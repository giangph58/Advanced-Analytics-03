{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4e9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am using the following SPARK_HOME: d:\\OneDrive - CGIAR\\Master\\Advanced Analytics\\assignments\\assignment-03\\spark\\spark-3.5.5-bin-hadoop3\n",
      "Windows detected: set HADOOP_HOME to: d:\\OneDrive - CGIAR\\Master\\Advanced Analytics\\assignments\\assignment-03\\spark\\winutils\n",
      "  Also added Hadoop bin directory to PATH: d:\\OneDrive - CGIAR\\Master\\Advanced Analytics\\assignments\\assignment-03\\spark\\winutils\\bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "spark_home = os.path.abspath(os.getcwd() + \"/spark/spark-3.5.5-bin-hadoop3\")\n",
    "hadoop_home = os.path.abspath(os.getcwd() + \"/spark/winutils\")\n",
    "print(f\"I am using the following SPARK_HOME: {spark_home}\")\n",
    "if os.name == 'nt':\n",
    "    os.environ[\"HADOOP_HOME\"] = f\"{hadoop_home}\"\n",
    "    print(f\"Windows detected: set HADOOP_HOME to: {os.environ['HADOOP_HOME']}\")\n",
    "    hadoop_bin = os.path.join(hadoop_home, \"bin\")\n",
    "    os.environ[\"PATH\"] = f\"{hadoop_bin};{os.environ['PATH']}\"\n",
    "    print(f\"  Also added Hadoop bin directory to PATH: {hadoop_bin}\")\n",
    "\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "findspark.init(spark_home)\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df = spark.read.json(\"data/interim\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488cb561",
   "metadata": {},
   "source": [
    "# Data structure\n",
    "\n",
    "Understand what you have and whether it's clean:\n",
    "- What is the schema of the dataset?\n",
    "- Are there missing values in key fields like title, summary, main_category, categories, or published?\n",
    "- Are there duplicated records (aid)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema\n",
    "papers_df.printSchema()\n",
    "\n",
    "# Get basic statistics and information\n",
    "print(f\"Dataset size: {papers_df.count()} papers\")\n",
    "print(f\"Number of columns: {len(papers_df.columns)}\")\n",
    "\n",
    "# Check for missing values\n",
    "from pyspark.sql.functions import col, count, when, isnan, isnull\n",
    "from pyspark.sql.functions import split, size, year, month, explode, round, sum as spark_sum\n",
    "\n",
    "papers_df.select([count(when(col(c).isNull() | isnan(col(c)), c)).alias(c) \n",
    "                          for c in papers_df.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb228599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first record in a dictionary format with better formatting\n",
    "from pprint import pprint\n",
    "\n",
    "# Get the first record as a dictionary\n",
    "first_record = papers_df.limit(1).toPandas().T.to_dict()[0]\n",
    "\n",
    "# Pretty print with formatting\n",
    "print(\"First record:\")\n",
    "pprint(first_record, width=100, compact=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c9d8cb",
   "metadata": {},
   "source": [
    "# Main category distribution\n",
    "\n",
    "Understand your labels before deciding on prediction strategy:\n",
    "- How many unique values are there in `main_category`?\n",
    "- What are the top N most frequent `main_category` values?\n",
    "- Are some `main_category` values very rare or dominant? (e.g., does one class dominate 50% of the data?)\n",
    "- How many unique `categories` values?\n",
    "- Whatâ€™s the average number of `categories` per paper (for multilabel setup)?\n",
    "- How do `main_category` distributions differ over week?\n",
    "- What's the distribution of Arxiv main category (before `.`)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d1bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Analysis: Main Category Distribution and Characteristics# Target Analysis: Main Category Distribution and Characteristics\n",
    "\n",
    "# ---------- 1. Count unique values in main_category ----------\n",
    "unique_main_categories = papers_df.select(\"main_category\").distinct().count()\n",
    "print(f\"Number of unique main_categories: {unique_main_categories}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 2. Top N most frequent main_categories ----------\n",
    "N = 10  # Change this to see more or fewer categories\n",
    "main_cat_dist = papers_df.groupBy(\"main_category\").count().orderBy(\"count\", ascending=False)\n",
    "print(f\"\\nTop {N} most frequent main categories:\")\n",
    "main_cat_dist.show(N, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca271f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 3. Check for category imbalance ----------\n",
    "total_papers = papers_df.count()\n",
    "main_cat_dist_with_pct = main_cat_dist.withColumn(\n",
    "    \"percentage\", round((col(\"count\") / total_papers) * 100, 2)\n",
    ")\n",
    "\n",
    "print(\"\\nCategory distribution with percentages:\")\n",
    "main_cat_dist_with_pct.show(20, truncate=False)\n",
    "\n",
    "# Find dominant categories (>10%) and rare categories (<1%)\n",
    "dominant_categories = main_cat_dist_with_pct.filter(\"percentage > 5\")\n",
    "rare_categories = main_cat_dist_with_pct.filter(\"percentage < 1\")\n",
    "\n",
    "print(f\"Dominant categories (>5%): {dominant_categories.count()}\")\n",
    "dominant_categories.show(truncate=False)\n",
    "\n",
    "print(f\"Rare categories (<1%): {rare_categories.count()}\")\n",
    "rare_categories.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef31a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 4. Unique subcategories in categories field ----------\n",
    "# Assuming categories field contains comma-separated category codes\n",
    "categories_exploded = papers_df.select(\n",
    "    explode(split(col(\"categories\"), \",\")).alias(\"subcategory\")\n",
    ")\n",
    "unique_subcategories = categories_exploded.select(\"subcategory\").distinct().count()\n",
    "print(f\"\\nNumber of unique subcategories: {unique_subcategories}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb8005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit\n",
    "\n",
    "cat_dist_with_count = papers_df.withColumn(\n",
    "    \"num_categories\", size(split(col(\"categories\"), \",\"))\n",
    ")\n",
    "\n",
    "avg_labels = cat_dist_with_count.agg(\n",
    "    {\"num_categories\": \"avg\"}\n",
    ").collect()[0][0]\n",
    "\n",
    "print(f\"\\nAverage number of categories per paper: {avg_labels:.2f}\")\n",
    "\n",
    "category_dist = cat_dist_with_count.groupBy(\"num_categories\").count()\n",
    "\n",
    "# Add percentage column\n",
    "category_dist_with_pct = category_dist.withColumn(\n",
    "    \"percentage\", round((col(\"count\") / total_papers) * 100, 2)\n",
    ")\n",
    "\n",
    "# Format the output to include both count and percentage\n",
    "print(\"\\nDistribution of number of categories per paper:\")\n",
    "category_dist_with_pct.select(\n",
    "    \"num_categories\", \n",
    "    \"count\", \n",
    "    concat(col(\"percentage\").cast(\"string\"), lit(\"%\")).alias(\"percentage\")\n",
    ").orderBy(\"num_categories\").show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 6. Category distributions over week ----------\n",
    "from pyspark.sql.functions import weekofyear, date_format, dayofweek, date_sub, expr, to_date\n",
    "\n",
    "# Convert published field to date components and find the Monday that starts each week\n",
    "papers_with_date = papers_df.withColumn(\n",
    "    \"pub_date\", to_date(col(\"published\"))\n",
    ").withColumn(\n",
    "    # Calculate days to subtract to get to Monday (Sunday=1, Monday=2, etc.)\n",
    "    # For Monday (2) we subtract 0, Tuesday (3) subtract 1, etc.\n",
    "    \"days_from_monday\", expr(\"dayofweek(pub_date) - 2\")\n",
    ").withColumn(\n",
    "    # Handle Sunday (need to subtract 6 days instead of -1)\n",
    "    \"days_from_monday\", \n",
    "    when(col(\"days_from_monday\") < 0, 6).otherwise(col(\"days_from_monday\"))\n",
    ").withColumn(\n",
    "    # Get the Monday date\n",
    "    \"week_start\", date_sub(col(\"pub_date\"), col(\"days_from_monday\"))\n",
    ").withColumn(\n",
    "    # Format it as \"Mar-31\" \n",
    "    \"week_date\", date_format(col(\"week_start\"), \"MMM-d\")\n",
    ")\n",
    "\n",
    "# Count papers by week and category\n",
    "weekly_cat_dist = papers_with_date.groupBy(\"week_date\", \"main_category\").count().orderBy(\"week_date\", \"count\", ascending=False)\n",
    "\n",
    "# print(\"\\nCategory distribution by week (showing first 20 rows):\")\n",
    "# weekly_cat_dist.show(20, truncate=False)\n",
    "\n",
    "# Calculate percentage distribution by week\n",
    "weekly_totals = papers_with_date.groupBy(\"week_date\").count().withColumnRenamed(\"count\", \"week_total\")\n",
    "\n",
    "weekly_cat_pct = weekly_cat_dist.join(weekly_totals, \"week_date\") \\\n",
    "    .withColumn(\"percentage\", round((col(\"count\") / col(\"week_total\")) * 100, 2)) \\\n",
    "    .orderBy(\"week_date\", \"percentage\", ascending=False)\n",
    "\n",
    "# print(\"\\nPercentage distribution of categories by week (showing first 20 rows):\")\n",
    "# weekly_cat_pct.select(\"week_date\", \"main_category\", \"count\", \"week_total\", \"percentage\").show(20, truncate=False)\n",
    "\n",
    "# Optional: Track specific categories over time\n",
    "top_categories = main_cat_dist.limit(10).select(\"main_category\").rdd.flatMap(lambda x: x).collect()\n",
    "top_cat_trend = weekly_cat_pct.filter(col(\"main_category\").isin(top_categories))\n",
    "\n",
    "print(\"\\nTrend for top 10 categories over time (by week):\")\n",
    "top_cat_trend.orderBy(\"week_date\", \"main_category\").show(50, truncate=False)\n",
    "\n",
    "# Add a visualization hint\n",
    "print(\"\\nNote: The week_date format is 'MMM-d' showing the Monday that starts each week\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae9a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Convert Spark DataFrame to Pandas and prepare data\n",
    "top_cat_pandas = top_cat_trend.orderBy(\"week_date\", \"main_category\").toPandas()\n",
    "\n",
    "# Create line chart with Plotly\n",
    "fig = px.line(\n",
    "    top_cat_pandas, \n",
    "    x='week_date', \n",
    "    y='percentage', \n",
    "    color='main_category',\n",
    "    markers=True,\n",
    "    title='Weekly Trends of Top 10 Categories',\n",
    "    labels={'week_date': 'Week', 'percentage': 'Percentage (%)'}\n",
    ")\n",
    "\n",
    "# Basic layout improvements\n",
    "fig.update_layout(\n",
    "    xaxis=dict(tickangle=45),\n",
    "    yaxis=dict(ticksuffix='%'),\n",
    "    legend=dict(\n",
    "        orientation='h',  # Horizontal orientation\n",
    "        yanchor='top', # Anchor to bottom of legend box\n",
    "        y=1.02,          # Position just above the plot area\n",
    "        xanchor='center', # Center horizontally\n",
    "        x=0.5\n",
    "    ),\n",
    "    margin=dict(t=50),   # Add top margin for legend space\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show(render=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5796a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 7. Archive distribution ----------\n",
    "\n",
    "arxiv_main_category = {\n",
    "    \"astro-ph\": [\n",
    "        \"astro-ph.CO\", \"astro-ph.EP\", \"astro-ph.GA\", \"astro-ph.HE\",\n",
    "        \"astro-ph.IM\", \"astro-ph.SR\"\n",
    "    ],\n",
    "    \"cond-mat\": [\n",
    "        \"cond-mat.dis-nn\", \"cond-mat.mes-hall\", \"cond-mat.mtrl-sci\",\n",
    "        \"cond-mat.other\", \"cond-mat.quant-gas\", \"cond-mat.soft\",\n",
    "        \"cond-mat.stat-mech\", \"cond-mat.str-el\", \"cond-mat.supr-con\"\n",
    "    ],\n",
    "    \"gr-qc\": [\n",
    "        \"gr-qc\"\n",
    "    ],\n",
    "    \"hep\": [\n",
    "        \"hep-ex\", \"hep-lat\", \"hep-ph\", \"hep-th\"\n",
    "    ],\n",
    "    \"math-ph\": [\n",
    "        \"math-ph\"\n",
    "    ],\n",
    "    \"nlin\": [\n",
    "        \"nlin.AO\", \"nlin.CG\", \"nlin.CD\", \"nlin.SI\", \"nlin.PS\"\n",
    "    ],\n",
    "    \"nucl\": [\n",
    "        \"nucl-ex\", \"nucl-th\"\n",
    "    ],\n",
    "    \"quant-ph\": [\n",
    "        \"quant-ph\"\n",
    "    ],\n",
    "    \"physics\": [\n",
    "        \"physics.acc-ph\", \"physics.ao-ph\", \"physics.app-ph\", \"physics.atm-clus\",\n",
    "        \"physics.atom-ph\", \"physics.bio-ph\", \"physics.chem-ph\", \"physics.class-ph\",\n",
    "        \"physics.comp-ph\", \"physics.data-an\", \"physics.ed-ph\", \"physics.flu-dyn\",\n",
    "        \"physics.gen-ph\", \"physics.geo-ph\", \"physics.hist-ph\", \"physics.ins-det\",\n",
    "        \"physics.med-ph\", \"physics.optics\", \"physics.plasm-ph\", \"physics.pop-ph\",\n",
    "        \"physics.soc-ph\", \"physics.space-ph\"\n",
    "    ],\n",
    "    \"math\": [\n",
    "        \"math.AC\", \"math.AG\", \"math.AP\", \"math.AT\", \"math.CA\", \"math.CO\",\n",
    "        \"math.CT\", \"math.CV\", \"math.DG\", \"math.DS\", \"math.FA\", \"math.GM\",\n",
    "        \"math.GN\", \"math.GR\", \"math.GT\", \"math.HO\", \"math.IT\", \"math.KT\",\n",
    "        \"math.LO\", \"math.MG\", \"math.MP\", \"math.NA\", \"math.NT\", \"math.OA\",\n",
    "        \"math.OC\", \"math.PR\", \"math.QA\", \"math.RA\", \"math.RT\", \"math.SG\",\n",
    "        \"math.SP\", \"math.ST\"\n",
    "    ],\n",
    "    \"cs\": [\n",
    "        \"cs.AI\", \"cs.AR\", \"cs.CC\", \"cs.CE\", \"cs.CG\", \"cs.CL\", \"cs.CR\", \"cs.CV\",\n",
    "        \"cs.CY\", \"cs.DB\", \"cs.DC\", \"cs.DL\", \"cs.DM\", \"cs.DS\", \"cs.ET\", \"cs.FL\",\n",
    "        \"cs.GL\", \"cs.GR\", \"cs.GT\", \"cs.HC\", \"cs.IR\", \"cs.IT\", \"cs.LG\", \"cs.LO\",\n",
    "        \"cs.MA\", \"cs.MM\", \"cs.MS\", \"cs.NA\", \"cs.NE\", \"cs.NI\", \"cs.OH\", \"cs.OS\",\n",
    "        \"cs.PF\", \"cs.PL\", \"cs.RO\", \"cs.SC\", \"cs.SD\", \"cs.SE\", \"cs.SI\", \"cs.SY\"\n",
    "    ],\n",
    "    \"q-bio\": [\n",
    "        \"q-bio.BM\", \"q-bio.CB\", \"q-bio.GN\", \"q-bio.MN\", \"q-bio.NC\", \"q-bio.OT\",\n",
    "        \"q-bio.PE\", \"q-bio.QM\", \"q-bio.SC\", \"q-bio.TO\"\n",
    "    ],\n",
    "    \"q-fin\": [\n",
    "        \"q-fin.CP\", \"q-fin.EC\", \"q-fin.GN\", \"q-fin.MF\", \"q-fin.PM\", \"q-fin.PR\",\n",
    "        \"q-fin.RM\", \"q-fin.ST\", \"q-fin.TR\"\n",
    "    ],\n",
    "    \"stat\": [\n",
    "        \"stat.AP\", \"stat.CO\", \"stat.ME\", \"stat.ML\", \"stat.OT\", \"stat.TH\"\n",
    "    ],\n",
    "    \"eess\": [\n",
    "        \"eess.AS\", \"eess.IV\", \"eess.SP\", \"eess.SY\"\n",
    "    ],\n",
    "    \"econ\": [\n",
    "        \"econ.EM\", \"econ.GN\", \"econ.TH\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Create mapping with dictionary comprehension\n",
    "category_to_parent = {child: parent for parent, children in arxiv_main_category.items() for child in children}\n",
    "\n",
    "# Define mapping UDF\n",
    "@udf(StringType())\n",
    "def map_category(category):\n",
    "    return category_to_parent.get(category, \"other\")\n",
    "\n",
    "# Add column with single function call\n",
    "papers_with_categories = papers_df.withColumn(\"arxiv_main_category\", map_category(col(\"main_category\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14271c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distribution of arxiv_main_category\n",
    "from pyspark.sql.functions import col, count, round\n",
    "from pyspark.sql.functions import max as spark_max\n",
    "\n",
    "# Count the unique arxiv_main_category values\n",
    "unique_arxiv_main_categories = papers_with_categories.select(\"arxiv_main_category\").distinct().count()\n",
    "print(f\"Number of unique arxiv_main_categories: {unique_arxiv_main_categories}\")\n",
    "\n",
    "\n",
    "# Count categories and calculate percentages\n",
    "total_papers = papers_with_categories.count()\n",
    "cat_distribution = papers_with_categories.groupBy(\"arxiv_main_category\") \\\n",
    "    .count() \\\n",
    "    .withColumn(\"percentage\", round((col(\"count\") / total_papers) * 100, 2)) \\\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Display distribution\n",
    "print(\"\\nArXiv Main Category Distribution:\")\n",
    "cat_distribution.show(truncate=False)\n",
    "\n",
    "# Check class imbalance\n",
    "max_count = cat_distribution.agg({\"count\": \"max\"}).collect()[0][0]\n",
    "min_count = cat_distribution.agg({\"count\": \"min\"}).collect()[0][0]\n",
    "print(f\"\\nClass Imbalance Ratio (max/min): {max_count/min_count:.2f}\")\n",
    "\n",
    "# Count classes with less than 1% representation\n",
    "rare_classes = cat_distribution.filter(\"percentage < 1\").count()\n",
    "print(f\"Classes with <1% representation: {rare_classes}\")\n",
    "\n",
    "# Optional: Quick visualization using pandas and matplotlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to pandas for easy plotting\n",
    "latest_date = papers_df.agg(spark_max(\"published\")).collect()[0][0]\n",
    "pd_distribution = cat_distribution.toPandas()\n",
    "\n",
    "# Create horizontal bar chart\n",
    "plt.figure(figsize=(12, 8))  # Slightly larger figure to accommodate annotations\n",
    "bars = plt.barh(pd_distribution['arxiv_main_category'], pd_distribution['percentage'])\n",
    "plt.xlabel('Percentage (%)')\n",
    "plt.ylabel('Category')\n",
    "plt.title(f'ArXiv Main Category Distribution (Total: {total_papers:,} papers)\\n(Data through {latest_date})', \n",
    "         fontsize=14)\n",
    "# Add count annotations to each bar\n",
    "for i, bar in enumerate(bars):\n",
    "    count = pd_distribution.iloc[i]['count']\n",
    "    plt.text(\n",
    "        bar.get_width() + 0.3,  # Position slightly to the right of the bar\n",
    "        bar.get_y() + bar.get_height()/2,  # Vertical center of the bar\n",
    "        f'{count:,}',  # Format with commas for thousands\n",
    "        va='center'  # Vertically centered\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c76152",
   "metadata": {},
   "source": [
    "# Input features\n",
    "\n",
    "Evaluate your input features `title` and `summary`:\n",
    "- What is the average/median/min/max length (in words or characters) of:\n",
    "    - title\n",
    "    - summary\n",
    "    - title + summary?\n",
    "- Are there unusually short or long texts that should be filtered?\n",
    "- Are there stopwords or LaTeX/math formatting (\\\\mathsf, etc.) that should be removed?\n",
    "- Are there phrases or token patterns in title and summary that can be related to `main_category`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77864d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length, size, split, col, expr, percentile_approx\n",
    "from pyspark.sql.functions import min as spark_min, max as spark_max, avg as spark_avg\n",
    "from pyspark.sql.functions import regexp_extract, regexp_replace\n",
    "from pyspark.ml.feature import StopWordsRemover, NGram, CountVectorizer, Tokenizer\n",
    "\n",
    "# ---------- 1. Text Length Statistics ----------\n",
    "text_stats = papers_df.select(\n",
    "    length(\"title\").alias(\"title_chars\"),\n",
    "    size(split(\"title\", \" \")).alias(\"title_words\"),\n",
    "    length(\"summary\").alias(\"summary_chars\"),\n",
    "    size(split(\"summary\", \" \")).alias(\"summary_words\")\n",
    ").withColumn(\n",
    "    \"combined_chars\", col(\"title_chars\") + col(\"summary_chars\")\n",
    ").withColumn(\n",
    "    \"combined_words\", col(\"title_words\") + col(\"summary_words\")\n",
    ")\n",
    "\n",
    "# Calculate statistics\n",
    "stats = text_stats.agg(\n",
    "    spark_min(\"title_chars\"), spark_max(\"title_chars\"), spark_avg(\"title_chars\"), percentile_approx(\"title_chars\", 0.5),\n",
    "    spark_min(\"title_words\"), spark_max(\"title_words\"), spark_avg(\"title_words\"),\n",
    "    spark_min(\"summary_chars\"), spark_max(\"summary_chars\"), spark_avg(\"summary_chars\"), \n",
    "    spark_min(\"summary_words\"), spark_max(\"summary_words\"), spark_avg(\"summary_words\"),\n",
    "    spark_min(\"combined_chars\"), spark_max(\"combined_chars\"), spark_avg(\"combined_chars\"),\n",
    "    spark_min(\"combined_words\"), spark_max(\"combined_words\"), spark_avg(\"combined_words\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"\\nText Statistics (min, max, avg):\")\n",
    "print(f\"Title: {stats[0]}-{stats[1]} chars ({stats[2]:.1f} avg), {stats[4]}-{stats[5]} words ({stats[6]:.1f} avg)\")\n",
    "print(f\"Summary: {stats[7]}-{stats[8]} chars ({stats[9]:.1f} avg), {stats[10]}-{stats[11]} words ({stats[12]:.1f} avg)\")\n",
    "print(f\"Combined: {stats[13]}-{stats[14]} chars ({stats[15]:.1f} avg), {stats[16]}-{stats[17]} words ({stats[18]:.1f} avg)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc0e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 2. Identify Outliers (1st/99th percentiles) ----------\n",
    "percentiles = text_stats.select(\n",
    "    percentile_approx(\"title_chars\", [0.01, 0.99]).alias(\"title_thresholds\"),\n",
    "    percentile_approx(\"summary_chars\", [0.01, 0.99]).alias(\"summary_thresholds\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nOutlier thresholds (1st-99th percentiles):\")\n",
    "print(f\"Title length: {percentiles[0][0]}-{percentiles[0][1]} chars\")\n",
    "print(f\"Summary length: {percentiles[1][0]}-{percentiles[1][1]} chars\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0369f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 3. Check for LaTeX and Math formatting ----------\n",
    "latex_pattern = r'\\\\[a-zA-Z]+'\n",
    "latex_counts = papers_df.select(\n",
    "    (size(expr(f\"regexp_extract_all(title, '{latex_pattern}', 0)\")) > 0).cast(\"int\").alias(\"title_has_latex\"),\n",
    "    (size(expr(f\"regexp_extract_all(summary, '{latex_pattern}', 0)\")) > 0).cast(\"int\").alias(\"summary_has_latex\")\n",
    ").agg(\n",
    "    spark_sum(\"title_has_latex\"), \n",
    "    spark_sum(\"summary_has_latex\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nLaTeX Usage:\")\n",
    "print(f\"Papers with LaTeX in title: {latex_counts[0]} ({latex_counts[0]/papers_df.count()*100:.1f}%)\")\n",
    "print(f\"Papers with LaTeX in summary: {latex_counts[1]} ({latex_counts[1]/papers_df.count()*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775363f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col, count as spark_count, trim, lower, regexp_replace\n",
    "\n",
    "# 1. Better text cleaning before processing\n",
    "clean_papers = papers_df.withColumn(\n",
    "    \"clean_title\", \n",
    "    trim(regexp_replace(lower(col(\"title\")), \"[^a-zA-Z0-9\\\\s]\", \" \"))\n",
    ")\n",
    "\n",
    "# 2. Apply tokenization pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"clean_title\", outputCol=\"tokens\")\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "bigram = NGram(n=2, inputCol=\"filtered\", outputCol=\"bigrams\")\n",
    "\n",
    "# 3. Generate and filter meaningful bigrams\n",
    "pipeline_result = bigram.transform(remover.transform(tokenizer.transform(clean_papers)))\n",
    "bigram_counts = pipeline_result.select(\n",
    "    explode(\"bigrams\").alias(\"bigram\")\n",
    ").filter(\n",
    "    # Multiple filters to ensure quality bigrams\n",
    "    (length(trim(col(\"bigram\"))) > 3) &                   # Reasonable length\n",
    "    (~col(\"bigram\").contains(\"  \")) &                     # No double spaces\n",
    "    (col(\"bigram\").rlike(\"^[a-zA-Z0-9].*[a-zA-Z0-9]$\"))   # Start/end with alphanumeric\n",
    ").groupBy(\n",
    "    \"bigram\"\n",
    ").agg(\n",
    "    spark_count(\"*\").alias(\"count\")\n",
    ").orderBy(\n",
    "    col(\"count\").desc()\n",
    ")\n",
    "\n",
    "# 4. Show results\n",
    "print(\"\\nTop 10 bigrams in titles:\")\n",
    "for row in bigram_counts.limit(10).collect():\n",
    "    print(f\"'{row.bigram}': {row['count']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f320ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col, count as spark_count\n",
    "\n",
    "# 1. Process all papers once with category information\n",
    "category_bigrams = clean_papers.select(\n",
    "    \"main_category\", \n",
    "    \"clean_title\"\n",
    ").transform(\n",
    "    # Apply the same pipeline to all papers at once\n",
    "    lambda df: bigram.transform(remover.transform(tokenizer.transform(df)))\n",
    ")\n",
    "\n",
    "# 2. Extract, filter and count bigrams with categories in one operation\n",
    "bigram_by_category = category_bigrams.select(\n",
    "    \"main_category\",\n",
    "    explode(\"bigrams\").alias(\"bigram\")\n",
    ").filter(\n",
    "    # Apply same quality filters as main analysis \n",
    "    (length(trim(col(\"bigram\"))) > 3) &\n",
    "    (~col(\"bigram\").contains(\"  \")) &\n",
    "    (col(\"bigram\").rlike(\"^[a-zA-Z0-9].*[a-zA-Z0-9]$\"))\n",
    ").groupBy(\n",
    "    \"main_category\", \"bigram\"\n",
    ").count().orderBy(\n",
    "    col(\"main_category\"), col(\"count\").desc()\n",
    ")\n",
    "\n",
    "# 3. Display top bigrams for selected categories in one efficient loop\n",
    "top_categories = papers_df.groupBy(\"main_category\").count() \\\n",
    "                          .orderBy(\"count\", ascending=False) \\\n",
    "                          .limit(10) \\\n",
    "                          .select(\"main_category\") \\\n",
    "                          .rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "print(\"\\nTop 5 bigrams per category:\")\n",
    "for category in top_categories[:10]:\n",
    "    print(f\"\\n{category}:\")\n",
    "    top_for_category = bigram_by_category.filter(col(\"main_category\") == category).limit(5).collect()\n",
    "    for row in top_for_category:\n",
    "        print(f\"  '{row.bigram}': {row['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col, count as spark_count, trim, lower, regexp_replace, length\n",
    "\n",
    "# ----------------- Summary Bigram Analysis -----------------\n",
    "\n",
    "# 1. Clean the summary text\n",
    "clean_papers_summary = papers_df.withColumn(\n",
    "    \"clean_summary\", \n",
    "    trim(regexp_replace(lower(col(\"summary\")), \"[^a-zA-Z0-9\\\\s]\", \" \"))\n",
    ")\n",
    "\n",
    "# 2. Apply tokenization pipeline\n",
    "sum_tokenizer = Tokenizer(inputCol=\"clean_summary\", outputCol=\"summary_tokens\")\n",
    "sum_remover = StopWordsRemover(inputCol=\"summary_tokens\", outputCol=\"summary_filtered\")\n",
    "sum_bigram = NGram(n=2, inputCol=\"summary_filtered\", outputCol=\"summary_bigrams\")\n",
    "\n",
    "# 3. Generate and filter meaningful bigrams\n",
    "summary_pipeline = sum_bigram.transform(\n",
    "    sum_remover.transform(\n",
    "        sum_tokenizer.transform(clean_papers_summary)\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4. Extract and count bigrams\n",
    "summary_bigram_counts = summary_pipeline.select(\n",
    "    explode(\"summary_bigrams\").alias(\"bigram\")\n",
    ").filter(\n",
    "    # Same quality filters as used for titles\n",
    "    (length(trim(col(\"bigram\"))) > 3) &\n",
    "    (~col(\"bigram\").contains(\"  \")) &\n",
    "    (col(\"bigram\").rlike(\"^[a-zA-Z0-9].*[a-zA-Z0-9]$\"))\n",
    ").groupBy(\n",
    "    \"bigram\"\n",
    ").agg(\n",
    "    spark_count(\"*\").alias(\"count\")\n",
    ").orderBy(\n",
    "    col(\"count\").desc()\n",
    ")\n",
    "\n",
    "# 5. Show top bigrams in summaries\n",
    "print(\"\\nTop 10 bigrams in summaries:\")\n",
    "for row in summary_bigram_counts.limit(10).collect():\n",
    "    print(f\"'{row.bigram}': {row['count']}\")\n",
    "\n",
    "# 6. Category-specific summary bigrams in a single efficient operation\n",
    "category_summary_bigrams = clean_papers_summary.select(\n",
    "    \"main_category\",\n",
    "    \"clean_summary\"\n",
    ").transform(\n",
    "    # Apply the pipeline to all papers at once\n",
    "    lambda df: sum_bigram.transform(sum_remover.transform(sum_tokenizer.transform(df)))\n",
    ")\n",
    "\n",
    "# 7. Process all categories at once\n",
    "summary_bigrams_by_category = category_summary_bigrams.select(\n",
    "    \"main_category\",\n",
    "    explode(\"summary_bigrams\").alias(\"bigram\")\n",
    ").filter(\n",
    "    # Same quality filters\n",
    "    (length(trim(col(\"bigram\"))) > 3) &\n",
    "    (~col(\"bigram\").contains(\"  \")) &\n",
    "    (col(\"bigram\").rlike(\"^[a-zA-Z0-9].*[a-zA-Z0-9]$\"))\n",
    ").groupBy(\n",
    "    \"main_category\", \"bigram\"\n",
    ").count().orderBy(\n",
    "    col(\"main_category\"), col(\"count\").desc()\n",
    ")\n",
    "\n",
    "# 8. Display top summary bigrams by category\n",
    "print(\"\\nTop 5 summary bigrams per category:\")\n",
    "for category in top_categories[:5]:  # Limiting to top 5 categories for brevity\n",
    "    print(f\"\\n{category}:\")\n",
    "    top_for_category = summary_bigrams_by_category.filter(col(\"main_category\") == category).limit(5).collect()\n",
    "    for row in top_for_category:\n",
    "        print(f\"  '{row.bigram}': {row['count']}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are certain keywords more correlated with certain categories?\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.sql.functions import array_contains\n",
    "from pyspark.sql.functions import col, expr, udf\n",
    "from pyspark.sql.types import FloatType, ArrayType, StringType\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1. Prepare data - clean text and add category\n",
    "keyword_data = papers_df.select(\n",
    "    \"main_category\",\n",
    "    regexp_replace(lower(col(\"title\")), \"[^a-zA-Z0-9\\\\s]\", \" \").alias(\"cleaned_text\")\n",
    ")\n",
    "\n",
    "# 2. Create ML pipeline for feature extraction\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf])\n",
    "\n",
    "# 3. Fit the pipeline and transform data\n",
    "model = pipeline.fit(keyword_data)\n",
    "transformed = model.transform(keyword_data)\n",
    "\n",
    "# 4. Calculate keyword significance for each category\n",
    "# Get vocabulary from the HashingTF model (using the vocabulary from token counts)\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"counts\", minDF=10)\n",
    "cv_model = cv.fit(transformed)\n",
    "vocab = cv_model.vocabulary\n",
    "\n",
    "# 5. For each category, find most distinctive keywords\n",
    "print(\"\\nTop keywords correlated with categories:\")\n",
    "for category in top_categories[:10]:  # Show top 20 categories\n",
    "    # Filter for this category and collect weighted term frequencies\n",
    "    category_docs = transformed.filter(col(\"main_category\") == category)\n",
    "    \n",
    "    # Extract top terms for this category using the TF-IDF weights\n",
    "    @udf(returnType=ArrayType(StringType()))\n",
    "    def get_top_terms(words, features):\n",
    "        # Pair words with their feature weights and sort by weight\n",
    "        return [w for _, w in sorted([(features[i], w) for i, w in enumerate(words) if i < len(features)], \n",
    "                                   key=lambda x: x[0], reverse=True)[:10]]\n",
    "    \n",
    "    # Apply the UDF to get top terms for each document\n",
    "    category_terms = category_docs.withColumn(\"top_terms\", get_top_terms(\"filtered\", \"features\"))\n",
    "    \n",
    "    # Count term frequency across all documents in this category\n",
    "    top_terms = category_terms.select(explode(\"filtered\").alias(\"term\")).groupBy(\"term\").count()\n",
    "    \n",
    "    # Get overall corpus frequency for comparison\n",
    "    corpus_freq = transformed.select(explode(\"filtered\").alias(\"term\")).groupBy(\"term\").count().withColumnRenamed(\"count\", \"corpus_count\")\n",
    "    \n",
    "    # Calculate term importance as ratio of category frequency to corpus frequency\n",
    "    term_importance = top_terms.join(corpus_freq, \"term\").withColumn(\n",
    "        \"importance\", col(\"count\") / col(\"corpus_count\")\n",
    "    ).orderBy(\"importance\", ascending=False)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n{category}:\")\n",
    "    for row in term_importance.filter(col(\"count\") > 5).limit(10).collect():\n",
    "        print(f\"  '{row.term}': {row['count']} occurrences, {row.importance:.2f} importance\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment-03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "630217fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, ClassLabel, Dataset\n",
    "from src.utils import map_category\n",
    "\n",
    "aux_data = load_dataset(\"real-jiakai/arxiver-with-category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63295454",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_data.set_format(type=\"pandas\")\n",
    "aux_df = aux_data[\"train\"][:]\n",
    "aux_df[\"label\"] = aux_df[\"primary_category\"].apply(map_category)\n",
    "aux_df[\"title\"] = aux_df[\"title\"].str.replace(\"\\n  \", \" \")\n",
    "aux_df[\"text\"] = aux_df[\"title\"] + \"\\n\" + aux_df[\"abstract\"]\n",
    "aux_df = aux_df[[\"text\", \"label\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2742cfe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d4a58daa86499fbb48d02eb97a6f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/57021 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0746b3127748558cb707e51ab99978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/6336 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43686ed130594a249cd8e0f336758925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/57021 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c891b9a154e24428be28c66c8c5a34cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6336 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    aux_df, \n",
    "    test_size=0.1,\n",
    "    stratify=aux_df[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cpt_data = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df, preserve_index=False),\n",
    "    \"validation\": Dataset.from_pandas(val_df, preserve_index=False)\n",
    "})\n",
    "\n",
    "labels = sorted(train_df[\"label\"].unique())\n",
    "class_label = ClassLabel(names=labels)\n",
    "\n",
    "cpt_data = cpt_data.cast_column(\"label\", class_label)\n",
    "\n",
    "cpt_data.save_to_disk(\"data/processed/cpt_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4615e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb1cf7935384ea59d4489b203a12a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57021 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf432f36fcd841aa9e5bf97b154f3ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6336 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DistilBertForMaskedLM, DistilBertTokenizer, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "model = DistilBertForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", \n",
    "                    truncation=True, max_length=512)\n",
    "\n",
    "tokenized_cpt_data = cpt_data.map(tokenize, batched=True, remove_columns=[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "decca37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12500' max='17820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12500/17820 39:29 < 16:48, 5.27 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.379900</td>\n",
       "      <td>2.087670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.141800</td>\n",
       "      <td>1.982328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.057600</td>\n",
       "      <td>1.896975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.995500</td>\n",
       "      <td>1.849949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.946000</td>\n",
       "      <td>1.822771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.918900</td>\n",
       "      <td>1.789547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.895100</td>\n",
       "      <td>1.776106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.862400</td>\n",
       "      <td>1.759627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.849200</td>\n",
       "      <td>1.749570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.836900</td>\n",
       "      <td>1.724996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.816300</td>\n",
       "      <td>1.715244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.798800</td>\n",
       "      <td>1.706665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.797500</td>\n",
       "      <td>1.698959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.776500</td>\n",
       "      <td>1.688880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.774600</td>\n",
       "      <td>1.679658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.767400</td>\n",
       "      <td>1.673733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.756800</td>\n",
       "      <td>1.659143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.740300</td>\n",
       "      <td>1.665558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.733000</td>\n",
       "      <td>1.656237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.732000</td>\n",
       "      <td>1.649310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.725900</td>\n",
       "      <td>1.643242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.718500</td>\n",
       "      <td>1.632437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.722100</td>\n",
       "      <td>1.641648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.709600</td>\n",
       "      <td>1.633195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.702300</td>\n",
       "      <td>1.634147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['vocab_projector.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12500, training_loss=1.846195576171875, metrics={'train_runtime': 2370.3894, 'train_samples_per_second': 240.555, 'train_steps_per_second': 7.518, 'total_flos': 5.302167865412198e+16, 'train_loss': 1.846195576171875, 'epoch': 7.014590347923681})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data collator for masked language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"distilbert-base-uncased-cpt-arxiv\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=True,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=1000,\n",
    "    logging_dir='logs',\n",
    "    logging_steps=500,              \n",
    "    dataloader_num_workers=4,       \n",
    "    eval_strategy=\"steps\",   \n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\"\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_cpt_data[\"train\"],\n",
    "    eval_dataset=tokenized_cpt_data[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Run continued pretraining\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51165954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('distilbert-arxiv-domain-adapted/tokenizer_config.json',\n",
       " 'distilbert-arxiv-domain-adapted/special_tokens_map.json',\n",
       " 'distilbert-arxiv-domain-adapted/vocab.txt',\n",
       " 'distilbert-arxiv-domain-adapted/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save domain-adapted model\n",
    "model.save_pretrained(\"distilbert-arxiv-domain-adapted\")\n",
    "tokenizer.save_pretrained(\"distilbert-arxiv-domain-adapted\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0072c2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am using the following SPARK_HOME: d:\\OneDrive - CGIAR\\Master\\Advanced Analytics\\assignments\\assignment-03\\spark\\spark-3.5.5-bin-hadoop3\n",
      "Windows detected: set HADOOP_HOME to: d:\\OneDrive - CGIAR\\Master\\Advanced Analytics\\assignments\\assignment-03\\spark\\winutils\n",
      "  Also added Hadoop bin directory to PATH: d:\\OneDrive - CGIAR\\Master\\Advanced Analytics\\assignments\\assignment-03\\spark\\winutils\\bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "spark_home = os.path.abspath(os.getcwd() + \"/spark/spark-3.5.5-bin-hadoop3\")\n",
    "hadoop_home = os.path.abspath(os.getcwd() + \"/spark/winutils\")\n",
    "print(f\"I am using the following SPARK_HOME: {spark_home}\")\n",
    "if os.name == 'nt':\n",
    "    os.environ[\"HADOOP_HOME\"] = f\"{hadoop_home}\"\n",
    "    print(f\"Windows detected: set HADOOP_HOME to: {os.environ['HADOOP_HOME']}\")\n",
    "    hadoop_bin = os.path.join(hadoop_home, \"bin\")\n",
    "    os.environ[\"PATH\"] = f\"{hadoop_bin};{os.environ['PATH']}\"\n",
    "    print(f\"  Also added Hadoop bin directory to PATH: {hadoop_bin}\")\n",
    "\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "findspark.init(spark_home)\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f80463",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.read.json(\"data/raw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68834bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2449ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Define the schema for the nested JSON\n",
    "paper_schema = StructType([\n",
    "    StructField(\"aid\", StringType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"summary\", StringType()),\n",
    "    StructField(\"main_category\", StringType()),\n",
    "    StructField(\"categories\", StringType()),\n",
    "    StructField(\"published\", StringType())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6955fbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+---------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+--------------+--------------------+\n",
      "|aid                              |title                                                                                                                |summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |main_category|categories    |published           |\n",
      "+---------------------------------+---------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+--------------+--------------------+\n",
      "|http://arxiv.org/abs/2503.23697v1|A Low-complexity Structured Neural Network to Realize States of\\n  Dynamical Systems                                 |Data-driven learning is rapidly evolving and places a new perspective on\\nrealizing state-space dynamical systems. However, dynamical systems derived\\nfrom nonlinear ordinary differential equations (ODEs) suffer from limitations\\nin computational efficiency. Thus, this paper stems from data-driven learning\\nto advance states of dynamical systems utilizing a structured neural network\\n(StNN). The proposed learning technique also seeks to identify an optimal,\\nlow-complexity operator to solve dynamical systems, the so-called Hankel\\noperator, derived from time-delay measurements. Thus, we utilize the StNN based\\non the Hankel operator to solve dynamical systems as an alternative to existing\\ndata-driven techniques. We show that the proposed StNN reduces the number of\\nparameters and computational complexity compared with the conventional neural\\nnetworks and also with the classical data-driven techniques, such as Sparse\\nIdentification of Nonlinear Dynamics (SINDy) and Hankel Alternative view of\\nKoopman (HAVOK), which is commonly known as delay-Dynamic Mode\\nDecomposition(DMD) or Hankel-DMD. More specifically, we present numerical\\nsimulations to solve dynamical systems utilizing the StNN based on the Hankel\\noperator beginning from the fundamental Lotka-Volterra model, where we compare\\nthe StNN with the LEarning Across Dynamical Systems (LEADS), and extend our\\nanalysis to highly nonlinear and chaotic Lorenz systems, comparing the StNN\\nwith conventional neural networks, SINDy, and HAVOK. Hence, we show that the\\nproposed StNN paves the way for realizing state-space dynamical systems with a\\nlow-complexity learning algorithm, enabling prediction and understanding of\\nfuture states.|cs.LG        |cs.LG,math.DS |2025-03-31T03:52:38Z|\n",
      "|http://arxiv.org/abs/2503.23715v1|HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video\\n  Generation                                    |Text-to-video (T2V) generation has made tremendous progress in generating\\ncomplicated scenes based on texts. However, human-object interaction (HOI)\\noften cannot be precisely generated by current T2V models due to the lack of\\nlarge-scale videos with accurate captions for HOI. To address this issue, we\\nintroduce HOIGen-1M, the first largescale dataset for HOI Generation,\\nconsisting of over one million high-quality videos collected from diverse\\nsources. In particular, to guarantee the high quality of videos, we first\\ndesign an efficient framework to automatically curate HOI videos using the\\npowerful multimodal large language models (MLLMs), and then the videos are\\nfurther cleaned by human annotators. Moreover, to obtain accurate textual\\ncaptions for HOI videos, we design a novel video description method based on a\\nMixture-of-Multimodal-Experts (MoME) strategy that not only generates\\nexpressive captions but also eliminates the hallucination by individual MLLM.\\nFurthermore, due to the lack of an evaluation framework for generated HOI\\nvideos, we propose two new metrics to assess the quality of generated videos in\\na coarse-to-fine manner. Extensive experiments reveal that current T2V models\\nstruggle to generate high-quality HOI videos and confirm that our HOIGen-1M\\ndataset is instrumental for improving HOI video generation. Project webpage is\\navailable at https://liuqi-creat.github.io/HOIGen.github.io.                                                                                                                                                                                                                                                                                  |cs.CV        |cs.CV         |2025-03-31T04:30:34Z|\n",
      "|http://arxiv.org/abs/2504.09892v1|Vermilion: A Traffic-Aware Reconfigurable Optical Interconnect with\\n  Formal Throughput Guarantees                  |The increasing gap between datacenter traffic volume and the capacity of\\nelectrical switches has driven the development of reconfigurable network\\ndesigns utilizing optical circuit switching. Recent advancements, particularly\\nthose featuring periodic fixed-duration reconfigurations, have achieved\\npractical end-to-end delays of just a few microseconds. However, current\\ndesigns rely on multi-hop routing to enhance utilization, which can lead to a\\nsignificant reduction in worst-case throughput and added overhead from\\ncongestion control and routing complexity. These factors pose significant\\noperational challenges for the large-scale deployment of these technologies.\\n  We present Vermilion, a reconfigurable optical interconnect that breaks the\\nthroughput barrier of existing periodic reconfigurable networks, without the\\nneed for multi-hop routing -- thus eliminating congestion control and\\nsimplifying routing to direct communication. Vermilion adopts a traffic-aware\\napproach while retaining the simplicity of periodic fixed-duration\\nreconfigurations, similar to RotorNet. We formally establish throughput bounds\\nfor Vermilion, demonstrating that it achieves at least $33\\%$ more throughput\\nin the worst-case compared to existing designs. The key innovation of Vermilion\\nis its short traffic-aware periodic schedule, derived using a matrix rounding\\ntechnique. This schedule is then combined with a traffic-oblivious periodic\\nschedule to efficiently manage any residual traffic. Our evaluation results\\nsupport our theoretical findings, revealing significant performance gains for\\ndatacenter workloads.                                                                                    |cs.NI        |cs.NI,cs.DS   |2025-04-14T05:35:42Z|\n",
      "|http://arxiv.org/abs/2504.09908v1|Laser-induced spectral diffusion and excited-state mixing of silicon T\\n  centres                                    |To find practical application as photon sources for entangled optical\\nresource states or as spin-photon interfaces in entangled networks,\\nsemiconductor emitters must produce indistinguishable photons with high\\nefficiency and spectral stability. Nanophotonic cavity integration increases\\nefficiency and bandwidth, but it also introduces environmental charge\\ninstability and spectral diffusion. Among various candidates, silicon colour\\ncentres have emerged as compelling platforms for integrated-emitter quantum\\ntechnologies. Here we investigate the dynamics of spectral wandering in\\nnanophotonics-coupled, individual silicon T centres using spectral correlation\\nmeasurements. We observe that spectral fluctuations are driven predominantly by\\nthe near-infrared excitation laser, consistent with a power-dependent\\nOrnstein-Uhlenbeck process, and show that the spectrum is stable for up to 1.5\\nms in the dark. We demonstrate a 35x narrowing of the emitter linewidth to 110\\nMHz using a resonance-check scheme and discuss the advantage for pairwise\\nentanglement rates and optical resource state generators. Finally, we report\\nlaser-induced spin-mixing in the excited state and discuss potential mechanisms\\ncommon to both phenomena. These effects must be considered in calibrating T\\ncentre devices for high-performance entanglement generation.                                                                                                                                                                                                                                                                                                                                                                      |quant-ph     |quant-ph      |2025-04-14T06:09:17Z|\n",
      "|http://arxiv.org/abs/2504.02359v1|First observation of ultra-long-range azimuthal correlations in low\\n  multiplicity pp and p-Pb collisions at the LHC|This study presents the first observation of ultra-long-range two-particle\\nazimuthal correlations with pseudorapidity separation of ($|\\Delta \\eta| >\\n5.0$) in proton-proton (pp) and ($|\\Delta \\eta| > 6.5$) in proton-lead (p-Pb)\\ncollisions at the LHC, down to and below the minimum-bias multiplicity.\\nTwo-particle correlation coefficients (${V}_{2\\Delta}$) are measured after\\nremoving non-flow (jets and resonance decays) contributions using the\\ntemplate-fit method across various multiplicity classes, providing novel\\ninsights into the origin of long-range correlations in small systems.\\nComparisons with the 3D-Glauber + MUSIC + UrQMD hydrodynamic model reveal\\nsignificant discrepancies at low multiplicities, indicating possible dynamics\\nbeyond typical hydrodynamic behavior. Initial-state models based on the Color\\nGlass Condensate framework generate only short-range correlations, while PYTHIA\\nsimulations implemented with the string-shoving mechanism also fail to describe\\nthese ultra-long-range correlations. The results challenge existing paradigms\\nand question the underlying mechanisms in low-multiplicity pp and p-Pb\\ncollisions. The findings impose significant constraints on models describing\\ncollective phenomena in small collision systems and advance the understanding\\nof origin of long-range correlations at Large Hadron Collider (LHC) energies.                                                                                                                                                                                                                                                                                                                                               |nucl-ex      |nucl-ex,hep-ex|2025-04-03T07:46:09Z|\n",
      "+---------------------------------+---------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse the nested JSON\n",
    "papers_df = raw_df.withColumn(\"paper_data\", \n",
    "                           from_json(col(\"value\"), paper_schema))\\\n",
    "                 .select(\"paper_data.*\")\n",
    "\n",
    "# Display sample data\n",
    "papers_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb987326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 23836\n"
     ]
    }
   ],
   "source": [
    "# Count records\n",
    "print(f\"Total number of rows: {papers_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fd59e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers with duplicate IDs: 6540\n",
      "Sample of papers with duplicate IDs:\n",
      "+---------------------------------+-------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+----------+--------------------+\n",
      "|aid                              |title                                                                                                        |summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |main_category|categories|published           |\n",
      "+---------------------------------+-------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+----------+--------------------+\n",
      "|http://arxiv.org/abs/2503.23679v1|The Devil is in the Distributions: Explicit Modeling of Scene Content is\\n  Key in Zero-Shot Video Captioning|Zero-shot video captioning requires that a model generate high-quality\\ncaptions without human-annotated video-text pairs for training.\\nState-of-the-art approaches to the problem leverage CLIP to extract\\nvisual-relevant textual prompts to guide language models in generating\\ncaptions. These methods tend to focus on one key aspect of the scene and build\\na caption that ignores the rest of the visual input. To address this issue, and\\ngenerate more accurate and complete captions, we propose a novel progressive\\nmulti-granularity textual prompting strategy for zero-shot video captioning.\\nOur approach constructs three distinct memory banks, encompassing noun phrases,\\nscene graphs of noun phrases, and entire sentences. Moreover, we introduce a\\ncategory-aware retrieval mechanism that models the distribution of natural\\nlanguage surrounding the specific topics in question. Extensive experiments\\ndemonstrate the effectiveness of our method with 5.7%, 16.2%, and 3.4%\\nimprovements in terms of the main metric CIDEr on MSR-VTT, MSVD, and VATEX\\nbenchmarks compared to existing state-of-the-art.|cs.CV        |cs.CV     |2025-03-31T03:00:19Z|\n",
      "|http://arxiv.org/abs/2503.23679v1|The Devil is in the Distributions: Explicit Modeling of Scene Content is\\n  Key in Zero-Shot Video Captioning|Zero-shot video captioning requires that a model generate high-quality\\ncaptions without human-annotated video-text pairs for training.\\nState-of-the-art approaches to the problem leverage CLIP to extract\\nvisual-relevant textual prompts to guide language models in generating\\ncaptions. These methods tend to focus on one key aspect of the scene and build\\na caption that ignores the rest of the visual input. To address this issue, and\\ngenerate more accurate and complete captions, we propose a novel progressive\\nmulti-granularity textual prompting strategy for zero-shot video captioning.\\nOur approach constructs three distinct memory banks, encompassing noun phrases,\\nscene graphs of noun phrases, and entire sentences. Moreover, we introduce a\\ncategory-aware retrieval mechanism that models the distribution of natural\\nlanguage surrounding the specific topics in question. Extensive experiments\\ndemonstrate the effectiveness of our method with 5.7%, 16.2%, and 3.4%\\nimprovements in terms of the main metric CIDEr on MSR-VTT, MSVD, and VATEX\\nbenchmarks compared to existing state-of-the-art.|cs.CV        |cs.CV     |2025-03-31T03:00:19Z|\n",
      "|http://arxiv.org/abs/2503.23679v1|The Devil is in the Distributions: Explicit Modeling of Scene Content is\\n  Key in Zero-Shot Video Captioning|Zero-shot video captioning requires that a model generate high-quality\\ncaptions without human-annotated video-text pairs for training.\\nState-of-the-art approaches to the problem leverage CLIP to extract\\nvisual-relevant textual prompts to guide language models in generating\\ncaptions. These methods tend to focus on one key aspect of the scene and build\\na caption that ignores the rest of the visual input. To address this issue, and\\ngenerate more accurate and complete captions, we propose a novel progressive\\nmulti-granularity textual prompting strategy for zero-shot video captioning.\\nOur approach constructs three distinct memory banks, encompassing noun phrases,\\nscene graphs of noun phrases, and entire sentences. Moreover, we introduce a\\ncategory-aware retrieval mechanism that models the distribution of natural\\nlanguage surrounding the specific topics in question. Extensive experiments\\ndemonstrate the effectiveness of our method with 5.7%, 16.2%, and 3.4%\\nimprovements in terms of the main metric CIDEr on MSR-VTT, MSVD, and VATEX\\nbenchmarks compared to existing state-of-the-art.|cs.CV        |cs.CV     |2025-03-31T03:00:19Z|\n",
      "|http://arxiv.org/abs/2503.23679v1|The Devil is in the Distributions: Explicit Modeling of Scene Content is\\n  Key in Zero-Shot Video Captioning|Zero-shot video captioning requires that a model generate high-quality\\ncaptions without human-annotated video-text pairs for training.\\nState-of-the-art approaches to the problem leverage CLIP to extract\\nvisual-relevant textual prompts to guide language models in generating\\ncaptions. These methods tend to focus on one key aspect of the scene and build\\na caption that ignores the rest of the visual input. To address this issue, and\\ngenerate more accurate and complete captions, we propose a novel progressive\\nmulti-granularity textual prompting strategy for zero-shot video captioning.\\nOur approach constructs three distinct memory banks, encompassing noun phrases,\\nscene graphs of noun phrases, and entire sentences. Moreover, we introduce a\\ncategory-aware retrieval mechanism that models the distribution of natural\\nlanguage surrounding the specific topics in question. Extensive experiments\\ndemonstrate the effectiveness of our method with 5.7%, 16.2%, and 3.4%\\nimprovements in terms of the main metric CIDEr on MSR-VTT, MSVD, and VATEX\\nbenchmarks compared to existing state-of-the-art.|cs.CV        |cs.CV     |2025-03-31T03:00:19Z|\n",
      "|http://arxiv.org/abs/2503.23679v1|The Devil is in the Distributions: Explicit Modeling of Scene Content is\\n  Key in Zero-Shot Video Captioning|Zero-shot video captioning requires that a model generate high-quality\\ncaptions without human-annotated video-text pairs for training.\\nState-of-the-art approaches to the problem leverage CLIP to extract\\nvisual-relevant textual prompts to guide language models in generating\\ncaptions. These methods tend to focus on one key aspect of the scene and build\\na caption that ignores the rest of the visual input. To address this issue, and\\ngenerate more accurate and complete captions, we propose a novel progressive\\nmulti-granularity textual prompting strategy for zero-shot video captioning.\\nOur approach constructs three distinct memory banks, encompassing noun phrases,\\nscene graphs of noun phrases, and entire sentences. Moreover, we introduce a\\ncategory-aware retrieval mechanism that models the distribution of natural\\nlanguage surrounding the specific topics in question. Extensive experiments\\ndemonstrate the effectiveness of our method with 5.7%, 16.2%, and 3.4%\\nimprovements in terms of the main metric CIDEr on MSR-VTT, MSVD, and VATEX\\nbenchmarks compared to existing state-of-the-art.|cs.CV        |cs.CV     |2025-03-31T03:00:19Z|\n",
      "+---------------------------------+-------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dup_ids = papers_df.groupBy(\"aid\").count().filter(\"count > 1\")\n",
    "print(f\"Papers with duplicate IDs: {dup_ids.count()}\")\n",
    "\n",
    "if dup_ids.count() > 0:\n",
    "    print(\"Sample of papers with duplicate IDs:\")\n",
    "    papers_df.join(dup_ids.select(\"aid\"), \"aid\").orderBy(\"aid\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "994ad15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing duplicates based on all columns: 6803 papers remain\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates based on all columns instead of just \"aid\"\n",
    "papers_unique_df = papers_df.dropDuplicates([\"aid\"])\n",
    "print(f\"After removing duplicates based on all columns: {papers_unique_df.count()} papers remain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa9824b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|    main_category|count|\n",
      "+-----------------+-----+\n",
      "|            cs.CV|  820|\n",
      "|            cs.LG|  446|\n",
      "|            cs.CL|  359|\n",
      "|         quant-ph|  314|\n",
      "|            cs.RO|  182|\n",
      "|            cs.AI|  153|\n",
      "|          math.AP|  145|\n",
      "|cond-mat.mtrl-sci|  139|\n",
      "|           hep-ph|  138|\n",
      "|          eess.SY|  127|\n",
      "|            gr-qc|  117|\n",
      "|          math.OC|  117|\n",
      "|          eess.SP|  111|\n",
      "|   physics.optics|  105|\n",
      "|            cs.CR|  102|\n",
      "|           hep-th|  101|\n",
      "|          math.NA|   95|\n",
      "|          math.CO|   95|\n",
      "|            cs.HC|   92|\n",
      "|      astro-ph.GA|   91|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See how many papers per category\n",
    "papers_unique_df.groupBy(\"main_category\").count().orderBy(\"count\", ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36d60ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_unique_df.write.mode(\"overwrite\").json(\"data/interim\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment-03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0072c2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am using the following SPARK_HOME: d:\\OneDrive - CGIAR\\Master\\Advanced Analytics\\assignments\\assignment-03\\spark\\spark-3.5.5-bin-hadoop3\n",
      "Windows detected: set HADOOP_HOME to: d:\\OneDrive - CGIAR\\Master\\Advanced Analytics\\assignments\\assignment-03\\spark\\winutils\n",
      "  Also added Hadoop bin directory to PATH: d:\\OneDrive - CGIAR\\Master\\Advanced Analytics\\assignments\\assignment-03\\spark\\winutils\\bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "spark_home = os.path.abspath(os.getcwd() + \"/spark/spark-3.5.5-bin-hadoop3\")\n",
    "hadoop_home = os.path.abspath(os.getcwd() + \"/spark/winutils\")\n",
    "print(f\"I am using the following SPARK_HOME: {spark_home}\")\n",
    "if os.name == 'nt':\n",
    "    os.environ[\"HADOOP_HOME\"] = f\"{hadoop_home}\"\n",
    "    print(f\"Windows detected: set HADOOP_HOME to: {os.environ['HADOOP_HOME']}\")\n",
    "    hadoop_bin = os.path.join(hadoop_home, \"bin\")\n",
    "    os.environ[\"PATH\"] = f\"{hadoop_bin};{os.environ['PATH']}\"\n",
    "    print(f\"  Also added Hadoop bin directory to PATH: {hadoop_bin}\")\n",
    "\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "findspark.init(spark_home)\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9f80463",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.read.json(\"data/raw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68834bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2449ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Define the schema for the nested JSON\n",
    "paper_schema = StructType([\n",
    "    StructField(\"aid\", StringType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"summary\", StringType()),\n",
    "    StructField(\"main_category\", StringType()),\n",
    "    StructField(\"categories\", StringType()),\n",
    "    StructField(\"published\", StringType())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6955fbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+---------------------+--------------------+\n",
      "|aid                              |title                                                                                                                                 |summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |main_category|categories           |published           |\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+---------------------+--------------------+\n",
      "|http://arxiv.org/abs/2503.23697v1|A Low-complexity Structured Neural Network to Realize States of\\n  Dynamical Systems                                                  |Data-driven learning is rapidly evolving and places a new perspective on\\nrealizing state-space dynamical systems. However, dynamical systems derived\\nfrom nonlinear ordinary differential equations (ODEs) suffer from limitations\\nin computational efficiency. Thus, this paper stems from data-driven learning\\nto advance states of dynamical systems utilizing a structured neural network\\n(StNN). The proposed learning technique also seeks to identify an optimal,\\nlow-complexity operator to solve dynamical systems, the so-called Hankel\\noperator, derived from time-delay measurements. Thus, we utilize the StNN based\\non the Hankel operator to solve dynamical systems as an alternative to existing\\ndata-driven techniques. We show that the proposed StNN reduces the number of\\nparameters and computational complexity compared with the conventional neural\\nnetworks and also with the classical data-driven techniques, such as Sparse\\nIdentification of Nonlinear Dynamics (SINDy) and Hankel Alternative view of\\nKoopman (HAVOK), which is commonly known as delay-Dynamic Mode\\nDecomposition(DMD) or Hankel-DMD. More specifically, we present numerical\\nsimulations to solve dynamical systems utilizing the StNN based on the Hankel\\noperator beginning from the fundamental Lotka-Volterra model, where we compare\\nthe StNN with the LEarning Across Dynamical Systems (LEADS), and extend our\\nanalysis to highly nonlinear and chaotic Lorenz systems, comparing the StNN\\nwith conventional neural networks, SINDy, and HAVOK. Hence, we show that the\\nproposed StNN paves the way for realizing state-space dynamical systems with a\\nlow-complexity learning algorithm, enabling prediction and understanding of\\nfuture states.|cs.LG        |cs.LG,math.DS        |2025-03-31T03:52:38Z|\n",
      "|http://arxiv.org/abs/2503.23715v1|HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video\\n  Generation                                                     |Text-to-video (T2V) generation has made tremendous progress in generating\\ncomplicated scenes based on texts. However, human-object interaction (HOI)\\noften cannot be precisely generated by current T2V models due to the lack of\\nlarge-scale videos with accurate captions for HOI. To address this issue, we\\nintroduce HOIGen-1M, the first largescale dataset for HOI Generation,\\nconsisting of over one million high-quality videos collected from diverse\\nsources. In particular, to guarantee the high quality of videos, we first\\ndesign an efficient framework to automatically curate HOI videos using the\\npowerful multimodal large language models (MLLMs), and then the videos are\\nfurther cleaned by human annotators. Moreover, to obtain accurate textual\\ncaptions for HOI videos, we design a novel video description method based on a\\nMixture-of-Multimodal-Experts (MoME) strategy that not only generates\\nexpressive captions but also eliminates the hallucination by individual MLLM.\\nFurthermore, due to the lack of an evaluation framework for generated HOI\\nvideos, we propose two new metrics to assess the quality of generated videos in\\na coarse-to-fine manner. Extensive experiments reveal that current T2V models\\nstruggle to generate high-quality HOI videos and confirm that our HOIGen-1M\\ndataset is instrumental for improving HOI video generation. Project webpage is\\navailable at https://liuqi-creat.github.io/HOIGen.github.io.                                                                                                                                                                                                                                                                                  |cs.CV        |cs.CV                |2025-03-31T04:30:34Z|\n",
      "|http://arxiv.org/abs/2504.02359v1|First observation of ultra-long-range azimuthal correlations in low\\n  multiplicity pp and p-Pb collisions at the LHC                 |This study presents the first observation of ultra-long-range two-particle\\nazimuthal correlations with pseudorapidity separation of ($|\\Delta \\eta| >\\n5.0$) in proton-proton (pp) and ($|\\Delta \\eta| > 6.5$) in proton-lead (p-Pb)\\ncollisions at the LHC, down to and below the minimum-bias multiplicity.\\nTwo-particle correlation coefficients (${V}_{2\\Delta}$) are measured after\\nremoving non-flow (jets and resonance decays) contributions using the\\ntemplate-fit method across various multiplicity classes, providing novel\\ninsights into the origin of long-range correlations in small systems.\\nComparisons with the 3D-Glauber + MUSIC + UrQMD hydrodynamic model reveal\\nsignificant discrepancies at low multiplicities, indicating possible dynamics\\nbeyond typical hydrodynamic behavior. Initial-state models based on the Color\\nGlass Condensate framework generate only short-range correlations, while PYTHIA\\nsimulations implemented with the string-shoving mechanism also fail to describe\\nthese ultra-long-range correlations. The results challenge existing paradigms\\nand question the underlying mechanisms in low-multiplicity pp and p-Pb\\ncollisions. The findings impose significant constraints on models describing\\ncollective phenomena in small collision systems and advance the understanding\\nof origin of long-range correlations at Large Hadron Collider (LHC) energies.                                                                                                                                                                                                                                                                                                                                               |nucl-ex      |nucl-ex,hep-ex       |2025-04-03T07:46:09Z|\n",
      "|http://arxiv.org/abs/2504.02375v1|A Comparative Study of MINLP and MPVC Formulations for Solving Complex\\n  Nonlinear Decision-Making Problems in Aerospace Applications|High-level decision-making for dynamical systems often involves performance\\nand safety specifications that are activated or deactivated depending on\\nconditions related to the system state and commands. Such decision-making\\nproblems can be naturally formulated as optimization problems where these\\nconditional activations are regulated by discrete variables. However, solving\\nthese problems can be challenging numerically, even on powerful computing\\nplatforms, especially when the dynamics are nonlinear. In this work, we\\nconsider decision-making for nonlinear systems where certain constraints, as\\nwell as possible terms in the cost function, are activated or deactivated\\ndepending on the system state and commands. We show that these problems can be\\nformulated either as mixed-integer nonlinear programs (MINLPs) or as\\nmathematical programs with vanishing constraints (MPVCs), where the former\\nformulation involves discrete decision variables, whereas the latter relies on\\ncontinuous variables subject to structured nonconvex constraints. We discuss\\nthe different solution methods available for both formulations and demonstrate\\nthem on optimal trajectory planning problems in various aerospace applications.\\nFinally, we compare the strengths and weaknesses of the MINLP and MPVC\\napproaches through a focused case study on powered descent guidance with\\ndivert-feasible regions.                                                                                                                                                                                                                                                                                                                           |math.OC      |math.OC,cs.SY,eess.SY|2025-04-03T08:08:52Z|\n",
      "|http://arxiv.org/abs/2504.02358v1|Interference trapping of populations in a semi-infinite\\n  coupled-resonator waveguide                                                |We study the energy structure and dynamics of a two-level emitter (2LE)\\nlocally coupled to a semi-infinite one-dimensional (1D) coupled-resonator array\\n(CRA). The energy spectrum in the single-excitation subspace features a\\ncontinuous band with scattering states, discrete levels with bound states, and\\na quantum phase transition characterized by the change of the number of bound\\nstates. The number of bound states is revealed by the behavior of the\\nexcited-state population at long times with quantum beat, residual oscillation,\\na constant with either non-zero or zero.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |quant-ph     |quant-ph             |2025-04-03T07:45:42Z|\n",
      "+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+---------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse the nested JSON\n",
    "papers_df = raw_df.withColumn(\"paper_data\", \n",
    "                           from_json(col(\"value\"), paper_schema))\\\n",
    "                 .select(\"paper_data.*\")\n",
    "\n",
    "# Display sample data\n",
    "papers_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb987326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 14594\n"
     ]
    }
   ],
   "source": [
    "# Count records\n",
    "print(f\"Total number of rows: {papers_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fd59e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers with duplicate IDs: 4179\n",
      "Sample of papers with duplicate IDs:\n",
      "+---------------------------------+-------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+----------+--------------------+\n",
      "|aid                              |title                                                                                                        |summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |main_category|categories|published           |\n",
      "+---------------------------------+-------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+----------+--------------------+\n",
      "|http://arxiv.org/abs/2503.23679v1|The Devil is in the Distributions: Explicit Modeling of Scene Content is\\n  Key in Zero-Shot Video Captioning|Zero-shot video captioning requires that a model generate high-quality\\ncaptions without human-annotated video-text pairs for training.\\nState-of-the-art approaches to the problem leverage CLIP to extract\\nvisual-relevant textual prompts to guide language models in generating\\ncaptions. These methods tend to focus on one key aspect of the scene and build\\na caption that ignores the rest of the visual input. To address this issue, and\\ngenerate more accurate and complete captions, we propose a novel progressive\\nmulti-granularity textual prompting strategy for zero-shot video captioning.\\nOur approach constructs three distinct memory banks, encompassing noun phrases,\\nscene graphs of noun phrases, and entire sentences. Moreover, we introduce a\\ncategory-aware retrieval mechanism that models the distribution of natural\\nlanguage surrounding the specific topics in question. Extensive experiments\\ndemonstrate the effectiveness of our method with 5.7%, 16.2%, and 3.4%\\nimprovements in terms of the main metric CIDEr on MSR-VTT, MSVD, and VATEX\\nbenchmarks compared to existing state-of-the-art.|cs.CV        |cs.CV     |2025-03-31T03:00:19Z|\n",
      "|http://arxiv.org/abs/2503.23679v1|The Devil is in the Distributions: Explicit Modeling of Scene Content is\\n  Key in Zero-Shot Video Captioning|Zero-shot video captioning requires that a model generate high-quality\\ncaptions without human-annotated video-text pairs for training.\\nState-of-the-art approaches to the problem leverage CLIP to extract\\nvisual-relevant textual prompts to guide language models in generating\\ncaptions. These methods tend to focus on one key aspect of the scene and build\\na caption that ignores the rest of the visual input. To address this issue, and\\ngenerate more accurate and complete captions, we propose a novel progressive\\nmulti-granularity textual prompting strategy for zero-shot video captioning.\\nOur approach constructs three distinct memory banks, encompassing noun phrases,\\nscene graphs of noun phrases, and entire sentences. Moreover, we introduce a\\ncategory-aware retrieval mechanism that models the distribution of natural\\nlanguage surrounding the specific topics in question. Extensive experiments\\ndemonstrate the effectiveness of our method with 5.7%, 16.2%, and 3.4%\\nimprovements in terms of the main metric CIDEr on MSR-VTT, MSVD, and VATEX\\nbenchmarks compared to existing state-of-the-art.|cs.CV        |cs.CV     |2025-03-31T03:00:19Z|\n",
      "|http://arxiv.org/abs/2503.23679v1|The Devil is in the Distributions: Explicit Modeling of Scene Content is\\n  Key in Zero-Shot Video Captioning|Zero-shot video captioning requires that a model generate high-quality\\ncaptions without human-annotated video-text pairs for training.\\nState-of-the-art approaches to the problem leverage CLIP to extract\\nvisual-relevant textual prompts to guide language models in generating\\ncaptions. These methods tend to focus on one key aspect of the scene and build\\na caption that ignores the rest of the visual input. To address this issue, and\\ngenerate more accurate and complete captions, we propose a novel progressive\\nmulti-granularity textual prompting strategy for zero-shot video captioning.\\nOur approach constructs three distinct memory banks, encompassing noun phrases,\\nscene graphs of noun phrases, and entire sentences. Moreover, we introduce a\\ncategory-aware retrieval mechanism that models the distribution of natural\\nlanguage surrounding the specific topics in question. Extensive experiments\\ndemonstrate the effectiveness of our method with 5.7%, 16.2%, and 3.4%\\nimprovements in terms of the main metric CIDEr on MSR-VTT, MSVD, and VATEX\\nbenchmarks compared to existing state-of-the-art.|cs.CV        |cs.CV     |2025-03-31T03:00:19Z|\n",
      "|http://arxiv.org/abs/2503.23679v1|The Devil is in the Distributions: Explicit Modeling of Scene Content is\\n  Key in Zero-Shot Video Captioning|Zero-shot video captioning requires that a model generate high-quality\\ncaptions without human-annotated video-text pairs for training.\\nState-of-the-art approaches to the problem leverage CLIP to extract\\nvisual-relevant textual prompts to guide language models in generating\\ncaptions. These methods tend to focus on one key aspect of the scene and build\\na caption that ignores the rest of the visual input. To address this issue, and\\ngenerate more accurate and complete captions, we propose a novel progressive\\nmulti-granularity textual prompting strategy for zero-shot video captioning.\\nOur approach constructs three distinct memory banks, encompassing noun phrases,\\nscene graphs of noun phrases, and entire sentences. Moreover, we introduce a\\ncategory-aware retrieval mechanism that models the distribution of natural\\nlanguage surrounding the specific topics in question. Extensive experiments\\ndemonstrate the effectiveness of our method with 5.7%, 16.2%, and 3.4%\\nimprovements in terms of the main metric CIDEr on MSR-VTT, MSVD, and VATEX\\nbenchmarks compared to existing state-of-the-art.|cs.CV        |cs.CV     |2025-03-31T03:00:19Z|\n",
      "|http://arxiv.org/abs/2503.23679v1|The Devil is in the Distributions: Explicit Modeling of Scene Content is\\n  Key in Zero-Shot Video Captioning|Zero-shot video captioning requires that a model generate high-quality\\ncaptions without human-annotated video-text pairs for training.\\nState-of-the-art approaches to the problem leverage CLIP to extract\\nvisual-relevant textual prompts to guide language models in generating\\ncaptions. These methods tend to focus on one key aspect of the scene and build\\na caption that ignores the rest of the visual input. To address this issue, and\\ngenerate more accurate and complete captions, we propose a novel progressive\\nmulti-granularity textual prompting strategy for zero-shot video captioning.\\nOur approach constructs three distinct memory banks, encompassing noun phrases,\\nscene graphs of noun phrases, and entire sentences. Moreover, we introduce a\\ncategory-aware retrieval mechanism that models the distribution of natural\\nlanguage surrounding the specific topics in question. Extensive experiments\\ndemonstrate the effectiveness of our method with 5.7%, 16.2%, and 3.4%\\nimprovements in terms of the main metric CIDEr on MSR-VTT, MSVD, and VATEX\\nbenchmarks compared to existing state-of-the-art.|cs.CV        |cs.CV     |2025-03-31T03:00:19Z|\n",
      "+---------------------------------+-------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dup_ids = papers_df.groupBy(\"aid\").count().filter(\"count > 1\")\n",
    "print(f\"Papers with duplicate IDs: {dup_ids.count()}\")\n",
    "\n",
    "if dup_ids.count() > 0:\n",
    "    print(\"Sample of papers with duplicate IDs:\")\n",
    "    papers_df.join(dup_ids.select(\"aid\"), \"aid\").orderBy(\"aid\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "994ad15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing duplicates based on all columns: 4442 papers remain\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates based on all columns instead of just \"aid\"\n",
    "papers_unique_df = papers_df.dropDuplicates([\"aid\"])\n",
    "print(f\"After removing duplicates based on all columns: {papers_unique_df.count()} papers remain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa9824b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|    main_category|count|\n",
      "+-----------------+-----+\n",
      "|            cs.CV|  511|\n",
      "|            cs.LG|  295|\n",
      "|            cs.CL|  234|\n",
      "|         quant-ph|  195|\n",
      "|            cs.RO|  118|\n",
      "|            cs.AI|  108|\n",
      "|          math.AP|   94|\n",
      "|          eess.SY|   91|\n",
      "|cond-mat.mtrl-sci|   90|\n",
      "|           hep-ph|   84|\n",
      "|          math.OC|   73|\n",
      "|            gr-qc|   70|\n",
      "|   physics.optics|   69|\n",
      "|          eess.SP|   68|\n",
      "|          math.CO|   67|\n",
      "|      astro-ph.GA|   66|\n",
      "|            cs.CR|   62|\n",
      "|           hep-th|   61|\n",
      "|cond-mat.mes-hall|   60|\n",
      "|          math.NA|   59|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See how many papers per category\n",
    "papers_unique_df.groupBy(\"main_category\").count().orderBy(\"count\", ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36d60ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_unique_df.write.mode(\"overwrite\").json(\"data/interim\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment-03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

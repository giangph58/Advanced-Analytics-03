{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945f86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.onnx\n",
    "\n",
    "model_path = \"models/scibert-finetuned-arxiv-42/checkpoint-6000\"\n",
    "\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Convert to ONNX\n",
    "torch.onnx.export(model, dummy_input, \"models/scibert_finetuned-arxiv-42.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545d48cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "stream_data = load_from_disk(\"data/processed/stream_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0bef54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To MT or not to MT: An eye-tracking study on the reception by Dutch\\n  readers of different translation and creativity levels\\nThis article presents the results of a pilot study involving the reception of\\na fictional short story translated from English into Dutch under four\\nconditions: machine translation (MT), post-editing (PE), human translation (HT)\\nand original source text (ST). The aim is to understand how creativity and\\nerrors in different translation modalities affect readers, specifically\\nregarding cognitive load. Eight participants filled in a questionnaire, read a\\nstory using an eye-tracker, and conducted a retrospective think-aloud (RTA)\\ninterview. The results show that units of creative potential (UCP) increase\\ncognitive load and that this effect is highest for HT and lowest for MT; no\\neffect of error was observed. Triangulating the data with RTAs leads us to\\nhypothesize that the higher cognitive load in UCPs is linked to increases in\\nreader enjoyment and immersion. The effect of translation creativity on\\ncognitive load in different translation modalities at word-level is novel and\\nopens up new avenues for further research. All the code and data are available\\nat https://github.com/INCREC/Pilot_to_MT_or_not_to_MT',\n",
       " 'A Nonlinear Logistic Model for Age-Structured Populations: Analysis of\\n  Long-Term Dynamics and Equilibria\\nThis paper investigates a nonlinear logistic model for age-structured\\npopulation dynamics. The model incorporates interdependent fertility and\\nmortality functions within a logistic framework, offering insights into\\nstationary solutions and asymptotic behavior. Theoretical findings establish\\nconditions for the existence and uniqueness of equilibrium solutions and\\nexplore long-term population dynamics. This study provides valuable tools for\\ndemographic modeling and opens avenues for further mathematical exploration.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_data[\"train\"][\"text\"][:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10e96b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ColBERT-serve: Efficient Multi-Stage Memory-Mapped Scoring\\n'\n",
      " 'We study serving retrieval models, specifically late interaction models '\n",
      " 'like\\n'\n",
      " 'ColBERT, to many concurrent users at once and under a small budget, in '\n",
      " 'which\\n'\n",
      " 'the index may not fit in memory. We present ColBERT-serve, a novel serving\\n'\n",
      " 'system that applies a memory-mapping strategy to the ColBERT index, '\n",
      " 'reducing\\n'\n",
      " 'RAM usage by 90% and permitting its deployment on cheap servers, and\\n'\n",
      " 'incorporates a multi-stage architecture with hybrid scoring, reducing '\n",
      " \"ColBERT's\\n\"\n",
      " 'query latency and supporting many concurrent queries in parallel.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(stream_data[\"train\"][\"text\"][4])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

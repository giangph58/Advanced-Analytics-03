{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da3f3913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from src.utils import map_category\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "318ad662",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_data = load_from_disk(\"data/processed/stream_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5bf88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_data.set_format(type=\"pandas\")\n",
    "train_df = stream_data[\"train\"][:]\n",
    "valid_df = stream_data[\"validation\"][:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d193ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def label_int2str(row):\n",
    "#     return stream_data[\"train\"].features[\"label\"].int2str(row)\n",
    "# train_df[\"label_name\"] = train_df[\"label\"].apply(label_int2str)\n",
    "# train_df[\"label_name\"].value_counts(ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8b4eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df[\"text\"], train_df[\"label\"]\n",
    "X_valid, y_valid = valid_df[\"text\"], valid_df[\"label\"]\n",
    "labels = stream_data[\"train\"].features[\"label\"].names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471e1b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9bb5fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define classification head\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.linear = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = features['sentence_embedding']\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Define the number of classes for a classification task.\n",
    "num_classes = 20\n",
    "classification_head = ClassificationHead(model.get_sentence_embedding_dimension(), num_classes)\n",
    "\n",
    "# Combine SentenceTransformer model and classification head.\"\n",
    "class SentenceTransformerWithHead(nn.Module):\n",
    "    def __init__(self, transformer, head):\n",
    "        super(SentenceTransformerWithHead, self).__init__()\n",
    "        self.transformer = transformer\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, input):\n",
    "        features = self.transformer(input)\n",
    "        logits = self.head(features)\n",
    "        return logits\n",
    "\n",
    "model_with_head = SentenceTransformerWithHead(model, classification_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9de63778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/processing/climate_llm/learn/aa/Advanced-Analytics-03/.venv/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 0/4457, Loss: 3.0041\n",
      "Epoch 0, Step 100/4457, Loss: 2.9559\n",
      "Epoch 0, Step 200/4457, Loss: 2.8772\n",
      "Epoch 0, Step 300/4457, Loss: 2.4159\n",
      "Epoch 0, Step 400/4457, Loss: 2.5326\n",
      "Epoch 0, Step 500/4457, Loss: 3.0054\n",
      "Epoch 0, Step 600/4457, Loss: 2.3365\n",
      "Epoch 0, Step 700/4457, Loss: 2.3214\n",
      "Epoch 0, Step 800/4457, Loss: 2.3561\n",
      "Epoch 0, Step 900/4457, Loss: 2.6650\n",
      "Epoch 0, Step 1000/4457, Loss: 2.4817\n",
      "Epoch 0, Step 1100/4457, Loss: 2.8405\n",
      "Epoch 0, Step 1200/4457, Loss: 2.4017\n",
      "Epoch 0, Step 1300/4457, Loss: 2.3789\n",
      "Epoch 0, Step 1400/4457, Loss: 2.1779\n",
      "Epoch 0, Step 1500/4457, Loss: 2.7591\n",
      "Epoch 0, Step 1600/4457, Loss: 2.3966\n",
      "Epoch 0, Step 1700/4457, Loss: 2.8235\n",
      "Epoch 0, Step 1800/4457, Loss: 2.7049\n",
      "Epoch 0, Step 1900/4457, Loss: 3.0158\n",
      "Epoch 0, Step 2000/4457, Loss: 2.3519\n",
      "Epoch 0, Step 2100/4457, Loss: 2.3287\n",
      "Epoch 0, Step 2200/4457, Loss: 2.2698\n",
      "Epoch 0, Step 2300/4457, Loss: 2.5066\n",
      "Epoch 0, Step 2400/4457, Loss: 2.8131\n",
      "Epoch 0, Step 2500/4457, Loss: 2.6059\n",
      "Epoch 0, Step 2600/4457, Loss: 2.2908\n",
      "Epoch 0, Step 2700/4457, Loss: 2.2522\n",
      "Epoch 0, Step 2800/4457, Loss: 2.5792\n",
      "Epoch 0, Step 2900/4457, Loss: 2.5008\n",
      "Epoch 0, Step 3000/4457, Loss: 2.3650\n",
      "Epoch 0, Step 3100/4457, Loss: 2.4768\n",
      "Epoch 0, Step 3200/4457, Loss: 2.1210\n",
      "Epoch 0, Step 3300/4457, Loss: 1.8281\n",
      "Epoch 0, Step 3400/4457, Loss: 1.9571\n",
      "Epoch 0, Step 3500/4457, Loss: 2.1989\n",
      "Epoch 0, Step 3600/4457, Loss: 1.7784\n",
      "Epoch 0, Step 3700/4457, Loss: 2.6958\n",
      "Epoch 0, Step 3800/4457, Loss: 1.7772\n",
      "Epoch 0, Step 3900/4457, Loss: 2.7112\n",
      "Epoch 0, Step 4000/4457, Loss: 1.7289\n",
      "Epoch 0, Step 4100/4457, Loss: 2.4225\n",
      "Epoch 0, Step 4200/4457, Loss: 1.6912\n",
      "Epoch 0, Step 4300/4457, Loss: 1.8365\n",
      "Epoch 0, Step 4400/4457, Loss: 2.1028\n",
      "Epoch 1/5, Time: 226.53s\n",
      "Train Loss: 2.4427, Val Loss: 2.1968\n",
      "Accuracy: 79.44%, F1 Score: 0.7632\n",
      "Saved best model with F1: 0.7632\n",
      "Epoch 1, Step 0/4457, Loss: 1.8277\n",
      "Epoch 1, Step 100/4457, Loss: 1.8139\n",
      "Epoch 1, Step 200/4457, Loss: 2.5070\n",
      "Epoch 1, Step 300/4457, Loss: 2.3668\n",
      "Epoch 1, Step 400/4457, Loss: 2.0910\n",
      "Epoch 1, Step 500/4457, Loss: 2.0202\n",
      "Epoch 1, Step 600/4457, Loss: 1.7386\n",
      "Epoch 1, Step 700/4457, Loss: 1.7353\n",
      "Epoch 1, Step 800/4457, Loss: 2.7159\n",
      "Epoch 1, Step 900/4457, Loss: 1.9976\n",
      "Epoch 1, Step 1000/4457, Loss: 2.3684\n",
      "Epoch 1, Step 1100/4457, Loss: 1.6629\n",
      "Epoch 1, Step 1200/4457, Loss: 1.4842\n",
      "Epoch 1, Step 1300/4457, Loss: 1.9287\n",
      "Epoch 1, Step 1400/4457, Loss: 1.8569\n",
      "Epoch 1, Step 1500/4457, Loss: 1.6316\n",
      "Epoch 1, Step 1600/4457, Loss: 1.6720\n",
      "Epoch 1, Step 1700/4457, Loss: 2.6845\n",
      "Epoch 1, Step 1800/4457, Loss: 2.6078\n",
      "Epoch 1, Step 1900/4457, Loss: 2.0946\n",
      "Epoch 1, Step 2000/4457, Loss: 2.7761\n",
      "Epoch 1, Step 2100/4457, Loss: 1.3676\n",
      "Epoch 1, Step 2200/4457, Loss: 1.3739\n",
      "Epoch 1, Step 2300/4457, Loss: 1.8693\n",
      "Epoch 1, Step 2400/4457, Loss: 2.0706\n",
      "Epoch 1, Step 2500/4457, Loss: 1.6331\n",
      "Epoch 1, Step 2600/4457, Loss: 1.5381\n",
      "Epoch 1, Step 2700/4457, Loss: 2.5584\n",
      "Epoch 1, Step 2800/4457, Loss: 1.2950\n",
      "Epoch 1, Step 2900/4457, Loss: 2.6047\n",
      "Epoch 1, Step 3000/4457, Loss: 1.2845\n",
      "Epoch 1, Step 3100/4457, Loss: 2.6680\n",
      "Epoch 1, Step 3200/4457, Loss: 2.4091\n",
      "Epoch 1, Step 3300/4457, Loss: 1.4489\n",
      "Epoch 1, Step 3400/4457, Loss: 2.2137\n",
      "Epoch 1, Step 3500/4457, Loss: 1.5320\n",
      "Epoch 1, Step 3600/4457, Loss: 1.2539\n",
      "Epoch 1, Step 3700/4457, Loss: 1.7632\n",
      "Epoch 1, Step 3800/4457, Loss: 2.2794\n",
      "Epoch 1, Step 3900/4457, Loss: 2.3708\n",
      "Epoch 1, Step 4000/4457, Loss: 2.6368\n",
      "Epoch 1, Step 4100/4457, Loss: 1.3366\n",
      "Epoch 1, Step 4200/4457, Loss: 2.0365\n",
      "Epoch 1, Step 4300/4457, Loss: 2.1480\n",
      "Epoch 1, Step 4400/4457, Loss: 1.6610\n",
      "Epoch 2/5, Time: 225.88s\n",
      "Train Loss: 2.0198, Val Loss: 1.8853\n",
      "Accuracy: 81.15%, F1 Score: 0.7783\n",
      "Saved best model with F1: 0.7783\n",
      "Epoch 2, Step 0/4457, Loss: 1.3772\n",
      "Epoch 2, Step 100/4457, Loss: 1.7267\n",
      "Epoch 2, Step 200/4457, Loss: 1.1104\n",
      "Epoch 2, Step 300/4457, Loss: 2.2526\n",
      "Epoch 2, Step 400/4457, Loss: 1.0939\n",
      "Epoch 2, Step 500/4457, Loss: 1.2651\n",
      "Epoch 2, Step 600/4457, Loss: 2.4461\n",
      "Epoch 2, Step 700/4457, Loss: 1.0728\n",
      "Epoch 2, Step 800/4457, Loss: 1.0657\n",
      "Epoch 2, Step 900/4457, Loss: 1.5901\n",
      "Epoch 2, Step 1000/4457, Loss: 1.8708\n",
      "Epoch 2, Step 1100/4457, Loss: 1.0422\n",
      "Epoch 2, Step 1200/4457, Loss: 2.4526\n",
      "Epoch 2, Step 1300/4457, Loss: 1.9010\n",
      "Epoch 2, Step 1400/4457, Loss: 1.8828\n",
      "Epoch 2, Step 1500/4457, Loss: 2.1470\n",
      "Epoch 2, Step 1600/4457, Loss: 1.5453\n",
      "Epoch 2, Step 1700/4457, Loss: 2.1858\n",
      "Epoch 2, Step 1800/4457, Loss: 0.9924\n",
      "Epoch 2, Step 1900/4457, Loss: 1.9754\n",
      "Epoch 2, Step 2000/4457, Loss: 1.7215\n",
      "Epoch 2, Step 2100/4457, Loss: 0.9802\n",
      "Epoch 2, Step 2200/4457, Loss: 2.8305\n",
      "Epoch 2, Step 2300/4457, Loss: 1.1374\n",
      "Epoch 2, Step 2400/4457, Loss: 2.5476\n",
      "Epoch 2, Step 2500/4457, Loss: 1.4158\n",
      "Epoch 2, Step 2600/4457, Loss: 1.8223\n",
      "Epoch 2, Step 2700/4457, Loss: 2.0831\n",
      "Epoch 2, Step 2800/4457, Loss: 1.1604\n",
      "Epoch 2, Step 2900/4457, Loss: 2.6422\n",
      "Epoch 2, Step 3000/4457, Loss: 0.9253\n",
      "Epoch 2, Step 3100/4457, Loss: 1.0079\n",
      "Epoch 2, Step 3200/4457, Loss: 1.5326\n",
      "Epoch 2, Step 3300/4457, Loss: 1.2261\n",
      "Epoch 2, Step 3400/4457, Loss: 2.2181\n",
      "Epoch 2, Step 3500/4457, Loss: 1.8571\n",
      "Epoch 2, Step 3600/4457, Loss: 1.4440\n",
      "Epoch 2, Step 3700/4457, Loss: 1.4533\n",
      "Epoch 2, Step 3800/4457, Loss: 1.1910\n",
      "Epoch 2, Step 3900/4457, Loss: 0.8697\n",
      "Epoch 2, Step 4000/4457, Loss: 1.5057\n",
      "Epoch 2, Step 4100/4457, Loss: 3.4381\n",
      "Epoch 2, Step 4200/4457, Loss: 1.0783\n",
      "Epoch 2, Step 4300/4457, Loss: 2.7461\n",
      "Epoch 2, Step 4400/4457, Loss: 0.8456\n",
      "Epoch 3/5, Time: 226.20s\n",
      "Train Loss: 1.7490, Val Loss: 1.7101\n",
      "Accuracy: 80.61%, F1 Score: 0.7721\n",
      "Epoch 3, Step 0/4457, Loss: 1.1628\n",
      "Epoch 3, Step 100/4457, Loss: 0.9557\n",
      "Epoch 3, Step 200/4457, Loss: 2.0704\n",
      "Epoch 3, Step 300/4457, Loss: 1.0488\n",
      "Epoch 3, Step 400/4457, Loss: 2.0841\n",
      "Epoch 3, Step 500/4457, Loss: 1.7733\n",
      "Epoch 3, Step 600/4457, Loss: 1.6328\n",
      "Epoch 3, Step 700/4457, Loss: 3.1431\n",
      "Epoch 3, Step 800/4457, Loss: 1.4562\n",
      "Epoch 3, Step 900/4457, Loss: 1.1267\n",
      "Epoch 3, Step 1000/4457, Loss: 0.7975\n",
      "Epoch 3, Step 1100/4457, Loss: 0.7925\n",
      "Epoch 3, Step 1200/4457, Loss: 2.9407\n",
      "Epoch 3, Step 1300/4457, Loss: 1.6378\n",
      "Epoch 3, Step 1400/4457, Loss: 2.7561\n",
      "Epoch 3, Step 1500/4457, Loss: 1.3713\n",
      "Epoch 3, Step 1600/4457, Loss: 1.6106\n",
      "Epoch 3, Step 1700/4457, Loss: 0.7738\n",
      "Epoch 3, Step 1800/4457, Loss: 1.0047\n",
      "Epoch 3, Step 1900/4457, Loss: 2.1327\n",
      "Epoch 3, Step 2000/4457, Loss: 3.3283\n",
      "Epoch 3, Step 2100/4457, Loss: 1.0934\n",
      "Epoch 3, Step 2200/4457, Loss: 0.7877\n",
      "Epoch 3, Step 2300/4457, Loss: 2.1506\n",
      "Epoch 3, Step 2400/4457, Loss: 1.6106\n",
      "Epoch 3, Step 2500/4457, Loss: 1.7076\n",
      "Epoch 3, Step 2600/4457, Loss: 0.9681\n",
      "Epoch 3, Step 2700/4457, Loss: 1.3270\n",
      "Epoch 3, Step 2800/4457, Loss: 2.3680\n",
      "Epoch 3, Step 2900/4457, Loss: 2.4423\n",
      "Epoch 3, Step 3000/4457, Loss: 1.6476\n",
      "Epoch 3, Step 3100/4457, Loss: 0.7380\n",
      "Epoch 3, Step 3200/4457, Loss: 2.1221\n",
      "Epoch 3, Step 3300/4457, Loss: 1.8278\n",
      "Epoch 3, Step 3400/4457, Loss: 1.5020\n",
      "Epoch 3, Step 3500/4457, Loss: 1.6339\n",
      "Epoch 3, Step 3600/4457, Loss: 1.2891\n",
      "Epoch 3, Step 3700/4457, Loss: 2.4096\n",
      "Epoch 3, Step 3800/4457, Loss: 2.2897\n",
      "Epoch 3, Step 3900/4457, Loss: 0.7156\n",
      "Epoch 3, Step 4000/4457, Loss: 1.6482\n",
      "Epoch 3, Step 4100/4457, Loss: 1.2877\n",
      "Epoch 3, Step 4200/4457, Loss: 1.7319\n",
      "Epoch 3, Step 4300/4457, Loss: 0.9143\n",
      "Epoch 3, Step 4400/4457, Loss: 2.2791\n",
      "Epoch 4/5, Time: 226.55s\n",
      "Train Loss: 1.5777, Val Loss: 1.6146\n",
      "Accuracy: 82.50%, F1 Score: 0.8041\n",
      "Saved best model with F1: 0.8041\n",
      "Epoch 4, Step 0/4457, Loss: 1.3594\n",
      "Epoch 4, Step 100/4457, Loss: 0.9814\n",
      "Epoch 4, Step 200/4457, Loss: 1.9103\n",
      "Epoch 4, Step 300/4457, Loss: 1.3580\n",
      "Epoch 4, Step 400/4457, Loss: 2.1758\n",
      "Epoch 4, Step 500/4457, Loss: 2.2432\n",
      "Epoch 4, Step 600/4457, Loss: 0.6860\n",
      "Epoch 4, Step 700/4457, Loss: 0.7483\n",
      "Epoch 4, Step 800/4457, Loss: 0.6839\n",
      "Epoch 4, Step 900/4457, Loss: 0.9271\n",
      "Epoch 4, Step 1000/4457, Loss: 2.0657\n",
      "Epoch 4, Step 1100/4457, Loss: 1.1416\n",
      "Epoch 4, Step 1200/4457, Loss: 0.6779\n",
      "Epoch 4, Step 1300/4457, Loss: 1.4526\n",
      "Epoch 4, Step 1400/4457, Loss: 2.0097\n",
      "Epoch 4, Step 1500/4457, Loss: 1.3356\n",
      "Epoch 4, Step 1600/4457, Loss: 1.2439\n",
      "Epoch 4, Step 1700/4457, Loss: 2.5085\n",
      "Epoch 4, Step 1800/4457, Loss: 1.7886\n",
      "Epoch 4, Step 1900/4457, Loss: 1.5696\n",
      "Epoch 4, Step 2000/4457, Loss: 1.3337\n",
      "Epoch 4, Step 2100/4457, Loss: 1.1137\n",
      "Epoch 4, Step 2200/4457, Loss: 1.3279\n",
      "Epoch 4, Step 2300/4457, Loss: 1.4905\n",
      "Epoch 4, Step 2400/4457, Loss: 1.3408\n",
      "Epoch 4, Step 2500/4457, Loss: 0.6628\n",
      "Epoch 4, Step 2600/4457, Loss: 1.8960\n",
      "Epoch 4, Step 2700/4457, Loss: 0.6728\n",
      "Epoch 4, Step 2800/4457, Loss: 2.2823\n",
      "Epoch 4, Step 2900/4457, Loss: 2.0087\n",
      "Epoch 4, Step 3000/4457, Loss: 1.9596\n",
      "Epoch 4, Step 3100/4457, Loss: 0.6552\n",
      "Epoch 4, Step 3200/4457, Loss: 1.4438\n",
      "Epoch 4, Step 3300/4457, Loss: 1.2273\n",
      "Epoch 4, Step 3400/4457, Loss: 2.0904\n",
      "Epoch 4, Step 3500/4457, Loss: 1.2444\n",
      "Epoch 4, Step 3600/4457, Loss: 0.8730\n",
      "Epoch 4, Step 3700/4457, Loss: 0.8707\n",
      "Epoch 4, Step 3800/4457, Loss: 0.9537\n",
      "Epoch 4, Step 3900/4457, Loss: 0.6509\n",
      "Epoch 4, Step 4000/4457, Loss: 1.4857\n",
      "Epoch 4, Step 4100/4457, Loss: 1.3194\n",
      "Epoch 4, Step 4200/4457, Loss: 1.9532\n",
      "Epoch 4, Step 4300/4457, Loss: 0.8665\n",
      "Epoch 4, Step 4400/4457, Loss: 1.8826\n",
      "Epoch 5/5, Time: 223.87s\n",
      "Train Loss: 1.4708, Val Loss: 1.5800\n",
      "Accuracy: 82.59%, F1 Score: 0.8029\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from sentence_transformers import InputExample\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move models to device once\n",
    "model = model.to(device)\n",
    "model_with_head = model_with_head.to(device)\n",
    "\n",
    "# training parameters\n",
    "num_epochs = 5\n",
    "batch_size = 2\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# Convert the dataset to PyTorch tensors\n",
    "train_examples = [InputExample(texts=[s], label=l) for s, l in zip(X_train, y_train)]\n",
    "valid_examples = [InputExample(texts=[s], label=l) for s, l in zip(X_valid, y_valid)]\n",
    "\n",
    "# Dataloaders\n",
    "def collate_fn(batch):\n",
    "    texts = [example.texts[0] for example in batch]\n",
    "    labels = torch.tensor([example.label for example in batch])\n",
    "    return texts, labels\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_examples, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# Define the loss function, optimizer, and scheduler\n",
    "y_train_array = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train_array), y=y_train_array)\n",
    "class_weights = torch.FloatTensor(class_weights)\n",
    "class_weights = class_weights.to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "optimizer = AdamW(model_with_head.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training loop\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "best_val_f1 = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model_with_head.train()\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for step, (texts, labels) in enumerate(train_dataloader):\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Encode text and pass through classification head\n",
    "        inputs = model.tokenize(texts)\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        input_attention_mask = inputs['attention_mask'].to(device)\n",
    "        inputs_final = {'input_ids': input_ids, 'attention_mask': input_attention_mask}\n",
    "        \n",
    "        logits = model_with_head(inputs_final)\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Step {step}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    train_loss_list.append(avg_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model_with_head.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels in valid_dataloader:\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            inputs = model.tokenize(texts)\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            input_attention_mask = inputs['attention_mask'].to(device)\n",
    "            inputs_final = {'input_ids': input_ids, 'attention_mask': input_attention_mask}\n",
    "            \n",
    "            logits = model_with_head(inputs_final)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Collect predictions for F1 score\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    val_loss = val_loss / len(valid_dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Time: {epoch_time:.2f}s')\n",
    "    print(f'Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    print(f'Accuracy: {accuracy:.2f}%, F1 Score: {val_f1:.4f}')\n",
    "    \n",
    "    # Save both the encoder model and the full model with head\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        model_save_path = f'data/interim/epoch-{epoch}'\n",
    "        model.save(model_save_path)  # Save encoder\n",
    "        \n",
    "        # Save the full model with classification head\n",
    "        torch.save({\n",
    "            'model_state_dict': model_with_head.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, f'{model_save_path}_with_head.pt')\n",
    "        print(f\"Saved best model with F1: {val_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cea0adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "# model_final_save_path='data/interim/st_ft_'\n",
    "# model.save(model_final_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1413c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"data/interim/epoch-3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faf13159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4bf6d43c7941c3831ecb89d5627855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/279 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_embed = model.encode(X_train, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "904b34e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d11121fced4009bdd571f9d9981d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_valid_embed = model.encode(X_valid, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e3c0842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8914, 384), (1114, 384))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embed.shape, X_valid_embed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47a45cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.extmath import density\n",
    "from time import time\n",
    "\n",
    "def benchmark(clf, custom_name=False):\n",
    "    print(\"_\" * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train_embed, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(f\"train time: {train_time:.3}s\")\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_valid_embed)\n",
    "    test_time = time() - t0\n",
    "    print(f\"inference time:  {test_time:.3}s\")\n",
    "\n",
    "    weighted_f1 = f1_score(y_valid, pred, average=\"weighted\")\n",
    "    print(f\"Weighted F1 score:    {weighted_f1:.3}\")\n",
    "\n",
    "    if hasattr(clf, \"coef_\"):\n",
    "        print(f\"dimensionality: {clf.coef_.shape[1]}\")\n",
    "        print(f\"density: {density(clf.coef_)}\")\n",
    "        print()\n",
    "    \n",
    "    print()\n",
    "    if custom_name:\n",
    "        clf_descr = str(custom_name)\n",
    "    else:\n",
    "        clf_descr = clf.__class__.__name__\n",
    "    return clf_descr, weighted_f1, train_time, test_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19d67383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Logistic Regression\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LogisticRegression(class_weight='balanced')\n",
      "train time: 7.57s\n",
      "inference time:  0.00116s\n",
      "Weighted F1 score:    0.811\n",
      "dimensionality: 384\n",
      "density: 1.0\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Ridge Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RidgeClassifier(class_weight='balanced')\n",
      "train time: 0.0737s\n",
      "inference time:  0.0305s\n",
      "Weighted F1 score:    0.808\n",
      "dimensionality: 384\n",
      "density: 1.0\n",
      "\n",
      "\n",
      "================================================================================\n",
      "kNN\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier()\n",
      "train time: 0.00202s\n",
      "inference time:  0.155s\n",
      "Weighted F1 score:    0.817\n",
      "\n",
      "================================================================================\n",
      "XGBoost\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              feature_weights=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
      "              n_jobs=None, num_parallel_tree=None, ...)\n",
      "train time: 7.87s\n",
      "inference time:  0.0157s\n",
      "Weighted F1 score:    0.818\n",
      "\n",
      "================================================================================\n",
      "Linear SVC\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(class_weight='balanced')\n",
      "train time: 7.17s\n",
      "inference time:  0.0203s\n",
      "Weighted F1 score:    0.823\n",
      "dimensionality: 384\n",
      "density: 1.0\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SGD Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(class_weight='balanced')\n",
      "train time: 1.42s\n",
      "inference time:  0.00119s\n",
      "Weighted F1 score:    0.817\n",
      "dimensionality: 384\n",
      "density: 1.0\n",
      "\n",
      "\n",
      "================================================================================\n",
      "NearestCentroid\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "NearestCentroid()\n",
      "train time: 0.0482s\n",
      "inference time:  0.0755s\n",
      "Weighted F1 score:    0.821\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "results_embed = []\n",
    "for clf, name in (\n",
    "    (LogisticRegression(class_weight=\"balanced\"), \"Logistic Regression\"),\n",
    "    (RidgeClassifier(class_weight=\"balanced\"), \"Ridge Classifier\"),\n",
    "    (KNeighborsClassifier(), \"kNN\"),\n",
    "    (XGBClassifier(), \"XGBoost\"),\n",
    "    (LinearSVC(class_weight=\"balanced\"), \"Linear SVC\"),\n",
    "    (SGDClassifier(class_weight=\"balanced\"), \"SGD Classifier\"),\n",
    "    (NearestCentroid(), \"NearestCentroid\"),\n",
    "):\n",
    "    print(\"=\" * 80)\n",
    "    print(name)\n",
    "    results_embed.append(benchmark(clf, name))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

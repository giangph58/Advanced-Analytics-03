{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am using the following SPARK_HOME: C:\\Users\\Seppe\\Desktop\\spark\\spark-3.5.5-bin-hadoop3\n",
      "Windows detected: set HADOOP_HOME to: C:\\Users\\Seppe\\Desktop\\spark\\winutils\n",
      "  Also added Hadoop bin directory to PATH: C:\\Users\\Seppe\\Desktop\\spark\\winutils\\bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "spark_home = os.path.abspath(os.getcwd() + \"/../spark-3.5.5-bin-hadoop3\")\n",
    "hadoop_home = os.path.abspath(os.getcwd() + \"/../winutils\")\n",
    "print(f\"I am using the following SPARK_HOME: {spark_home}\")\n",
    "if os.name == 'nt':\n",
    "    os.environ[\"HADOOP_HOME\"] = f\"{hadoop_home}\"\n",
    "    print(f\"Windows detected: set HADOOP_HOME to: {os.environ['HADOOP_HOME']}\")\n",
    "    hadoop_bin = os.path.join(hadoop_home, \"bin\")\n",
    "    os.environ[\"PATH\"] = f\"{hadoop_bin};{os.environ['PATH']}\"\n",
    "    print(f\"  Also added Hadoop bin directory to PATH: {hadoop_bin}\")\n",
    "\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "findspark.init(spark_home)\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seppe\\Desktop\\spark\\.pixi\\envs\\default\\Lib\\site-packages\\pyspark\\streaming\\context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2025-03-28 20:50:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2025-03-28 20:50:50\n",
      "-------------------------------------------\n",
      "{\"aid\": \"http://arxiv.org/abs/2503.19351v1\", \"title\": \"Multi-Object Sketch Animation by Scene Decomposition and Motion Planning\", \"summary\": \"Sketch animation, which brings static sketches to life by generating dynamic\\nvideo sequences, has found widespread applications in GIF design, cartoon\\nproduction, and daily entertainment. While current sketch animation methods\\nperform well in single-object sketch animation, they struggle in multi-object\\nscenarios. By analyzing their failures, we summarize two challenges of\\ntransitioning from single-object to multi-object sketch animation: object-aware\\nmotion modeling and complex motion optimization. For multi-object sketch\\nanimation, we propose MoSketch based on iterative optimization through Score\\nDistillation Sampling (SDS), without any other data for training. We propose\\nfour modules: LLM-based scene decomposition, LLM-based motion planning, motion\\nrefinement network and compositional SDS, to tackle the two challenges in a\\ndivide-and-conquer strategy. Extensive qualitative and quantitative experiments\\ndemonstrate the superiority of our method over existing sketch animation\\napproaches. MoSketch takes a pioneering step towards multi-object sketch\\nanimation, opening new avenues for future research and applications. The code\\nwill be released.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-25T05:00:11Z\"}\n",
      "{\"aid\": \"http://arxiv.org/abs/2503.19352v1\", \"title\": \"Evolution of clustering in cosmological models with time-varying dark\\n  energy\", \"summary\": \"Observations favor cosmological models with a time-varying dark energy\\ncomponent. But how does dynamical dark energy (DDE) influence the growth of\\nstructure in an expanding Universe? We investigate this question using\\nhigh-resolution $N$-body simulations based on a DDE cosmology constrained by\\nfirst-year DESI data (DESIY1$+$DDE), characterized by a 4% lower Hubble\\nconstant ($H_0$) and 10% higher matter density ($\\\\Omega_0$) than the\\nPlanck-2018 $\\\\Lambda$CDM model. We examine the impact on the matter power\\nspectrum, halo abundances, clustering, and Baryonic Acoustic Oscillations\\n(BAO). We find that DESIY1$+$DDE exhibits a 10% excess in power at small scales\\nand a 15% suppression at large scales, driven primarily by its higher\\n$\\\\Omega_0$. This trend is reflected in the halo mass function: DESIY1$+$DDE\\npredicts up to 70% more massive halos at $z = 2$ and a 40% excess at $z = 0.3$.\\nClustering analysis reveals a 3.71% shift of the BAO peak towards smaller\\nscales in DESIY1$+$DDE, consistent with its reduced sound horizon compared to\\nPlanck18 Measurements of the BAO dilation parameter $\\\\alpha$, using halo\\nsamples with DESI-like tracer number densities across $0 < z < 1.5$, agree with\\nthe expected DESIY1$+$DDE-to-Planck18 sound horizon ratio. After accounting for\\ncosmology-dependent distances, the simulation-based observational dilation\\nparameter closely matches DESI Y1 data. We find that the impact of DDE is\\nseverely limited by current observational constraints, which strongly favor\\ncosmological models -- whether including DDE or not -- with a tightly\\nconstrained parameter $\\\\Omega_0h^2\\\\approx 0.143$, within 1-2% uncertainty.\\nIndeed, our results demonstrate that variations in cosmological parameters,\\nparticularly $\\\\Omega_0$, have a greater influence on structure formation than\\nthe DDE component alone.\", \"main_category\": \"astro-ph.CO\", \"categories\": \"astro-ph.CO\", \"published\": \"2025-03-25T05:02:44Z\"}\n",
      "{\"aid\": \"http://arxiv.org/abs/2503.19353v1\", \"title\": \"QUAD: Quantization and Parameter-Efficient Tuning of LLM with Activation\\n  Decomposition\", \"summary\": \"Large Language Models (LLMs) excel in diverse applications but suffer\\ninefficiency due to massive scale. While quantization reduces computational\\ncosts, existing methods degrade accuracy in medium-sized LLMs (e.g.,\\nLlama-3-8B) due to activation outliers. To address this, we propose QUAD\\n(Quantization with Activation Decomposition), a framework leveraging Singular\\nValue Decomposition (SVD) to suppress activation outliers for effective 4-bit\\nquantization. QUAD estimates activation singular vectors offline using\\ncalibration data to construct an orthogonal transformation matrix P, shifting\\noutliers to additional dimensions in full precision while quantizing rest\\ncomponents to 4-bit. Additionally, QUAD enables parameter-efficient fine-tuning\\nvia adaptable full-precision outlier weights, narrowing the accuracy gap\\nbetween quantized and full-precision models. Experiments demonstrate that QUAD\\nachieves 94% ~ 96% accuracy under W4A4 quantization and 98% accuracy with\\nW4A4/A8 and parameter-efficient fine-tuning for Llama-3 and Qwen-2.5 models.\\nOur code is available at \\\\href{https://github.com/hyx1999/Quad}{repository}.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL,I.2.7\", \"published\": \"2025-03-25T05:03:56Z\"}\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2025-03-28 20:51:00\n",
      "-------------------------------------------\n",
      "{\"aid\": \"http://arxiv.org/abs/2503.19354v1\", \"title\": \"Data-driven Mesoscale Weather Forecasting Combining Swin-Unet and\\n  Diffusion Models\", \"summary\": \"Data-driven weather prediction models exhibit promising performance and\\nadvance continuously. In particular, diffusion models represent fine-scale\\ndetails without spatial smoothing, which is crucial for mesoscale predictions,\\nsuch as heavy rainfall forecasting. However, the applications of diffusion\\nmodels to mesoscale prediction remain limited. To address this gap, this study\\nproposes an architecture that combines a diffusion model with Swin-Unet as a\\ndeterministic model, achieving mesoscale predictions while maintaining\\nflexibility. The proposed architecture trains the two models independently,\\nallowing the diffusion model to remain unchanged when the deterministic model\\nis updated. Comparisons using the Fractions Skill Score and power spectral\\nanalysis demonstrate that incorporating the diffusion model leads to improved\\naccuracy compared to predictions without it. These findings underscore the\\npotential of the proposed architecture to enhance mesoscale predictions,\\nparticularly for strong rainfall events, while maintaining flexibility.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,physics.ao-ph\", \"published\": \"2025-03-25T05:07:31Z\"}\n",
      "{\"aid\": \"http://arxiv.org/abs/2503.19355v1\", \"title\": \"ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in\\n  Vision-Language Models\", \"summary\": \"Spatio-temporal reasoning is essential in understanding real-world\\nenvironments in various fields, eg, autonomous driving and sports analytics.\\nRecent advances have improved the spatial reasoning ability of Vision-Language\\nModels (VLMs) by introducing large-scale data, but these models still struggle\\nto analyze kinematic elements like traveled distance and speed of moving\\nobjects. To bridge this gap, we construct a spatio-temporal reasoning dataset\\nand benchmark involving kinematic instruction tuning, referred to as STKit and\\nSTKit-Bench. They consist of real-world videos with 3D annotations, detailing\\nobject motion dynamics: traveled distance, speed, movement direction,\\ninter-object distance comparisons, and relative movement direction. To further\\nscale such data construction to videos without 3D labels, we propose an\\nautomatic pipeline to generate pseudo-labels using 4D reconstruction in\\nreal-world scale. With our kinematic instruction tuning data for\\nspatio-temporal reasoning, we present ST-VLM, a VLM enhanced for\\nspatio-temporal reasoning, which exhibits outstanding performance on\\nSTKit-Bench. Furthermore, we show that ST-VLM generalizes robustly across\\ndiverse domains and tasks, outperforming baselines on other spatio-temporal\\nbenchmarks (eg, ActivityNet, TVQA+). Finally, by integrating learned\\nspatio-temporal reasoning with existing abilities, ST-VLM enables complex\\nmulti-step reasoning. Project page: https://ikodoh.github.io/ST-VLM.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-25T05:08:06Z\"}\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2025-03-28 20:51:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2025-03-28 20:51:20\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n"
     ]
    }
   ],
   "source": [
    "# Wait a bit before running this cell until you see output appear in the previous cell\n",
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "I am using the following SPARK_HOME: C:\\Users\\Seppe\\Desktop\\spark\\spark-3.5.5-bin-hadoop3\n",
                  "Windows detected: set HADOOP_HOME to: C:\\Users\\Seppe\\Desktop\\spark\\winutils\n"
               ]
            }
         ],
         "source": [
            "import os\n",
            "spark_home = os.path.abspath(os.getcwd() + \"/../spark-3.5.5-bin-hadoop3\")\n",
            "hadoop_home = os.path.abspath(os.getcwd() + \"/../winutils\")\n",
            "print(f\"I am using the following SPARK_HOME: {spark_home}\")\n",
            "if os.name == 'nt':\n",
            "    os.environ[\"HADOOP_HOME\"] = f\"{hadoop_home}\"\n",
            "    print(f\"Windows detected: set HADOOP_HOME to: {os.environ['HADOOP_HOME']}\")\n",
            "    hadoop_bin = os.path.join(hadoop_home, \"bin\")\n",
            "    os.environ[\"PATH\"] = f\"{hadoop_bin};{os.environ['PATH']}\"\n",
            "    print(f\"  Also added Hadoop bin directory to PATH: {hadoop_bin}\")\n",
            "\n",
            "import findspark\n",
            "import pyspark\n",
            "from pyspark.streaming import StreamingContext\n",
            "\n",
            "findspark.init(spark_home)\n",
            "sc = pyspark.SparkContext()\n",
            "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import threading\n",
            "\n",
            "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
            "        \n",
            "class StreamingThread(threading.Thread):\n",
            "    def __init__(self, ssc):\n",
            "        super().__init__()\n",
            "        self.ssc = ssc\n",
            "    def run(self):\n",
            "        self.ssc.start()\n",
            "        self.ssc.awaitTermination()\n",
            "    def stop(self):\n",
            "        print('----- Stopping... this may take a few seconds -----')\n",
            "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
            "        "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import random\n",
            "from pyspark.streaming import StreamingContext\n",
            "from pyspark.sql import Row\n",
            "from pyspark.sql.functions import udf, struct, array, col, lit\n",
            "from pyspark.sql.types import StringType\n",
            "\n",
            "import numpy as np\n",
            "import pickle\n",
            "import pandas as pd\n",
            "from pyspark.sql.functions import udf, col, concat_ws, lit\n",
            "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
            "from sentence_transformers import SentenceTransformer\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "globals()['models_loaded'] = False\n",
            "globals()['my_model'] = None\n",
            "\n",
            "# Toy predict function that returns a random probability. Normally you'd use your loaded globals()['my_model'] here\n",
            "def predict(df):\n",
            "    return random.random()\n",
            "\n",
            "predict_udf = udf(predict, StringType())\n",
            "\n",
            "def process(time, rdd):\n",
            "    if rdd.isEmpty():\n",
            "        return\n",
            "    \n",
            "    print(\"========= %s =========\" % str(time))\n",
            "    \n",
            "    # Convert to data frame\n",
            "    df = spark.read.json(rdd)\n",
            "    df.show()\n",
            "    \n",
            "    # Utilize our predict function\n",
            "    df_withpreds = df.withColumn(\"pred\", predict_udf(\n",
            "        struct([df[x] for x in df.columns])\n",
            "    ))\n",
            "    df_withpreds.show()\n",
            "    \n",
            "    # Normally, you wouldn't use a UDF (User Defined Function) Python function to predict as we did here (you can)\n",
            "    # but an MLlib model you've built and saved with Spark\n",
            "    # In this case, you need to prevent loading your model in every call to \"process\" as follows:\n",
            "    \n",
            "    # Load in the model if not yet loaded:\n",
            "    if not globals()['models_loaded']:\n",
            "        # load in your models here\n",
            "        globals()['my_model'] = '***' # Replace '***' with e.g.:    [...].load('my_logistic_regression')\n",
            "        globals()['models_loaded'] = True\n",
            "        \n",
            "    # And then predict using the loaded model (uncomment below):\n",
            "    \n",
            "    # df_result = globals()['my_model'].transform(df)\n",
            "    # df_result.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "C:\\Users\\Seppe\\Desktop\\spark\\.pixi\\envs\\default\\Lib\\site-packages\\pyspark\\streaming\\context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
                  "  warnings.warn(\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "========= 2025-03-28 20:23:10 =========\n",
                  "+--------------------+----------+-------------+--------------------+--------------------+--------------------+\n",
                  "|                 aid|categories|main_category|           published|             summary|               title|\n",
                  "+--------------------+----------+-------------+--------------------+--------------------+--------------------+\n",
                  "|http://arxiv.org/...|     cs.CV|        cs.CV|2025-03-25T05:00:11Z|Sketch animation,...|Multi-Object Sket...|\n",
                  "+--------------------+----------+-------------+--------------------+--------------------+--------------------+\n",
                  "\n",
                  "+--------------------+----------+-------------+--------------------+--------------------+--------------------+------------------+\n",
                  "|                 aid|categories|main_category|           published|             summary|               title|              pred|\n",
                  "+--------------------+----------+-------------+--------------------+--------------------+--------------------+------------------+\n",
                  "|http://arxiv.org/...|     cs.CV|        cs.CV|2025-03-25T05:00:11Z|Sketch animation,...|Multi-Object Sket...|0.9724068042789026|\n",
                  "+--------------------+----------+-------------+--------------------+--------------------+--------------------+------------------+\n",
                  "\n",
                  "========= 2025-03-28 20:23:20 =========\n",
                  "+--------------------+-------------------+-------------+--------------------+--------------------+--------------------+\n",
                  "|                 aid|         categories|main_category|           published|             summary|               title|\n",
                  "+--------------------+-------------------+-------------+--------------------+--------------------+--------------------+\n",
                  "|http://arxiv.org/...|        astro-ph.CO|  astro-ph.CO|2025-03-25T05:02:44Z|Observations favo...|Evolution of clus...|\n",
                  "|http://arxiv.org/...|  cs.LG,cs.CL,I.2.7|        cs.LG|2025-03-25T05:03:56Z|Large Language Mo...|QUAD: Quantizatio...|\n",
                  "|http://arxiv.org/...|cs.LG,physics.ao-ph|        cs.LG|2025-03-25T05:07:31Z|Data-driven weath...|Data-driven Mesos...|\n",
                  "|http://arxiv.org/...|              cs.CV|        cs.CV|2025-03-25T05:08:06Z|Spatio-temporal r...|ST-VLM: Kinematic...|\n",
                  "+--------------------+-------------------+-------------+--------------------+--------------------+--------------------+\n",
                  "\n",
                  "+--------------------+-------------------+-------------+--------------------+--------------------+--------------------+-------------------+\n",
                  "|                 aid|         categories|main_category|           published|             summary|               title|               pred|\n",
                  "+--------------------+-------------------+-------------+--------------------+--------------------+--------------------+-------------------+\n",
                  "|http://arxiv.org/...|        astro-ph.CO|  astro-ph.CO|2025-03-25T05:02:44Z|Observations favo...|Evolution of clus...|0.11651166294853799|\n",
                  "|http://arxiv.org/...|  cs.LG,cs.CL,I.2.7|        cs.LG|2025-03-25T05:03:56Z|Large Language Mo...|QUAD: Quantizatio...| 0.9067992619949217|\n",
                  "|http://arxiv.org/...|cs.LG,physics.ao-ph|        cs.LG|2025-03-25T05:07:31Z|Data-driven weath...|Data-driven Mesos...| 0.9936869132334399|\n",
                  "|http://arxiv.org/...|              cs.CV|        cs.CV|2025-03-25T05:08:06Z|Spatio-temporal r...|ST-VLM: Kinematic...| 0.6828431104810759|\n",
                  "+--------------------+-------------------+-------------+--------------------+--------------------+--------------------+-------------------+\n",
                  "\n"
               ]
            }
         ],
         "source": [
            "ssc = StreamingContext(sc, 10)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
            "lines.foreachRDD(process)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "ssc_t = StreamingThread(ssc)\n",
            "ssc_t.start()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "----- Stopping... this may take a few seconds -----\n"
               ]
            }
         ],
         "source": [
            "ssc_t.stop()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.12"
      },
      "toc": {
         "base_numbering": 1,
         "nav_menu": {},
         "number_sections": true,
         "sideBar": true,
         "skip_h1_title": false,
         "title_cell": "Table of Contents",
         "title_sidebar": "Contents",
         "toc_cell": false,
         "toc_position": {},
         "toc_section_display": true,
         "toc_window_display": false
      }
   },
   "nbformat": 4,
   "nbformat_minor": 4
}
{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "spark_home = os.path.abspath(os.getcwd() + \"/spark/spark-3.5.5-bin-hadoop3\")\n",
            "hadoop_home = os.path.abspath(os.getcwd() + \"/spark/winutils\")\n",
            "print(f\"I am using the following SPARK_HOME: {spark_home}\")\n",
            "if os.name == 'nt':\n",
            "    os.environ[\"HADOOP_HOME\"] = f\"{hadoop_home}\"\n",
            "    print(f\"Windows detected: set HADOOP_HOME to: {os.environ['HADOOP_HOME']}\")\n",
            "    hadoop_bin = os.path.join(hadoop_home, \"bin\")\n",
            "    os.environ[\"PATH\"] = f\"{hadoop_bin};{os.environ['PATH']}\"\n",
            "    print(f\"  Also added Hadoop bin directory to PATH: {hadoop_bin}\")\n",
            "\n",
            "import findspark\n",
            "import pyspark\n",
            "from pyspark.streaming import StreamingContext\n",
            "\n",
            "findspark.init(spark_home)\n",
            "sc = pyspark.SparkContext()\n",
            "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import threading\n",
            "\n",
            "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
            "        \n",
            "class StreamingThread(threading.Thread):\n",
            "    def __init__(self, ssc):\n",
            "        super().__init__()\n",
            "        self.ssc = ssc\n",
            "    def run(self):\n",
            "        self.ssc.start()\n",
            "        self.ssc.awaitTermination()\n",
            "    def stop(self):\n",
            "        print('----- Stopping... this may take a few seconds -----')\n",
            "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
            "        "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import random\n",
            "from pyspark.streaming import StreamingContext\n",
            "from pyspark.sql import Row\n",
            "from pyspark.sql.functions import udf, struct, array, col, lit\n",
            "from pyspark.sql.types import StringType\n",
            "\n",
            "import numpy as np\n",
            "import pickle\n",
            "import pandas as pd\n",
            "from pyspark.sql.functions import udf, col, concat_ws, lit\n",
            "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
            "from sentence_transformers import SentenceTransformer\n",
            "from transformers import pipeline\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Run this separately to convert your model to ONNX\n",
            "from transformers import pipeline\n",
            "import torch\n",
            "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
            "from transformers import AutoTokenizer\n",
            "\n",
            "def convert_to_onnx():\n",
            "    model_name = \"gpham/scibert-finetuned-arxiv-42\"\n",
            "    \n",
            "    # Load the original model\n",
            "    model = ORTModelForSequenceClassification.from_pretrained(\n",
            "        model_name, \n",
            "        from_transformers=True,\n",
            "        export=True\n",
            "    )\n",
            "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
            "    \n",
            "    # Save ONNX model locally\n",
            "    model.save_pretrained(\"./models/scibert-finetuned-arxiv-42-onnx\")\n",
            "    tokenizer.save_pretrained(\"./models/scibert-finetuned-arxiv-42-onnx\")\n",
            "    \n",
            "    print(\"Model converted to ONNX format!\")\n",
            "\n",
            "# Run this once to create the ONNX model\n",
            "convert_to_onnx()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# def load_model():\n",
            "#     \"\"\"Load and configure the model with proper labels\"\"\"\n",
            "#     print(\"Loading model...\")\n",
            "#     model_name = \"gpham/scibert-finetuned-arxiv-42\"\n",
            "\n",
            "#     # Create pipeline\n",
            "#     classifier = pipeline(\n",
            "#         \"text-classification\", \n",
            "#         model=model_name,\n",
            "#         max_length=256)\n",
            "#     print(\"Model loaded successfully!\")\n",
            "#     return classifier\n",
            "\n",
            "\n",
            "# def load_and_broadcast_model():\n",
            "#     \"\"\"Load model on driver and broadcast to executors\"\"\"\n",
            "#     print(\"Loading model on driver...\")\n",
            "#     model_name = \"gpham/scibert-finetuned-arxiv-42\"\n",
            "#     classifier = pipeline(\"text-classification\", model=model_name, max_length=256)\n",
            "    \n",
            "#     # Broadcast the model to all executors\n",
            "#     broadcast_model = sc.broadcast(classifier)\n",
            "#     return broadcast_model\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Toy predict function that returns a random probability. Normally you'd use your loaded globals()['my_model'] here\n",
            "\n",
            "# import random\n",
            "\n",
            "# globals()['models_loaded'] = False\n",
            "# globals()['my_model'] = None\n",
            "\n",
            "# broadcast_model = load_and_broadcast_model()\n",
            "\n",
            "# def predict(row):\n",
            "#     \"\"\"Predict function using broadcast model\"\"\"\n",
            "#     try:\n",
            "#         # Access the broadcast model\n",
            "#         model = broadcast_model.value\n",
            "#         text = row.title + \"\\n\" + row.summary\n",
            "#         result = model(text)\n",
            "        \n",
            "#         if result and len(result) > 0:\n",
            "#             return result[0][\"label\"]\n",
            "#         else:\n",
            "#             return \"prediction_failed\"\n",
            "#     except Exception as e:\n",
            "#         print(f\"Error in prediction: {e}\")\n",
            "#         return \"prediction_failed\"\n",
            "\n",
            "def predict(row):\n",
            "    \"\"\"Predict function with per-executor lazy loading\"\"\"\n",
            "    \n",
            "    # Check if model exists in current executor's global scope\n",
            "    if 'executor_model' not in globals():\n",
            "        try:\n",
            "            print(\"Loading model on executor...\")\n",
            "            model_name = \"gpham/scibert-finetuned-arxiv-42\"\n",
            "            globals()['executor_model'] = pipeline(\n",
            "                \"text-classification\", \n",
            "                model=model_name,\n",
            "                device=-1,  # Force CPU\n",
            "                max_length=256,\n",
            "                truncation=True\n",
            "            )\n",
            "            print(\"Model loaded successfully on executor!\")\n",
            "        except Exception as e:\n",
            "            print(f\"Failed to load model on executor: {e}\")\n",
            "            globals()['executor_model'] = None\n",
            "    \n",
            "    try:\n",
            "        text = row.title + \"\\n\" + row.summary\n",
            "        \n",
            "        # Fallback to mock if model loading failed\n",
            "        if globals()['executor_model'] is None:\n",
            "            print(\"Using fallback mock prediction\")\n",
            "            categories = [\"cs.AI\", \"cs.LG\", \"cs.CV\", \"cs.CL\", \"cs.IT\"]\n",
            "            # Simple heuristic fallback\n",
            "            if \"neural\" in text.lower() or \"deep\" in text.lower():\n",
            "                return \"cs.LG\"\n",
            "            elif \"vision\" in text.lower() or \"image\" in text.lower():\n",
            "                return \"cs.CV\"\n",
            "            elif \"language\" in text.lower() or \"text\" in text.lower():\n",
            "                return \"cs.CL\"\n",
            "            elif \"algorithm\" in text.lower():\n",
            "                return \"cs.AI\"\n",
            "            else:\n",
            "                return random.choice(categories)\n",
            "        \n",
            "        # Use the real model\n",
            "        result = globals()['executor_model'](text)\n",
            "        \n",
            "        if result and len(result) > 0:\n",
            "            return result[0][\"label\"]\n",
            "        else:\n",
            "            return \"prediction_failed\"\n",
            "            \n",
            "    except Exception as e:\n",
            "        print(f\"Error in prediction: {e}\")\n",
            "        return \"prediction_failed\"\n",
            "\n",
            "# def predict(row):\n",
            "#     \"\"\"Simple predict function that takes combined text and returns category\"\"\"\n",
            "    \n",
            "#     # Load model once per executor (lazy loading)\n",
            "#     if not globals().get('models_loaded', False):\n",
            "#         globals()['my_model'] = load_model()\n",
            "#         globals()['models_loaded'] = True\n",
            "\n",
            "#     text = row.title + \"\\n\" + row.summary\n",
            "#     result = globals()['my_model'](text)\n",
            "        \n",
            "#     if result and len(result) > 0:\n",
            "#         return result[0][\"label\"]\n",
            "#     else:\n",
            "#         return \"prediction_failed\"\n",
            "    \n",
            "# def predict(row):\n",
            "#     \"\"\"Simple predict function that takes combined text and returns category\"\"\"\n",
            "    \n",
            "#     try:\n",
            "#         # Simple mock prediction instead of loading heavy ML models\n",
            "#         # This avoids serialization and memory issues in Spark\n",
            "#         text = row.title + \"\\n\" + row.summary\n",
            "        \n",
            "#         # Mock categories based on simple text analysis\n",
            "#         categories = [\"cs.AI\", \"cs.LG\", \"cs.CV\", \"cs.CL\", \"cs.IT\"]\n",
            "        \n",
            "#         # Simple heuristic based on text length and content\n",
            "#         if \"neural\" in text.lower() or \"deep\" in text.lower():\n",
            "#             return \"cs.LG\"\n",
            "#         elif \"vision\" in text.lower() or \"image\" in text.lower():\n",
            "#             return \"cs.CV\"\n",
            "#         elif \"language\" in text.lower() or \"text\" in text.lower():\n",
            "#             return \"cs.CL\"\n",
            "#         elif \"algorithm\" in text.lower():\n",
            "#             return \"cs.AI\"\n",
            "#         else:\n",
            "#             # Random selection for other cases\n",
            "#             return random.choice(categories)\n",
            "            \n",
            "#     except Exception as e:\n",
            "#         print(f\"Error in prediction: {e}\")\n",
            "#         return \"prediction_failed\"\n",
            "\n",
            "predict_udf = udf(predict, StringType())\n",
            "\n",
            "def process(time, rdd):\n",
            "    if rdd.isEmpty():\n",
            "        return\n",
            "    \n",
            "    print(\"========= %s =========\" % str(time))\n",
            "    \n",
            "    # Convert to data frame\n",
            "    df = spark.read.json(rdd)\n",
            "        \n",
            "    # Utilize our predict function\n",
            "    df_withpreds = df.withColumn(\"pred\", predict_udf(\n",
            "        struct(col(\"title\"), col(\"summary\"))\n",
            "    ))\n",
            "    df_withpreds.show()   \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pyspark.sql.types import StructType, StructField\n",
            "from pyspark.sql import Row\n",
            "\n",
            "def predict_partition(iterator):\n",
            "    \"\"\"Process entire partition with one model load\"\"\"\n",
            "    \n",
            "    # Convert iterator to list to process all rows\n",
            "    rows = list(iterator)\n",
            "    if not rows:\n",
            "        return []\n",
            "    \n",
            "    # Try to load model once per partition\n",
            "    model = None\n",
            "    try:\n",
            "        print(f\"Loading model for partition with {len(rows)} rows...\")\n",
            "        model_name = \"gpham/scibert-finetuned-arxiv-42\"\n",
            "        model = pipeline(\n",
            "            \"text-classification\", \n",
            "            model=model_name,\n",
            "            device=-1,  # Force CPU\n",
            "            max_length=256,\n",
            "            truncation=True\n",
            "        )\n",
            "        print(\"Model loaded successfully for partition!\")\n",
            "    except Exception as e:\n",
            "        print(f\"Failed to load model for partition: {e}\")\n",
            "        model = None\n",
            "    \n",
            "    # Process all rows in the partition\n",
            "    results = []\n",
            "    categories = [\"cs.AI\", \"cs.LG\", \"cs.CV\", \"cs.CL\", \"cs.IT\"]\n",
            "    \n",
            "    for row in rows:\n",
            "        try:\n",
            "            text = row.title + \"\\n\" + row.summary\n",
            "            \n",
            "            if model is not None:\n",
            "                # Use real model\n",
            "                result = model(text)\n",
            "                pred = result[0][\"label\"] if result and len(result) > 0 else \"prediction_failed\"\n",
            "            else:\n",
            "                # Fallback to mock prediction\n",
            "                if \"neural\" in text.lower() or \"deep\" in text.lower():\n",
            "                    pred = \"cs.LG\"\n",
            "                elif \"vision\" in text.lower() or \"image\" in text.lower():\n",
            "                    pred = \"cs.CV\"\n",
            "                elif \"language\" in text.lower() or \"text\" in text.lower():\n",
            "                    pred = \"cs.CL\"\n",
            "                elif \"algorithm\" in text.lower():\n",
            "                    pred = \"cs.AI\"\n",
            "                else:\n",
            "                    pred = random.choice(categories)\n",
            "            \n",
            "            # Return original row data plus prediction\n",
            "            results.append(Row(title=row.title, summary=row.summary, pred=pred))\n",
            "            \n",
            "        except Exception as e:\n",
            "            print(f\"Error processing row: {e}\")\n",
            "            results.append(Row(title=row.title, summary=row.summary, pred=\"prediction_failed\"))\n",
            "    \n",
            "    return results\n",
            "\n",
            "def process(time, rdd):\n",
            "    if rdd.isEmpty():\n",
            "        return\n",
            "    \n",
            "    print(\"========= %s =========\" % str(time))\n",
            "    \n",
            "    try:\n",
            "        # Convert to DataFrame first\n",
            "        df = spark.read.json(rdd)\n",
            "        print(f\"Processing {df.count()} rows\")\n",
            "        \n",
            "        # Convert to RDD and process by partitions\n",
            "        results_rdd = df.rdd.mapPartitions(predict_partition)\n",
            "        \n",
            "        # Convert back to DataFrame\n",
            "        schema = StructType([\n",
            "            StructField(\"title\", StringType(), True),\n",
            "            StructField(\"summary\", StringType(), True),\n",
            "            StructField(\"pred\", StringType(), True)\n",
            "        ])\n",
            "        \n",
            "        result_df = spark.createDataFrame(results_rdd, schema)\n",
            "        result_df.show(truncate=False)\n",
            "        \n",
            "    except Exception as e:\n",
            "        print(f\"Error in process function: {e}\")\n",
            "        import traceback\n",
            "        traceback.print_exc()\n",
            "        "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
            "from transformers import AutoTokenizer, pipeline\n",
            "from pyspark.sql.types import StructType, StructField\n",
            "from pyspark.sql import Row\n",
            "\n",
            "def predict_partition_onnx(iterator):\n",
            "    \"\"\"Process entire partition with ONNX model\"\"\"\n",
            "    \n",
            "    # Convert iterator to list to process all rows\n",
            "    rows = list(iterator)\n",
            "    if not rows:\n",
            "        return []\n",
            "    \n",
            "    # Try to load ONNX model once per partition\n",
            "    model = None\n",
            "    try:\n",
            "        print(f\"Loading ONNX model for partition with {len(rows)} rows...\")\n",
            "        \n",
            "        # Load ONNX model (much faster than original)\n",
            "        onnx_model = ORTModelForSequenceClassification.from_pretrained(\"./models/scibert-finetuned-arxiv-42-onnx\")\n",
            "        tokenizer = AutoTokenizer.from_pretrained(\"./models/scibert-finetuned-arxiv-42-onnx\")\n",
            "        \n",
            "        # Create pipeline with ONNX model\n",
            "        model = pipeline(\n",
            "            \"text-classification\",\n",
            "            model=onnx_model,\n",
            "            tokenizer=tokenizer,\n",
            "            max_length=256,\n",
            "            device=-1  # CPU only\n",
            "        )\n",
            "        print(\"ONNX model loaded successfully for partition!\")\n",
            "        \n",
            "    except Exception as e:\n",
            "        print(f\"Failed to load ONNX model for partition: {e}\")\n",
            "        model = None\n",
            "    \n",
            "    # Process all rows in the partition\n",
            "    results = []\n",
            "    categories = [\"cs.AI\", \"cs.LG\", \"cs.CV\", \"cs.CL\", \"cs.IT\"]\n",
            "    \n",
            "    for row in rows:\n",
            "        try:\n",
            "            text = row.title + \"\\n\" + row.summary\n",
            "            \n",
            "            if model is not None:\n",
            "                # Use ONNX model\n",
            "                result = model(text)\n",
            "                pred = result[0][\"label\"] if result and len(result) > 0 else \"prediction_failed\"\n",
            "            else:\n",
            "                # Fallback to mock prediction\n",
            "                if \"neural\" in text.lower() or \"deep\" in text.lower():\n",
            "                    pred = \"cs.LG\"\n",
            "                elif \"vision\" in text.lower() or \"image\" in text.lower():\n",
            "                    pred = \"cs.CV\"\n",
            "                elif \"language\" in text.lower() or \"text\" in text.lower():\n",
            "                    pred = \"cs.CL\"\n",
            "                elif \"algorithm\" in text.lower():\n",
            "                    pred = \"cs.AI\"\n",
            "                else:\n",
            "                    pred = random.choice(categories)\n",
            "            \n",
            "            results.append(Row(title=row.title, summary=row.summary, pred=pred))\n",
            "            \n",
            "        except Exception as e:\n",
            "            print(f\"Error processing row: {e}\")\n",
            "            results.append(Row(title=row.title, summary=row.summary, pred=\"prediction_failed\"))\n",
            "    \n",
            "    return results\n",
            "\n",
            "def process_onnx(time, rdd):\n",
            "    if rdd.isEmpty():\n",
            "        return\n",
            "    \n",
            "    print(\"========= %s =========\" % str(time))\n",
            "    \n",
            "    try:\n",
            "        # Convert to DataFrame first\n",
            "        df = spark.read.json(rdd)\n",
            "        print(f\"Processing {df.count()} rows with ONNX\")\n",
            "        \n",
            "        # Convert to RDD and process by partitions\n",
            "        results_rdd = df.rdd.mapPartitions(predict_partition_onnx)\n",
            "        \n",
            "        # Convert back to DataFrame\n",
            "        schema = StructType([\n",
            "            StructField(\"title\", StringType(), True),\n",
            "            StructField(\"summary\", StringType(), True),\n",
            "            StructField(\"pred\", StringType(), True)\n",
            "        ])\n",
            "        \n",
            "        result_df = spark.createDataFrame(results_rdd, schema)\n",
            "        result_df.show(truncate=False)\n",
            "        \n",
            "    except Exception as e:\n",
            "        print(f\"Error in process function: {e}\")\n",
            "        import traceback\n",
            "        traceback.print_exc()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
            "from transformers import AutoTokenizer, pipeline\n",
            "from pyspark.sql.types import StructType, StructField, StringType\n",
            "from pyspark.sql import Row\n",
            "from pyspark.sql.functions import udf, struct, col\n",
            "\n",
            "def load_and_broadcast_onnx_model():\n",
            "    \"\"\"Load ONNX model on driver and broadcast to executors\"\"\"\n",
            "    try:\n",
            "        print(\"Loading ONNX model on driver...\")\n",
            "        \n",
            "        # Load ONNX model (much faster and smaller)\n",
            "        onnx_model = ORTModelForSequenceClassification.from_pretrained(\"./models/scibert-finetuned-arxiv-42-onnx\")\n",
            "        tokenizer = AutoTokenizer.from_pretrained(\"./models/scibert-finetuned-arxiv-42-onnx\")\n",
            "        \n",
            "        # Create pipeline with ONNX model\n",
            "        classifier = pipeline(\n",
            "            \"text-classification\",\n",
            "            model=onnx_model,\n",
            "            tokenizer=tokenizer,\n",
            "            max_length=256,\n",
            "            device=-1  # CPU only\n",
            "        )\n",
            "        \n",
            "        print(\"ONNX model loaded successfully on driver!\")\n",
            "        \n",
            "        # Broadcast the model to all executors\n",
            "        broadcast_model = sc.broadcast(classifier)\n",
            "        return broadcast_model\n",
            "        \n",
            "    except Exception as e:\n",
            "        print(f\"Failed to load ONNX model on driver: {e}\")\n",
            "        return None\n",
            "\n",
            "# Load and broadcast the ONNX model once\n",
            "broadcast_onnx_model = load_and_broadcast_onnx_model()\n",
            "\n",
            "def predict_with_broadcast_onnx(row):\n",
            "    \"\"\"Predict function using broadcast ONNX model\"\"\"\n",
            "    try:\n",
            "        # Access the broadcast model\n",
            "        if broadcast_onnx_model is not None and broadcast_onnx_model.value is not None:\n",
            "            model = broadcast_onnx_model.value\n",
            "            text = row.title + \"\\n\" + row.summary\n",
            "            result = model(text)\n",
            "            \n",
            "            if result and len(result) > 0:\n",
            "                return result[0][\"label\"]\n",
            "            else:\n",
            "                return \"prediction_failed\"\n",
            "        else:\n",
            "            # Fallback to mock prediction if model not available\n",
            "            text = row.title + \"\\n\" + row.summary\n",
            "            categories = [\"cs.AI\", \"cs.LG\", \"cs.CV\", \"cs.CL\", \"cs.IT\"]\n",
            "            \n",
            "            if \"neural\" in text.lower() or \"deep\" in text.lower():\n",
            "                return \"cs.LG\"\n",
            "            elif \"vision\" in text.lower() or \"image\" in text.lower():\n",
            "                return \"cs.CV\"\n",
            "            elif \"language\" in text.lower() or \"text\" in text.lower():\n",
            "                return \"cs.CL\"\n",
            "            elif \"algorithm\" in text.lower():\n",
            "                return \"cs.AI\"\n",
            "            else:\n",
            "                return random.choice(categories)\n",
            "                \n",
            "    except Exception as e:\n",
            "        print(f\"Error in prediction: {e}\")\n",
            "        return \"prediction_failed\"\n",
            "\n",
            "# Create UDF with the broadcast ONNX model\n",
            "predict_onnx_udf = udf(predict_with_broadcast_onnx, StringType())\n",
            "\n",
            "def process_broadcast_onnx(time, rdd):\n",
            "    if rdd.isEmpty():\n",
            "        return\n",
            "    \n",
            "    print(\"========= %s =========\" % str(time))\n",
            "    \n",
            "    try:\n",
            "        # Convert to DataFrame\n",
            "        df = spark.read.json(rdd)\n",
            "        print(f\"Processing {df.count()} rows with broadcast ONNX\")\n",
            "        \n",
            "        # Apply predictions using UDF\n",
            "        df_withpreds = df.withColumn(\"pred\", predict_onnx_udf(\n",
            "            struct(col(\"title\"), col(\"summary\"))\n",
            "        ))\n",
            "        \n",
            "        df_withpreds.show(truncate=False)\n",
            "        \n",
            "    except Exception as e:\n",
            "        print(f\"Error in process function: {e}\")\n",
            "        import traceback\n",
            "        traceback.print_exc()\n",
            "        "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
            "from transformers import AutoTokenizer, pipeline\n",
            "from pyspark.sql.types import StructType, StructField, StringType\n",
            "from pyspark.sql import Row\n",
            "\n",
            "def predict_partition_onnx_optimized(iterator):\n",
            "    \"\"\"Optimized ONNX partition processing\"\"\"\n",
            "    \n",
            "    rows = list(iterator)\n",
            "    if not rows:\n",
            "        return []\n",
            "    \n",
            "    model = None\n",
            "    try:\n",
            "        print(f\"Loading ONNX model for partition with {len(rows)} rows...\")\n",
            "        \n",
            "        # Try to load ONNX model with error handling\n",
            "        import os\n",
            "        model_path = \"./models/scibert-finetuned-arxiv-42-onnx\"\n",
            "        \n",
            "        if os.path.exists(model_path):\n",
            "            onnx_model = ORTModelForSequenceClassification.from_pretrained(model_path)\n",
            "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
            "            \n",
            "            model = pipeline(\n",
            "                \"text-classification\",\n",
            "                model=onnx_model,\n",
            "                tokenizer=tokenizer,\n",
            "                max_length=256,\n",
            "                device=-1,\n",
            "                truncation=True\n",
            "            )\n",
            "            print(\"ONNX model loaded successfully for partition!\")\n",
            "        else:\n",
            "            print(f\"ONNX model path not found: {os.path.abspath(model_path)}\")\n",
            "            model = None\n",
            "            \n",
            "    except Exception as e:\n",
            "        print(f\"Failed to load ONNX model: {e}\")\n",
            "        # Try fallback to original model\n",
            "        try:\n",
            "            print(\"Trying original model as fallback...\")\n",
            "            model = pipeline(\n",
            "                \"text-classification\",\n",
            "                model=\"gpham/scibert-finetuned-arxiv-42\",\n",
            "                device=-1,\n",
            "                max_length=256,\n",
            "                truncation=True\n",
            "            )\n",
            "            print(\"Original model loaded as fallback!\")\n",
            "        except Exception as e2:\n",
            "            print(f\"Original model also failed: {e2}\")\n",
            "            model = None\n",
            "    \n",
            "    # Process rows\n",
            "    results = []\n",
            "    categories = [\"cs.AI\", \"cs.LG\", \"cs.CV\", \"cs.CL\", \"cs.IT\"]\n",
            "    \n",
            "    for row in rows:\n",
            "        try:\n",
            "            text = row.title + \"\\n\" + row.summary\n",
            "            \n",
            "            if model is not None:\n",
            "                result = model(text)\n",
            "                pred = result[0][\"label\"] if result and len(result) > 0 else \"prediction_failed\"\n",
            "            else:\n",
            "                # Enhanced fallback with better heuristics\n",
            "                text_lower = text.lower()\n",
            "                if any(word in text_lower for word in [\"neural\", \"deep\", \"learning\", \"network\"]):\n",
            "                    pred = \"cs.LG\"\n",
            "                elif any(word in text_lower for word in [\"vision\", \"image\", \"visual\", \"computer vision\"]):\n",
            "                    pred = \"cs.CV\"\n",
            "                elif any(word in text_lower for word in [\"language\", \"text\", \"nlp\", \"linguistic\"]):\n",
            "                    pred = \"cs.CL\"\n",
            "                elif any(word in text_lower for word in [\"algorithm\", \"optimization\", \"artificial intelligence\"]):\n",
            "                    pred = \"cs.AI\"\n",
            "                elif any(word in text_lower for word in [\"information\", \"theory\", \"data\"]):\n",
            "                    pred = \"cs.IT\"\n",
            "                else:\n",
            "                    pred = random.choice(categories)\n",
            "            \n",
            "            results.append(Row(title=row.title, summary=row.summary, pred=pred))\n",
            "            \n",
            "        except Exception as e:\n",
            "            print(f\"Error processing row: {e}\")\n",
            "            results.append(Row(title=row.title, summary=row.summary, pred=\"prediction_failed\"))\n",
            "    \n",
            "    return results\n",
            "\n",
            "def process_onnx_optimized(time, rdd):\n",
            "    if rdd.isEmpty():\n",
            "        return\n",
            "    \n",
            "    print(\"========= %s =========\" % str(time))\n",
            "    \n",
            "    try:\n",
            "        df = spark.read.json(rdd)\n",
            "        print(f\"Processing {df.count()} rows with optimized ONNX\")\n",
            "        \n",
            "        # Use optimized partition processing\n",
            "        results_rdd = df.rdd.mapPartitions(predict_partition_onnx_optimized)\n",
            "        \n",
            "        schema = StructType([\n",
            "            StructField(\"title\", StringType(), True),\n",
            "            StructField(\"summary\", StringType(), True),\n",
            "            StructField(\"pred\", StringType(), True)\n",
            "        ])\n",
            "        \n",
            "        result_df = spark.createDataFrame(results_rdd, schema)\n",
            "        result_df.show(truncate=False)\n",
            "        \n",
            "    except Exception as e:\n",
            "        print(f\"Error in process function: {e}\")\n",
            "        import traceback\n",
            "        traceback.print_exc()\n",
            "        "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import requests\n",
            "# import random\n",
            "from pyspark.sql.functions import udf, struct, col\n",
            "from pyspark.sql.types import StringType\n",
            "\n",
            "def predict_with_api(row):\n",
            "    try:\n",
            "        response = requests.post(\"http://localhost:8000/predict\", \n",
            "                               json={\"title\": str(row.title), \"summary\": str(row.summary)},\n",
            "                               timeout=5)\n",
            "        if response.status_code == 200:\n",
            "            return response.json()[\"prediction\"]\n",
            "    except:\n",
            "        pass\n",
            "    \n",
            "    # Fallback\n",
            "    text = str(row.title) + str(row.summary)\n",
            "    if \"neural\" in text.lower(): return \"cs.LG\"\n",
            "    elif \"vision\" in text.lower(): return \"cs.CV\"\n",
            "    elif \"language\" in text.lower(): return \"cs.CL\"\n",
            "    else: return \"cs.AI\"\n",
            "\n",
            "predict_api_udf = udf(predict_with_api, StringType())\n",
            "\n",
            "def process_api(time, rdd):\n",
            "    if rdd.isEmpty(): return\n",
            "    print(\"========= %s =========\" % str(time))\n",
            "    \n",
            "    df = spark.read.json(rdd)\n",
            "    df_withpreds = df.withColumn(\"pred\", predict_api_udf(struct(col(\"title\"), col(\"summary\"))))\n",
            "    df_withpreds.show()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "ssc = StreamingContext(sc, 10)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
            "lines.foreachRDD(process_api)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "ssc_t = StreamingThread(ssc)\n",
            "ssc_t.start()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "ssc_t.stop()\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.12"
      },
      "toc": {
         "base_numbering": 1,
         "nav_menu": {},
         "number_sections": true,
         "sideBar": true,
         "skip_h1_title": false,
         "title_cell": "Table of Contents",
         "title_sidebar": "Contents",
         "toc_cell": false,
         "toc_position": {},
         "toc_section_display": true,
         "toc_window_display": false
      }
   },
   "nbformat": 4,
   "nbformat_minor": 4
}

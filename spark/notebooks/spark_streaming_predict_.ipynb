{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "I am using the following SPARK_HOME: d:\\OneDrive - CGIAR\\Master\\Advanced Analytics\\assignments\\assignment-03\\spark\\spark-3.5.5-bin-hadoop3\n",
                  "Windows detected: set HADOOP_HOME to: d:\\OneDrive - CGIAR\\Master\\Advanced Analytics\\assignments\\assignment-03\\spark\\winutils\n",
                  "  Also added Hadoop bin directory to PATH: d:\\OneDrive - CGIAR\\Master\\Advanced Analytics\\assignments\\assignment-03\\spark\\winutils\\bin\n"
               ]
            }
         ],
         "source": [
            "import os\n",
            "spark_home = os.path.abspath(os.getcwd() + \"/spark/spark-3.5.5-bin-hadoop3\")\n",
            "hadoop_home = os.path.abspath(os.getcwd() + \"/spark/winutils\")\n",
            "print(f\"I am using the following SPARK_HOME: {spark_home}\")\n",
            "if os.name == 'nt':\n",
            "    os.environ[\"HADOOP_HOME\"] = f\"{hadoop_home}\"\n",
            "    print(f\"Windows detected: set HADOOP_HOME to: {os.environ['HADOOP_HOME']}\")\n",
            "    hadoop_bin = os.path.join(hadoop_home, \"bin\")\n",
            "    os.environ[\"PATH\"] = f\"{hadoop_bin};{os.environ['PATH']}\"\n",
            "    print(f\"  Also added Hadoop bin directory to PATH: {hadoop_bin}\")\n",
            "\n",
            "import findspark\n",
            "import pyspark\n",
            "from pyspark.streaming import StreamingContext\n",
            "\n",
            "findspark.init(spark_home)\n",
            "sc = pyspark.SparkContext()\n",
            "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "import threading\n",
            "\n",
            "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
            "        \n",
            "class StreamingThread(threading.Thread):\n",
            "    def __init__(self, ssc):\n",
            "        super().__init__()\n",
            "        self.ssc = ssc\n",
            "    def run(self):\n",
            "        self.ssc.start()\n",
            "        self.ssc.awaitTermination()\n",
            "    def stop(self):\n",
            "        print('----- Stopping... this may take a few seconds -----')\n",
            "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
            "        "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "import requests\n",
            "import time\n",
            "from collections import Counter\n",
            "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
            "from pyspark.sql.functions import udf, struct, col\n",
            "from pyspark.sql.types import StringType\n",
            "from src.utils import map_category\n",
            "\n",
            "# Global monitoring variables\n",
            "predictions_log = []\n",
            "inference_times = []\n",
            "\n",
            "def predict(row):\n",
            "    \"\"\"Predict using FastAPI service with monitoring\"\"\"\n",
            "    start_time = time.time()\n",
            "    \n",
            "    try:\n",
            "        response = requests.post(\"http://localhost:8000/predict\", \n",
            "                               json={\"title\": str(row.title), \"summary\": str(row.summary)},\n",
            "                               timeout=5)\n",
            "        \n",
            "        inference_time = (time.time() - start_time) * 1000  # ms\n",
            "        inference_times.append(inference_time)\n",
            "        \n",
            "        if response.status_code == 200:\n",
            "            result = response.json()\n",
            "            prediction = result[\"prediction\"]\n",
            "            \n",
            "            # Extract true label from main_category and map to parent category\n",
            "            true_category = getattr(row, 'main_category', None)\n",
            "            true_label = map_category(true_category) if true_category else None\n",
            "            \n",
            "            # Log prediction for metrics calculation\n",
            "            predictions_log.append({\n",
            "                'prediction': prediction,\n",
            "                'true_label': true_label,\n",
            "                'inference_time_ms': inference_time\n",
            "            })\n",
            "            \n",
            "            return prediction\n",
            "            \n",
            "    except Exception as e:\n",
            "        inference_times.append((time.time() - start_time) * 1000)\n",
            "        print(f\"API call failed: {e}\")\n",
            "        return \"api_error\"\n",
            "\n",
            "predict_udf = udf(predict, StringType())\n",
            "\n",
            "def process(time_batch, rdd):\n",
            "    \"\"\"Process streaming batch with predictions and monitoring\"\"\"\n",
            "    if rdd.isEmpty(): \n",
            "        return\n",
            "        \n",
            "    print(f\"========= {str(time_batch)} =========\")\n",
            "    \n",
            "    df = spark.read.json(rdd)\n",
            "    \n",
            "    # Show original data structure\n",
            "    if df.count() > 0:\n",
            "        print(\"Sample data:\")\n",
            "        df.select(\"title\", \"main_category\").show(2, truncate=True)\n",
            "        \n",
            "        # Apply predictions with monitoring\n",
            "        df_withpreds = df.withColumn(\"pred\", predict_udf(\n",
            "            struct(col(\"title\"), col(\"summary\"), col(\"main_category\"))\n",
            "        ))\n",
            "        \n",
            "        df_withpreds.select(\"title\", \"main_category\", \"pred\").show(5, truncate=True)\n",
            "        print_performance_metrics()\n",
            "\n",
            "def print_performance_metrics():\n",
            "    \"\"\"Print classification performance and inference speed metrics\"\"\"\n",
            "    if len(predictions_log) < 2: \n",
            "        print(\"Need more predictions for metrics calculation\")\n",
            "        return\n",
            "    \n",
            "    # Filter predictions with valid true labels\n",
            "    valid_preds = [p for p in predictions_log if p['true_label'] is not None]\n",
            "    \n",
            "    if len(valid_preds) < 2:\n",
            "        print(\"Need more predictions with valid true labels\")\n",
            "        return\n",
            "    \n",
            "    # Extract predictions and true labels\n",
            "    preds = [p['prediction'] for p in valid_preds]\n",
            "    trues = [p['true_label'] for p in valid_preds]\n",
            "    \n",
            "    # Classification performance metrics\n",
            "    try:\n",
            "        macro_f1 = f1_score(trues, preds, average='macro', zero_division=0)\n",
            "        balanced_acc = balanced_accuracy_score(trues, preds)\n",
            "        \n",
            "        print(f\"\\n--- PERFORMANCE METRICS ---\")\n",
            "        print(f\"Valid predictions: {len(valid_preds)}\")\n",
            "        print(f\"Macro F1: {macro_f1:.3f}\")\n",
            "        print(f\"Balanced Accuracy: {balanced_acc:.3f}\")\n",
            "    except Exception as e:\n",
            "        print(f\"Error calculating metrics: {e}\")\n",
            "    \n",
            "    # Inference speed metrics\n",
            "    if len(inference_times) > 0:\n",
            "        avg_time = sum(inference_times) / len(inference_times)\n",
            "        min_time = min(inference_times)\n",
            "        max_time = max(inference_times)\n",
            "        \n",
            "        print(f\"\\n--- INFERENCE SPEED ---\")\n",
            "        print(f\"Avg: {avg_time:.1f}ms | Min: {min_time:.1f}ms | Max: {max_time:.1f}ms\")\n",
            "        print(f\"Total predictions: {len(predictions_log)}\")\n",
            "    \n",
            "    print(\"=\" * 40)\n",
            "\n",
            "def get_category_distribution():\n",
            "    \"\"\"Show distribution of true vs predicted categories\"\"\"\n",
            "    if len(predictions_log) < 1: \n",
            "        return\n",
            "    \n",
            "    valid_preds = [p for p in predictions_log if p['true_label'] is not None]\n",
            "    if len(valid_preds) < 1: \n",
            "        return\n",
            "        \n",
            "    true_dist = Counter([p['true_label'] for p in valid_preds])\n",
            "    pred_dist = Counter([p['prediction'] for p in valid_preds])\n",
            "    \n",
            "    print(\"\\n--- CATEGORY DISTRIBUTION ---\")\n",
            "    print(\"True labels:\", dict(true_dist))\n",
            "    print(\"Predictions:\", dict(pred_dist))\n",
            "\n",
            "def reset_metrics():\n",
            "    \"\"\"Reset all monitoring statistics\"\"\"\n",
            "    global predictions_log, inference_times\n",
            "    predictions_log = []\n",
            "    inference_times = []\n",
            "    print(\"Metrics reset!\")\n",
            "    "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "c:\\Users\\GPham\\miniforge3\\envs\\assignment-03\\Lib\\site-packages\\pyspark\\streaming\\context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
                  "  warnings.warn(\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "========= 2025-05-31 08:32:30 =========\n",
                  "Sample data:\n",
                  "+--------------------+-------------+\n",
                  "|               title|main_category|\n",
                  "+--------------------+-------------+\n",
                  "|Institutionalizin...|        cs.HC|\n",
                  "|POLAR: A Benchmar...|        cs.CL|\n",
                  "+--------------------+-------------+\n",
                  "only showing top 2 rows\n",
                  "\n",
                  "+--------------------+-------------+----+\n",
                  "|               title|main_category|pred|\n",
                  "+--------------------+-------------+----+\n",
                  "|Institutionalizin...|        cs.HC|  cs|\n",
                  "|POLAR: A Benchmar...|        cs.CL|  cs|\n",
                  "|Long Context Scal...|        cs.CL|  cs|\n",
                  "|ConsiStyle: Style...|        cs.CV|  cs|\n",
                  "|Fundamental Limit...|        cs.GT|  cs|\n",
                  "+--------------------+-------------+----+\n",
                  "only showing top 5 rows\n",
                  "\n",
                  "Need more predictions for metrics calculation\n",
                  "========= 2025-05-31 08:32:40 =========\n"
               ]
            }
         ],
         "source": [
            "ssc = StreamingContext(sc, 10)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
            "lines.foreachRDD(process)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "ssc_t = StreamingThread(ssc)\n",
            "ssc_t.start()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "ssc_t.stop()\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "assignment-03",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.11"
      },
      "toc": {
         "base_numbering": 1,
         "nav_menu": {},
         "number_sections": true,
         "sideBar": true,
         "skip_h1_title": false,
         "title_cell": "Table of Contents",
         "title_sidebar": "Contents",
         "toc_cell": false,
         "toc_position": {},
         "toc_section_display": true,
         "toc_window_display": false
      }
   },
   "nbformat": 4,
   "nbformat_minor": 4
}

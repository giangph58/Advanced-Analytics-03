{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "spark_home = os.path.abspath(os.getcwd() + \"/spark/spark-3.5.5-bin-hadoop3\")\n",
            "hadoop_home = os.path.abspath(os.getcwd() + \"/spark/winutils\")\n",
            "print(f\"I am using the following SPARK_HOME: {spark_home}\")\n",
            "if os.name == 'nt':\n",
            "    os.environ[\"HADOOP_HOME\"] = f\"{hadoop_home}\"\n",
            "    print(f\"Windows detected: set HADOOP_HOME to: {os.environ['HADOOP_HOME']}\")\n",
            "    hadoop_bin = os.path.join(hadoop_home, \"bin\")\n",
            "    os.environ[\"PATH\"] = f\"{hadoop_bin};{os.environ['PATH']}\"\n",
            "    print(f\"  Also added Hadoop bin directory to PATH: {hadoop_bin}\")\n",
            "\n",
            "import findspark\n",
            "import pyspark\n",
            "from pyspark.streaming import StreamingContext\n",
            "\n",
            "findspark.init(spark_home)\n",
            "sc = pyspark.SparkContext()\n",
            "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import threading\n",
            "\n",
            "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
            "        \n",
            "class StreamingThread(threading.Thread):\n",
            "    def __init__(self, ssc):\n",
            "        super().__init__()\n",
            "        self.ssc = ssc\n",
            "    def run(self):\n",
            "        self.ssc.start()\n",
            "        self.ssc.awaitTermination()\n",
            "    def stop(self):\n",
            "        print('----- Stopping... this may take a few seconds -----')\n",
            "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
            "        "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import requests\n",
            "import time\n",
            "from collections import Counter\n",
            "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
            "from pyspark.sql.functions import udf, struct, col\n",
            "from pyspark.sql.types import StringType\n",
            "from src.utils import map_category\n",
            "\n",
            "# Use Spark accumulators instead of global lists\n",
            "predictions_accumulator = sc.accumulator(0)  # Count of predictions\n",
            "valid_predictions_accumulator = sc.accumulator(0)  # Count of valid predictions\n",
            "total_inference_time_accumulator = sc.accumulator(0.0)  # Sum of inference times\n",
            "\n",
            "# Keep a driver-side log for detailed analysis\n",
            "predictions_log = []\n",
            "\n",
            "def predict(row):\n",
            "    \"\"\"Predict using FastAPI service with monitoring\"\"\"\n",
            "    start_time = time.time()\n",
            "    \n",
            "    try:\n",
            "        response = requests.post(\"http://localhost:8000/predict\", \n",
            "                               json={\"title\": str(row.title), \"summary\": str(row.summary)},\n",
            "                               timeout=10)  # Increase timeout\n",
            "        \n",
            "        inference_time = (time.time() - start_time) * 1000\n",
            "        \n",
            "        if response.status_code == 200:\n",
            "            result = response.json()\n",
            "            prediction = result[\"prediction\"]\n",
            "            \n",
            "            # Update accumulators\n",
            "            predictions_accumulator.add(1)\n",
            "            total_inference_time_accumulator.add(inference_time)\n",
            "            \n",
            "            return prediction\n",
            "        else:\n",
            "            print(f\"API returned status: {response.status_code}\")\n",
            "            return \"api_error\"\n",
            "            \n",
            "    except requests.exceptions.Timeout:\n",
            "        print(f\"API call timed out after 10 seconds\")\n",
            "        return \"timeout_error\"\n",
            "    except requests.exceptions.ConnectionError:\n",
            "        print(f\"Cannot connect to FastAPI service\")\n",
            "        return \"connection_error\"\n",
            "    except Exception as e:\n",
            "        print(f\"API call failed: {e}\")\n",
            "        return \"api_error\"\n",
            "\n",
            "\n",
            "predict_udf = udf(predict, StringType())\n",
            "map_category_udf = udf(map_category, StringType())\n",
            "\n",
            "def process(time_batch, rdd):\n",
            "    \"\"\"Process streaming batch with predictions and monitoring\"\"\"\n",
            "    if rdd.isEmpty(): \n",
            "        return\n",
            "        \n",
            "    print(f\"========= {str(time_batch)} =========\")\n",
            "    \n",
            "    df = spark.read.json(rdd)\n",
            "    \n",
            "    if df.count() > 0:\n",
            "        print(\"Sample data:\")\n",
            "        # Add label column to verify mapping works correctly\n",
            "        df = df.withColumn(\"label\", map_category_udf(col(\"main_category\")))\n",
            "        df.select(\"title\", \"published\", \"main_category\", \"label\").show()\n",
            "\n",
            "        # Apply predictions with monitoring\n",
            "        df_withpreds = df.withColumn(\"pred\", predict_udf(\n",
            "            struct(col(\"title\"), col(\"summary\"), col(\"main_category\"))\n",
            "        ))\n",
            "        \n",
            "        df_withpreds.select(\"title\", \"main_category\", \"label\", \"pred\").show()\n",
            "        \n",
            "        # Collect results to driver for detailed analysis\n",
            "        results = df_withpreds.select(\"main_category\", \"label\", \"pred\").collect()\n",
            "        for row in results:\n",
            "            true_label = row.label\n",
            "            prediction = row.pred\n",
            "            if true_label and prediction and prediction != \"api_error\":\n",
            "                predictions_log.append({\n",
            "                    'prediction': prediction,\n",
            "                    'true_label': true_label\n",
            "                })\n",
            "        \n",
            "        print_performance_metrics()\n",
            "\n",
            "def print_performance_metrics():\n",
            "    \"\"\"Print classification performance and inference speed metrics\"\"\"\n",
            "    print(f\"\\n--- ACCUMULATOR METRICS ---\")\n",
            "    print(f\"Total predictions (accumulator): {predictions_accumulator.value}\")\n",
            "    print(f\"Valid predictions (accumulator): {valid_predictions_accumulator.value}\")\n",
            "    print(f\"Driver-side log: {len(predictions_log)} entries\")\n",
            "    \n",
            "    if predictions_accumulator.value > 0:\n",
            "        avg_inference_time = total_inference_time_accumulator.value / predictions_accumulator.value\n",
            "        print(f\"Average inference time: {avg_inference_time:.1f}ms\")\n",
            "    \n",
            "    # Use driver-side log for detailed metrics\n",
            "    if len(predictions_log) < 2:\n",
            "        print(\"Need more predictions in driver log for detailed metrics\")\n",
            "        return\n",
            "    \n",
            "    valid_preds = [p for p in predictions_log if p['true_label'] is not None]\n",
            "    \n",
            "    if len(valid_preds) < 2:\n",
            "        print(\"Need more valid predictions for detailed metrics\")\n",
            "        return\n",
            "    \n",
            "    # Extract predictions and true labels\n",
            "    preds = [p['prediction'] for p in valid_preds]\n",
            "    trues = [p['true_label'] for p in valid_preds]\n",
            "    \n",
            "    # Classification performance metrics\n",
            "    try:\n",
            "        macro_f1 = f1_score(trues, preds, average='macro', zero_division=0)\n",
            "        balanced_acc = balanced_accuracy_score(trues, preds)\n",
            "        \n",
            "        print(f\"\\n--- PERFORMANCE METRICS ---\")\n",
            "        print(f\"Valid predictions: {len(valid_preds)}\")\n",
            "        print(f\"Macro F1: {macro_f1:.3f}\")\n",
            "        print(f\"Balanced Accuracy: {balanced_acc:.3f}\")\n",
            "    except Exception as e:\n",
            "        print(f\"Error calculating metrics: {e}\")\n",
            "    \n",
            "    print(\"=\" * 40)\n",
            "\n",
            "def reset_metrics():\n",
            "    \"\"\"Reset all monitoring statistics\"\"\"\n",
            "    global predictions_log\n",
            "    predictions_log = []\n",
            "    \n",
            "    # Reset accumulators by recreating them\n",
            "    global predictions_accumulator, valid_predictions_accumulator, total_inference_time_accumulator\n",
            "    predictions_accumulator = sc.accumulator(0)\n",
            "    valid_predictions_accumulator = sc.accumulator(0)\n",
            "    total_inference_time_accumulator = sc.accumulator(0.0)\n",
            "    \n",
            "    print(\"Metrics reset!\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "ssc = StreamingContext(sc, 10)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "scrolled": true
         },
         "outputs": [],
         "source": [
            "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
            "lines.foreachRDD(process)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "ssc_t = StreamingThread(ssc)\n",
            "ssc_t.start()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "ssc_t.stop()\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "assignment-03",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.11"
      },
      "toc": {
         "base_numbering": 1,
         "nav_menu": {},
         "number_sections": true,
         "sideBar": true,
         "skip_h1_title": false,
         "title_cell": "Table of Contents",
         "title_sidebar": "Contents",
         "toc_cell": false,
         "toc_position": {},
         "toc_section_display": true,
         "toc_window_display": false
      }
   },
   "nbformat": 4,
   "nbformat_minor": 4
}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02639v1\", \"title\": \"Reservoir Computing: A New Paradigm for Neural Networks\", \"summary\": \"A Literature Review of Reservoir Computing.\\n  Even before Artificial Intelligence was its own field of computational\\nscience, humanity has tried to mimic the activity of the human brain. In the\\nearly 1940s the first artificial neuron models were created as purely\\nmathematical concepts. Over the years, ideas from neuroscience and computer\\nscience were used to develop the modern Neural Network. The interest in these\\nmodels rose quickly but fell when they failed to be successfully applied to\\npractical applications, and rose again in the late 2000s with the drastic\\nincrease in computing power, notably in the field of natural language\\nprocessing, for example with the state-of-the-art speech recognizer making\\nheavy use of deep neural networks.\\n  Recurrent Neural Networks (RNNs), a class of neural networks with cycles in\\nthe network, exacerbates the difficulties of traditional neural nets. Slow\\nconvergence limiting the use to small networks, and difficulty to train through\\ngradient-descent methods because of the recurrent dynamics have hindered\\nresearch on RNNs, yet their biological plausibility and their capability to\\nmodel dynamical systems over simple functions makes then interesting for\\ncomputational researchers.\\n  Reservoir Computing emerges as a solution to these problems that RNNs\\ntraditionally face. Promising to be both theoretically sound and\\ncomputationally fast, Reservoir Computing has already been applied successfully\\nto numerous fields: natural language processing, computational biology and\\nneuroscience, robotics, even physics. This survey will explore the history and\\nappeal of both traditional feed-forward and recurrent neural networks, before\\ndescribing the theory and models of this new reservoir computing paradigm.\\nFinally recent papers using reservoir computing in a variety of scientific\\nfields will be reviewed.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-03T14:34:51Z\"}"}

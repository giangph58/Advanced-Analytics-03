{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04847v1\", \"title\": \"Nonlocal techniques for the analysis of deep ReLU neural network\\n  approximations\", \"summary\": \"Recently, Daubechies, DeVore, Foucart, Hanin, and Petrova introduced a system\\nof piece-wise linear functions, which can be easily reproduced by artificial\\nneural networks with the ReLU activation function and which form a Riesz basis\\nof $L_2([0,1])$. This work was generalized by two of the authors to the\\nmultivariate setting. We show that this system serves as a Riesz basis also for\\nSobolev spaces $W^s([0,1]^d)$ and Barron classes ${\\\\mathbb B}^s([0,1]^d)$ with\\nsmoothness $0<s<1$. We apply this fact to re-prove some recent results on the\\napproximation of functions from these classes by deep neural networks. Our\\nproof method avoids using local approximations and allows us to track also the\\nimplicit constants as well as to show that we can avoid the curse of dimension.\\nMoreover, we also study how well one can approximate Sobolev and Barron\\nfunctions by ANNs if only function values are known.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CC,cs.NA,math.NA\", \"published\": \"2025-04-07T09:00:22Z\"}"}

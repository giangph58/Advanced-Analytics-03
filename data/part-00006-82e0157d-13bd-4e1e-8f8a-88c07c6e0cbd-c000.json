{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02395v1\", \"title\": \"The quasi-semantic competence of LLMs: a case study on the part-whole\\n  relation\", \"summary\": \"Understanding the extent and depth of the semantic competence of \\\\emph{Large\\nLanguage Models} (LLMs) is at the center of the current scientific agenda in\\nArtificial Intelligence (AI) and Computational Linguistics (CL). We contribute\\nto this endeavor by investigating their knowledge of the \\\\emph{part-whole}\\nrelation, a.k.a. \\\\emph{meronymy}, which plays a crucial role in lexical\\norganization, but it is significantly understudied. We used data from\\nConceptNet relations \\\\citep{speer2016conceptnet} and human-generated semantic\\nfeature norms \\\\citep{McRae:2005} to explore the abilities of LLMs to deal with\\n\\\\textit{part-whole} relations. We employed several methods based on three\\nlevels of analysis: i.) \\\\textbf{behavioral} testing via prompting, where we\\ndirectly queried the models on their knowledge of meronymy, ii.) sentence\\n\\\\textbf{probability} scoring, where we tested models' abilities to discriminate\\ncorrect (real) and incorrect (asymmetric counterfactual) \\\\textit{part-whole}\\nrelations, and iii.) \\\\textbf{concept representation} analysis in vector space,\\nwhere we proved the linear organization of the \\\\textit{part-whole} concept in\\nthe embedding and unembedding spaces. These analyses present a complex picture\\nthat reveals that the LLMs' knowledge of this relation is only partial. They\\nhave just a ``\\\\emph{quasi}-semantic'' competence and still fall short of\\ncapturing deep inferential properties.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-03T08:41:26Z\"}"}

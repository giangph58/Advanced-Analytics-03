{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24322v1\", \"title\": \"NoProp: Training Neural Networks without Back-propagation or\\n  Forward-propagation\", \"summary\": \"The canonical deep learning approach for learning requires computing a\\ngradient term at each layer by back-propagating the error signal from the\\noutput towards each learnable parameter. Given the stacked structure of neural\\nnetworks, where each layer builds on the representation of the layer below,\\nthis approach leads to hierarchical representations. More abstract features\\nlive on the top layers of the model, while features on lower layers are\\nexpected to be less abstract. In contrast to this, we introduce a new learning\\nmethod named NoProp, which does not rely on either forward or backwards\\npropagation. Instead, NoProp takes inspiration from diffusion and flow matching\\nmethods, where each layer independently learns to denoise a noisy target. We\\nbelieve this work takes a first step towards introducing a new family of\\ngradient-free learning methods, that does not learn hierarchical\\nrepresentations -- at least not in the usual sense. NoProp needs to fix the\\nrepresentation at each layer beforehand to a noised version of the target,\\nlearning a local denoising process that can then be exploited at inference. We\\ndemonstrate the effectiveness of our method on MNIST, CIFAR-10, and CIFAR-100\\nimage classification benchmarks. Our results show that NoProp is a viable\\nlearning algorithm which achieves superior accuracy, is easier to use and\\ncomputationally more efficient compared to other existing back-propagation-free\\nmethods. By departing from the traditional gradient based learning paradigm,\\nNoProp alters how credit assignment is done within the network, enabling more\\nefficient distributed learning as well as potentially impacting other\\ncharacteristics of the learning process.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,stat.ML\", \"published\": \"2025-03-31T17:08:57Z\"}"}

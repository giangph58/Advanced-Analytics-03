{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01875v1\", \"title\": \"Architect Your Landscape Approach (AYLA) for Optimizations in Deep\\n  Learning\", \"summary\": \"Stochastic Gradient Descent (SGD) and its variants, such as ADAM, are\\nfoundational to deep learning optimization, adjusting model parameters using\\nfixed or adaptive learning rates based on loss function gradients. However,\\nthese methods often face challenges in balancing adaptability and efficiency in\\nnon-convex, high-dimensional settings. This paper introduces AYLA, a novel\\noptimization technique that enhances training dynamics through loss function\\ntransformations. By applying a tunable power-law transformation, AYLA preserves\\ncritical points while scaling loss values to amplify gradient sensitivity,\\naccelerating convergence. We further propose a dynamic (effective) learning\\nrate that adapts to the transformed loss, improving optimization efficiency.\\nEmpirical tests on finding minimum of a synthetic non-convex polynomial, a\\nnon-convex curve-fitting dataset, and digit classification (MNIST) demonstrate\\nthat AYLA surpasses SGD and ADAM in convergence speed and stability. This\\napproach redefines the loss landscape for better optimization outcomes,\\noffering a promising advancement for deep neural networks and can be applied to\\nany optimization method and potentially improve the performance of it.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-02T16:31:39Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05147v1\", \"title\": \"Pr$\\u03b5\\u03b5$mpt: Sanitizing Sensitive Prompts for LLMs\", \"summary\": \"The rise of large language models (LLMs) has introduced new privacy\\nchallenges, particularly during inference where sensitive information in\\nprompts may be exposed to proprietary LLM APIs. In this paper, we address the\\nproblem of formally protecting the sensitive information contained in a prompt\\nwhile maintaining response quality. To this end, first, we introduce a\\ncryptographically inspired notion of a prompt sanitizer which transforms an\\ninput prompt to protect its sensitive tokens. Second, we propose\\nPr$\\\\epsilon\\\\epsilon$mpt, a novel system that implements a prompt sanitizer.\\nPr$\\\\epsilon\\\\epsilon$mpt categorizes sensitive tokens into two types: (1) those\\nwhere the LLM's response depends solely on the format (such as SSNs, credit\\ncard numbers), for which we use format-preserving encryption (FPE); and (2)\\nthose where the response depends on specific values, (such as age, salary) for\\nwhich we apply metric differential privacy (mDP). Our evaluation demonstrates\\nthat Pr$\\\\epsilon\\\\epsilon$mpt is a practical method to achieve meaningful\\nprivacy guarantees, while maintaining high utility compared to unsanitized\\nprompts, and outperforming prior methods\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.LG\", \"published\": \"2025-04-07T14:52:40Z\"}"}

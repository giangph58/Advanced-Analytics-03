{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24016v1\", \"title\": \"Bayesian Predictive Coding\", \"summary\": \"Predictive coding (PC) is an influential theory of information processing in\\nthe brain, providing a biologically plausible alternative to backpropagation.\\nIt is motivated in terms of Bayesian inference, as hidden states and parameters\\nare optimised via gradient descent on variational free energy. However,\\nimplementations of PC rely on maximum \\\\textit{a posteriori} (MAP) estimates of\\nhidden states and maximum likelihood (ML) estimates of parameters, limiting\\ntheir ability to quantify epistemic uncertainty. In this work, we investigate a\\nBayesian extension to PC that estimates a posterior distribution over network\\nparameters. This approach, termed Bayesian Predictive coding (BPC), preserves\\nthe locality of PC and results in closed-form Hebbian weight updates. Compared\\nto PC, our BPC algorithm converges in fewer epochs in the full-batch setting\\nand remains competitive in the mini-batch setting. Additionally, we demonstrate\\nthat BPC offers uncertainty quantification comparable to existing methods in\\nBayesian deep learning, while also improving convergence properties. Together,\\nthese results suggest that BPC provides a biologically plausible method for\\nBayesian learning in the brain, as well as an attractive approach to\\nuncertainty quantification in deep learning.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,stat.ML\", \"published\": \"2025-03-31T12:40:50Z\"}"}

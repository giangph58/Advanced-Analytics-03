{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05020v1\", \"title\": \"Batch Aggregation: An Approach to Enhance Text Classification with\\n  Correlated Augmented Data\", \"summary\": \"Natural language processing models often face challenges due to limited\\nlabeled data, especially in domain specific areas, e.g., clinical trials. To\\novercome this, text augmentation techniques are commonly used to increases\\nsample size by transforming the original input data into artificial ones with\\nthe label preserved. However, traditional text classification methods ignores\\nthe relationship between augmented texts and treats them as independent samples\\nwhich may introduce classification error. Therefore, we propose a novel\\napproach called 'Batch Aggregation' (BAGG) which explicitly models the\\ndependence of text inputs generated through augmentation by incorporating an\\nadditional layer that aggregates results from correlated texts. Through\\nstudying multiple benchmark data sets across different domains, we found that\\nBAGG can improve classification accuracy. We also found that the increase of\\nperformance with BAGG is more obvious in domain specific data sets, with\\naccuracy improvements of up to 10-29%. Through the analysis of benchmark data,\\nthe proposed method addresses limitations of traditional techniques and\\nimproves robustness in text classification tasks. Our result demonstrates that\\nBAGG offers more robust results and outperforms traditional approaches when\\ntraining data is limited.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-07T12:46:07Z\"}"}

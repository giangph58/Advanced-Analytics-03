{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24307v1\", \"title\": \"A Systematic Evaluation of LLM Strategies for Mental Health Text\\n  Analysis: Fine-tuning vs. Prompt Engineering vs. RAG\", \"summary\": \"This study presents a systematic comparison of three approaches for the\\nanalysis of mental health text using large language models (LLMs): prompt\\nengineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA\\n3, we evaluate these approaches on emotion classification and mental health\\ncondition detection tasks across two datasets. Fine-tuning achieves the highest\\naccuracy (91% for emotion classification, 80% for mental health conditions) but\\nrequires substantial computational resources and large training sets, while\\nprompt engineering and RAG offer more flexible deployment with moderate\\nperformance (40-68% accuracy). Our findings provide practical insights for\\nimplementing LLM-based solutions in mental health applications, highlighting\\nthe trade-offs between accuracy, computational requirements, and deployment\\nflexibility.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.IR,cs.LG\", \"published\": \"2025-03-31T16:54:04Z\"}"}

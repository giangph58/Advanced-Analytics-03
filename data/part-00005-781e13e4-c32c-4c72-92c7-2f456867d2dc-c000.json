{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05239v1\", \"title\": \"LLM-based Automated Grading with Human-in-the-Loop\", \"summary\": \"The rise of artificial intelligence (AI) technologies, particularly large\\nlanguage models (LLMs), has brought significant advancements to the field of\\neducation. Among various applications, automatic short answer grading (ASAG),\\nwhich focuses on evaluating open-ended textual responses, has seen remarkable\\nprogress with the introduction of LLMs. These models not only enhance grading\\nperformance compared to traditional ASAG approaches but also move beyond simple\\ncomparisons with predefined \\\"golden\\\" answers, enabling more sophisticated\\ngrading scenarios, such as rubric-based evaluation. However, existing\\nLLM-powered methods still face challenges in achieving human-level grading\\nperformance in rubric-based assessments due to their reliance on fully\\nautomated approaches. In this work, we explore the potential of LLMs in ASAG\\ntasks by leveraging their interactive capabilities through a human-in-the-loop\\n(HITL) approach. Our proposed framework, GradeHITL, utilizes the generative\\nproperties of LLMs to pose questions to human experts, incorporating their\\ninsights to refine grading rubrics dynamically. This adaptive process\\nsignificantly improves grading accuracy, outperforming existing methods and\\nbringing ASAG closer to human-level evaluation.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-07T16:23:07Z\"}"}

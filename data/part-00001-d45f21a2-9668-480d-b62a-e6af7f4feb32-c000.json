{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05291v1\", \"title\": \"Using Physiological Measures, Gaze, and Facial Expressions to Model\\n  Human Trust in a Robot Partner\", \"summary\": \"With robots becoming increasingly prevalent in various domains, it has become\\ncrucial to equip them with tools to achieve greater fluency in interactions\\nwith humans. One of the promising areas for further exploration lies in human\\ntrust. A real-time, objective model of human trust could be used to maximize\\nproductivity, preserve safety, and mitigate failure. In this work, we attempt\\nto use physiological measures, gaze, and facial expressions to model human\\ntrust in a robot partner. We are the first to design an in-person, human-robot\\nsupervisory interaction study to create a dedicated trust dataset. Using this\\ndataset, we train machine learning algorithms to identify the objective\\nmeasures that are most indicative of trust in a robot partner, advancing trust\\nprediction in human-robot interactions. Our findings indicate that a\\ncombination of sensor modalities (blood volume pulse, electrodermal activity,\\nskin temperature, and gaze) can enhance the accuracy of detecting human trust\\nin a robot partner. Furthermore, the Extra Trees, Random Forest, and Decision\\nTrees classifiers exhibit consistently better performance in measuring the\\nperson's trust in the robot partner. These results lay the groundwork for\\nconstructing a real-time trust model for human-robot interaction, which could\\nfoster more efficient interactions between humans and robots.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO\", \"published\": \"2025-04-07T17:45:17Z\"}"}

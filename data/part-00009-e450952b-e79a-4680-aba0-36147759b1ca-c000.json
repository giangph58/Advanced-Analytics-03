{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06577v1\", \"title\": \"Bypassing Safety Guardrails in LLMs Using Humor\", \"summary\": \"In this paper, we show it is possible to bypass the safety guardrails of\\nlarge language models (LLMs) through a humorous prompt including the unsafe\\nrequest. In particular, our method does not edit the unsafe request and follows\\na fixed template -- it is simple to implement and does not need additional LLMs\\nto craft prompts. Extensive experiments show the effectiveness of our method\\nacross different LLMs. We also show that both removing and adding more humor to\\nour method can reduce its effectiveness -- excessive humor possibly distracts\\nthe LLM from fulfilling its unsafe request. Thus, we argue that LLM\\njailbreaking occurs when there is a proper balance between focus on the unsafe\\nrequest and presence of humor.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-09T04:58:14Z\"}"}

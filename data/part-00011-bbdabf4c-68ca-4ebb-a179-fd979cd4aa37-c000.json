{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05288v1\", \"title\": \"LiveVQA: Live Visual Knowledge Seeking\", \"summary\": \"We introduce LiveVQA, an automatically collected dataset of latest visual\\nknowledge from the Internet with synthesized VQA problems. LiveVQA consists of\\n3,602 single- and multi-hop visual questions from 6 news websites across 14\\nnews categories, featuring high-quality image-text coherence and authentic\\ninformation. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and\\nQwen-2.5-VL family) demonstrates that stronger models perform better overall,\\nwith advanced visual reasoning capabilities proving crucial for complex\\nmulti-hop questions. Despite excellent performance on textual problems, models\\nwith tools like search engines still show significant gaps when addressing\\nvisual questions requiring latest visual knowledge, highlighting important\\nareas for future research.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.CL\", \"published\": \"2025-04-07T17:39:31Z\"}"}

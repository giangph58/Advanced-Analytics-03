{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01819v1\", \"title\": \"Implicit Bias Injection Attacks against Text-to-Image Diffusion Models\", \"summary\": \"The proliferation of text-to-image diffusion models (T2I DMs) has led to an\\nincreased presence of AI-generated images in daily life. However, biased T2I\\nmodels can generate content with specific tendencies, potentially influencing\\npeople's perceptions. Intentional exploitation of these biases risks conveying\\nmisleading information to the public. Current research on bias primarily\\naddresses explicit biases with recognizable visual patterns, such as skin color\\nand gender. This paper introduces a novel form of implicit bias that lacks\\nexplicit visual features but can manifest in diverse ways across various\\nsemantic contexts. This subtle and versatile nature makes this bias challenging\\nto detect, easy to propagate, and adaptable to a wide range of scenarios. We\\nfurther propose an implicit bias injection attack framework (IBI-Attacks)\\nagainst T2I diffusion models by precomputing a general bias direction in the\\nprompt embedding space and adaptively adjusting it based on different inputs.\\nOur attack module can be seamlessly integrated into pre-trained diffusion\\nmodels in a plug-and-play manner without direct manipulation of user input or\\nmodel retraining. Extensive experiments validate the effectiveness of our\\nscheme in introducing bias through subtle and diverse modifications while\\npreserving the original semantics. The strong concealment and transferability\\nof our attack across various scenarios further underscore the significance of\\nour approach. Code is available at https://github.com/Hannah1102/IBI-attacks.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-02T15:24:12Z\"}"}

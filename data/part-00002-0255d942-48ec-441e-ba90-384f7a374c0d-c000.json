{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02477v1\", \"title\": \"Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision\", \"summary\": \"Robot vision has greatly benefited from advancements in multimodal fusion\\ntechniques and vision-language models (VLMs). We systematically review the\\napplications of multimodal fusion in key robotic vision tasks, including\\nsemantic scene understanding, simultaneous localization and mapping (SLAM), 3D\\nobject detection, navigation and localization, and robot manipulation. We\\ncompare VLMs based on large language models (LLMs) with traditional multimodal\\nfusion methods, analyzing their advantages, limitations, and synergies.\\nAdditionally, we conduct an in-depth analysis of commonly used datasets,\\nevaluating their applicability and challenges in real-world robotic scenarios.\\nFurthermore, we identify critical research challenges such as cross-modal\\nalignment, efficient fusion strategies, real-time deployment, and domain\\nadaptation, and propose future research directions, including self-supervised\\nlearning for robust multimodal representations, transformer-based fusion\\narchitectures, and scalable multimodal frameworks. Through a comprehensive\\nreview, comparative analysis, and forward-looking discussion, we provide a\\nvaluable reference for advancing multimodal perception and interaction in\\nrobotic vision. A comprehensive list of studies in this survey is available at\\nhttps://github.com/Xiaofeng-Han-Res/MF-RV.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.CV\", \"published\": \"2025-04-03T10:53:07Z\"}"}

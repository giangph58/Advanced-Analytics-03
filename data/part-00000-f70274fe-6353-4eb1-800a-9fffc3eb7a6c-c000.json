{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06866v1\", \"title\": \"GraspClutter6D: A Large-scale Real-world Dataset for Robust Perception\\n  and Grasping in Cluttered Scenes\", \"summary\": \"Robust grasping in cluttered environments remains an open challenge in\\nrobotics. While benchmark datasets have significantly advanced deep learning\\nmethods, they mainly focus on simplistic scenes with light occlusion and\\ninsufficient diversity, limiting their applicability to practical scenarios. We\\npresent GraspClutter6D, a large-scale real-world grasping dataset featuring:\\n(1) 1,000 highly cluttered scenes with dense arrangements (14.1 objects/scene,\\n62.6\\\\% occlusion), (2) comprehensive coverage across 200 objects in 75\\nenvironment configurations (bins, shelves, and tables) captured using four\\nRGB-D cameras from multiple viewpoints, and (3) rich annotations including 736K\\n6D object poses and 9.3B feasible robotic grasps for 52K RGB-D images. We\\nbenchmark state-of-the-art segmentation, object pose estimation, and grasping\\ndetection methods to provide key insights into challenges in cluttered\\nenvironments. Additionally, we validate the dataset's effectiveness as a\\ntraining resource, demonstrating that grasping networks trained on\\nGraspClutter6D significantly outperform those trained on existing datasets in\\nboth simulation and real-world experiments. The dataset, toolkit, and\\nannotation tools are publicly available on our project website:\\nhttps://sites.google.com/view/graspclutter6d.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.AI,cs.CV\", \"published\": \"2025-04-09T13:15:46Z\"}"}

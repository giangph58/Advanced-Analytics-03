{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02587v1\", \"title\": \"Rethinking RL Scaling for Vision Language Models: A Transparent,\\n  From-Scratch Framework and Comprehensive Evaluation Scheme\", \"summary\": \"Reinforcement learning (RL) has recently shown strong potential in improving\\nthe reasoning capabilities of large language models and is now being actively\\nextended to vision-language models (VLMs). However, existing RL applications in\\nVLMs often rely on heavily engineered frameworks that hinder reproducibility\\nand accessibility, while lacking standardized evaluation protocols, making it\\ndifficult to compare results or interpret training dynamics. This work\\nintroduces a transparent, from-scratch framework for RL in VLMs, offering a\\nminimal yet functional four-step pipeline validated across multiple models and\\ndatasets. In addition, a standardized evaluation scheme is proposed to assess\\ntraining dynamics and reflective behaviors. Extensive experiments on visual\\nreasoning tasks uncover key empirical findings: response length is sensitive to\\nrandom seeds, reflection correlates with output length, and RL consistently\\noutperforms supervised fine-tuning (SFT) in generalization, even with\\nhigh-quality data. These findings, together with the proposed framework, aim to\\nestablish a reproducible baseline and support broader engagement in RL-based\\nVLM research.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL,cs.CV\", \"published\": \"2025-04-03T13:53:28Z\"}"}

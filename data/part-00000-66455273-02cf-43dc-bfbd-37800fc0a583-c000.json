{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23899v1\", \"title\": \"Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the\\n  CUBE dataset\", \"summary\": \"The performance and usability of Large-Language Models (LLMs) are driving\\ntheir use in explanation generation tasks. However, despite their widespread\\nadoption, LLM explanations have been found to be unreliable, making it\\ndifficult for users to distinguish good from bad explanations. To address this\\nissue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of\\n26k explanations, written and later quality-annotated using the rubric by both\\nhumans and six open- and closed-source LLMs. The CUBE dataset focuses on two\\nreasoning and two language tasks, providing the necessary diversity for us to\\neffectively test our proposed rubric. Using Rubrik, we find that explanations\\nare influenced by both task and perceived difficulty. Low quality stems\\nprimarily from a lack of conciseness in LLM-generated explanations, rather than\\ncohesion and word choice. The full dataset, rubric, and code will be made\\navailable upon acceptance.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,I.2.7\", \"published\": \"2025-03-31T09:48:59Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06861v1\", \"title\": \"EIDT-V: Exploiting Intersections in Diffusion Trajectories for\\n  Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation\", \"summary\": \"Zero-shot, training-free, image-based text-to-video generation is an emerging\\narea that aims to generate videos using existing image-based diffusion models.\\nCurrent methods in this space require specific architectural changes to image\\ngeneration models, which limit their adaptability and scalability. In contrast\\nto such methods, we provide a model-agnostic approach. We use intersections in\\ndiffusion trajectories, working only with the latent values. We could not\\nobtain localized frame-wise coherence and diversity using only the intersection\\nof trajectories. Thus, we instead use a grid-based approach. An in-context\\ntrained LLM is used to generate coherent frame-wise prompts; another is used to\\nidentify differences between frames. Based on these, we obtain a CLIP-based\\nattention mask that controls the timing of switching the prompts for each grid\\ncell. Earlier switching results in higher variance, while later switching\\nresults in more coherence. Therefore, our approach can ensure appropriate\\ncontrol between coherence and variance for the frames. Our approach results in\\nstate-of-the-art performance while being more flexible when working with\\ndiverse image-generation models. The empirical analysis using quantitative\\nmetrics and user studies confirms our model's superior temporal consistency,\\nvisual fidelity and user satisfaction, thus providing a novel way to obtain\\ntraining-free, image-based text-to-video generation.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-09T13:11:09Z\"}"}

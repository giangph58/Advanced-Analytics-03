{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05806v1\", \"title\": \"Meta-Continual Learning of Neural Fields\", \"summary\": \"Neural Fields (NF) have gained prominence as a versatile framework for\\ncomplex data representation. This work unveils a new problem setting termed\\n\\\\emph{Meta-Continual Learning of Neural Fields} (MCL-NF) and introduces a novel\\nstrategy that employs a modular architecture combined with optimization-based\\nmeta-learning. Focused on overcoming the limitations of existing methods for\\ncontinual learning of neural fields, such as catastrophic forgetting and slow\\nconvergence, our strategy achieves high-quality reconstruction with\\nsignificantly improved learning speed. We further introduce Fisher Information\\nMaximization loss for neural radiance fields (FIM-NeRF), which maximizes\\ninformation gains at the sample level to enhance learning generalization, with\\nproved convergence guarantee and generalization bound. We perform extensive\\nevaluations across image, audio, video reconstruction, and view synthesis tasks\\non six diverse datasets, demonstrating our method's superiority in\\nreconstruction quality and speed over existing MCL and CL-NF approaches.\\nNotably, our approach attains rapid adaptation of neural fields for city-scale\\nNeRF rendering with reduced parameter requirement.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-08T08:38:37Z\"}"}

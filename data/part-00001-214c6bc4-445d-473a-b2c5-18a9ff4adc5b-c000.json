{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06643v1\", \"title\": \"AMAD: AutoMasked Attention for Unsupervised Multivariate Time Series\\n  Anomaly Detection\", \"summary\": \"Unsupervised multivariate time series anomaly detection (UMTSAD) plays a\\ncritical role in various domains, including finance, networks, and sensor\\nsystems. In recent years, due to the outstanding performance of deep learning\\nin general sequential tasks, many models have been specialized for deep UMTSAD\\ntasks and have achieved impressive results, particularly those based on the\\nTransformer and self-attention mechanisms. However, the sequence anomaly\\nassociation assumptions underlying these models are often limited to specific\\npredefined patterns and scenarios, such as concentrated or peak anomaly\\npatterns. These limitations hinder their ability to generalize to diverse\\nanomaly situations, especially where the lack of labels poses significant\\nchallenges. To address these issues, we propose AMAD, which integrates\\n\\\\textbf{A}uto\\\\textbf{M}asked Attention for UMTS\\\\textbf{AD} scenarios. AMAD\\nintroduces a novel structure based on the AutoMask mechanism and an attention\\nmixup module, forming a simple yet generalized anomaly association\\nrepresentation framework. This framework is further enhanced by a Max-Min\\ntraining strategy and a Local-Global contrastive learning approach. By\\ncombining multi-scale feature extraction with automatic relative association\\nmodeling, AMAD provides a robust and adaptable solution to UMTSAD challenges.\\nExtensive experimental results demonstrate that the proposed model achieving\\ncompetitive performance results compared to SOTA benchmarks across a variety of\\ndatasets.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,I.5.1\", \"published\": \"2025-04-09T07:32:59Z\"}"}

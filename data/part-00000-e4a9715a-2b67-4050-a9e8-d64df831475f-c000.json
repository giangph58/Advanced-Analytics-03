{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01916v1\", \"title\": \"FineLIP: Extending CLIP's Reach via Fine-Grained Alignment with Longer\\n  Text Inputs\", \"summary\": \"As a pioneering vision-language model, CLIP (Contrastive Language-Image\\nPre-training) has achieved significant success across various domains and a\\nwide range of downstream vision-language tasks. However, the text encoders in\\npopular CLIP models are limited to processing only 77 text tokens, which\\nconstrains their ability to effectively handle longer, detail-rich captions.\\nAdditionally, CLIP models often struggle to effectively capture detailed visual\\nand textual information, which hampers their performance on tasks that require\\nfine-grained analysis. To address these limitations, we present a novel\\napproach, \\\\textbf{FineLIP}, that extends the capabilities of CLIP. FineLIP\\nenhances cross-modal text-image mapping by incorporating \\\\textbf{Fine}-grained\\nalignment with \\\\textbf{L}onger text input within the CL\\\\textbf{IP}-style\\nframework. FineLIP first extends the positional embeddings to handle longer\\ntext, followed by the dynamic aggregation of local image and text tokens. The\\naggregated results are then used to enforce fine-grained token-to-token\\ncross-modal alignment. We validate our model on datasets with long, detailed\\ncaptions across two tasks: zero-shot cross-modal retrieval and text-to-image\\ngeneration. Quantitative and qualitative experimental results demonstrate the\\neffectiveness of FineLIP, outperforming existing state-of-the-art approaches.\\nFurthermore, comprehensive ablation studies validate the benefits of key design\\nelements within FineLIP.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.CL\", \"published\": \"2025-04-02T17:19:59Z\"}"}

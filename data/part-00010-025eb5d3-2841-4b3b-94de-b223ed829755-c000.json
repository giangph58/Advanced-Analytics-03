{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02826v1\", \"title\": \"Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\\n  Editing\", \"summary\": \"Large Multi-modality Models (LMMs) have made significant progress in visual\\nunderstanding and generation, but they still face challenges in General Visual\\nEditing, particularly in following complex instructions, preserving appearance\\nconsistency, and supporting flexible input formats. To address this gap, we\\nintroduce RISEBench, the first benchmark for evaluating Reasoning-Informed\\nviSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal,\\nCausal, Spatial, and Logical Reasoning. We curate high-quality test cases for\\neach category and propose an evaluation framework that assesses Instruction\\nReasoning, Appearance Consistency, and Visual Plausibility with both human\\njudges and an LMM-as-a-judge approach. Our experiments reveal that while\\nGPT-4o-Native significantly outperforms other open-source and proprietary\\nmodels, even this state-of-the-art system struggles with logical reasoning\\ntasks, highlighting an area that remains underexplored. As an initial effort,\\nRISEBench aims to provide foundational insights into reasoning-aware visual\\nediting and to catalyze future research. Though still in its early stages, we\\nare committed to continuously expanding and refining the benchmark to support\\nmore comprehensive, reliable, and scalable evaluations of next-generation\\nmultimodal systems. Our code and data will be released at\\nhttps://github.com/PhoenixZ810/RISEBench.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-03T17:59:56Z\"}"}

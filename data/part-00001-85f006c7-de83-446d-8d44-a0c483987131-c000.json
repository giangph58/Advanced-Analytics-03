{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06925v1\", \"title\": \"Are Vision-Language Models Ready for Dietary Assessment? Exploring the\\n  Next Frontier in AI-Powered Food Image Recognition\", \"summary\": \"Automatic dietary assessment based on food images remains a challenge,\\nrequiring precise food detection, segmentation, and classification.\\nVision-Language Models (VLMs) offer new possibilities by integrating visual and\\ntextual reasoning. In this study, we evaluate six state-of-the-art VLMs\\n(ChatGPT, Gemini, Claude, Moondream, DeepSeek, and LLaVA), analyzing their\\ncapabilities in food recognition at different levels. For the experimental\\nframework, we introduce the FoodNExTDB, a unique food image database that\\ncontains 9,263 expert-labeled images across 10 categories (e.g., \\\"protein\\nsource\\\"), 62 subcategories (e.g., \\\"poultry\\\"), and 9 cooking styles (e.g.,\\n\\\"grilled\\\"). In total, FoodNExTDB includes 50k nutritional labels generated by\\nseven experts who manually annotated all images in the database. Also, we\\npropose a novel evaluation metric, Expert-Weighted Recall (EWR), that accounts\\nfor the inter-annotator variability. Results show that closed-source models\\noutperform open-source ones, achieving over 90% EWR in recognizing food\\nproducts in images containing a single product. Despite their potential,\\ncurrent VLMs face challenges in fine-grained food recognition, particularly in\\ndistinguishing subtle differences in cooking styles and visually similar food\\nitems, which limits their reliability for automatic dietary assessment. The\\nFoodNExTDB database is publicly available at\\nhttps://github.com/AI4Food/FoodNExtDB.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-09T14:33:59Z\"}"}

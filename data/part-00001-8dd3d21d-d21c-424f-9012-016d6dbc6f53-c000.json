{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04718v1\", \"title\": \"T1: Tool-integrated Self-verification for Test-time Compute Scaling in\\n  Small Language Models\", \"summary\": \"Recent studies have demonstrated that test-time compute scaling effectively\\nimproves the performance of small language models (sLMs). However, prior\\nresearch has mainly examined test-time compute scaling with an additional\\nlarger model as a verifier, leaving self-verification by sLMs underexplored. In\\nthis work, we investigate whether sLMs can reliably self-verify their outputs\\nunder test-time scaling. We find that even with knowledge distillation from\\nlarger verifiers, sLMs struggle with verification tasks requiring memorization,\\nsuch as numerical calculations and fact-checking. To address this limitation,\\nwe propose Tool-integrated self-verification (T1), which delegates\\nmemorization-heavy verification steps to external tools, such as a code\\ninterpreter. Our theoretical analysis shows that tool integration reduces\\nmemorization demands and improves test-time scaling performance. Experiments on\\nthe MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under\\ntest-time scaling outperforms the significantly larger Llama-3.1 8B model.\\nMoreover, T1 generalizes effectively to both mathematical (MATH500) and\\nmulti-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the\\npotential of tool integration to substantially improve the self-verification\\nabilities of sLMs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-07T04:01:17Z\"}"}

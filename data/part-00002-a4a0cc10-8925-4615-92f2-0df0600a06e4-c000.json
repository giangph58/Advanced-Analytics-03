{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24270v1\", \"title\": \"Visual Acoustic Fields\", \"summary\": \"Objects produce different sounds when hit, and humans can intuitively infer\\nhow an object might sound based on its appearance and material properties.\\nInspired by this intuition, we propose Visual Acoustic Fields, a framework that\\nbridges hitting sounds and visual signals within a 3D space using 3D Gaussian\\nSplatting (3DGS). Our approach features two key modules: sound generation and\\nsound localization. The sound generation module leverages a conditional\\ndiffusion model, which takes multiscale features rendered from a\\nfeature-augmented 3DGS to generate realistic hitting sounds. Meanwhile, the\\nsound localization module enables querying the 3D scene, represented by the\\nfeature-augmented 3DGS, to localize hitting positions based on the sound\\nsources. To support this framework, we introduce a novel pipeline for\\ncollecting scene-level visual-sound sample pairs, achieving alignment between\\ncaptured images, impact locations, and corresponding sounds. To the best of our\\nknowledge, this is the first dataset to connect visual and acoustic signals in\\na 3D context. Extensive experiments on our dataset demonstrate the\\neffectiveness of Visual Acoustic Fields in generating plausible impact sounds\\nand accurately localizing impact sources. Our project page is at\\nhttps://yuelei0428.github.io/projects/Visual-Acoustic-Fields/.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-03-31T16:16:10Z\"}"}

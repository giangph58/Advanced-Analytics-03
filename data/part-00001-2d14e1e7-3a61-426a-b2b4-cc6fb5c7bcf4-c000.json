{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07583v1\", \"title\": \"Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with\\n  Question Answering\", \"summary\": \"Despite the steady progress in machine translation evaluation, existing\\nautomatic metrics struggle to capture how well meaning is preserved beyond\\nsentence boundaries. We posit that reliance on a single intrinsic quality\\nscore, trained to mimic human judgments, might be insufficient for evaluating\\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\\nassesses how accurately key information is conveyed by a translation in context\\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\\na framework that extrinsically evaluates translation quality by assessing how\\naccurately candidate translations answer reading comprehension questions that\\ntarget key information in the original source or reference texts. In\\nchallenging domains that require long-range understanding, such as literary\\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\\nstate-of-the-art neural and LLM-based metrics in ranking alternative\\nparagraph-level translations, despite never being explicitly optimized to\\ncorrelate with human judgments. Furthermore, the generated questions and\\nanswers offer interpretability: empirical analysis shows that they effectively\\ntarget translation errors identified by experts in evaluated datasets. Our code\\nis available at https://github.com/deep-spin/treqa\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-10T09:24:54Z\"}"}

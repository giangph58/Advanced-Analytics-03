{"value":"{\"aid\": \"http://arxiv.org/abs/2503.21668v1\", \"title\": \"Cognitive Science-Inspired Evaluation of Core Capabilities for Object\\n  Understanding in AI\", \"summary\": \"One of the core components of our world models is 'intuitive physics' - an\\nunderstanding of objects, space, and causality. This capability enables us to\\npredict events, plan action and navigate environments, all of which rely on a\\ncomposite sense of objecthood. Despite its importance, there is no single,\\nunified account of objecthood, though multiple theoretical frameworks provide\\ninsights. In the first part of this paper, we present a comprehensive overview\\nof the main theoretical frameworks in objecthood research - Gestalt psychology,\\nenactive cognition, and developmental psychology - and identify the core\\ncapabilities each framework attributes to object understanding, as well as what\\nfunctional roles they play in shaping world models in biological agents. Given\\nthe foundational role of objecthood in world modelling, understanding\\nobjecthood is also essential in AI. In the second part of the paper, we\\nevaluate how current AI paradigms approach and test objecthood capabilities\\ncompared to those in cognitive science. We define an AI paradigm as a\\ncombination of how objecthood is conceptualised, the methods used for studying\\nobjecthood, the data utilised, and the evaluation techniques. We find that,\\nwhilst benchmarks can detect that AI systems model isolated aspects of\\nobjecthood, the benchmarks cannot detect when AI systems lack functional\\nintegration across these capabilities, not solving the objecthood challenge\\nfully. Finally, we explore novel evaluation approaches that align with the\\nintegrated vision of objecthood outlined in this paper. These methods are\\npromising candidates for advancing from isolated object capabilities toward\\ngeneral-purpose AI with genuine object understanding in real-world contexts.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CV,cs.LG\", \"published\": \"2025-03-27T16:35:02Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04722v1\", \"title\": \"TactileNet: Bridging the Accessibility Gap with AI-Generated Tactile\\n  Graphics for Individuals with Vision Impairment\", \"summary\": \"Tactile graphics are essential for providing access to visual information for\\nthe 43 million people globally living with vision loss, as estimated by global\\nprevalence data. However, traditional methods for creating these tactile\\ngraphics are labor-intensive and struggle to meet demand. We introduce\\nTactileNet, the first comprehensive dataset and AI-driven framework for\\ngenerating tactile graphics using text-to-image Stable Diffusion (SD) models.\\nBy integrating Low-Rank Adaptation (LoRA) and DreamBooth, our method fine-tunes\\nSD models to produce high-fidelity, guideline-compliant tactile graphics while\\nreducing computational costs. Evaluations involving tactile experts show that\\ngenerated graphics achieve 92.86% adherence to tactile standards and 100%\\nalignment with natural images in posture and features. Our framework also\\ndemonstrates scalability, generating 32,000 images (7,050 filtered for quality)\\nacross 66 classes, with prompt editing enabling customizable outputs (e.g.,\\nadding/removing details). Our work empowers designers to focus on refinement,\\nsignificantly accelerating accessibility efforts. It underscores the\\ntransformative potential of AI for social good, offering a scalable solution to\\nbridge the accessibility gap in education and beyond.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-07T04:21:31Z\"}"}

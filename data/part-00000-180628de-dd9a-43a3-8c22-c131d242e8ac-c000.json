{"value":"{\"aid\": \"http://arxiv.org/abs/2503.21686v1\", \"title\": \"Molecular Quantum Transformer\", \"summary\": \"The Transformer model, renowned for its powerful attention mechanism, has\\nachieved state-of-the-art performance in various artificial intelligence tasks\\nbut faces challenges such as high computational cost and memory usage.\\nResearchers are exploring quantum computing to enhance the Transformer's\\ndesign, though it still shows limited success with classical data. With a\\ngrowing focus on leveraging quantum machine learning for quantum data,\\nparticularly in quantum chemistry, we propose the Molecular Quantum Transformer\\n(MQT) for modeling interactions in molecular quantum systems. By utilizing\\nquantum circuits to implement the attention mechanism on the molecular\\nconfigurations, MQT can efficiently calculate ground-state energies for all\\nconfigurations. Numerical demonstrations show that in calculating ground-state\\nenergies for H_2, LiH, BeH_2, and H_4, MQT outperforms the classical\\nTransformer, highlighting the promise of quantum effects in Transformer\\nstructures. Furthermore, its pretraining capability on diverse molecular data\\nfacilitates the efficient learning of new molecules, extending its\\napplicability to complex molecular systems with minimal additional effort. Our\\nmethod offers an alternative to existing quantum algorithms for estimating\\nground-state energies, opening new avenues in quantum chemistry and materials\\nscience.\", \"main_category\": \"quant-ph\", \"categories\": \"quant-ph,cs.LG\", \"published\": \"2025-03-27T16:54:15Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06704v1\", \"title\": \"CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers\", \"summary\": \"Transformers have driven remarkable breakthroughs in natural language\\nprocessing and computer vision, yet their standard attention mechanism still\\nimposes O(N^2) complexity, hindering scalability to longer sequences. We\\nintroduce Circular-convolutional ATtention (CAT), a Fourier-based approach that\\nefficiently applies circular convolutions to reduce complexity without\\nsacrificing representational power. CAT achieves O(NlogN) computations,\\nrequires fewer learnable parameters by streamlining fully-connected layers, and\\nintroduces no heavier operations, resulting in consistent accuracy improvements\\nand about a 10% speedup in naive PyTorch implementations on large-scale\\nbenchmarks such as ImageNet-1k and WikiText-103. Grounded in an\\nengineering-isomorphism framework, CAT's design not only offers practical\\nefficiency and ease of implementation but also provides insights to guide the\\ndevelopment of next-generation, high-performance Transformer architectures.\\nFinally, our ablation studies highlight the key conditions underlying CAT's\\nsuccess, shedding light on broader principles for scalable attention\\nmechanisms.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL,cs.CV\", \"published\": \"2025-04-09T09:08:26Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24102v1\", \"title\": \"Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?\", \"summary\": \"Low-Resource Languages (LRLs) present significant challenges in natural\\nlanguage processing due to their limited linguistic resources and\\nunderrepresentation in standard datasets. While recent advancements in Large\\nLanguage Models (LLMs) and Neural Machine Translation (NMT) have substantially\\nimproved translation capabilities for high-resource languages, performance\\ndisparities persist for LRLs, particularly impacting privacy-sensitive and\\nresource-constrained scenarios. This paper systematically evaluates the\\nlimitations of current LLMs across 200 languages using benchmarks such as\\nFLORES-200. We also explore alternative data sources, including news articles\\nand bilingual dictionaries, and demonstrate how knowledge distillation from\\nlarge pre-trained models can significantly improve smaller LRL translations.\\nAdditionally, we investigate various fine-tuning strategies, revealing that\\nincremental enhancements markedly reduce performance gaps on smaller LLMs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-03-31T13:56:03Z\"}"}

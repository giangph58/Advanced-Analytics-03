{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05262v1\", \"title\": \"Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning\\n  vs. Memorization in Large Language Models\", \"summary\": \"Despite high benchmark scores, Large Language Models (LLMs) often fail simple\\nproblem, raising a critical question: Do LLMs learn mathematical principles or\\nmerely memorize patterns? Rather than designing increasingly complex benchmarks\\nlike recent works, we investigate this using elementary two-integer addition\\n($0$ to $2^{64}$), probing two core properties: commutativity ($A+B=B+A$) and\\ncompositional generalization (via isomorphic symbolic mappings, e.g., $7\\n\\\\rightarrow y$). While state-of-the-art LLMs achieve 73.8-99.8\\\\% accuracy on\\nnumerical addition, performance collapses to $\\\\leq$7.5\\\\% under symbolic\\nmapping, indicating failure to generalize learned rules. Non-monotonic\\nperformance scaling with digit count and frequent commutativity violations\\n(over 1,700 cases of $A+B \\\\neq B+A$) further support this. Explicitly providing\\naddition rules degrades performance by 81.2\\\\% on average, while\\nself-explanation maintains baseline accuracy, suggesting LLM arithmetic\\nprocessing is misaligned with human-defined principles. Our findings indicate\\ncurrent LLMs rely on memory pattern over genuine rule learning, highlighting\\narchitectural limitations and the need for new approaches to achieve true\\nmathematical reasoning.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-07T16:57:10Z\"}"}

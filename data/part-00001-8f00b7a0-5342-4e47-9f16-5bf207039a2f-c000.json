{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02277v1\", \"title\": \"Beyond Conventional Transformers: The Medical X-ray Attention (MXA)\\n  Block for Improved Multi-Label Diagnosis Using Knowledge Distillation\", \"summary\": \"Medical imaging, particularly X-ray analysis, often involves detecting\\nmultiple conditions simultaneously within a single scan, making multi-label\\nclassification crucial for real-world clinical applications. We present the\\nMedical X-ray Attention (MXA) block, a novel attention mechanism tailored\\nspecifically to address the unique challenges of X-ray abnormality detection.\\nThe MXA block enhances traditional Multi-Head Self Attention (MHSA) by\\nintegrating a specialized module that efficiently captures both detailed local\\ninformation and broader global context. To the best of our knowledge, this is\\nthe first work to propose a task-specific attention mechanism for diagnosing\\nchest X-rays, as well as to attempt multi-label classification using an\\nEfficient Vision Transformer (EfficientViT). By embedding the MXA block within\\nthe EfficientViT architecture and employing knowledge distillation, our\\nproposed model significantly improves performance on the CheXpert dataset, a\\nwidely used benchmark for multi-label chest X-ray abnormality detection. Our\\napproach achieves an area under the curve (AUC) of 0.85, an absolute\\nimprovement of 0.19 compared to our baseline model's AUC of 0.66, corresponding\\nto a substantial approximate 233% relative improvement over random guessing\\n(AUC = 0.5).\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.LG\", \"published\": \"2025-04-03T04:55:42Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02217v1\", \"title\": \"The Plot Thickens: Quantitative Part-by-Part Exploration of MLLM\\n  Visualization Literacy\", \"summary\": \"Multimodal Large Language Models (MLLMs) can interpret data visualizations,\\nbut what makes a visualization understandable to these models? Do factors like\\ncolor, shape, and text influence legibility, and how does this compare to human\\nperception? In this paper, we build on prior work to systematically assess\\nwhich visualization characteristics impact MLLM interpretability. We expanded\\nthe Visualization Literacy Assessment Test (VLAT) test set from 12 to 380\\nvisualizations by varying plot types, colors, and titles. This allowed us to\\nstatistically analyze how these features affect model performance. Our findings\\nsuggest that while color palettes have no significant impact on accuracy, plot\\ntypes and the type of title significantly affect MLLM performance. We observe\\nsimilar trends for model omissions. Based on these insights, we look into which\\nplot types are beneficial for MLLMs in different tasks and propose\\nvisualization design principles that enhance MLLM readability. Additionally, we\\nmake the extended VLAT test set, VLAT ex, publicly available on\\nhttps://osf.io/ermwx/ together with our supplemental material for future model\\ntesting and evaluation.\", \"main_category\": \"cs.HC\", \"categories\": \"cs.HC\", \"published\": \"2025-04-03T02:13:00Z\"}"}

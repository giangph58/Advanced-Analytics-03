{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07513v1\", \"title\": \"GPT Carry-On: Training Foundation Model for Customization Could Be\\n  Simple, Scalable and Affordable\", \"summary\": \"Modern large language foundation models (LLM) have now entered the daily\\nlives of millions of users. We ask a natural question whether it is possible to\\ncustomize LLM for every user or every task. From system and industrial economy\\nconsideration, general continue-training or fine-tuning still require\\nsubstantial computation and memory of training GPU nodes, whereas most\\ninference nodes under deployment, possibly with lower-end GPUs, are configured\\nto make forward pass fastest possible. We propose a framework to take full\\nadvantages of existing LLMs and systems of online service. We train an\\nadditional branch of transformer blocks on the final-layer embedding of\\npretrained LLMs, which is the base, then a carry-on module merge the base\\nmodels to compose a customized LLM. We can mix multiple layers, or multiple\\nLLMs specialized in different domains such as chat, coding, math, to form a new\\nmixture of LLM that best fit a new task. As the base model don't need to update\\nparameters, we are able to outsource most computation of the training job on\\ninference nodes, and only train a lightweight carry-on on training nodes, where\\nwe consume less than 1GB GPU memory to train a 100M carry-on layer on 30B LLM.\\nWe tested Qwen and DeepSeek opensourced models for continue-pretraining and got\\nfaster loss convergence. We use it to improve solving math questions with\\nextremely small computation and model size, with 1000 data samples of\\nchain-of-thoughts, and as small as 1 MB parameters of two layer layer carry-on,\\nand the results are promising.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.DC,stat.ML\", \"published\": \"2025-04-10T07:15:40Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07433v1\", \"title\": \"From Token to Line: Enhancing Code Generation with a Long-Term\\n  Perspective\", \"summary\": \"The emergence of large language models (LLMs) has significantly promoted the\\ndevelopment of code generation task, sparking a surge in pertinent literature.\\nCurrent research is hindered by redundant generation results and a tendency to\\noverfit local patterns in the short term. Although existing studies attempt to\\nalleviate the issue by adopting a multi-token prediction strategy, there\\nremains limited focus on choosing the appropriate processing length for\\ngenerations. By analyzing the attention between tokens during the generation\\nprocess of LLMs, it can be observed that the high spikes of the attention\\nscores typically appear at the end of lines. This insight suggests that it is\\nreasonable to treat each line of code as a fundamental processing unit and\\ngenerate them sequentially. Inspired by this, we propose the \\\\textbf{LSR-MCTS}\\nalgorithm, which leverages MCTS to determine the code line-by-line and select\\nthe optimal path. Further, we integrate a self-refine mechanism at each node to\\nenhance diversity and generate higher-quality programs through error\\ncorrection. Extensive experiments and comprehensive analyses on three public\\ncoding benchmarks demonstrate that our method outperforms the state-of-the-art\\nperformance approaches.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-10T04:03:25Z\"}"}

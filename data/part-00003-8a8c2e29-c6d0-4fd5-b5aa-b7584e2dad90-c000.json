{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06027v1\", \"title\": \"OSDM-MReg: Multimodal Image Registration based One Step Diffusion Model\", \"summary\": \"Multimodal remote sensing image registration aligns images from different\\nsensors for data fusion and analysis. However, current methods often fail to\\nextract modality-invariant features when aligning image pairs with large\\nnonlinear radiometric differences. To address this issues, we propose\\nOSDM-MReg, a novel multimodal image registration framework based image-to-image\\ntranslation to eliminate the gap of multimodal images. Firstly, we propose a\\nnovel one-step unaligned target-guided conditional denoising diffusion\\nprobabilistic models(UTGOS-CDDPM)to translate multimodal images into a unified\\ndomain. In the inference stage, traditional conditional DDPM generate\\ntranslated source image by a large number of iterations, which severely slows\\ndown the image registration task. To address this issues, we use the unaligned\\ntraget image as a condition to promote the generation of low-frequency features\\nof the translated source image. Furthermore, during the training stage, we add\\nthe inverse process of directly predicting the translated image to ensure that\\nthe translated source image can be generated in one step during the testing\\nstage. Additionally, to supervised the detail features of translated source\\nimage, we propose a new perceptual loss that focuses on the high-frequency\\nfeature differences between the translated and ground-truth images. Finally, a\\nmultimodal multiscale image registration network (MM-Reg) fuse the multimodal\\nfeature of the unimodal images and multimodal images by proposed multimodal\\nfeature fusion strategy. Experiments demonstrate superior accuracy and\\nefficiency across various multimodal registration tasks, particularly for\\nSAR-optical image pairs.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,eess.IV\", \"published\": \"2025-04-08T13:32:56Z\"}"}

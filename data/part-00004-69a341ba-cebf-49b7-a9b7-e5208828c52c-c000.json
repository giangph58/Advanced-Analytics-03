{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01708v1\", \"title\": \"TransforMerger: Transformer-based Voice-Gesture Fusion for Robust\\n  Human-Robot Communication\", \"summary\": \"As human-robot collaboration advances, natural and flexible communication\\nmethods are essential for effective robot control. Traditional methods relying\\non a single modality or rigid rules struggle with noisy or misaligned data as\\nwell as with object descriptions that do not perfectly fit the predefined\\nobject names (e.g. 'Pick that red object'). We introduce TransforMerger, a\\ntransformer-based reasoning model that infers a structured action command for\\nrobotic manipulation based on fused voice and gesture inputs. Our approach\\nmerges multimodal data into a single unified sentence, which is then processed\\nby the language model. We employ probabilistic embeddings to handle uncertainty\\nand we integrate contextual scene understanding to resolve ambiguous references\\n(e.g., gestures pointing to multiple objects or vague verbal cues like \\\"this\\\").\\nWe evaluate TransforMerger in simulated and real-world experiments,\\ndemonstrating its robustness to noise, misalignment, and missing information.\\nOur results show that TransforMerger outperforms deterministic baselines,\\nespecially in scenarios requiring more contextual knowledge, enabling more\\nrobust and flexible human-robot communication. Code and datasets are available\\nat: http://imitrob.ciirc.cvut.cz/publications/transformerger.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.HC,cs.LG\", \"published\": \"2025-04-02T13:15:59Z\"}"}

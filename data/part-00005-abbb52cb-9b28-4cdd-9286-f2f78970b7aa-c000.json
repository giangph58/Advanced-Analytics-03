{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04842v1\", \"title\": \"FantasyTalking: Realistic Talking Portrait Generation via Coherent\\n  Motion Synthesis\", \"summary\": \"Creating a realistic animatable avatar from a single static portrait remains\\nchallenging. Existing approaches often struggle to capture subtle facial\\nexpressions, the associated global body movements, and the dynamic background.\\nTo address these limitations, we propose a novel framework that leverages a\\npretrained video diffusion transformer model to generate high-fidelity,\\ncoherent talking portraits with controllable motion dynamics. At the core of\\nour work is a dual-stage audio-visual alignment strategy. In the first stage,\\nwe employ a clip-level training scheme to establish coherent global motion by\\naligning audio-driven dynamics across the entire scene, including the reference\\nportrait, contextual objects, and background. In the second stage, we refine\\nlip movements at the frame level using a lip-tracing mask, ensuring precise\\nsynchronization with audio signals. To preserve identity without compromising\\nmotion flexibility, we replace the commonly used reference network with a\\nfacial-focused cross-attention module that effectively maintains facial\\nconsistency throughout the video. Furthermore, we integrate a motion intensity\\nmodulation module that explicitly controls expression and body motion\\nintensity, enabling controllable manipulation of portrait movements beyond mere\\nlip motion. Extensive experimental results show that our proposed approach\\nachieves higher quality with better realism, coherence, motion intensity, and\\nidentity preservation. Ours project page:\\nhttps://fantasy-amap.github.io/fantasy-talking/.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-07T08:56:01Z\"}"}

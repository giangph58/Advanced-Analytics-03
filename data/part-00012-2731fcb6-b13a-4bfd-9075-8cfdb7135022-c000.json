{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07878v1\", \"title\": \"Token Level Routing Inference System for Edge Devices\", \"summary\": \"The computational complexity of large language model (LLM) inference\\nsignificantly constrains their deployment efficiency on edge devices. In\\ncontrast, small language models offer faster decoding and lower resource\\nconsumption but often suffer from degraded response quality and heightened\\nsusceptibility to hallucinations. To address this trade-off, collaborative\\ndecoding, in which a large model assists in generating critical tokens, has\\nemerged as a promising solution. This paradigm leverages the strengths of both\\nmodel types by enabling high-quality inference through selective intervention\\nof the large model, while maintaining the speed and efficiency of the smaller\\nmodel. In this work, we present a novel collaborative decoding inference system\\nthat allows small models to perform on-device inference while selectively\\nconsulting a cloud-based large model for critical token generation. Remarkably,\\nthe system achieves a 60% performance gain on CommonsenseQA using only a 0.5B\\nmodel on an M1 MacBook, with under 7% of tokens generation uploaded to the\\nlarge model in the cloud.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.DC\", \"published\": \"2025-04-10T15:54:19Z\"}"}

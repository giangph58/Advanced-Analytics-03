{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24278v1\", \"title\": \"AutoEval: Autonomous Evaluation of Generalist Robot Manipulation\\n  Policies in the Real World\", \"summary\": \"Scalable and reproducible policy evaluation has been a long-standing\\nchallenge in robot learning. Evaluations are critical to assess progress and\\nbuild better policies, but evaluation in the real world, especially at a scale\\nthat would provide statistically reliable results, is costly in terms of human\\ntime and hard to obtain. Evaluation of increasingly generalist robot policies\\nrequires an increasingly diverse repertoire of evaluation environments, making\\nthe evaluation bottleneck even more pronounced. To make real-world evaluation\\nof robotic policies more practical, we propose AutoEval, a system to\\nautonomously evaluate generalist robot policies around the clock with minimal\\nhuman intervention. Users interact with AutoEval by submitting evaluation jobs\\nto the AutoEval queue, much like how software jobs are submitted with a cluster\\nscheduling system, and AutoEval will schedule the policies for evaluation\\nwithin a framework supplying automatic success detection and automatic scene\\nresets. We show that AutoEval can nearly fully eliminate human involvement in\\nthe evaluation process, permitting around the clock evaluations, and the\\nevaluation results correspond closely to ground truth evaluations conducted by\\nhand. To facilitate the evaluation of generalist policies in the robotics\\ncommunity, we provide public access to multiple AutoEval scenes in the popular\\nBridgeData robot setup with WidowX robot arms. In the future, we hope that\\nAutoEval scenes can be set up across institutions to form a diverse and\\ndistributed evaluation network.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.AI\", \"published\": \"2025-03-31T16:23:44Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06768v1\", \"title\": \"FedMerge: Federated Personalization via Model Merging\", \"summary\": \"One global model in federated learning (FL) might not be sufficient to serve\\nmany clients with non-IID tasks and distributions. While there has been\\nadvances in FL to train multiple global models for better personalization, they\\nonly provide limited choices to clients so local finetuning is still\\nindispensable. In this paper, we propose a novel ``FedMerge'' approach that can\\ncreate a personalized model per client by simply merging multiple global models\\nwith automatically optimized and customized weights. In FedMerge, a few global\\nmodels can serve many non-IID clients, even without further local finetuning.\\nWe formulate this problem as a joint optimization of global models and the\\nmerging weights for each client. Unlike existing FL approaches where the server\\nbroadcasts one or multiple global models to all clients, the server only needs\\nto send a customized, merged model to each client. Moreover, instead of\\nperiodically interrupting the local training and re-initializing it to a global\\nmodel, the merged model aligns better with each client's task and data\\ndistribution, smoothening the local-global gap between consecutive rounds\\ncaused by client drift. We evaluate FedMerge on three different non-IID\\nsettings applied to different domains with diverse tasks and data types, in\\nwhich FedMerge consistently outperforms existing FL approaches, including\\nclustering-based and mixture-of-experts (MoE) based methods.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-09T10:44:14Z\"}"}

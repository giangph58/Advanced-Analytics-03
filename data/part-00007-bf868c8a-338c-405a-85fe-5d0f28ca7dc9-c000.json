{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01924v1\", \"title\": \"Gen-C: Populating Virtual Worlds with Generative Crowds\", \"summary\": \"Over the past two decades, researchers have made significant advancements in\\nsimulating human crowds, yet these efforts largely focus on low-level tasks\\nlike collision avoidance and a narrow range of behaviors such as path following\\nand flocking. However, creating compelling crowd scenes demands more than just\\nfunctional movement-it requires capturing high-level interactions between\\nagents, their environment, and each other over time. To address this issue, we\\nintroduce Gen-C, a generative model to automate the task of authoring\\nhigh-level crowd behaviors. Gen-C bypasses the labor-intensive and challenging\\ntask of collecting and annotating real crowd video data by leveraging a large\\nlanguage model (LLM) to generate a limited set of crowd scenarios, which are\\nsubsequently expanded and generalized through simulations to construct\\ntime-expanded graphs that model the actions and interactions of virtual agents.\\nOur method employs two Variational Graph Auto-Encoders guided by a condition\\nprior network: one dedicated to learning a latent space for graph structures\\n(agent interactions) and the other for node features (agent actions and\\nnavigation). This setup enables the flexible generation of dynamic crowd\\ninteractions. The trained model can be conditioned on natural language,\\nempowering users to synthesize novel crowd behaviors from text descriptions. We\\ndemonstrate the effectiveness of our approach in two scenarios, a University\\nCampus and a Train Station, showcasing its potential for populating diverse\\nvirtual environments with agents exhibiting varied and dynamic behaviors that\\nreflect complex interactions and high-level decision-making patterns.\", \"main_category\": \"cs.GR\", \"categories\": \"cs.GR,cs.LG\", \"published\": \"2025-04-02T17:33:53Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23959v1\", \"title\": \"Local Information Matters: Inference Acceleration For Grounded\\n  Conversation Generation Models Through Adaptive Local-Aware Token Pruning\", \"summary\": \"Grounded Conversation Generation (GCG) is an emerging vision-language task\\nthat requires models to generate natural language responses seamlessly\\nintertwined with corresponding object segmentation masks. Recent models, such\\nas GLaMM and OMG-LLaVA, achieve pixel-level grounding but incur significant\\ncomputational costs due to processing a large number of visual tokens. Existing\\ntoken pruning methods, like FastV and PyramidDrop, fail to preserve the local\\nvisual features critical for accurate grounding, leading to substantial\\nperformance drops in GCG tasks. To address this, we propose Adaptive\\nLocal-Aware Token Pruning (ALTP), a simple yet effective framework that\\naccelerates GCG models by prioritizing local object information. ALTP\\nintroduces two key components: (1) Detail Density Capture (DDC), which uses\\nsuperpixel segmentation to retain tokens in object-centric regions, preserving\\nfine-grained details, and (2) Dynamic Density Formation (DDF), which\\ndynamically allocates tokens based on information density, ensuring higher\\nretention in semantically rich areas. Extensive experiments on the GranDf\\ndataset demonstrate that ALTP significantly outperforms existing token pruning\\nmethods, such as FastV and PyramidDrop, on both GLaMM and OMG-LLaVA models.\\nNotably, when applied to GLaMM, ALTP achieves a 90% reduction in visual tokens\\nwith a 4.9% improvement in AP50 and a 5.0% improvement in Recall compared to\\nPyramidDrop. Similarly, on OMG-LLaVA, ALTP improves AP by 2.1% and mIOU by 3.0%\\nat a 90% token reduction compared with PDrop.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-31T11:18:27Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07097v1\", \"title\": \"Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual\\n  Learning\", \"summary\": \"Continual learning in large language models (LLMs) is prone to catastrophic\\nforgetting, where adapting to new tasks significantly degrades performance on\\npreviously learned ones. Existing methods typically rely on low-rank,\\nparameter-efficient updates that limit the model's expressivity and introduce\\nadditional parameters per task, leading to scalability issues. To address these\\nlimitations, we propose a novel continual full fine-tuning approach leveraging\\nadaptive singular value decomposition (SVD). Our method dynamically identifies\\ntask-specific low-rank parameter subspaces and constrains updates to be\\northogonal to critical directions associated with prior tasks, thus effectively\\nminimizing interference without additional parameter overhead or storing\\nprevious task gradients. We evaluate our approach extensively on standard\\ncontinual learning benchmarks using both encoder-decoder (T5-Large) and\\ndecoder-only (LLaMA-2 7B) models, spanning diverse tasks including\\nclassification, generation, and reasoning. Empirically, our method achieves\\nstate-of-the-art results, up to 7% higher average accuracy than recent\\nbaselines like O-LoRA, and notably maintains the model's general linguistic\\ncapabilities, instruction-following accuracy, and safety throughout the\\ncontinual learning process by reducing forgetting to near-negligible levels.\\nOur adaptive SVD framework effectively balances model plasticity and knowledge\\nretention, providing a practical, theoretically grounded, and computationally\\nscalable solution for continual learning scenarios in large language models.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL,math.PR,stat.ML\", \"published\": \"2025-04-09T17:59:42Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24071v1\", \"title\": \"From Colors to Classes: Emergence of Concepts in Vision Transformers\", \"summary\": \"Vision Transformers (ViTs) are increasingly utilized in various computer\\nvision tasks due to their powerful representation capabilities. However, it\\nremains understudied how ViTs process information layer by layer. Numerous\\nstudies have shown that convolutional neural networks (CNNs) extract features\\nof increasing complexity throughout their layers, which is crucial for tasks\\nlike domain adaptation and transfer learning. ViTs, lacking the same inductive\\nbiases as CNNs, can potentially learn global dependencies from the first layers\\ndue to their attention mechanisms. Given the increasing importance of ViTs in\\ncomputer vision, there is a need to improve the layer-wise understanding of\\nViTs. In this work, we present a novel, layer-wise analysis of concepts encoded\\nin state-of-the-art ViTs using neuron labeling. Our findings reveal that ViTs\\nencode concepts with increasing complexity throughout the network. Early layers\\nprimarily encode basic features such as colors and textures, while later layers\\nrepresent more specific classes, including objects and animals. As the\\ncomplexity of encoded concepts increases, the number of concepts represented in\\neach layer also rises, reflecting a more diverse and specific set of features.\\nAdditionally, different pretraining strategies influence the quantity and\\ncategory of encoded concepts, with finetuning to specific downstream tasks\\ngenerally reducing the number of encoded concepts and shifting the concepts to\\nmore relevant categories.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG\", \"published\": \"2025-03-31T13:28:43Z\"}"}

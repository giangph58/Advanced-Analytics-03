{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24198v1\", \"title\": \"TwT: Thinking without Tokens by Habitual Reasoning Distillation with\\n  Multi-Teachers' Guidance\", \"summary\": \"Large Language Models (LLMs) have made significant strides in problem-solving\\nby incorporating reasoning processes. However, this enhanced reasoning\\ncapability results in an increased number of output tokens during inference,\\nleading to higher computational costs. To address this challenge, we propose\\nTwT (Thinking without Tokens), a method that reduces inference-time costs\\nthrough habitual reasoning distillation with multi-teachers' guidance, while\\nmaintaining high performance. Our approach introduces a Habitual Reasoning\\nDistillation method, which internalizes explicit reasoning into the model's\\nhabitual behavior through a Teacher-Guided compression strategy inspired by\\nhuman cognition. Additionally, we propose Dual-Criteria Rejection Sampling\\n(DCRS), a technique that generates a high-quality and diverse distillation\\ndataset using multiple teacher models, making our method suitable for\\nunsupervised scenarios. Experimental results demonstrate that TwT effectively\\nreduces inference costs while preserving superior performance, achieving up to\\na 13.6% improvement in accuracy with fewer output tokens compared to other\\ndistillation methods, offering a highly practical solution for efficient LLM\\ndeployment.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-03-31T15:16:31Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01792v1\", \"title\": \"UniViTAR: Unified Vision Transformer with Native Resolution\", \"summary\": \"Conventional Vision Transformer simplifies visual modeling by standardizing\\ninput resolutions, often disregarding the variability of natural visual data\\nand compromising spatial-contextual fidelity. While preliminary explorations\\nhave superficially investigated native resolution modeling, existing approaches\\nstill lack systematic analysis from a visual representation perspective. To\\nbridge this gap, we introduce UniViTAR, a family of homogeneous vision\\nfoundation models tailored for unified visual modality and native resolution\\nscenario in the era of multimodal. Our framework first conducts architectural\\nupgrades to the vanilla paradigm by integrating multiple advanced components.\\nBuilding upon these improvements, a progressive training paradigm is\\nintroduced, which strategically combines two core mechanisms: (1) resolution\\ncurriculum learning, transitioning from fixed-resolution pretraining to native\\nresolution tuning, thereby leveraging ViT's inherent adaptability to\\nvariable-length sequences, and (2) visual modality adaptation via inter-batch\\nimage-video switching, which balances computational efficiency with enhanced\\ntemporal reasoning. In parallel, a hybrid training framework further synergizes\\nsigmoid-based contrastive loss with feature distillation from a frozen teacher\\nmodel, thereby accelerating early-stage convergence. Finally, trained\\nexclusively on public datasets, externsive experiments across multiple model\\nscales from 0.3B to 1B demonstrate its effectiveness.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-02T14:59:39Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01801v1\", \"title\": \"Investigating and Scaling up Code-Switching for Multilingual Language\\n  Model Pre-Training\", \"summary\": \"Large language models (LLMs) exhibit remarkable multilingual capabilities\\ndespite the extreme language imbalance in the pre-training data. In this paper,\\nwe closely examine the reasons behind this phenomenon, focusing on the\\npre-training corpus. We find that the existence of code-switching, alternating\\nbetween different languages within a context, is key to multilingual\\ncapabilities. We conduct an analysis to investigate code-switching in the\\npre-training corpus, examining its presence and categorizing it into four types\\nwithin two quadrants. We then assess its impact on multilingual performance.\\nThese types of code-switching data are unbalanced in proportions and\\ndemonstrate different effects on facilitating language transfer. To better\\nexplore the power of code-switching for language alignment during pre-training,\\nwe investigate the strategy of synthetic code-switching. We continuously scale\\nup the synthetic code-switching data and observe remarkable improvements in\\nboth benchmarks and representation space. Extensive experiments indicate that\\nincorporating synthetic code-switching data enables better language alignment\\nand generalizes well to high, medium, and low-resource languages with\\npre-training corpora of varying qualities.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-02T15:09:58Z\"}"}

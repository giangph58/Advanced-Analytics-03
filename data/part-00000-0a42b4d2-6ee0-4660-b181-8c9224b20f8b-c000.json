{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23993v1\", \"title\": \"DenseFormer: Learning Dense Depth Map from Sparse Depth and Image via\\n  Conditional Diffusion Model\", \"summary\": \"The depth completion task is a critical problem in autonomous driving,\\ninvolving the generation of dense depth maps from sparse depth maps and RGB\\nimages. Most existing methods employ a spatial propagation network to\\niteratively refine the depth map after obtaining an initial dense depth. In\\nthis paper, we propose DenseFormer, a novel method that integrates the\\ndiffusion model into the depth completion task. By incorporating the denoising\\nmechanism of the diffusion model, DenseFormer generates the dense depth map by\\nprogressively refining an initial random depth distribution through multiple\\niterations. We propose a feature extraction module that leverages a feature\\npyramid structure, along with multi-layer deformable attention, to effectively\\nextract and integrate features from sparse depth maps and RGB images, which\\nserve as the guiding condition for the diffusion process. Additionally, this\\npaper presents a depth refinement module that applies multi-step iterative\\nrefinement across various ranges to the dense depth results generated by the\\ndiffusion process. The module utilizes image features enriched with multi-scale\\ninformation and sparse depth input to further enhance the accuracy of the\\npredicted depth map. Extensive experiments on the KITTI outdoor scene dataset\\ndemonstrate that DenseFormer outperforms classical depth completion methods.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-03-31T12:11:01Z\"}"}

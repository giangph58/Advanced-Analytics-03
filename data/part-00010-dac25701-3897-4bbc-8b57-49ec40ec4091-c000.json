{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06863v1\", \"title\": \"MovSAM: A Single-image Moving Object Segmentation Framework Based on\\n  Deep Thinking\", \"summary\": \"Moving object segmentation plays a vital role in understanding dynamic visual\\nenvironments. While existing methods rely on multi-frame image sequences to\\nidentify moving objects, single-image MOS is critical for applications like\\nmotion intention prediction and handling camera frame drops. However,\\nsegmenting moving objects from a single image remains challenging for existing\\nmethods due to the absence of temporal cues. To address this gap, we propose\\nMovSAM, the first framework for single-image moving object segmentation. MovSAM\\nleverages a Multimodal Large Language Model (MLLM) enhanced with\\nChain-of-Thought (CoT) prompting to search the moving object and generate text\\nprompts based on deep thinking for segmentation. These prompts are cross-fused\\nwith visual features from the Segment Anything Model (SAM) and a\\nVision-Language Model (VLM), enabling logic-driven moving object segmentation.\\nThe segmentation results then undergo a deep thinking refinement loop, allowing\\nMovSAM to iteratively improve its understanding of the scene context and\\ninter-object relationships with logical reasoning. This innovative approach\\nenables MovSAM to segment moving objects in single images by considering scene\\nunderstanding. We implement MovSAM in the real world to validate its practical\\napplication and effectiveness for autonomous driving scenarios where the\\nmulti-frame methods fail. Furthermore, despite the inherent advantage of\\nmulti-frame methods in utilizing temporal information, MovSAM achieves\\nstate-of-the-art performance across public MOS benchmarks, reaching 92.5\\\\% on\\nJ\\\\&F. Our implementation will be available at\\nhttps://github.com/IRMVLab/MovSAM.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-09T13:12:58Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24187v1\", \"title\": \"NeuRaLaTeX: A machine learning library written in pure LaTeX\", \"summary\": \"In this paper, we introduce NeuRaLaTeX, which we believe to be the first deep\\nlearning library written entirely in LaTeX. As part of your LaTeX document you\\ncan specify the architecture of a neural network and its loss functions, define\\nhow to generate or load training data, and specify training hyperparameters and\\nexperiments. When the document is compiled, the LaTeX compiler will generate or\\nload training data, train the network, run experiments, and generate figures.\\nThis paper generates a random 100 point spiral dataset, trains a two layer MLP\\non it, evaluates on a different random spiral dataset, produces plots and\\ntables of results. The paper took 48 hours to compile and the entire source\\ncode for NeuRaLaTeX is contained within the source code of the paper. We\\npropose two new metrics: the Written In Latex (WIL) metric measures the\\nproportion of a machine learning library that is written in pure LaTeX, while\\nthe Source Code Of Method in Source Code of Paper (SCOMISCOP) metric measures\\nthe proportion of a paper's implementation that is contained within the paper\\nsource. We are state-of-the-art for both metrics, outperforming the ResNet and\\nTransformer papers, as well as the PyTorch and Tensorflow libraries. Source\\ncode, documentation, videos, crypto scams and an invitation to invest in the\\ncommercialisation of NeuRaLaTeX are available at https://www.neuralatex.com\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-03-31T15:05:19Z\"}"}

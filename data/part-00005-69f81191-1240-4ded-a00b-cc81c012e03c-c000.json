{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02244v1\", \"title\": \"SocialGesture: Delving into Multi-person Gesture Understanding\", \"summary\": \"Previous research in human gesture recognition has largely overlooked\\nmulti-person interactions, which are crucial for understanding the social\\ncontext of naturally occurring gestures. This limitation in existing datasets\\npresents a significant challenge in aligning human gestures with other\\nmodalities like language and speech. To address this issue, we introduce\\nSocialGesture, the first large-scale dataset specifically designed for\\nmulti-person gesture analysis. SocialGesture features a diverse range of\\nnatural scenarios and supports multiple gesture analysis tasks, including\\nvideo-based recognition and temporal localization, providing a valuable\\nresource for advancing the study of gesture during complex social interactions.\\nFurthermore, we propose a novel visual question answering (VQA) task to\\nbenchmark vision language models'(VLMs) performance on social gesture\\nunderstanding. Our findings highlight several limitations of current gesture\\nrecognition models, offering insights into future directions for improvement in\\nthis field. SocialGesture is available at\\nhuggingface.co/datasets/IrohXu/SocialGesture.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-03T03:21:06Z\"}"}

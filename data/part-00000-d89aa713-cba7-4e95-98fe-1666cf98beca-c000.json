{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23982v1\", \"title\": \"Deep Nets as Hamiltonians\", \"summary\": \"Neural networks are complex functions of both their inputs and parameters.\\nMuch prior work in deep learning theory analyzes the distribution of network\\noutputs at a fixed a set of inputs (e.g. a training dataset) over random\\ninitializations of the network parameters. The purpose of this article is to\\nconsider the opposite situation: we view a randomly initialized Multi-Layer\\nPerceptron (MLP) as a Hamiltonian over its inputs. For typical realizations of\\nthe network parameters, we study the properties of the energy landscape induced\\nby this Hamiltonian, focusing on the structure of near-global minimum in the\\nlimit of infinite width. Specifically, we use the replica trick to perform an\\nexact analytic calculation giving the entropy (log volume of space) at a given\\nenergy. We further derive saddle point equations that describe the overlaps\\nbetween inputs sampled iid from the Gibbs distribution induced by the random\\nMLP. For linear activations we solve these saddle point equations exactly. But\\nwe also solve them numerically for a variety of depths and activation\\nfunctions, including $\\\\tanh, \\\\sin, \\\\text{ReLU}$, and shaped non-linearities. We\\nfind even at infinite width a rich range of behaviors. For some\\nnon-linearities, such as $\\\\sin$, for instance, we find that the landscapes of\\nrandom MLPs exhibit full replica symmetry breaking, while shallow $\\\\tanh$ and\\nReLU networks or deep shaped MLPs are instead replica symmetric.\", \"main_category\": \"cond-mat.dis-nn\", \"categories\": \"cond-mat.dis-nn,cond-mat.stat-mech,cs.AI,cs.LG,math.PR\", \"published\": \"2025-03-31T11:51:10Z\"}"}

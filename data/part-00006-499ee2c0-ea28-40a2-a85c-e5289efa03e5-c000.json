{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24277v1\", \"title\": \"Evaluating and Designing Sparse Autoencoders by Approximating\\n  Quasi-Orthogonality\", \"summary\": \"Sparse autoencoders (SAEs) have emerged as a workhorse of modern mechanistic\\ninterpretability, but leading SAE approaches with top-$k$ style activation\\nfunctions lack theoretical grounding for selecting the hyperparameter $k$. SAEs\\nare based on the linear representation hypothesis (LRH), which assumes that the\\nrepresentations of large language models (LLMs) are linearly encoded, and the\\nsuperposition hypothesis (SH), which states that there can be more features in\\nthe model than its dimensionality. We show that, based on the formal\\ndefinitions of the LRH and SH, the magnitude of sparse feature vectors (the\\nlatent representations learned by SAEs of the dense embeddings of LLMs) can be\\napproximated using their corresponding dense vector with a closed-form error\\nbound. To visualize this, we propose the ZF plot, which reveals a previously\\nunknown relationship between LLM hidden embeddings and SAE feature vectors,\\nallowing us to make the first empirical measurement of the extent to which\\nfeature vectors of pre-trained SAEs are over- or under-activated for a given\\ninput. Correspondingly, we introduce Approximate Feature Activation (AFA),\\nwhich approximates the magnitude of the ground-truth sparse feature vector, and\\npropose a new evaluation metric derived from AFA to assess the alignment\\nbetween inputs and activations. We also leverage AFA to introduce a novel SAE\\narchitecture, the top-AFA SAE, leading to SAEs that: (a) are more in line with\\ntheoretical justifications; and (b) obviate the need to tune SAE sparsity\\nhyperparameters. Finally, we empirically demonstrate that top-AFA SAEs achieve\\nreconstruction loss comparable to that of state-of-the-art top-k SAEs, without\\nrequiring the hyperparameter $k$ to be tuned. Our code is available at:\\nhttps://github.com/SewoongLee/top-afa-sae.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-03-31T16:22:11Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23819v1\", \"title\": \"Conformal uncertainty quantification to evaluate predictive fairness of\\n  foundation AI model for skin lesion classes across patient demographics\", \"summary\": \"Deep learning based diagnostic AI systems based on medical images are\\nstarting to provide similar performance as human experts. However these data\\nhungry complex systems are inherently black boxes and therefore slow to be\\nadopted for high risk applications like healthcare. This problem of lack of\\ntransparency is exacerbated in the case of recent large foundation models,\\nwhich are trained in a self supervised manner on millions of data points to\\nprovide robust generalisation across a range of downstream tasks, but the\\nembeddings generated from them happen through a process that is not\\ninterpretable, and hence not easily trustable for clinical applications. To\\naddress this timely issue, we deploy conformal analysis to quantify the\\npredictive uncertainty of a vision transformer (ViT) based foundation model\\nacross patient demographics with respect to sex, age and ethnicity for the\\ntasks of skin lesion classification using several public benchmark datasets.\\nThe significant advantage of this method is that conformal analysis is method\\nindependent and it not only provides a coverage guarantee at population level\\nbut also provides an uncertainty score for each individual. We used a\\nmodel-agnostic dynamic F1-score-based sampling during model training, which\\nhelped to stabilize the class imbalance and we investigate the effects on\\nuncertainty quantification (UQ) with or without this bias mitigation step. Thus\\nwe show how this can be used as a fairness metric to evaluate the robustness of\\nthe feature embeddings of the foundation model (Google DermFoundation) and thus\\nadvance the trustworthiness and fairness of clinical AI.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CV\", \"published\": \"2025-03-31T08:06:00Z\"}"}

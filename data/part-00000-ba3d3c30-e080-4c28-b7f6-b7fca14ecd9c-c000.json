{"value":"{\"aid\": \"http://arxiv.org/abs/2503.21747v1\", \"title\": \"CTRL-O: Language-Controllable Object-Centric Visual Representation\\n  Learning\", \"summary\": \"Object-centric representation learning aims to decompose visual scenes into\\nfixed-size vectors called \\\"slots\\\" or \\\"object files\\\", where each slot captures a\\ndistinct object. Current state-of-the-art object-centric models have shown\\nremarkable success in object discovery in diverse domains, including complex\\nreal-world scenes. However, these models suffer from a key limitation: they\\nlack controllability. Specifically, current object-centric models learn\\nrepresentations based on their preconceived understanding of objects, without\\nallowing user input to guide which objects are represented. Introducing\\ncontrollability into object-centric models could unlock a range of useful\\ncapabilities, such as the ability to extract instance-specific representations\\nfrom a scene. In this work, we propose a novel approach for user-directed\\ncontrol over slot representations by conditioning slots on language\\ndescriptions. The proposed ConTRoLlable Object-centric representation learning\\napproach, which we term CTRL-O, achieves targeted object-language binding in\\ncomplex real-world scenes without requiring mask supervision. Next, we apply\\nthese controllable slot representations on two downstream vision language\\ntasks: text-to-image generation and visual question answering. The proposed\\napproach enables instance-specific text-to-image generation and also achieves\\nstrong performance on visual question answering.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.LG\", \"published\": \"2025-03-27T17:53:50Z\"}"}

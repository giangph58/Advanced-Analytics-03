{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01429v1\", \"title\": \"Refining Interactions: Enhancing Anisotropy in Graph Neural Networks\\n  with Language Semantics\", \"summary\": \"The integration of Large Language Models (LLMs) with Graph Neural Networks\\n(GNNs) has recently been explored to enhance the capabilities of Text Attribute\\nGraphs (TAGs). Most existing methods feed textual descriptions of the graph\\nstructure or neighbouring nodes' text directly into LLMs. However, these\\napproaches often cause LLMs to treat structural information simply as general\\ncontextual text, thus limiting their effectiveness in graph-related tasks. In\\nthis paper, we introduce LanSAGNN (Language Semantic Anisotropic Graph Neural\\nNetwork), a framework that extends the concept of anisotropic GNNs to the\\nnatural language level. This model leverages LLMs to extract tailor-made\\nsemantic information for node pairs, effectively capturing the unique\\ninteractions within node relationships. In addition, we propose an efficient\\ndual-layer LLMs finetuning architecture to better align LLMs' outputs with\\ngraph tasks. Experimental results demonstrate that LanSAGNN significantly\\nenhances existing LLM-based methods without increasing complexity while also\\nexhibiting strong robustness against interference.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-02T07:32:45Z\"}"}

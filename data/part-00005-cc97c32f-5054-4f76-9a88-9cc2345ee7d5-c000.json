{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24374v1\", \"title\": \"ERUPT: Efficient Rendering with Unposed Patch Transformer\", \"summary\": \"This work addresses the problem of novel view synthesis in diverse scenes\\nfrom small collections of RGB images. We propose ERUPT (Efficient Rendering\\nwith Unposed Patch Transformer) a state-of-the-art scene reconstruction model\\ncapable of efficient scene rendering using unposed imagery. We introduce\\npatch-based querying, in contrast to existing pixel-based queries, to reduce\\nthe compute required to render a target view. This makes our model highly\\nefficient both during training and at inference, capable of rendering at 600\\nfps on commercial hardware. Notably, our model is designed to use a learned\\nlatent camera pose which allows for training using unposed targets in datasets\\nwith sparse or inaccurate ground truth camera pose. We show that our approach\\ncan generalize on large real-world data and introduce a new benchmark dataset\\n(MSVS-1M) for latent view synthesis using street-view imagery collected from\\nMapillary. In contrast to NeRF and Gaussian Splatting, which require dense\\nimagery and precise metadata, ERUPT can render novel views of arbitrary scenes\\nwith as few as five unposed input images. ERUPT achieves better rendered image\\nquality than current state-of-the-art methods for unposed image synthesis\\ntasks, reduces labeled data requirements by ~95\\\\% and decreases computational\\nrequirements by an order of magnitude, providing efficient novel view synthesis\\nfor diverse real-world scenes.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-31T17:53:05Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01452v1\", \"title\": \"BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting\\n  Binary Segmentation in Segment Anything Models\", \"summary\": \"Accurate segmentation of polyps and skin lesions is essential for diagnosing\\ncolorectal and skin cancers. While various segmentation methods for polyps and\\nskin lesions using fully supervised deep learning techniques have been\\ndeveloped, the pixel-level annotation of medical images by doctors is both\\ntime-consuming and costly. Foundational vision models like the Segment Anything\\nModel (SAM) have demonstrated superior performance; however, directly applying\\nSAM to medical segmentation may not yield satisfactory results due to the lack\\nof domain-specific medical knowledge. In this paper, we propose BiSeg-SAM, a\\nSAM-guided weakly supervised prompting and boundary refinement network for the\\nsegmentation of polyps and skin lesions. Specifically, we fine-tune SAM\\ncombined with a CNN module to learn local features. We introduce a WeakBox with\\ntwo functions: automatically generating box prompts for the SAM model and using\\nour proposed Multi-choice Mask-to-Box (MM2B) transformation for rough\\nmask-to-box conversion, addressing the mismatch between coarse labels and\\nprecise predictions. Additionally, we apply scale consistency (SC) loss for\\nprediction scale alignment. Our DetailRefine module enhances boundary precision\\nand segmentation accuracy by refining coarse predictions using a limited amount\\nof ground truth labels. This comprehensive approach enables BiSeg-SAM to\\nachieve excellent multi-task segmentation performance. Our method demonstrates\\nsignificant superiority over state-of-the-art (SOTA) methods when tested on\\nfive polyp datasets and one skin cancer dataset.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.LG\", \"published\": \"2025-04-02T08:04:37Z\"}"}

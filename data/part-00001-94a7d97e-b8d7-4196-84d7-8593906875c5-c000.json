{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06805v1\", \"title\": \"Robust Classification with Noisy Labels Based on Posterior Maximization\", \"summary\": \"Designing objective functions robust to label noise is crucial for real-world\\nclassification algorithms. In this paper, we investigate the robustness to\\nlabel noise of an $f$-divergence-based class of objective functions recently\\nproposed for supervised classification, herein referred to as $f$-PML. We show\\nthat, in the presence of label noise, any of the $f$-PML objective functions\\ncan be corrected to obtain a neural network that is equal to the one learned\\nwith the clean dataset. Additionally, we propose an alternative and novel\\ncorrection approach that, during the test phase, refines the posterior\\nestimated by the neural network trained in the presence of label noise. Then,\\nwe demonstrate that, even if the considered $f$-PML objective functions are not\\nsymmetric, they are robust to symmetric label noise for any choice of\\n$f$-divergence, without the need for any correction approach. This allows us to\\nprove that the cross-entropy, which belongs to the $f$-PML class, is robust to\\nsymmetric label noise. Finally, we show that such a class of objective\\nfunctions can be used together with refined training strategies, achieving\\ncompetitive performance against state-of-the-art techniques of classification\\nwith label noise.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-09T11:52:51Z\"}"}

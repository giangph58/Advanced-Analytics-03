{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05702v1\", \"title\": \"Evaluating Speech-to-Text Systems with PennSound\", \"summary\": \"A random sample of nearly 10 hours of speech from PennSound, the world's\\nlargest online collection of poetry readings and discussions, was used as a\\nbenchmark to evaluate several commercial and open-source speech-to-text\\nsystems. PennSound's wide variation in recording conditions and speech styles\\nmakes it a good representative for many other untranscribed audio collections.\\nReference transcripts were created by trained annotators, and system\\ntranscripts were produced from AWS, Azure, Google, IBM, NeMo, Rev.ai, Whisper,\\nand Whisper.cpp. Based on word error rate, Rev.ai was the top performer, and\\nWhisper was the top open source performer (as long as hallucinations were\\navoided). AWS had the best diarization error rates among three systems.\\nHowever, WER and DER differences were slim, and various tradeoffs may motivate\\nchoosing different systems for different end users. We also examine the issue\\nof hallucinations in Whisper. Users of Whisper should be cautioned to be aware\\nof runtime options, and whether the speed vs accuracy trade off is acceptable.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-08T05:49:53Z\"}"}

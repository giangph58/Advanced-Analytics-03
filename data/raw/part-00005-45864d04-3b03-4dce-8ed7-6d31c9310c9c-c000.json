{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03500v1\", \"title\": \"Task Reconstruction and Extrapolation for $\\u03c0_0$ using Text Latent\", \"summary\": \"Vision-language-action models (VLAs) often achieve high performance on\\ndemonstrated tasks but struggle significantly when required to extrapolate,\\ncombining skills learned from different tasks in novel ways. For instance, VLAs\\nmight successfully put the cream cheese in the bowl and put the bowl on top of\\nthe cabinet, yet still fail to put the cream cheese on top of the cabinet. In\\nthis work, we demonstrate that behaviors from distinct tasks can be effectively\\nrecombined by manipulating the VLA's internal representations at inference\\ntime. Concretely, we identify the text latent by averaging the text tokens'\\nhidden states across all demonstrated trajectories for a specific base task.\\nFor executing an extrapolated task, we can temporally interpolate the text\\nlatent of the two base tasks and add it back to the text hidden states, so\\nsub-behaviors from the two tasks will be activated sequentially. We evaluate\\nthis approach using the newly created libero-ood benchmark, featuring 20 tasks\\nextrapolated from standard LIBERO suites. The results on libero-ood show that\\nall SOTA VLAs achieve < 15% success rate, while $\\\\pi0$ with text latent\\ninterpolation reaches an 83% success rate. Further qualitative analysis reveals\\na tendency for VLAs to exhibit spatial overfitting, mapping object names to\\ndemonstrated locations rather than achieving genuine object and goal\\nunderstanding. Additionally, we find that decoding the text latent yields\\nhuman-unreadable prompts that can nevertheless instruct the VLA to achieve a\\n70% success rate on standard LIBERO suites, enabling private instruction or\\nbackdoor attacks.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO\", \"published\": \"2025-05-06T13:05:04Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16768v1\", \"title\": \"How Effective are Generative Large Language Models in Performing\\n  Requirements Classification?\", \"summary\": \"In recent years, transformer-based large language models (LLMs) have\\nrevolutionised natural language processing (NLP), with generative models\\nopening new possibilities for tasks that require context-aware text generation.\\nRequirements engineering (RE) has also seen a surge in the experimentation of\\nLLMs for different tasks, including trace-link detection, regulatory\\ncompliance, and others. Requirements classification is a common task in RE.\\nWhile non-generative LLMs like BERT have been successfully applied to this\\ntask, there has been limited exploration of generative LLMs. This gap raises an\\nimportant question: how well can generative LLMs, which produce context-aware\\noutputs, perform in requirements classification? In this study, we explore the\\neffectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing\\nboth binary and multi-class requirements classification. We design an extensive\\nexperimental study involving over 400 experiments across three widely used\\ndatasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes\\nthat while factors like prompt design and LLM architecture are universally\\nimportant, others-such as dataset variations-have a more situational impact,\\ndepending on the complexity of the classification task. This insight can guide\\nfuture model development and deployment strategies, focusing on optimising\\nprompt structures and aligning model architectures with task-specific needs for\\nimproved performance.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.SE\", \"published\": \"2025-04-23T14:41:11Z\"}"}

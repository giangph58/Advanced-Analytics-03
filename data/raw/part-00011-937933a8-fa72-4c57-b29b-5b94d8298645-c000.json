{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05283v1\", \"title\": \"Software Development Life Cycle Perspective: A Survey of Benchmarks for\\n  CodeLLMs and Agents\", \"summary\": \"Code large language models (CodeLLMs) and agents have shown great promise in\\ntackling complex software engineering tasks.Compared to traditional software\\nengineering methods, CodeLLMs and agents offer stronger abilities, and can\\nflexibly process inputs and outputs in both natural and code. Benchmarking\\nplays a crucial role in evaluating the capabilities of CodeLLMs and agents,\\nguiding their development and deployment. However, despite their growing\\nsignificance, there remains a lack of comprehensive reviews of benchmarks for\\nCodeLLMs and agents. To bridge this gap, this paper provides a comprehensive\\nreview of existing benchmarks for CodeLLMs and agents, studying and analyzing\\n181 benchmarks from 461 relevant papers, covering the different phases of the\\nsoftware development life cycle (SDLC). Our findings reveal a notable imbalance\\nin the coverage of current benchmarks, with approximately 60% focused on the\\nsoftware development phase in SDLC, while requirements engineering and software\\ndesign phases receive minimal attention at only 5% and 3%, respectively.\\nAdditionally, Python emerges as the dominant programming language across the\\nreviewed benchmarks. Finally, this paper highlights the challenges of current\\nresearch and proposes future directions, aiming to narrow the gap between the\\ntheoretical capabilities of CodeLLMs and agents and their application in\\nreal-world scenarios.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE,cs.AI\", \"published\": \"2025-05-08T14:27:45Z\"}"}

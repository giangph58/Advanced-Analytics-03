{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17584v1\", \"title\": \"L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\\n  Long-Context LLM Inference\", \"summary\": \"Large Language Models (LLMs) increasingly require processing long text\\nsequences, but GPU memory limitations force difficult trade-offs between memory\\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\\ncapacity remains constrained. Offloading data to host-side DIMMs improves\\ncapacity but introduces costly data swapping overhead. We identify that the\\ncritical memory bottleneck lies in the decoding phase of multi-head attention\\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\\nhigh bandwidth for attention computation. Our key insight reveals this\\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\\narchitectures, which offers scalability of both capacity and bandwidth.\\n  Based on this observation and insight, we propose L3, a hardware-software\\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\\ninnovations: First, hardware redesigns resolve data layout mismatches and\\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\\nutilization. Second, communication optimization enables hiding the data\\ntransfer overhead with the computation. Third, an adaptive scheduler\\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\\nEvaluations using real-world traces show L3 achieves up to 6.1$\\\\times$ speedup\\nover state-of-the-art HBM-PIM solutions while significantly improving batch\\nsizes.\", \"main_category\": \"cs.AR\", \"categories\": \"cs.AR,cs.LG\", \"published\": \"2025-04-24T14:14:07Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11840v1\", \"title\": \"GT-SVQ: A Linear-Time Graph Transformer for Node Classification Using\\n  Spiking Vector Quantization\", \"summary\": \"Graph Transformers (GTs), which simultaneously integrate message-passing and\\nself-attention mechanisms, have achieved promising empirical results in some\\ngraph prediction tasks. Although these approaches show the potential of\\nTransformers in capturing long-range graph topology information, issues\\nconcerning the quadratic complexity and high computing energy consumption\\nseverely limit the scalability of GTs on large-scale graphs. Recently, as\\nbrain-inspired neural networks, Spiking Neural Networks (SNNs), facilitate the\\ndevelopment of graph representation learning methods with lower computational\\nand storage overhead through the unique event-driven spiking neurons. Inspired\\nby these characteristics, we propose a linear-time Graph Transformer using\\nSpiking Vector Quantization (GT-SVQ) for node classification. GT-SVQ\\nreconstructs codebooks based on rate coding outputs from spiking neurons, and\\ninjects the codebooks into self-attention blocks to aggregate global\\ninformation in linear complexity. Besides, spiking vector quantization\\neffectively alleviates codebook collapse and the reliance on complex machinery\\n(distance measure, auxiliary loss, etc.) present in previous vector\\nquantization-based graph learning methods. In experiments, we compare GT-SVQ\\nwith other state-of-the-art baselines on node classification datasets ranging\\nfrom small to large. Experimental results show that GT-SVQ has achieved\\ncompetitive performances on most datasets while maintaining up to 130x faster\\ninference speed compared to other GTs.\", \"main_category\": \"cs.NE\", \"categories\": \"cs.NE,cs.LG\", \"published\": \"2025-04-16T07:57:42Z\"}"}

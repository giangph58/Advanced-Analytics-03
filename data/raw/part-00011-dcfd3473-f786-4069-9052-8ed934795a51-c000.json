{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04531v1\", \"title\": \"Overcoming Data Scarcity in Generative Language Modelling for\\n  Low-Resource Languages: A Systematic Review\", \"summary\": \"Generative language modelling has surged in popularity with the emergence of\\nservices such as ChatGPT and Google Gemini. While these models have\\ndemonstrated transformative potential in productivity and communication, they\\noverwhelmingly cater to high-resource languages like English. This has\\namplified concerns over linguistic inequality in natural language processing\\n(NLP). This paper presents the first systematic review focused specifically on\\nstrategies to address data scarcity in generative language modelling for\\nlow-resource languages (LRL). Drawing from 54 studies, we identify, categorise\\nand evaluate technical approaches, including monolingual data augmentation,\\nback-translation, multilingual training, and prompt engineering, across\\ngenerative tasks. We also analyse trends in architecture choices, language\\nfamily representation, and evaluation methods. Our findings highlight a strong\\nreliance on transformer-based models, a concentration on a small subset of\\nLRLs, and a lack of consistent evaluation across studies. We conclude with\\nrecommendations for extending these methods to a wider range of LRLs and\\noutline open challenges in building equitable generative language systems.\\nUltimately, this review aims to support researchers and developers in building\\ninclusive AI tools for underrepresented languages, a necessary step toward\\nempowering LRL speakers and the preservation of linguistic diversity in a world\\nincreasingly shaped by large-scale language technologies.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-05-07T16:04:45Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12175v1\", \"title\": \"Approximation Bounds for Transformer Networks with Application to\\n  Regression\", \"summary\": \"We explore the approximation capabilities of Transformer networks for\\nH\\\\\\\"older and Sobolev functions, and apply these results to address\\nnonparametric regression estimation with dependent observations. First, we\\nestablish novel upper bounds for standard Transformer networks approximating\\nsequence-to-sequence mappings whose component functions are H\\\\\\\"older continuous\\nwith smoothness index $\\\\gamma \\\\in (0,1]$. To achieve an approximation error\\n$\\\\varepsilon$ under the $L^p$-norm for $p \\\\in [1, \\\\infty]$, it suffices to use\\na fixed-depth Transformer network whose total number of parameters scales as\\n$\\\\varepsilon^{-d_x n / \\\\gamma}$. This result not only extends existing findings\\nto include the case $p = \\\\infty$, but also matches the best known upper bounds\\non number of parameters previously obtained for fixed-depth FNNs and RNNs.\\nSimilar bounds are also derived for Sobolev functions. Second, we derive\\nexplicit convergence rates for the nonparametric regression problem under\\nvarious $\\\\beta$-mixing data assumptions, which allow the dependence between\\nobservations to weaken over time. Our bounds on the sample complexity impose no\\nconstraints on weight magnitudes. Lastly, we propose a novel proof strategy to\\nestablish approximation bounds, inspired by the Kolmogorov-Arnold\\nrepresentation theorem. We show that if the self-attention layer in a\\nTransformer can perform column averaging, the network can approximate\\nsequence-to-sequence H\\\\\\\"older functions, offering new insights into the\\ninterpretability of self-attention mechanisms.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.LG\", \"published\": \"2025-04-16T15:25:58Z\"}"}

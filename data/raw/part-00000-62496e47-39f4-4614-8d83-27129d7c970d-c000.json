{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03679v1\", \"title\": \"CaRaFFusion: Improving 2D Semantic Segmentation with Camera-Radar Point\\n  Cloud Fusion and Zero-Shot Image Inpainting\", \"summary\": \"Segmenting objects in an environment is a crucial task for autonomous driving\\nand robotics, as it enables a better understanding of the surroundings of each\\nagent. Although camera sensors provide rich visual details, they are vulnerable\\nto adverse weather conditions. In contrast, radar sensors remain robust under\\nsuch conditions, but often produce sparse and noisy data. Therefore, a\\npromising approach is to fuse information from both sensors. In this work, we\\npropose a novel framework to enhance camera-only baselines by integrating a\\ndiffusion model into a camera-radar fusion architecture. We leverage radar\\npoint features to create pseudo-masks using the Segment-Anything model,\\ntreating the projected radar points as point prompts. Additionally, we propose\\na noise reduction unit to denoise these pseudo-masks, which are further used to\\ngenerate inpainted images that complete the missing information in the original\\nimages. Our method improves the camera-only segmentation baseline by 2.63% in\\nmIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on the\\nWaterscenes dataset. This demonstrates the effectiveness of our approach for\\nsemantic segmentation using camera-radar fusion under adverse weather\\nconditions.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-06T16:25:38Z\"}"}

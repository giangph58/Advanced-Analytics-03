{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01591v1\", \"title\": \"Leveraging Modality Tags for Enhanced Cross-Modal Video Retrieval\", \"summary\": \"Video retrieval requires aligning visual content with corresponding natural\\nlanguage descriptions. In this paper, we introduce Modality Auxiliary Concepts\\nfor Video Retrieval (MAC-VR), a novel approach that leverages modality-specific\\ntags -- automatically extracted from foundation models -- to enhance video\\nretrieval. We propose to align modalities in a latent space, along with\\nlearning and aligning auxiliary latent concepts, derived from the features of a\\nvideo and its corresponding caption. We introduce these auxiliary concepts to\\nimprove the alignment of visual and textual latent concepts, and so are able to\\ndistinguish concepts from one other. We conduct extensive experiments on five\\ndiverse datasets: MSR-VTT, DiDeMo, TGIF, Charades and YouCook2. The\\nexperimental results consistently demonstrate that modality-specific tags\\nimprove cross-modal alignment, outperforming current state-of-the-art methods\\nacross three datasets and performing comparably or better across the other two.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-02T10:56:01Z\"}"}

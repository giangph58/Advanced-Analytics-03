{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04457v1\", \"title\": \"Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale\\n  Data Restoration\", \"summary\": \"Training data cleaning is a new application for generative model-based speech\\nrestoration (SR). This paper introduces Miipher-2, an SR model designed for\\nmillion-hour scale data, for training data cleaning for large-scale generative\\nmodels like large language models. Key challenges addressed include\\ngeneralization to unseen languages, operation without explicit conditioning\\n(e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a\\nfrozen, pre-trained Universal Speech Model (USM), supporting over 300\\nlanguages, as a robust, conditioning-free feature extractor. To optimize\\nefficiency and minimize memory, Miipher-2 incorporates parallel adapters for\\npredicting clean USM features from noisy inputs and employs the WaneFit neural\\nvocoder for waveform synthesis. These components were trained on 3,000 hours of\\nmulti-lingual, studio-quality recordings with augmented degradations, while USM\\nparameters remained fixed. Experimental results demonstrate Miipher-2's\\nsuperior or comparable performance to conventional SR models in\\nword-error-rate, speaker similarity, and both objective and subjective sound\\nquality scores across all tested languages. Miipher-2 operates efficiently on\\nconsumer-grade accelerators, achieving a real-time factor of 0.0078, enabling\\nthe processing of a million-hour speech dataset in approximately three days\\nusing only 100 such accelerators.\", \"main_category\": \"cs.SD\", \"categories\": \"cs.SD,cs.CL,eess.AS\", \"published\": \"2025-05-07T14:27:46Z\"}"}

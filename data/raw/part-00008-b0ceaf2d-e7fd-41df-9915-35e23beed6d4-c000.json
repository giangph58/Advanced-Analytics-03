{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20493v1\", \"title\": \"Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM\\n  Reasoning via Adaptive Token Compression\", \"summary\": \"While reasoning large language models (LLMs) demonstrate remarkable\\nperformance across various tasks, they also contain notable security\\nvulnerabilities. Recent research has uncovered a \\\"thinking-stopped\\\"\\nvulnerability in DeepSeek-R1, where model-generated reasoning tokens can\\nforcibly interrupt the inference process, resulting in empty responses that\\ncompromise LLM-integrated applications. However, existing methods triggering\\nthis vulnerability require complex mathematical word problems with long\\nprompts--even exceeding 5,000 tokens. To reduce the token cost and formally\\ndefine this vulnerability, we propose a novel prompt injection attack named\\n\\\"Reasoning Interruption Attack\\\", based on adaptive token compression. We\\ndemonstrate that simple standalone arithmetic tasks can effectively trigger\\nthis vulnerability, and the prompts based on such tasks exhibit simpler logical\\nstructures than mathematical word problems. We develop a systematic approach to\\nefficiently collect attack prompts and an adaptive token compression framework\\nthat utilizes LLMs to automatically compress these prompts. Experiments show\\nour compression framework significantly reduces prompt length while maintaining\\neffective attack capabilities. We further investigate the attack's performance\\nvia output prefix and analyze the underlying causes of the vulnerability,\\nproviding valuable insights for improving security in reasoning LLMs.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.AI\", \"published\": \"2025-04-29T07:34:22Z\"}"}

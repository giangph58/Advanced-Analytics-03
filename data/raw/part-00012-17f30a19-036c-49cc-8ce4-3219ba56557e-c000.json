{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16518v1\", \"title\": \"Preconditioning Natural and Second Order Gradient Descent in Quantum\\n  Optimization: A Performance Benchmark\", \"summary\": \"The optimization of parametric quantum circuits is technically hindered by\\nthree major obstacles: the non-convex nature of the objective function, noisy\\ngradient evaluations, and the presence of barren plateaus. As a result, the\\nselection of classical optimizer becomes a critical factor in assessing and\\nexploiting quantum-classical applications. One promising approach to tackle\\nthese challenges involves incorporating curvature information into the\\nparameter update. The most prominent methods in this field are quasi-Newton and\\nquantum natural gradient methods, which can facilitate faster convergence\\ncompared to first-order approaches. Second order methods however exhibit a\\nsignificant trade-off between computational cost and accuracy, as well as\\nheightened sensitivity to noise. This study evaluates the performance of three\\nfamilies of optimizers on synthetically generated MaxCut problems on a shallow\\nQAOA algorithm. To address noise sensitivity and iteration cost, we demonstrate\\nthat incorporating secant-penalization in the BFGS update rule (SP-BFGS) yields\\nimproved outcomes for QAOA optimization problems, introducing a novel approach\\nto stabilizing BFGS updates against gradient noise.\", \"main_category\": \"cs.CE\", \"categories\": \"cs.CE,quant-ph\", \"published\": \"2025-04-23T08:44:18Z\"}"}

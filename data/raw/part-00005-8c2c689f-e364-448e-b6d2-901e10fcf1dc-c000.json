{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11059v1\", \"title\": \"Quantifying Group Fairness in Community Detection\", \"summary\": \"Understanding community structures is crucial for analyzing networks, as\\nnodes join communities that collectively shape large-scale networks. In\\nreal-world settings, the formation of communities is often impacted by several\\nsocial factors, such as ethnicity, gender, wealth, or other attributes. These\\nfactors may introduce structural inequalities; for instance, real-world\\nnetworks can have a few majority groups and many minority groups. Community\\ndetection algorithms, which identify communities based on network topology, may\\ngenerate unfair outcomes if they fail to account for existing structural\\ninequalities, particularly affecting underrepresented groups. In this work, we\\npropose a set of novel group fairness metrics to assess the fairness of\\ncommunity detection methods. Additionally, we conduct a comparative evaluation\\nof the most common community detection methods, analyzing the trade-off between\\nperformance and fairness. Experiments are performed on synthetic networks\\ngenerated using LFR, ABCD, and HICH-BA benchmark models, as well as on\\nreal-world networks. Our results demonstrate that the fairness-performance\\ntrade-off varies widely across methods, with no single class of approaches\\nconsistently excelling in both aspects. We observe that Infomap and\\nSignificance methods are high-performing and fair with respect to different\\ntypes of communities across most networks. The proposed metrics and findings\\nprovide valuable insights for designing fair and effective community detection\\nalgorithms.\", \"main_category\": \"cs.SI\", \"categories\": \"cs.SI\", \"published\": \"2025-04-15T10:45:58Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20468v1\", \"title\": \"Antidote: A Unified Framework for Mitigating LVLM Hallucinations in\\n  Counterfactual Presupposition and Object Perception\", \"summary\": \"Large Vision-Language Models (LVLMs) have achieved impressive results across\\nvarious cross-modal tasks. However, hallucinations, i.e., the models generating\\ncounterfactual responses, remain a challenge. Though recent studies have\\nattempted to alleviate object perception hallucinations, they focus on the\\nmodels' response generation, and overlooking the task question itself. This\\npaper discusses the vulnerability of LVLMs in solving counterfactual\\npresupposition questions (CPQs), where the models are prone to accept the\\npresuppositions of counterfactual objects and produce severe hallucinatory\\nresponses. To this end, we introduce \\\"Antidote\\\", a unified, synthetic\\ndata-driven post-training framework for mitigating both types of hallucination\\nabove. It leverages synthetic data to incorporate factual priors into questions\\nto achieve self-correction, and decouple the mitigation process into a\\npreference optimization problem. Furthermore, we construct \\\"CP-Bench\\\", a novel\\nbenchmark to evaluate LVLMs' ability to correctly handle CPQs and produce\\nfactual responses. Applied to the LLaVA series, Antidote can simultaneously\\nenhance performance on CP-Bench by over 50%, POPE by 1.8-3.3%, and CHAIR & SHR\\nby 30-50%, all without relying on external supervision from stronger LVLMs or\\nhuman feedback and introducing noticeable catastrophic forgetting issues.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-29T07:05:24Z\"}"}

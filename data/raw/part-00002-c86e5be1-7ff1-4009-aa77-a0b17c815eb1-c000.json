{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16795v1\", \"title\": \"Random Long-Context Access for Mamba via Hardware-aligned Hierarchical\\n  Sparse Attention\", \"summary\": \"A key advantage of Recurrent Neural Networks (RNNs) over Transformers is\\ntheir linear computational and space complexity enables faster training and\\ninference for long sequences. However, RNNs are fundamentally unable to\\nrandomly access historical context, and simply integrating attention mechanisms\\nmay undermine their efficiency advantages. To overcome this limitation, we\\npropose \\\\textbf{H}ierarchical \\\\textbf{S}parse \\\\textbf{A}ttention (HSA), a novel\\nattention mechanism that enhances RNNs with long-range random access\\nflexibility while preserving their merits in efficiency and length\\ngeneralization. HSA divides inputs into chunks, selecting the top-$k$ chunks\\nand hierarchically aggregates information. The core innovation lies in learning\\ntoken-to-chunk relevance based on fine-grained token-level information inside\\neach chunk. This approach enhances the precision of chunk selection across both\\nin-domain and out-of-domain context lengths. To make HSA efficient, we further\\nintroduce a hardware-aligned kernel design. By combining HSA with Mamba, we\\nintroduce RAMba, which achieves perfect accuracy in passkey retrieval across 64\\nmillion contexts despite pre-training on only 4K-length contexts, and\\nsignificant improvements on various downstream tasks, with nearly constant\\nmemory footprint. These results show RAMba's huge potential in long-context\\nmodeling.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-23T15:15:06Z\"}"}

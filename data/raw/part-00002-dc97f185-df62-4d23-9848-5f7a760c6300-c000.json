{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16064v1\", \"title\": \"Boosting Generative Image Modeling via Joint Image-Feature Synthesis\", \"summary\": \"Latent diffusion models (LDMs) dominate high-quality image generation, yet\\nintegrating representation learning with generative modeling remains a\\nchallenge. We introduce a novel generative image modeling framework that\\nseamlessly bridges this gap by leveraging a diffusion model to jointly model\\nlow-level image latents (from a variational autoencoder) and high-level\\nsemantic features (from a pretrained self-supervised encoder like DINO). Our\\nlatent-semantic diffusion approach learns to generate coherent image-feature\\npairs from pure noise, significantly enhancing both generative quality and\\ntraining efficiency, all while requiring only minimal modifications to standard\\nDiffusion Transformer architectures. By eliminating the need for complex\\ndistillation objectives, our unified design simplifies training and unlocks a\\npowerful new inference strategy: Representation Guidance, which leverages\\nlearned semantics to steer and refine image generation. Evaluated in both\\nconditional and unconditional settings, our method delivers substantial\\nimprovements in image quality and training convergence speed, establishing a\\nnew direction for representation-aware generative modeling.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-22T17:41:42Z\"}"}

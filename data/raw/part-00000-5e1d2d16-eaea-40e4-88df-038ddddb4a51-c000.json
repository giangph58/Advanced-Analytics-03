{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15604v1\", \"title\": \"Exploring Next Token Prediction in Theory of Mind (ToM) Tasks:\\n  Comparative Experiments with GPT-2 and LLaMA-2 AI Models\", \"summary\": \"Language models have made significant progress in generating coherent text\\nand predicting next tokens based on input prompts. This study compares the\\nnext-token prediction performance of two well-known models: OpenAI's GPT-2 and\\nMeta's Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. To evaluate their\\ncapabilities, we built a dataset from 10 short stories sourced from the Explore\\nToM Dataset. We enhanced these stories by programmatically inserting additional\\nsentences (infills) using GPT-4, creating variations that introduce different\\nlevels of contextual complexity. This setup enables analysis of how increasing\\ncontext affects model performance. We tested both models under four temperature\\nsettings (0.01, 0.5, 1.0, 2.0) and evaluated their ability to predict the next\\ntoken across three reasoning levels. Zero-order reasoning involves tracking the\\nstate, either current (ground truth) or past (memory). First-order reasoning\\nconcerns understanding another's mental state (e.g., \\\"Does Anne know the apple\\nis salted?\\\"). Second-order reasoning adds recursion (e.g., \\\"Does Anne think\\nthat Charles knows the apple is salted?\\\").\\n  Our results show that adding more infill sentences slightly reduces\\nprediction accuracy, as added context increases complexity and ambiguity.\\nLlama-2 consistently outperforms GPT-2 in prediction accuracy, especially at\\nlower temperatures, demonstrating greater confidence in selecting the most\\nprobable token. As reasoning complexity rises, model responses diverge more.\\nNotably, GPT-2 and Llama-2 display greater variability in predictions during\\nfirst- and second-order reasoning tasks. These findings illustrate how model\\narchitecture, temperature, and contextual complexity influence next-token\\nprediction, contributing to a better understanding of the strengths and\\nlimitations of current language models.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-22T05:52:55Z\"}"}

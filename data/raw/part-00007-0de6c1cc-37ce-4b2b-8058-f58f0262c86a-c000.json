{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13157v1\", \"title\": \"AerialMegaDepth: Learning Aerial-Ground Reconstruction and View\\n  Synthesis\", \"summary\": \"We explore the task of geometric reconstruction of images captured from a\\nmixture of ground and aerial views. Current state-of-the-art learning-based\\napproaches fail to handle the extreme viewpoint variation between aerial-ground\\nimage pairs. Our hypothesis is that the lack of high-quality, co-registered\\naerial-ground datasets for training is a key reason for this failure. Such data\\nis difficult to assemble precisely because it is difficult to reconstruct in a\\nscalable way. To overcome this challenge, we propose a scalable framework\\ncombining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google\\nEarth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The\\npseudo-synthetic data simulates a wide range of aerial viewpoints, while the\\nreal, crowd-sourced images help improve visual fidelity for ground-level images\\nwhere mesh-based renderings lack sufficient detail, effectively bridging the\\ndomain gap between real images and pseudo-synthetic renderings. Using this\\nhybrid dataset, we fine-tune several state-of-the-art algorithms and achieve\\nsignificant improvements on real-world, zero-shot aerial-ground tasks. For\\nexample, we observe that baseline DUSt3R localizes fewer than 5% of\\naerial-ground pairs within 5 degrees of camera rotation error, while\\nfine-tuning with our data raises accuracy to nearly 56%, addressing a major\\nfailure point in handling large viewpoint changes. Beyond camera estimation and\\nscene reconstruction, our dataset also improves performance on downstream tasks\\nlike novel-view synthesis in challenging aerial-ground scenarios, demonstrating\\nthe practical value of our approach in real-world applications.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-17T17:57:05Z\"}"}

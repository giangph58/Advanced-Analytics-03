{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12140v1\", \"title\": \"Multilingual Contextualization of Large Language Models for\\n  Document-Level Machine Translation\", \"summary\": \"Large language models (LLMs) have demonstrated strong performance in\\nsentence-level machine translation, but scaling to document-level translation\\nremains challenging, particularly in modeling long-range dependencies and\\ndiscourse phenomena across sentences and paragraphs. In this work, we propose a\\nmethod to improve LLM-based long-document translation through targeted\\nfine-tuning on high-quality document-level data, which we curate and introduce\\nas DocBlocks. Our approach supports multiple translation paradigms, including\\ndirect document-to-document and chunk-level translation, by integrating\\ninstructions both with and without surrounding context. This enables models to\\nbetter capture cross-sentence dependencies while maintaining strong\\nsentence-level translation performance. Experimental results show that\\nincorporating multiple translation paradigms improves document-level\\ntranslation quality and inference speed compared to prompting and agent-based\\nmethods.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-16T14:52:22Z\"}"}

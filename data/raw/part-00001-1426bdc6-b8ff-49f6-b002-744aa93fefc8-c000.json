{"value":"{\"aid\": \"http://arxiv.org/abs/2503.21657v1\", \"title\": \"Model Assembly Learning with Heterogeneous Layer Weight Merging\", \"summary\": \"Model merging acquires general capabilities without extra data or training by\\ncombining multiple models' parameters. Previous approaches achieve linear mode\\nconnectivity by aligning parameters into the same loss basin using permutation\\ninvariance. In this paper, we introduce Model Assembly Learning (MAL), a novel\\nparadigm for model merging that iteratively integrates parameters from diverse\\nmodels in an open-ended model zoo to enhance the base model's capabilities.\\nUnlike previous works that require identical architectures, MAL allows the\\nmerging of heterogeneous architectures and selective parameters across layers.\\nSpecifically, the base model can incorporate parameters from different layers\\nof multiple pre-trained models. We systematically investigate the conditions\\nand fundamental settings of heterogeneous parameter merging, addressing all\\npossible mismatches in layer widths between the base and target models.\\nFurthermore, we establish key laws and provide practical guidelines for\\neffectively implementing MAL.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL\", \"published\": \"2025-03-27T16:21:53Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16665v1\", \"title\": \"A Diff-Attention Aware State Space Fusion Model for Remote Sensing\\n  Classification\", \"summary\": \"Multispectral (MS) and panchromatic (PAN) images describe the same land\\nsurface, so these images not only have their own advantages, but also have a\\nlot of similar information. In order to separate these similar information and\\ntheir respective advantages, reduce the feature redundancy in the fusion stage.\\nThis paper introduces a diff-attention aware state space fusion model\\n(DAS2F-Model) for multimodal remote sensing image classification. Based on the\\nselective state space model, a cross-modal diff-attention module (CMDA-Module)\\nis designed to extract and separate the common features and their respective\\ndominant features of MS and PAN images. Among this, space preserving visual\\nmamba (SPVM) retains image spatial features and captures local features by\\noptimizing visual mamba's input reasonably. Considering that features in the\\nfusion stage will have large semantic differences after feature separation and\\nsimple fusion operations struggle to effectively integrate these significantly\\ndifferent features, an attention-aware linear fusion module (AALF-Module) is\\nproposed. It performs pixel-wise linear fusion by calculating influence\\ncoefficients. This mechanism can fuse features with large semantic differences\\nwhile keeping the feature size unchanged. Empirical evaluations indicate that\\nthe presented method achieves better results than alternative approaches. The\\nrelevant code can be found at:https://github.com/AVKSKVL/DAS-F-Model\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-23T12:34:32Z\"}"}

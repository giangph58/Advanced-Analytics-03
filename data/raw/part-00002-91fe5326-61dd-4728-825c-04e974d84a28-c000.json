{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20547v1\", \"title\": \"Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for\\n  Electronic Health Records\", \"summary\": \"The lack of standardized evaluation benchmarks in the medical domain for text\\ninputs can be a barrier to widely adopting and leveraging the potential of\\nnatural language models for health-related downstream tasks. This paper\\nrevisited an openly available MIMIC-IV benchmark for electronic health records\\n(EHRs) to address this issue. First, we integrate the MIMIC-IV data within the\\nHugging Face datasets library to allow an easy share and use of this\\ncollection. Second, we investigate the application of templates to convert EHR\\ntabular data to text. Experiments using fine-tuned and zero-shot LLMs on the\\nmortality of patients task show that fine-tuned text-based models are\\ncompetitive against robust tabular classifiers. In contrast, zero-shot LLMs\\nstruggle to leverage EHR representations. This study underlines the potential\\nof text-based approaches in the medical field and highlights areas for further\\nimprovement.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-29T08:49:38Z\"}"}

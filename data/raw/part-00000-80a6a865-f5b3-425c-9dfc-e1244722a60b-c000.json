{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05782v1\", \"title\": \"MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in\\n  Multimodal Large Language Models\", \"summary\": \"Multimodal reasoning, which integrates language and visual cues into problem\\nsolving and decision making, is a fundamental aspect of human intelligence and\\na crucial step toward artificial general intelligence. However, the evaluation\\nof multimodal reasoning capabilities in Multimodal Large Language Models\\n(MLLMs) remains inadequate. Most existing reasoning benchmarks are constrained\\nby limited data size, narrow domain coverage, and unstructured knowledge\\ndistribution. To close these gaps, we introduce MDK12-Bench, a\\nmulti-disciplinary benchmark assessing the reasoning capabilities of MLLMs via\\nreal-world K-12 examinations. Spanning six disciplines (math, physics,\\nchemistry, biology, geography, and information science), our benchmark\\ncomprises 140K reasoning instances across diverse difficulty levels from\\nprimary school to 12th grade. It features 6,827 instance-level knowledge point\\nannotations based on a well-organized knowledge structure, detailed answer\\nexplanations, difficulty labels and cross-year partitions, providing a robust\\nplatform for comprehensive evaluation. Additionally, we present a novel dynamic\\nevaluation framework to mitigate data contamination issues by bootstrapping\\nquestion forms, question types, and image styles during evaluation. Extensive\\nexperiment on MDK12-Bench reveals the significant limitation of current MLLMs\\nin multimodal reasoning. The findings on our benchmark provide insights into\\nthe development of the next-generation models. Our data and codes are available\\nat https://github.com/LanceZPF/MDK12.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-08T08:06:53Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16460v1\", \"title\": \"T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic\\n  Understanding via Deep Triplet Loss Fine-Tuning\", \"summary\": \"The specialized vocabulary and complex concepts of the telecommunications\\nindustry present significant challenges for standard Natural Language\\nProcessing models. Generic text embeddings often fail to capture\\ntelecom-specific semantics, hindering downstream task performance. We introduce\\nT-VEC (Telecom Vectorization Model), a novel embedding model tailored for the\\ntelecom domain through deep fine-tuning. Developed by NetoAI, T-VEC is created\\nby adapting the state-of-the-art gte-Qwen2-1.5B-instruct model using a triplet\\nloss objective on a meticulously curated, large-scale dataset of\\ntelecom-specific data. Crucially, this process involved substantial\\nmodification of weights across 338 layers of the base model, ensuring deep\\nintegration of domain knowledge, far exceeding superficial adaptation\\ntechniques. We quantify this deep change via weight difference analysis. A key\\ncontribution is the development and open-sourcing (MIT License) of the first\\ndedicated telecom-specific tokenizer, enhancing the handling of industry\\njargon. T-VEC achieves a leading average MTEB score (0.825) compared to\\nestablished models and demonstrates vastly superior performance (0.9380 vs.\\nless than 0.07) on our internal telecom-specific triplet evaluation benchmark,\\nindicating an exceptional grasp of domain-specific nuances, visually confirmed\\nby improved embedding separation. This work positions NetoAI at the forefront\\nof telecom AI innovation, providing the community with a powerful, deeply\\nadapted, open-source tool.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-23T07:10:37Z\"}"}

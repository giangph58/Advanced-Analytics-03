{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12870v1\", \"title\": \"CST-former: Multidimensional Attention-based Transformer for Sound Event\\n  Localization and Detection in Real Scenes\", \"summary\": \"Sound event localization and detection (SELD) is a task for the\\nclassification of sound events and the identification of direction of arrival\\n(DoA) utilizing multichannel acoustic signals. For effective classification and\\nlocalization, a channel-spectro-temporal transformer (CST-former) was\\nsuggested. CST-former employs multidimensional attention mechanisms across the\\nspatial, spectral, and temporal domains to enlarge the model's capacity to\\nlearn the domain information essential for event detection and DoA estimation\\nover time. In this work, we present an enhanced version of CST-former with\\nmultiscale unfolded local embedding (MSULE) developed to capture and aggregate\\ndomain information over multiple time-frequency scales. Also, we propose\\nfinetuning and post-processing techniques beneficial for conducting the SELD\\ntask over limited training datasets. In-depth ablation studies of the proposed\\narchitecture and detailed analysis on the proposed modules are carried out to\\nvalidate the efficacy of multidimensional attentions on the SELD task.\\nEmpirical validation through experimentation on STARSS22 and STARSS23 datasets\\ndemonstrates the remarkable performance of CST-former and post-processing\\ntechniques without using external data.\", \"main_category\": \"eess.AS\", \"categories\": \"eess.AS\", \"published\": \"2025-04-17T11:56:13Z\"}"}

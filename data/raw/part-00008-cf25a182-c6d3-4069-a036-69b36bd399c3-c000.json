{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15176v1\", \"title\": \"DSPO: Direct Semantic Preference Optimization for Real-World Image\\n  Super-Resolution\", \"summary\": \"Recent advances in diffusion models have improved Real-World Image\\nSuper-Resolution (Real-ISR), but existing methods lack human feedback\\nintegration, risking misalignment with human preference and may leading to\\nartifacts, hallucinations and harmful content generation. To this end, we are\\nthe first to introduce human preference alignment into Real-ISR, a technique\\nthat has been successfully applied in Large Language Models and Text-to-Image\\ntasks to effectively enhance the alignment of generated outputs with human\\npreferences. Specifically, we introduce Direct Preference Optimization (DPO)\\ninto Real-ISR to achieve alignment, where DPO serves as a general alignment\\ntechnique that directly learns from the human preference dataset. Nevertheless,\\nunlike high-level tasks, the pixel-level reconstruction objectives of Real-ISR\\nare difficult to reconcile with the image-level preferences of DPO, which can\\nlead to the DPO being overly sensitive to local anomalies, leading to reduced\\ngeneration quality. To resolve this dichotomy, we propose Direct Semantic\\nPreference Optimization (DSPO) to align instance-level human preferences by\\nincorporating semantic guidance, which is through two strategies: (a) semantic\\ninstance alignment strategy, implementing instance-level alignment to ensure\\nfine-grained perceptual consistency, and (b) user description feedback\\nstrategy, mitigating hallucinations through semantic textual feedback on\\ninstance-level images. As a plug-and-play solution, DSPO proves highly\\neffective in both one-step and multi-step SR frameworks.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-21T15:35:48Z\"}"}

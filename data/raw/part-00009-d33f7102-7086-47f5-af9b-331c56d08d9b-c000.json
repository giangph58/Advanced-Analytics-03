{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10342v1\", \"title\": \"VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain\\n  Knowledge\", \"summary\": \"Current multimodal benchmarks often conflate reasoning with domain-specific\\nknowledge, making it difficult to isolate and evaluate general reasoning\\nabilities in non-expert settings. To address this, we introduce VisualPuzzles,\\na benchmark that targets visual reasoning while deliberately minimizing\\nreliance on specialized knowledge. VisualPuzzles consists of diverse questions\\nspanning five categories: algorithmic, analogical, deductive, inductive, and\\nspatial reasoning. One major source of our questions is manually translated\\nlogical reasoning questions from the Chinese Civil Service Examination.\\nExperiments show that VisualPuzzles requires significantly less intensive\\ndomain-specific knowledge and more complex reasoning compared to benchmarks\\nlike MMMU, enabling us to better evaluate genuine multimodal reasoning.\\nEvaluations show that state-of-the-art multimodal large language models\\nconsistently lag behind human performance on VisualPuzzles, and that strong\\nperformance on knowledge-intensive benchmarks does not necessarily translate to\\nsuccess on reasoning-focused, knowledge-light tasks. Additionally, reasoning\\nenhancements such as scaling up inference compute (with \\\"thinking\\\" modes) yield\\ninconsistent gains across models and task types, and we observe no clear\\ncorrelation between model size and performance. We also found that models\\nexhibit different reasoning and answering patterns on VisualPuzzles compared to\\nbenchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer\\nlens through which to evaluate reasoning capabilities beyond factual recall and\\ndomain knowledge.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-14T15:50:39Z\"}"}

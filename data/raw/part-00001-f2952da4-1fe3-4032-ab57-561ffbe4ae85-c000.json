{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10110v1\", \"title\": \"Eigengap Sparsity for Covariance Parsimony\", \"summary\": \"Covariance estimation is a central problem in statistics. An important issue\\nis that there are rarely enough samples $n$ to accurately estimate the $p (p+1)\\n/ 2$ coefficients in dimension $p$. Parsimonious covariance models are\\ntherefore preferred, but the discrete nature of model selection makes inference\\ncomputationally challenging. In this paper, we propose a relaxation of\\ncovariance parsimony termed \\\"eigengap sparsity\\\" and motivated by the good\\naccuracy-parsimony tradeoff of eigenvalue-equalization in covariance matrices.\\nThis new penalty can be included in a penalized-likelihood framework that we\\npropose to solve with a projected gradient descent on a monotone cone. The\\nalgorithm turns out to resemble an isotonic regression of mutually-attracted\\nsample eigenvalues, drawing an interesting link between covariance parsimony\\nand shrinkage.\", \"main_category\": \"stat.ME\", \"categories\": \"stat.ME\", \"published\": \"2025-04-14T11:18:02Z\"}"}

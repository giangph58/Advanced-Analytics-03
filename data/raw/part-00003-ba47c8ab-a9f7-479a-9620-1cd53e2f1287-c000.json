{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15900v1\", \"title\": \"SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement\\n  Learning\", \"summary\": \"Recent work shows that reinforcement learning(RL) can markedly sharpen the\\nreasoning ability of large language models (LLMs) by prompting them to \\\"think\\nbefore answering.\\\" Yet whether and how these gains transfer to audio-language\\nreasoning remains largely unexplored. We extend the Group-Relative Policy\\nOptimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model\\n(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage\\nregimen supervised fine-tuning on structured and unstructured\\nchains-of-thought, followed by curriculum-guided GRPO, we systematically\\ncompare implicit vs. explicit, and structured vs. free form reasoning under\\nidentical architectures. Our structured audio reasoning model, SARI (Structured\\nAudio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a\\n16.35% improvement in average accuracy over the base model\\nQwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni\\nreaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.\\nAblation experiments show that on the base model we use: (i) SFT warm-up is\\nimportant for stable RL training, (ii) structured chains yield more robust\\ngeneralization than unstructured ones, and (iii) easy-to-hard curricula\\naccelerate convergence and improve final performance. These findings\\ndemonstrate that explicit, structured reasoning and curriculum learning\\nsubstantially enhances audio-language understanding.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-22T13:41:26Z\"}"}

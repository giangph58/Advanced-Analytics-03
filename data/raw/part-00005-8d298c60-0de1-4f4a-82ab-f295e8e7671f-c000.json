{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11232v1\", \"title\": \"Leveraging multimodal explanatory annotations for video interpretation\\n  with Modality Specific Dataset\", \"summary\": \"We examine the impact of concept-informed supervision on multimodal video\\ninterpretation models using MOByGaze, a dataset containing human-annotated\\nexplanatory concepts. We introduce Concept Modality Specific Datasets (CMSDs),\\nwhich consist of data subsets categorized by the modality (visual, textual, or\\naudio) of annotated concepts. Models trained on CMSDs outperform those using\\ntraditional legacy training in both early and late fusion approaches. Notably,\\nthis approach enables late fusion models to achieve performance close to that\\nof early fusion models. These findings underscore the importance of\\nmodality-specific annotations in developing robust, self-explainable video\\nmodels and contribute to advancing interpretable multimodal learning in complex\\nvideo analysis.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.MM\", \"published\": \"2025-04-15T14:33:25Z\"}"}

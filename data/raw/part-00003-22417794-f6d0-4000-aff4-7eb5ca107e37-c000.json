{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12679v1\", \"title\": \"TongUI: Building Generalized GUI Agents by Learning from Multimodal Web\\n  Tutorials\", \"summary\": \"Building Graphical User Interface (GUI) agents is a promising research\\ndirection, which simulates human interaction with computers or mobile phones to\\nperform diverse GUI tasks. However, a major challenge in developing generalized\\nGUI agents is the lack of sufficient trajectory data across various operating\\nsystems and applications, mainly due to the high cost of manual annotations. In\\nthis paper, we propose the TongUI framework that builds generalized GUI agents\\nby learning from rich multimodal web tutorials. Concretely, we crawl and\\nprocess online GUI tutorials (such as videos and articles) into GUI agent\\ntrajectory data, through which we produce the GUI-Net dataset containing 143K\\ntrajectory data across five operating systems and more than 200 applications.\\nWe develop the TongUI agent by fine-tuning Qwen2.5-VL-3B/7B models on GUI-Net,\\nwhich show remarkable performance improvements on commonly used grounding and\\nnavigation benchmarks, outperforming baseline agents about 10\\\\% on multiple\\nbenchmarks, showing the effectiveness of the GUI-Net dataset and underscoring\\nthe significance of our TongUI framework. We will fully open-source the code,\\nthe GUI-Net dataset, and the trained models soon.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-17T06:15:56Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20972v1\", \"title\": \"SetKE: Knowledge Editing for Knowledge Elements Overlap\", \"summary\": \"Large Language Models (LLMs) excel in tasks such as retrieval and question\\nanswering but require updates to incorporate new knowledge and reduce\\ninaccuracies and hallucinations. Traditional updating methods, like fine-tuning\\nand incremental learning, face challenges such as overfitting and high\\ncomputational costs. Knowledge Editing (KE) provides a promising alternative\\nbut often overlooks the Knowledge Element Overlap (KEO) phenomenon, where\\nmultiple triplets share common elements, leading to editing conflicts. We\\nidentify the prevalence of KEO in existing KE datasets and show its significant\\nimpact on current KE methods, causing performance degradation in handling such\\ntriplets. To address this, we propose a new formulation, Knowledge Set Editing\\n(KSE), and introduce SetKE, a method that edits sets of triplets\\nsimultaneously. Experimental results demonstrate that SetKE outperforms\\nexisting methods in KEO scenarios on mainstream LLMs. Additionally, we\\nintroduce EditSet, a dataset containing KEO triplets, providing a comprehensive\\nbenchmark.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-29T17:40:29Z\"}"}

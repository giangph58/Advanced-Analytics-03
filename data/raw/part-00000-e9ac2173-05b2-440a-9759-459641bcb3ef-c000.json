{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10415v1\", \"title\": \"LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\\n  Large Language Models\", \"summary\": \"Scientific equation discovery is a fundamental task in the history of\\nscientific progress, enabling the derivation of laws governing natural\\nphenomena. Recently, Large Language Models (LLMs) have gained interest for this\\ntask due to their potential to leverage embedded scientific knowledge for\\nhypothesis generation. However, evaluating the true discovery capabilities of\\nthese methods remains challenging, as existing benchmarks often rely on common\\nequations that are susceptible to memorization by LLMs, leading to inflated\\nperformance metrics that do not reflect discovery. In this paper, we introduce\\nLLM-SRBench, a comprehensive benchmark with 239 challenging problems across\\nfour scientific domains specifically designed to evaluate LLM-based scientific\\nequation discovery methods while preventing trivial memorization. Our benchmark\\ncomprises two main categories: LSR-Transform, which transforms common physical\\nmodels into less common mathematical representations to test reasoning beyond\\nmemorized forms, and LSR-Synth, which introduces synthetic, discovery-driven\\nproblems requiring data-driven reasoning. Through extensive evaluation of\\nseveral state-of-the-art methods, using both open and closed LLMs, we find that\\nthe best-performing system so far achieves only 31.5% symbolic accuracy. These\\nfindings highlight the challenges of scientific equation discovery, positioning\\nLLM-SRBench as a valuable resource for future research.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-14T17:00:13Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12108v1\", \"title\": \"Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust\\n  and Traceable Text Generation\", \"summary\": \"The rapid development of Large Language Models (LLMs) has intensified\\nconcerns about content traceability and potential misuse. Existing watermarking\\nschemes for sampled text often face trade-offs between maintaining text quality\\nand ensuring robust detection against various attacks. To address these issues,\\nwe propose a novel watermarking scheme that improves both detectability and\\ntext quality by introducing a cumulative watermark entropy threshold. Our\\napproach is compatible with and generalizes existing sampling functions,\\nenhancing adaptability. Experimental results across multiple LLMs show that our\\nscheme significantly outperforms existing methods, achieving over 80\\\\%\\nimprovements on widely-used datasets, e.g., MATH and GSM8K, while maintaining\\nhigh detection accuracy.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-16T14:16:38Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00254v1\", \"title\": \"Empowering Agentic Video Analytics Systems with Video Language Models\", \"summary\": \"AI-driven video analytics has become increasingly pivotal across diverse\\ndomains. However, existing systems are often constrained to specific,\\npredefined tasks, limiting their adaptability in open-ended analytical\\nscenarios. The recent emergence of Video-Language Models (VLMs) as\\ntransformative technologies offers significant potential for enabling\\nopen-ended video understanding, reasoning, and analytics. Nevertheless, their\\nlimited context windows present challenges when processing ultra-long video\\ncontent, which is prevalent in real-world applications. To address this, we\\nintroduce AVA, a VLM-powered system designed for open-ended, advanced video\\nanalytics. AVA incorporates two key innovations: (1) the near real-time\\nconstruction of Event Knowledge Graphs (EKGs) for efficient indexing of long or\\ncontinuous video streams, and (2) an agentic retrieval-generation mechanism\\nthat leverages EKGs to handle complex and diverse queries. Comprehensive\\nevaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that\\nAVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,\\nrespectively, significantly surpassing existing VLM and video\\nRetrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video\\nanalytics in ultra-long and open-world video scenarios, we introduce a new\\nbenchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours\\nin duration, along with 120 manually annotated, diverse, and complex\\nquestion-answer pairs. On AVA-100, AVA achieves top-tier performance with an\\naccuracy of 75.8%.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-05-01T02:40:23Z\"}"}

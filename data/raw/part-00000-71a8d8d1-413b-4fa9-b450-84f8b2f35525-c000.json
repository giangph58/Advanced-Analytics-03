{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04916v1\", \"title\": \"An Open-Source Dual-Loss Embedding Model for Semantic Retrieval in\\n  Higher Education\", \"summary\": \"Recent advances in AI have catalyzed the adoption of intelligent educational\\ntools, yet many semantic retrieval systems remain ill-suited to the unique\\nlinguistic and structural characteristics of academic content. This study\\npresents two open-source embedding models fine-tuned for educational question\\nanswering, particularly in the context of course syllabi. A synthetic dataset\\nof 3,197 sentence pairs, spanning synonymous terminology, paraphrased\\nquestions, and implicit-explicit mappings, was constructed through a\\ncombination of manual curation and large language model (LLM)-assisted\\ngeneration. Two training strategies were evaluated: (1) a baseline model\\nfine-tuned using MultipleNegativesRankingLoss (MNRL), and (2) a dual-loss model\\nthat combines MNRL with CosineSimilarityLoss to improve both semantic ranking\\nand similarity calibration. Evaluations were conducted on 28 university course\\nsyllabi using a fixed set of natural language questions categorized into\\ncourse, faculty, and teaching assistant information. Results demonstrate that\\nboth fine-tuned models outperform strong open-source baselines, including\\nall-MiniLM-L6-v2 and multi-qa-MiniLM-L6-cos-v1, and that the dual-loss model\\nnarrows the performance gap with high-performing proprietary embeddings such as\\nOpenAI's text-embedding-3 series. This work contributes reusable,\\ndomain-aligned embedding models and provides a replicable framework for\\neducational semantic retrieval, supporting downstream applications such as\\nacademic chatbots, retrieval-augmented generation (RAG) systems, and learning\\nmanagement system (LMS) integrations.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.IR\", \"published\": \"2025-05-08T03:14:14Z\"}"}

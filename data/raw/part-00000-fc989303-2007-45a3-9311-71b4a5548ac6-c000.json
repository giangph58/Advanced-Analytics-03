{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03581v1\", \"title\": \"DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer\\n  Questions in Dynamic Scenes\", \"summary\": \"The analysis of events in dynamic environments poses a fundamental challenge\\nin the development of intelligent agents and robots capable of interacting with\\nhumans. Current approaches predominantly utilize visual models. However, these\\nmethods often capture information implicitly from images, lacking interpretable\\nspatial-temporal object representations. To address this issue we introduce\\nDyGEnc - a novel method for Encoding a Dynamic Graph. This method integrates\\ncompressed spatial-temporal structural observation representation with the\\ncognitive capabilities of large language models. The purpose of this\\nintegration is to enable advanced question answering based on a sequence of\\ntextual scene graphs. Extended evaluations on the STAR and AGQA datasets\\nindicate that DyGEnc outperforms existing visual methods by a large margin of\\n15-25% in addressing queries regarding the history of human-to-object\\ninteractions. Furthermore, the proposed method can be seamlessly extended to\\nprocess raw input images utilizing foundational models for extracting explicit\\ntextual scene graphs, as substantiated by the results of a robotic experiment\\nconducted with a wheeled manipulator platform. We hope that these findings will\\ncontribute to the implementation of robust and compressed graph-based robotic\\nmemory for long-horizon reasoning. Code is available at\\ngithub.com/linukc/DyGEnc.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-06T14:41:42Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20835v1\", \"title\": \"Enhancing Non-Core Language Instruction-Following in Speech LLMs via\\n  Semi-Implicit Cross-Lingual CoT Reasoning\", \"summary\": \"Large language models have been extended to the speech domain, leading to the\\ndevelopment of speech large language models (SLLMs). While existing SLLMs\\ndemonstrate strong performance in speech instruction-following for core\\nlanguages (e.g., English), they often struggle with non-core languages due to\\nthe scarcity of paired speech-text data and limited multilingual semantic\\nreasoning capabilities. To address this, we propose the semi-implicit\\nCross-lingual Speech Chain-of-Thought (XS-CoT) framework, which integrates\\nspeech-to-text translation into the reasoning process of SLLMs. The XS-CoT\\ngenerates four types of tokens: instruction and response tokens in both core\\nand non-core languages, enabling cross-lingual transfer of reasoning\\ncapabilities. To mitigate inference latency in generating target non-core\\nresponse tokens, we incorporate a semi-implicit CoT scheme into XS-CoT, which\\nprogressively compresses the first three types of intermediate reasoning tokens\\nwhile retaining global reasoning logic during training. By leveraging the\\nrobust reasoning capabilities of the core language, XS-CoT improves responses\\nfor non-core languages by up to 45\\\\% in GPT-4 score when compared to direct\\nsupervised fine-tuning on two representative SLLMs, Qwen2-Audio and SALMONN.\\nMoreover, the semi-implicit XS-CoT reduces token delay by more than 50\\\\% with a\\nslight drop in GPT-4 scores. Importantly, XS-CoT requires only a small amount\\nof high-quality training data for non-core languages by leveraging the\\nreasoning capabilities of core languages. To support training, we also develop\\na data pipeline and open-source speech instruction-following datasets in\\nJapanese, German, and French.\", \"main_category\": \"cs.SD\", \"categories\": \"cs.SD,eess.AS\", \"published\": \"2025-04-29T14:59:42Z\"}"}

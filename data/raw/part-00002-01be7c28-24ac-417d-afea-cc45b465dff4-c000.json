{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10449v1\", \"title\": \"M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models\", \"summary\": \"Effective reasoning is crucial to solving complex mathematical problems.\\nRecent large language models (LLMs) have boosted performance by scaling\\ntest-time computation through long chain-of-thought reasoning. However,\\ntransformer-based models are inherently limited in extending context length due\\nto their quadratic computational complexity and linear memory requirements. In\\nthis paper, we introduce a novel hybrid linear RNN reasoning model, M1, built\\non the Mamba architecture, which allows memory-efficient inference. Our\\napproach leverages a distillation process from existing reasoning models and is\\nfurther enhanced through RL training. Experimental results on the AIME and MATH\\nbenchmarks show that M1 not only outperforms previous linear RNN models but\\nalso matches the performance of state-of-the-art Deepseek R1 distilled\\nreasoning models at a similar scale. We also compare our generation speed with\\na highly performant general purpose inference engine, vLLM, and observe more\\nthan a 3x speedup compared to a same size transformer. With throughput speedup,\\nwe are able to achieve higher accuracy compared to DeepSeek R1 distilled\\ntransformer reasoning models under a fixed generation time budget using\\nself-consistency voting. Overall, we introduce a hybrid Mamba reasoning model\\nand provide a more effective approach to scaling test-time generation using\\nself-consistency or long chain of thought reasoning.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-14T17:38:25Z\"}"}

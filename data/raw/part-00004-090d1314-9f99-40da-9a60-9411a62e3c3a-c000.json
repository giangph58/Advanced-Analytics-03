{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11182v1\", \"title\": \"Exploring Backdoor Attack and Defense for LLM-empowered Recommendations\", \"summary\": \"The fusion of Large Language Models (LLMs) with recommender systems (RecSys)\\nhas dramatically advanced personalized recommendations and drawn extensive\\nattention. Despite the impressive progress, the safety of LLM-based RecSys\\nagainst backdoor attacks remains largely under-explored. In this paper, we\\nraise a new problem: Can a backdoor with a specific trigger be injected into\\nLLM-based Recsys, leading to the manipulation of the recommendation responses\\nwhen the backdoor trigger is appended to an item's title? To investigate the\\nvulnerabilities of LLM-based RecSys under backdoor attacks, we propose a new\\nattack framework termed Backdoor Injection Poisoning for RecSys (BadRec).\\nBadRec perturbs the items' titles with triggers and employs several fake users\\nto interact with these items, effectively poisoning the training set and\\ninjecting backdoors into LLM-based RecSys. Comprehensive experiments reveal\\nthat poisoning just 1% of the training data with adversarial examples is\\nsufficient to successfully implant backdoors, enabling manipulation of\\nrecommendations. To further mitigate such a security threat, we propose a\\nuniversal defense strategy called Poison Scanner (P-Scanner). Specifically, we\\nintroduce an LLM-based poison scanner to detect the poisoned items by\\nleveraging the powerful language understanding and rich knowledge of LLMs. A\\ntrigger augmentation agent is employed to generate diverse synthetic triggers\\nto guide the poison scanner in learning domain-specific knowledge of the\\npoisoned item detection task. Extensive experiments on three real-world\\ndatasets validate the effectiveness of the proposed P-Scanner.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.AI\", \"published\": \"2025-04-15T13:37:38Z\"}"}

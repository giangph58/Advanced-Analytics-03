{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03373v1\", \"title\": \"SPAP: Structured Pruning via Alternating Optimization and Penalty\\n  Methods\", \"summary\": \"The deployment of large language models (LLMs) is often constrained by their\\nsubstantial computational and memory demands. While structured pruning presents\\na viable approach by eliminating entire network components, existing methods\\nsuffer from performance degradation, reliance on heuristic metrics, or\\nexpensive finetuning. To address these challenges, we propose SPAP (Structured\\nPruning via Alternating Optimization and Penalty Methods), a novel and\\nefficient structured pruning framework for LLMs grounded in optimization\\ntheory. SPAP formulates the pruning problem through a mixed-integer\\noptimization model, employs a penalty method that effectively makes pruning\\ndecisions to minimize pruning errors, and introduces an alternating\\nminimization algorithm tailored to the splittable problem structure for\\nefficient weight updates and performance recovery. Extensive experiments on\\nOPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over\\nstate-of-the-art methods, delivering linear inference speedups (1.29$\\\\times$ at\\n30% sparsity) and proportional memory reductions. Our work offers a practical,\\noptimization-driven solution for pruning LLMs while preserving model\\nperformance.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,math.OC\", \"published\": \"2025-05-06T09:47:53Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15251v1\", \"title\": \"On Learning Parallel Pancakes with Mostly Uniform Weights\", \"summary\": \"We study the complexity of learning $k$-mixtures of Gaussians ($k$-GMMs) on\\n$\\\\mathbb{R}^d$. This task is known to have complexity $d^{\\\\Omega(k)}$ in full\\ngenerality. To circumvent this exponential lower bound on the number of\\ncomponents, research has focused on learning families of GMMs satisfying\\nadditional structural properties. A natural assumption posits that the\\ncomponent weights are not exponentially small and that the components have the\\nsame unknown covariance. Recent work gave a $d^{O(\\\\log(1/w_{\\\\min}))}$-time\\nalgorithm for this class of GMMs, where $w_{\\\\min}$ is the minimum weight. Our\\nfirst main result is a Statistical Query (SQ) lower bound showing that this\\nquasi-polynomial upper bound is essentially best possible, even for the special\\ncase of uniform weights. Specifically, we show that it is SQ-hard to\\ndistinguish between such a mixture and the standard Gaussian. We further\\nexplore how the distribution of weights affects the complexity of this task.\\nOur second main result is a quasi-polynomial upper bound for the aforementioned\\ntesting task when most of the weights are uniform while a small fraction of the\\nweights are potentially arbitrary.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.DS,math.ST,stat.ML,stat.TH\", \"published\": \"2025-04-21T17:31:55Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12880v1\", \"title\": \"Can Masked Autoencoders Also Listen to Birds?\", \"summary\": \"Masked Autoencoders (MAEs) pretrained on AudioSet fail to capture the\\nfine-grained acoustic characteristics of specialized domains such as\\nbioacoustic monitoring. Bird sound classification is critical for assessing\\nenvironmental health, yet general-purpose models inadequately address its\\nunique acoustic challenges. To address this, we introduce Bird-MAE, a\\ndomain-specialized MAE pretrained on the large-scale BirdSet dataset. We\\nexplore adjustments to pretraining, fine-tuning and utilizing frozen\\nrepresentations. Bird-MAE achieves state-of-the-art results across all BirdSet\\ndownstream tasks, substantially improving multi-label classification\\nperformance compared to the general-purpose Audio-MAE baseline. Additionally,\\nwe propose prototypical probing, a parameter-efficient method for leveraging\\nMAEs' frozen representations. Bird-MAE's prototypical probes outperform linear\\nprobing by up to 37\\\\% in MAP and narrow the gap to fine-tuning to approximately\\n3\\\\% on average on BirdSet.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.SD,eess.AS\", \"published\": \"2025-04-17T12:13:25Z\"}"}

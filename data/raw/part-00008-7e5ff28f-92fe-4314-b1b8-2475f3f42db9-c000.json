{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03638v1\", \"title\": \"Towards Smart Point-and-Shoot Photography\", \"summary\": \"Hundreds of millions of people routinely take photos using their smartphones\\nas point and shoot (PAS) cameras, yet very few would have the photography\\nskills to compose a good shot of a scene. While traditional PAS cameras have\\nbuilt-in functions to ensure a photo is well focused and has the right\\nbrightness, they cannot tell the users how to compose the best shot of a scene.\\nIn this paper, we present a first of its kind smart point and shoot (SPAS)\\nsystem to help users to take good photos. Our SPAS proposes to help users to\\ncompose a good shot of a scene by automatically guiding the users to adjust the\\ncamera pose live on the scene. We first constructed a large dataset containing\\n320K images with camera pose information from 4000 scenes. We then developed an\\ninnovative CLIP-based Composition Quality Assessment (CCQA) model to assign\\npseudo labels to these images. The CCQA introduces a unique learnable text\\nembedding technique to learn continuous word embeddings capable of discerning\\nsubtle visual quality differences in the range covered by five levels of\\nquality description words {bad, poor, fair, good, perfect}. And finally we have\\ndeveloped a camera pose adjustment model (CPAM) which first determines if the\\ncurrent view can be further improved and if so it outputs the adjust suggestion\\nin the form of two camera pose adjustment angles. The two tasks of CPAM make\\ndecisions in a sequential manner and each involves different sets of training\\nsamples, we have developed a mixture-of-experts model with a gated loss\\nfunction to train the CPAM in an end-to-end manner. We will present extensive\\nresults to demonstrate the performances of our SPAS system using publicly\\navailable image composition datasets.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-06T15:40:14Z\"}"}

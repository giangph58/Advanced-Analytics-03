{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07901v1\", \"title\": \"Redefining Machine Translation on Social Network Services with Large\\n  Language Models\", \"summary\": \"The globalization of social interactions has heightened the need for machine\\ntranslation (MT) on Social Network Services (SNS), yet traditional models\\nstruggle with culturally nuanced content like memes, slang, and pop culture\\nreferences. While large language models (LLMs) have advanced general-purpose\\ntranslation, their performance on SNS-specific content remains limited due to\\ninsufficient specialized training data and evaluation benchmarks. This paper\\nintroduces RedTrans, a 72B LLM tailored for SNS translation, trained on a novel\\ndataset developed through three innovations: (1) Supervised Finetuning with\\nDual-LLM Back-Translation Sampling, an unsupervised sampling method using\\nLLM-based back-translation to select diverse data for large-scale finetuning;\\n(2) Rewritten Preference Optimization (RePO), an algorithm that identifies and\\ncorrects erroneous preference pairs through expert annotation, building\\nreliable preference corpora; and (3) RedTrans-Bench, the first benchmark for\\nSNS translation, evaluating phenomena like humor localization, emoji semantics,\\nand meme adaptation. Experiments show RedTrans outperforms state-of-the-art\\nLLMs. Besides, RedTrans has already been deployed in a real-world production\\nenvironment, demonstrating that domain-specific adaptation, effectively bridges\\nthe gap between generic and culturally grounded translation systems.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-10T16:24:28Z\"}"}

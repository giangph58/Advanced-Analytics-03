{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07718v1\", \"title\": \"Multi-modal Reference Learning for Fine-grained Text-to-Image Retrieval\", \"summary\": \"Fine-grained text-to-image retrieval aims to retrieve a fine-grained target\\nimage with a given text query. Existing methods typically assume that each\\ntraining image is accurately depicted by its textual descriptions. However,\\ntextual descriptions can be ambiguous and fail to depict discriminative visual\\ndetails in images, leading to inaccurate representation learning. To alleviate\\nthe effects of text ambiguity, we propose a Multi-Modal Reference learning\\nframework to learn robust representations. We first propose a multi-modal\\nreference construction module to aggregate all visual and textual details of\\nthe same object into a comprehensive multi-modal reference. The multi-modal\\nreference hence facilitates the subsequent representation learning and\\nretrieval similarity computation. Specifically, a reference-guided\\nrepresentation learning module is proposed to use multi-modal references to\\nlearn more accurate visual and textual representations. Additionally, we\\nintroduce a reference-based refinement method that employs the object\\nreferences to compute a reference-based similarity that refines the initial\\nretrieval results. Extensive experiments are conducted on five fine-grained\\ntext-to-image retrieval datasets for different text-to-image retrieval tasks.\\nThe proposed method has achieved superior performance over state-of-the-art\\nmethods. For instance, on the text-to-person image retrieval dataset RSTPReid,\\nour method achieves the Rank1 accuracy of 56.2\\\\%, surpassing the recent CFine\\nby 5.6\\\\%.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-10T13:09:52Z\"}"}

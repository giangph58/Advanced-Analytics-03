{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05831v1\", \"title\": \"Leveraging Robust Optimization for LLM Alignment under Distribution\\n  Shifts\", \"summary\": \"Large language models (LLMs) increasingly rely on preference alignment\\nmethods to steer outputs toward human values, yet these methods are often\\nconstrained by the scarcity of high-quality human-annotated data. To tackle\\nthis, recent approaches have turned to synthetic data generated by LLMs as a\\nscalable alternative. However, synthetic data can introduce distribution\\nshifts, compromising the nuanced human preferences that are essential for\\ndesirable outputs. In this paper, we propose a novel distribution-aware\\noptimization framework that improves preference alignment in the presence of\\nsuch shifts. Our approach first estimates the likelihood ratios between the\\ntarget and training distributions leveraging a learned classifier, then it\\nminimizes the worst-case loss over data regions that reflect the target\\nhuman-preferred distribution. By explicitly prioritizing the target\\ndistribution during optimization, our method mitigates the adverse effects of\\ndistributional variation and enhances the generation of responses that\\nfaithfully reflect human values.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-08T09:14:38Z\"}"}

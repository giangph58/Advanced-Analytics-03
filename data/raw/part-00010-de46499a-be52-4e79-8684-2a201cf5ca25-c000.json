{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16489v1\", \"title\": \"Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based\\n  Multi-Agent Debate\", \"summary\": \"Multi-Agent Debate (MAD), leveraging collaborative interactions among Large\\nLanguage Models (LLMs), aim to enhance reasoning capabilities in complex tasks.\\nHowever, the security implications of their iterative dialogues and\\nrole-playing characteristics, particularly susceptibility to jailbreak attacks\\neliciting harmful content, remain critically underexplored. This paper\\nsystematically investigates the jailbreak vulnerabilities of four prominent MAD\\nframeworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo,\\nand DeepSeek) without compromising internal agents. We introduce a novel\\nstructured prompt-rewriting framework specifically designed to exploit MAD\\ndynamics via narrative encapsulation, role-driven escalation, iterative\\nrefinement, and rhetorical obfuscation. Our extensive experiments demonstrate\\nthat MAD systems are inherently more vulnerable than single-agent setups.\\nCrucially, our proposed attack methodology significantly amplifies this\\nfragility, increasing average harmfulness from 28.14% to 80.34% and achieving\\nattack success rates as high as 80% in certain scenarios. These findings reveal\\nintrinsic vulnerabilities in MAD architectures and underscore the urgent need\\nfor robust, specialized defenses prior to real-world deployment.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.AI\", \"published\": \"2025-04-23T08:01:50Z\"}"}

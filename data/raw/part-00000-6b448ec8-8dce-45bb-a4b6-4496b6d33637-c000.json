{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17490v1\", \"title\": \"Plasticine: Accelerating Research in Plasticity-Motivated Deep\\n  Reinforcement Learning\", \"summary\": \"Developing lifelong learning agents is crucial for artificial general\\nintelligence. However, deep reinforcement learning (RL) systems often suffer\\nfrom plasticity loss, where neural networks gradually lose their ability to\\nadapt during training. Despite its significance, this field lacks unified\\nbenchmarks and evaluation protocols. We introduce Plasticine, the first\\nopen-source framework for benchmarking plasticity optimization in deep RL.\\nPlasticine provides single-file implementations of over 13 mitigation methods,\\n10 evaluation metrics, and learning scenarios with increasing non-stationarity\\nlevels from standard to open-ended environments. This framework enables\\nresearchers to systematically quantify plasticity loss, evaluate mitigation\\nstrategies, and analyze plasticity dynamics across different contexts. Our\\ndocumentation, examples, and source code are available at\\nhttps://github.com/RLE-Foundation/Plasticine.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-24T12:32:13Z\"}"}

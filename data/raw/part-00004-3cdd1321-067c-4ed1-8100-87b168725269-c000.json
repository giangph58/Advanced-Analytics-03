{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03400v1\", \"title\": \"Close-Fitting Dressing Assistance Based on State Estimation of Feet and\\n  Garments with Semantic-based Visual Attention\", \"summary\": \"As the population continues to age, a shortage of caregivers is expected in\\nthe future. Dressing assistance, in particular, is crucial for opportunities\\nfor social participation. Especially dressing close-fitting garments, such as\\nsocks, remains challenging due to the need for fine force adjustments to handle\\nthe friction or snagging against the skin, while considering the shape and\\nposition of the garment. This study introduces a method uses multi-modal\\ninformation including not only robot's camera images, joint angles, joint\\ntorques, but also tactile forces for proper force interaction that can adapt to\\nindividual differences in humans. Furthermore, by introducing semantic\\ninformation based on object concepts, rather than relying solely on RGB data,\\nit can be generalized to unseen feet and background. In addition, incorporating\\ndepth data helps infer relative spatial relationship between the sock and the\\nfoot. To validate its capability for semantic object conceptualization and to\\nensure safety, training data were collected using a mannequin, and subsequent\\nexperiments were conducted with human subjects. In experiments, the robot\\nsuccessfully adapted to previously unseen human feet and was able to put socks\\non 10 participants, achieving a higher success rate than Action Chunking with\\nTransformer and Diffusion Policy. These results demonstrate that the proposed\\nmodel can estimate the state of both the garment and the foot, enabling precise\\ndressing assistance for close-fitting garments.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO\", \"published\": \"2025-05-06T10:28:39Z\"}"}

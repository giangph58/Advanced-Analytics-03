{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12113v1\", \"title\": \"Clarifying Ambiguities: on the Role of Ambiguity Types in Prompting\\n  Methods for Clarification Generation\", \"summary\": \"In information retrieval (IR), providing appropriate clarifications to better\\nunderstand users' information needs is crucial for building a proactive\\nsearch-oriented dialogue system. Due to the strong in-context learning ability\\nof large language models (LLMs), recent studies investigate prompting methods\\nto generate clarifications using few-shot or Chain of Thought (CoT) prompts.\\nHowever, vanilla CoT prompting does not distinguish the characteristics of\\ndifferent information needs, making it difficult to understand how LLMs resolve\\nambiguities in user queries. In this work, we focus on the concept of ambiguity\\nfor clarification, seeking to model and integrate ambiguities in the\\nclarification process. To this end, we comprehensively study the impact of\\nprompting schemes based on reasoning and ambiguity for clarification. The idea\\nis to enhance the reasoning abilities of LLMs by limiting CoT to predict first\\nambiguity types that can be interpreted as instructions to clarify, then\\ncorrespondingly generate clarifications. We name this new prompting scheme\\nAmbiguity Type-Chain of Thought (AT-CoT). Experiments are conducted on various\\ndatasets containing human-annotated clarifying questions to compare AT-CoT with\\nmultiple baselines. We also perform user simulations to implicitly measure the\\nquality of generated clarifications under various IR scenarios.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR\", \"published\": \"2025-04-16T14:21:02Z\"}"}

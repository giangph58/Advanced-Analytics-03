{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02708v1\", \"title\": \"The Hidden Space of Safety: Understanding Preference-Tuned LLMs in\\n  Multilingual context\", \"summary\": \"Alignment tuning has enabled large language models to excel in reasoning,\\ninstruction-following, and minimizing harmful generations. However, despite\\ntheir widespread deployment, these models exhibit a monolingual bias, raising\\nconcerns about the effectiveness of alignment across languages. Current\\nalignment methods predominantly focus on English, leaving it unclear how\\nalignment mechanism generalize to multilingual settings. To address this, we\\nconduct a systematic analysis of distributional shifts in the embedding space\\nof LLMs before and after alignment, uncovering its impact on model behavior\\nacross diverse languages. We leverage the alignment-induced separation in\\nsafety space as a quantitative tool to measure how alignment enforces safety\\nconstraints. Our study evaluates seven LLMs using balanced toxicity datasets\\nand parallel text-detoxification benchmarks, revealing substantial disparities\\nin the latent representation space between high-resource and low-resource\\nlanguages. These findings underscore the need for language-specific fine-tuning\\nto ensure fair, reliable and robust multilingual alignment. Our insights\\nprovide a foundation for developing truly safe multilingual LLMs, emphasizing\\nthe urgency of addressing alignment gaps in underrepresented languages.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-03T15:46:46Z\"}"}

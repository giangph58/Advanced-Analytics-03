{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03736v1\", \"title\": \"Decentralized Nonconvex Optimization under Heavy-Tailed Noise:\\n  Normalization and Optimal Convergence\", \"summary\": \"Heavy-tailed noise in nonconvex stochastic optimization has garnered\\nincreasing research interest, as empirical studies, including those on training\\nattention models, suggest it is a more realistic gradient noise condition. This\\npaper studies first-order nonconvex stochastic optimization under heavy-tailed\\ngradient noise in a decentralized setup, where each node can only communicate\\nwith its direct neighbors in a predefined graph. Specifically, we consider a\\nclass of heavy-tailed gradient noise that is zero-mean and has only $p$-th\\nmoment for $p \\\\in (1, 2]$. We propose GT-NSGDm, Gradient Tracking based\\nNormalized Stochastic Gradient Descent with momentum, that utilizes\\nnormalization, in conjunction with gradient tracking and momentum, to cope with\\nheavy-tailed noise on distributed nodes. We show that, when the communication\\ngraph admits primitive and doubly stochastic weights, GT-NSGDm guarantees, for\\nthe \\\\textit{first} time in the literature, that the expected gradient norm\\nconverges at an optimal non-asymptotic rate $O\\\\big(1/T^{(p-1)/(3p-2)}\\\\big)$,\\nwhich matches the lower bound in the centralized setup. When tail index $p$ is\\nunknown, GT-NSGDm attains a non-asymptotic rate $O\\\\big( 1/T^{(p-1)/(2p)} \\\\big)$\\nthat is, for $p < 2$, topology independent and has a speedup factor $n^{1-1/p}$\\nin terms of the number of nodes $n$. Finally, experiments on nonconvex linear\\nregression with tokenized synthetic data and decentralized training of language\\nmodels on a real-world corpus demonstrate that GT-NSGDm is more robust and\\nefficient than baselines.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC,cs.DC\", \"published\": \"2025-05-06T17:59:36Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15865v1\", \"title\": \"MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search\", \"summary\": \"Deep learning (DL) has achieved remarkable progress in the field of medical\\nimaging. However, adapting DL models to medical tasks remains a significant\\nchallenge, primarily due to two key factors: (1) architecture selection, as\\ndifferent tasks necessitate specialized model designs, and (2) weight\\ninitialization, which directly impacts the convergence speed and final\\nperformance of the models. Although transfer learning from ImageNet is a widely\\nadopted strategy, its effectiveness is constrained by the substantial\\ndifferences between natural and medical images. To address these challenges, we\\nintroduce Medical Neural Network Search (MedNNS), the first Neural Network\\nSearch framework for medical imaging applications. MedNNS jointly optimizes\\narchitecture selection and weight initialization by constructing a meta-space\\nthat encodes datasets and models based on how well they perform together. We\\nbuild this space using a Supernetwork-based approach, expanding the model zoo\\nsize by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we\\nintroduce rank loss and Fr\\\\'echet Inception Distance (FID) loss into the\\nconstruction of the space to capture inter-model and inter-dataset\\nrelationships, thereby achieving more accurate alignment in the meta-space.\\nExperimental results across multiple datasets demonstrate that MedNNS\\nsignificantly outperforms both ImageNet pre-trained DL models and SOTA Neural\\nArchitecture Search (NAS) methods, achieving an average accuracy improvement of\\n1.7% across datasets while converging substantially faster. The code and the\\nprocessed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.LG\", \"published\": \"2025-04-22T13:04:40Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00663v1\", \"title\": \"Wasserstein Policy Optimization\", \"summary\": \"We introduce Wasserstein Policy Optimization (WPO), an actor-critic algorithm\\nfor reinforcement learning in continuous action spaces. WPO can be derived as\\nan approximation to Wasserstein gradient flow over the space of all policies\\nprojected into a finite-dimensional parameter space (e.g., the weights of a\\nneural network), leading to a simple and completely general closed-form update.\\nThe resulting algorithm combines many properties of deterministic and classic\\npolicy gradient methods. Like deterministic policy gradients, it exploits\\nknowledge of the gradient of the action-value function with respect to the\\naction. Like classic policy gradients, it can be applied to stochastic policies\\nwith arbitrary distributions over actions -- without using the\\nreparameterization trick. We show results on the DeepMind Control Suite and a\\nmagnetic confinement fusion task which compare favorably with state-of-the-art\\ncontinuous control methods.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-05-01T17:07:01Z\"}"}

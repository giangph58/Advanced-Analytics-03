{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07825v1\", \"title\": \"What the HellaSwag? On the Validity of Common-Sense Reasoning Benchmarks\", \"summary\": \"Common-sense reasoning is a key language model capability because it\\nencapsulates not just specific factual knowledge but rather general language\\nand world understanding. Measuring common-sense reasoning, therefore, is\\ncrucial for language models of different sizes and applications. One of the\\nmost widely used benchmarks for evaluating such capabilities is HellaSwag;\\nhowever, in this paper, we show that it has severe construct validity issues.\\nThese issues range from basic ungrammaticality and numerous typos to misleading\\nprompts or equally correct options. Furthermore, we show that if models are\\nevaluated only on answer texts, or with \\\"Lorem ipsum dolor...\\\" instead of the\\nquestion, more than 65% of model predictions remain the same, and this cannot\\nbe attributed merely to contamination. Since benchmark scores are an essential\\npart of model selection in both research and commercial applications, these\\nvalidity issues can have severe consequences. In particular, knowing that\\ntaking benchmark scores at face value is ubiquitous, inadequate evaluation\\nleads to ill-informed decisions about models. In this paper, we thoroughly\\ninvestigate critical validity issues posed by HellaSwag and illustrate them\\nwith various evaluations using generative language models of different sizes.\\nWe argue that this benchmark does not accurately measure common-sense reasoning\\nand, therefore, should not be used for evaluation in its current state. Based\\non the results of our study, we propose requirements that should be met by\\nfuture common-sense reasoning benchmarks. In addition, we release GoldenSwag, a\\ncorrected subset of HellaSwag, which, to our belief, facilitates acceptable\\ncommon-sense reasoning evaluation.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-10T15:01:46Z\"}"}

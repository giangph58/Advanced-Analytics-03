{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05732v1\", \"title\": \"LLM$\\\\times$MapReduce-V2: Entropy-Driven Convolutional Test-Time Scaling\\n  for Generating Long-Form Articles from Extremely Long Resources\", \"summary\": \"Long-form generation is crucial for a wide range of practical applications,\\ntypically categorized into short-to-long and long-to-long generation. While\\nshort-to-long generations have received considerable attention, generating long\\ntexts from extremely long resources remains relatively underexplored. The\\nprimary challenge in long-to-long generation lies in effectively integrating\\nand analyzing relevant information from extensive inputs, which remains\\ndifficult for current large language models (LLMs). In this paper, we propose\\nLLM$\\\\times$MapReduce-V2, a novel test-time scaling strategy designed to enhance\\nthe ability of LLMs to process extremely long inputs. Drawing inspiration from\\nconvolutional neural networks, which iteratively integrate local features into\\nhigher-level global representations, LLM$\\\\times$MapReduce-V2 utilizes stacked\\nconvolutional scaling layers to progressively expand the understanding of input\\nmaterials. Both quantitative and qualitative experimental results demonstrate\\nthat our approach substantially enhances the ability of LLMs to process long\\ninputs and generate coherent, informative long-form articles, outperforming\\nseveral representative baselines.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-08T07:03:48Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.09966v1\", \"title\": \"SemiETS: Integrating Spatial and Content Consistencies for\\n  Semi-Supervised End-to-end Text Spotting\", \"summary\": \"Most previous scene text spotting methods rely on high-quality manual\\nannotations to achieve promising performance. To reduce their expensive costs,\\nwe study semi-supervised text spotting (SSTS) to exploit useful information\\nfrom unlabeled images. However, directly applying existing semi-supervised\\nmethods of general scenes to SSTS will face new challenges: 1) inconsistent\\npseudo labels between detection and recognition tasks, and 2) sub-optimal\\nsupervisions caused by inconsistency between teacher/student. Thus, we propose\\na new Semi-supervised framework for End-to-end Text Spotting, namely SemiETS\\nthat leverages the complementarity of text detection and recognition.\\nSpecifically, it gradually generates reliable hierarchical pseudo labels for\\neach task, thereby reducing noisy labels. Meanwhile, it extracts important\\ninformation in locations and transcriptions from bidirectional flows to improve\\nconsistency. Extensive experiments on three datasets under various settings\\ndemonstrate the effectiveness of SemiETS on arbitrary-shaped text. For example,\\nit outperforms previous state-of-the-art SSL methods by a large margin on\\nend-to-end spotting (+8.7%, +5.6%, and +2.6% H-mean under 0.5%, 1%, and 2%\\nlabeled data settings on Total-Text, respectively). More importantly, it still\\nimproves upon a strongly supervised text spotter trained with plenty of labeled\\ndata by 2.0%. Compelling domain adaptation ability shows practical potential.\\nMoreover, our method demonstrates consistent improvement on different text\\nspotters.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-14T08:09:17Z\"}"}

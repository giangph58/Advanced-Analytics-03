{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21700v1\", \"title\": \"XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs\", \"summary\": \"Large Language Models are fundamental actors in the modern IT landscape\\ndominated by AI solutions. However, security threats associated with them might\\nprevent their reliable adoption in critical application scenarios such as\\ngovernment organizations and medical institutions. For this reason, commercial\\nLLMs typically undergo a sophisticated censoring mechanism to eliminate any\\nharmful output they could possibly produce. In response to this, LLM\\nJailbreaking is a significant threat to such protections, and many previous\\napproaches have already demonstrated its effectiveness across diverse domains.\\nExisting jailbreak proposals mostly adopt a generate-and-test strategy to craft\\nmalicious input. To improve the comprehension of censoring mechanisms and\\ndesign a targeted jailbreak attack, we propose an Explainable-AI solution that\\ncomparatively analyzes the behavior of censored and uncensored models to derive\\nunique exploitable alignment patterns. Then, we propose XBreaking, a novel\\njailbreak attack that exploits these unique patterns to break the security\\nconstraints of LLMs by targeted noise injection. Our thorough experimental\\ncampaign returns important insights about the censoring mechanisms and\\ndemonstrates the effectiveness and performance of our attack.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.AI,cs.LG\", \"published\": \"2025-04-30T14:44:24Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19746v1\", \"title\": \"FineQ: Software-Hardware Co-Design for Low-Bit Fine-Grained\\n  Mixed-Precision Quantization of LLMs\", \"summary\": \"Large language models (LLMs) have significantly advanced the natural language\\nprocessing paradigm but impose substantial demands on memory and computational\\nresources. Quantization is one of the most effective ways to reduce memory\\nconsumption of LLMs. However, advanced single-precision quantization methods\\nexperience significant accuracy degradation when quantizing to ultra-low bits.\\nExisting mixed-precision quantization methods are quantized by groups with\\ncoarse granularity. Employing high precision for group data leads to\\nsubstantial memory overhead, whereas low precision severely impacts model\\naccuracy. To address this issue, we propose FineQ, software-hardware co-design\\nfor low-bit fine-grained mixed-precision quantization of LLMs. First, FineQ\\npartitions the weights into finer-grained clusters and considers the\\ndistribution of outliers within these clusters, thus achieving a balance\\nbetween model accuracy and memory overhead. Then, we propose an outlier\\nprotection mechanism within clusters that uses 3 bits to represent outliers and\\nintroduce an encoding scheme for index and data concatenation to enable aligned\\nmemory access. Finally, we introduce an accelerator utilizing temporal coding\\nthat effectively supports the quantization algorithm while simplifying the\\nmultipliers in the systolic array. FineQ achieves higher model accuracy\\ncompared to the SOTA mixed-precision quantization algorithm at a close average\\nbit-width. Meanwhile, the accelerator achieves up to 1.79x energy efficiency\\nand reduces the area of the systolic array by 61.2%.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AR\", \"published\": \"2025-04-28T12:47:23Z\"}"}

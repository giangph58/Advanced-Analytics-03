{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03442v1\", \"title\": \"Knowledge Distillation for Speech Denoising by Latent Representation\\n  Alignment with Cosine Distance\", \"summary\": \"Speech denoising is a generally adopted and impactful task, appearing in many\\ncommon and everyday-life use cases. Although there are very powerful methods\\npublished, most of those are too complex for deployment in everyday and\\nlow-resources computational environments, like hand-held devices, intelligent\\nglasses, hearing aids, etc. Knowledge distillation (KD) is a prominent way for\\nalleviating this complexity mismatch and is based on the\\ntransferring/distilling of knowledge from a pre-trained complex model, the\\nteacher, to another less complex one, the student. Existing KD methods for\\nspeech denoising are based on processes that potentially hamper the KD by\\nbounding the learning of the student to the distribution, information ordering,\\nand feature dimensionality learned by the teacher. In this paper, we present\\nand assess a method that tries to treat this issue, by exploiting the\\nwell-known denoising-autoencoder framework, the linear inverted bottlenecks,\\nand the properties of the cosine similarity. We use a public dataset and\\nconduct repeated experiments with different mismatching scenarios between the\\nteacher and the student, reporting the mean and standard deviation of the\\nmetrics of our method and another, state-of-the-art method that is used as a\\nbaseline. Our results show that with the proposed method, the student can\\nperform better and can also retain greater mismatching conditions compared to\\nthe teacher.\", \"main_category\": \"cs.SD\", \"categories\": \"cs.SD,cs.LG,eess.AS\", \"published\": \"2025-05-06T11:28:28Z\"}"}

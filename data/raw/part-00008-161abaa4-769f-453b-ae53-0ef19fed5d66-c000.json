{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17723v1\", \"title\": \"Towards Robust LLMs: an Adversarial Robustness Measurement Framework\", \"summary\": \"The rise of Large Language Models (LLMs) has revolutionized artificial\\nintelligence, yet these models remain vulnerable to adversarial perturbations,\\nundermining their reliability in high-stakes applications. While adversarial\\nrobustness in vision-based neural networks has been extensively studied, LLM\\nrobustness remains under-explored. We adapt the Robustness Measurement and\\nAssessment (RoMA) framework to quantify LLM resilience against adversarial\\ninputs without requiring access to model parameters. By comparing RoMA's\\nestimates to those of formal verification methods, we demonstrate its accuracy\\nwith minimal error margins while maintaining computational efficiency. Our\\nempirical evaluation reveals that robustness varies significantly not only\\nbetween different models but also across categories within the same task and\\nbetween various types of perturbations. This non-uniformity underscores the\\nneed for task-specific robustness evaluations, enabling practitioners to\\ncompare and select models based on application-specific robustness\\nrequirements. Our work provides a systematic methodology to assess LLM\\nrobustness, advancing the development of more reliable language models for\\nreal-world deployment.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-24T16:36:19Z\"}"}

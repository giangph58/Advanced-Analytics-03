{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03501v1\", \"title\": \"BadLingual: A Novel Lingual-Backdoor Attack against Large Language\\n  Models\", \"summary\": \"In this paper, we present a new form of backdoor attack against Large\\nLanguage Models (LLMs): lingual-backdoor attacks. The key novelty of\\nlingual-backdoor attacks is that the language itself serves as the trigger to\\nhijack the infected LLMs to generate inflammatory speech. They enable the\\nprecise targeting of a specific language-speaking group, exacerbating racial\\ndiscrimination by malicious entities. We first implement a baseline\\nlingual-backdoor attack, which is carried out by poisoning a set of training\\ndata for specific downstream tasks through translation into the trigger\\nlanguage. However, this baseline attack suffers from poor task generalization\\nand is impractical in real-world settings. To address this challenge, we design\\nBadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any\\ndownstream tasks within the chat LLMs, regardless of the specific questions of\\nthese tasks. We design a new approach using PPL-constrained Greedy Coordinate\\nGradient-based Search (PGCG) based adversarial training to expand the decision\\nboundary of lingual-backdoor, thereby enhancing the generalization ability of\\nlingual-backdoor across various tasks. We perform extensive experiments to\\nvalidate the effectiveness of our proposed attacks. Specifically, the baseline\\nattack achieves an ASR of over 90% on the specified tasks. However, its ASR\\nreaches only 37.61% across six tasks in the task-agnostic scenario. In\\ncontrast, BadLingual brings up to 37.35% improvement over the baseline. Our\\nstudy sheds light on a new perspective of vulnerabilities in LLMs with\\nmultilingual capabilities and is expected to promote future research on the\\npotential defenses to enhance the LLMs' robustness\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.CL\", \"published\": \"2025-05-06T13:07:57Z\"}"}

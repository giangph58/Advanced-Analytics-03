{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07029v1\", \"title\": \"Distilling Textual Priors from LLM to Efficient Image Fusion\", \"summary\": \"Multi-modality image fusion aims to synthesize a single, comprehensive image\\nfrom multiple source inputs. Traditional approaches, such as CNNs and GANs,\\noffer efficiency but struggle to handle low-quality or complex inputs. Recent\\nadvances in text-guided methods leverage large model priors to overcome these\\nlimitations, but at the cost of significant computational overhead, both in\\nmemory and inference time. To address this challenge, we propose a novel\\nframework for distilling large model priors, eliminating the need for text\\nguidance during inference while dramatically reducing model size. Our framework\\nutilizes a teacher-student architecture, where the teacher network incorporates\\nlarge model priors and transfers this knowledge to a smaller student network\\nvia a tailored distillation process. Additionally, we introduce spatial-channel\\ncross-fusion module to enhance the model's ability to leverage textual priors\\nacross both spatial and channel dimensions. Our method achieves a favorable\\ntrade-off between computational efficiency and fusion quality. The distilled\\nnetwork, requiring only 10\\\\% of the parameters and inference time of the\\nteacher network, retains 90\\\\% of its performance and outperforms existing SOTA\\nmethods. Extensive experiments demonstrate the effectiveness of our approach.\\nThe implementation will be made publicly available as an open-source resource.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-09T16:44:19Z\"}"}

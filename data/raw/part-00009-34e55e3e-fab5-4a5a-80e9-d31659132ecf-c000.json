{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11900v1\", \"title\": \"Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models\\n  via Plot Hole Detection\", \"summary\": \"Stories are a fundamental aspect of human experience. Engaging deeply with\\nstories and spotting plot holes -- inconsistencies in a storyline that break\\nthe internal logic or rules of a story's world -- requires nuanced reasoning\\nskills, including tracking entities and events and their interplay, abstract\\nthinking, pragmatic narrative understanding, commonsense and social reasoning,\\nand theory of mind. As Large Language Models (LLMs) increasingly generate,\\ninterpret, and modify text, rigorously assessing their narrative consistency\\nand deeper language understanding becomes critical. However, existing\\nbenchmarks focus mainly on surface-level comprehension. In this work, we\\npropose plot hole detection in stories as a proxy to evaluate language\\nunderstanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel\\nalgorithm to controllably and carefully synthesize plot holes in human-written\\nstories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot\\nhole detection abilities in stories -- FlawedFictions -- , which is robust to\\ncontamination, with human filtering ensuring high quality. We find that\\nstate-of-the-art LLMs struggle in accurately solving FlawedFictions regardless\\nof the reasoning effort allowed, with performance significantly degrading as\\nstory length increases. Finally, we show that LLM-based story summarization and\\nstory generation are prone to introducing plot holes, with more than 50% and\\n100% increases in plot hole detection rates with respect to human-written\\noriginals.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-16T09:25:54Z\"}"}

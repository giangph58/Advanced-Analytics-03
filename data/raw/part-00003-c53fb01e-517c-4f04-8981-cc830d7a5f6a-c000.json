{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12609v1\", \"title\": \"Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One\\n  Human Demonstration\", \"summary\": \"Teaching robots dexterous manipulation skills often requires collecting\\nhundreds of demonstrations using wearables or teleoperation, a process that is\\nchallenging to scale. Videos of human-object interactions are easier to collect\\nand scale, but leveraging them directly for robot learning is difficult due to\\nthe lack of explicit action labels from videos and morphological differences\\nbetween robot and human hands. We propose Human2Sim2Robot, a novel\\nreal-to-sim-to-real framework for training dexterous manipulation policies\\nusing only one RGB-D video of a human demonstrating a task. Our method utilizes\\nreinforcement learning (RL) in simulation to cross the human-robot embodiment\\ngap without relying on wearables, teleoperation, or large-scale data collection\\ntypically necessary for imitation learning methods. From the demonstration, we\\nextract two task-specific components: (1) the object pose trajectory to define\\nan object-centric, embodiment-agnostic reward function, and (2) the\\npre-manipulation hand pose to initialize and guide exploration during RL\\ntraining. We found that these two components are highly effective for learning\\nthe desired task, eliminating the need for task-specific reward shaping and\\ntuning. We demonstrate that Human2Sim2Robot outperforms object-aware open-loop\\ntrajectory replay by 55% and imitation learning with data augmentation by 68%\\nacross grasping, non-prehensile manipulation, and multi-step tasks. Project\\nSite: https://human2sim2robot.github.io\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.AI\", \"published\": \"2025-04-17T03:15:20Z\"}"}

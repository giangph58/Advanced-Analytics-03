{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01755v1\", \"title\": \"Bridge the Gap between SNN and ANN for Image Restoration\", \"summary\": \"Models of dense prediction based on traditional Artificial Neural Networks\\n(ANNs) require a lot of energy, especially for image restoration tasks.\\nCurrently, neural networks based on the SNN (Spiking Neural Network) framework\\nare beginning to make their mark in the field of image restoration, especially\\nas they typically use less than 10\\\\% of the energy of ANNs with the same\\narchitecture. However, training an SNN is much more expensive than training an\\nANN, due to the use of the heuristic gradient descent strategy. In other words,\\nthe process of SNN's potential membrane signal changing from sparse to dense is\\nvery slow, which affects the convergence of the whole model.To tackle this\\nproblem, we propose a novel distillation technique, called asymmetric framework\\n(ANN-SNN) distillation, in which the teacher is an ANN and the student is an\\nSNN. Specifically, we leverage the intermediate features (feature maps) learned\\nby the ANN as hints to guide the training process of the SNN. This approach not\\nonly accelerates the convergence of the SNN but also improves its final\\nperformance, effectively bridging the gap between the efficiency of the SNN and\\nthe superior learning capabilities of ANN. Extensive experimental results show\\nthat our designed SNN-based image restoration model, which has only 1/300 the\\nnumber of parameters of the teacher network and 1/50 the energy consumption of\\nthe teacher network, is as good as the teacher network in some denoising tasks.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-02T14:12:06Z\"}"}

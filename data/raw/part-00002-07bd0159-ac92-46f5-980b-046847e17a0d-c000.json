{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10857v1\", \"title\": \"ZeroGrasp: Zero-Shot Shape Reconstruction Enabled Robotic Grasping\", \"summary\": \"Robotic grasping is a cornerstone capability of embodied systems. Many\\nmethods directly output grasps from partial information without modeling the\\ngeometry of the scene, leading to suboptimal motion and even collisions. To\\naddress these issues, we introduce ZeroGrasp, a novel framework that\\nsimultaneously performs 3D reconstruction and grasp pose prediction in near\\nreal-time. A key insight of our method is that occlusion reasoning and modeling\\nthe spatial relationships between objects is beneficial for both accurate\\nreconstruction and grasping. We couple our method with a novel large-scale\\nsynthetic dataset, which comprises 1M photo-realistic images, high-resolution\\n3D reconstructions and 11.3B physically-valid grasp pose annotations for 12K\\nobjects from the Objaverse-LVIS dataset. We evaluate ZeroGrasp on the\\nGraspNet-1B benchmark as well as through real-world robot experiments.\\nZeroGrasp achieves state-of-the-art performance and generalizes to novel\\nreal-world objects by leveraging synthetic data.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.CV\", \"published\": \"2025-04-15T04:37:39Z\"}"}

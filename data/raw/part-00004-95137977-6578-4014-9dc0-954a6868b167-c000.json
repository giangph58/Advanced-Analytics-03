{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03205v1\", \"title\": \"Transformers for Learning on Noisy and Task-Level Manifolds:\\n  Approximation and Generalization Insights\", \"summary\": \"Transformers serve as the foundational architecture for large language and\\nvideo generation models, such as GPT, BERT, SORA and their successors.\\nEmpirical studies have demonstrated that real-world data and learning tasks\\nexhibit low-dimensional structures, along with some noise or measurement error.\\nThe performance of transformers tends to depend on the intrinsic dimension of\\nthe data/tasks, though theoretical understandings remain largely unexplored for\\ntransformers. This work establishes a theoretical foundation by analyzing the\\nperformance of transformers for regression tasks involving noisy input data on\\na manifold. Specifically, the input data are in a tubular neighborhood of a\\nmanifold, while the ground truth function depends on the projection of the\\nnoisy data onto the manifold. We prove approximation and generalization errors\\nwhich crucially depend on the intrinsic dimension of the manifold. Our results\\ndemonstrate that transformers can leverage low-complexity structures in\\nlearning task even when the input data are perturbed by high-dimensional noise.\\nOur novel proof technique constructs representations of basic arithmetic\\noperations by transformers, which may hold independent interest.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.NA,math.NA,math.ST,stat.TH\", \"published\": \"2025-05-06T05:41:46Z\"}"}

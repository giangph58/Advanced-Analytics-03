{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11816v1\", \"title\": \"Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\\n  Offloading\", \"summary\": \"LLM inference is essential for applications like text summarization,\\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\\nService Providers (CSPs) like AWS is a major burden. This paper proposes\\nInferSave, a cost-efficient VM selection framework for cloud based LLM\\ninference. InferSave optimizes KV cache offloading based on Service Level\\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\\nand recommending cost-effective VM instances. Additionally, the Compute Time\\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\\nfor discrepancies between theoretical and actual GPU performance. Experiments\\non AWS GPU instances show that selecting lower-cost instances without KV cache\\noffloading improves cost efficiency by up to 73.7% for online workloads, while\\nKV cache offloading saves up to 20.19% for offline workloads.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.DC\", \"published\": \"2025-04-16T07:02:38Z\"}"}

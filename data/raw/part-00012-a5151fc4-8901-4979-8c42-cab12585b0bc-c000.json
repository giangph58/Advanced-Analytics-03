{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11164v1\", \"title\": \"TSAL: Few-shot Text Segmentation Based on Attribute Learning\", \"summary\": \"Recently supervised learning rapidly develops in scene text segmentation.\\nHowever, the lack of high-quality datasets and the high cost of pixel\\nannotation greatly limit the development of them. Considering the\\nwell-performed few-shot learning methods for downstream tasks, we investigate\\nthe application of the few-shot learning method to scene text segmentation. We\\npropose TSAL, which leverages CLIP's prior knowledge to learn text attributes\\nfor segmentation. To fully utilize the semantic and texture information in the\\nimage, a visual-guided branch is proposed to separately extract text and\\nbackground features. To reduce data dependency and improve text detection\\naccuracy, the adaptive prompt-guided branch employs effective adaptive prompt\\ntemplates to capture various text attributes. To enable adaptive prompts\\ncapture distinctive text features and complex background distribution, we\\npropose Adaptive Feature Alignment module(AFA). By aligning learnable tokens of\\ndifferent attributes with visual features and prompt prototypes, AFA enables\\nadaptive prompts to capture both general and distinctive attribute information.\\nTSAL can capture the unique attributes of text and achieve precise segmentation\\nusing only few images. Experiments demonstrate that our method achieves SOTA\\nperformance on multiple text segmentation datasets under few-shot settings and\\nshow great potential in text-related domains.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-15T13:12:42Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04270v1\", \"title\": \"Object-Shot Enhanced Grounding Network for Egocentric Video\", \"summary\": \"Egocentric video grounding is a crucial task for embodied intelligence\\napplications, distinct from exocentric video moment localization. Existing\\nmethods primarily focus on the distributional differences between egocentric\\nand exocentric videos but often neglect key characteristics of egocentric\\nvideos and the fine-grained information emphasized by question-type queries. To\\naddress these limitations, we propose OSGNet, an Object-Shot enhanced Grounding\\nNetwork for egocentric video. Specifically, we extract object information from\\nvideos to enrich video representation, particularly for objects highlighted in\\nthe textual query but not directly captured in the video features.\\nAdditionally, we analyze the frequent shot movements inherent to egocentric\\nvideos, leveraging these features to extract the wearer's attention\\ninformation, which enhances the model's ability to perform modality alignment.\\nExperiments conducted on three datasets demonstrate that OSGNet achieves\\nstate-of-the-art performance, validating the effectiveness of our approach. Our\\ncode can be found at https://github.com/Yisen-Feng/OSGNet.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-05-07T09:20:12Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21787v1\", \"title\": \"Estimation of discrete distributions in relative entropy, and the\\n  deviations of the missing mass\", \"summary\": \"We study the problem of estimating a distribution over a finite alphabet from\\nan i.i.d. sample, with accuracy measured in relative entropy (Kullback-Leibler\\ndivergence). While optimal expected risk bounds are known, high-probability\\nguarantees remain less well-understood. First, we analyze the classical Laplace\\n(add-$1$) estimator, obtaining matching upper and lower bounds on its\\nperformance and showing its optimality among confidence-independent estimators.\\nWe then characterize the minimax-optimal high-probability risk achievable by\\nany estimator, which is attained via a simple confidence-dependent smoothing\\ntechnique. Interestingly, the optimal non-asymptotic risk contains an\\nadditional logarithmic factor over the ideal asymptotic risk. Next, motivated\\nby scenarios where the alphabet exceeds the sample size, we investigate methods\\nthat adapt to the sparsity of the distribution at hand. We introduce an\\nestimator using data-dependent smoothing, for which we establish a\\nhigh-probability risk bound depending on two effective sparsity parameters. As\\npart of the analysis, we also derive a sharp high-probability upper bound on\\nthe missing mass.\", \"main_category\": \"math.ST\", \"categories\": \"math.ST,cs.IT,cs.LG,math.IT,stat.ML,stat.TH\", \"published\": \"2025-04-30T16:47:10Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20651v1\", \"title\": \"Learning and Generalization with Mixture Data\", \"summary\": \"In many, if not most, machine learning applications the training data is\\nnaturally heterogeneous (e.g. federated learning, adversarial attacks and\\ndomain adaptation in neural net training). Data heterogeneity is identified as\\none of the major challenges in modern day large-scale learning. A classical way\\nto represent heterogeneous data is via a mixture model. In this paper, we study\\ngeneralization performance and statistical rates when data is sampled from a\\nmixture distribution. We first characterize the heterogeneity of the mixture in\\nterms of the pairwise total variation distance of the sub-population\\ndistributions. Thereafter, as a central theme of this paper, we characterize\\nthe range where the mixture may be treated as a single (homogeneous)\\ndistribution for learning. In particular, we study the generalization\\nperformance under the classical PAC framework and the statistical error rates\\nfor parametric (linear regression, mixture of hyperplanes) as well as\\nnon-parametric (Lipschitz, convex and H\\\\\\\"older-smooth) regression problems. In\\norder to do this, we obtain Rademacher complexity and (local) Gaussian\\ncomplexity bounds with mixture data, and apply them to get the generalization\\nand convergence rates respectively. We observe that as the (regression)\\nfunction classes get more complex, the requirement on the pairwise total\\nvariation distance gets stringent, which matches our intuition. We also do a\\nfiner analysis for the case of mixed linear regression and provide a tight\\nbound on the generalization error in terms of heterogeneity.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.LG\", \"published\": \"2025-04-29T11:21:15Z\"}"}

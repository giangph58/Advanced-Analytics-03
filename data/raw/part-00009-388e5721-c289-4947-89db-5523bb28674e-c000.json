{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19724v1\", \"title\": \"RepText: Rendering Visual Text via Replicating\", \"summary\": \"Although contemporary text-to-image generation models have achieved\\nremarkable breakthroughs in producing visually appealing images, their capacity\\nto generate precise and flexible typographic elements, especially non-Latin\\nalphabets, remains constrained. To address these limitations, we start from an\\nnaive assumption that text understanding is only a sufficient condition for\\ntext rendering, but not a necessary condition. Based on this, we present\\nRepText, which aims to empower pre-trained monolingual text-to-image generation\\nmodels with the ability to accurately render, or more precisely, replicate,\\nmultilingual visual text in user-specified fonts, without the need to really\\nunderstand them. Specifically, we adopt the setting from ControlNet and\\nadditionally integrate language agnostic glyph and position of rendered text to\\nenable generating harmonized visual text, allowing users to customize text\\ncontent, font and position on their needs. To improve accuracy, a text\\nperceptual loss is employed along with the diffusion loss. Furthermore, to\\nstabilize rendering process, at the inference phase, we directly initialize\\nwith noisy glyph latent instead of random initialization, and adopt region\\nmasks to restrict the feature injection to only the text region to avoid\\ndistortion of the background. We conducted extensive experiments to verify the\\neffectiveness of our RepText relative to existing works, our approach\\noutperforms existing open-source methods and achieves comparable results to\\nnative multi-language closed-source models. To be more fair, we also\\nexhaustively discuss its limitations in the end.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-28T12:19:53Z\"}"}

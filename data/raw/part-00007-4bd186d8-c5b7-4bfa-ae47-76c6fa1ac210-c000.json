{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06923v1\", \"title\": \"The Importance of Being Discrete: Measuring the Impact of Discretization\\n  in End-to-End Differentially Private Synthetic Data\", \"summary\": \"Differentially Private (DP) generative marginal models are often used in the\\nwild to release synthetic tabular datasets in lieu of sensitive data while\\nproviding formal privacy guarantees. These models approximate low-dimensional\\nmarginals or query workloads; crucially, they require the training data to be\\npre-discretized, i.e., continuous values need to first be partitioned into\\nbins. However, as the range of values (or their domain) is often inferred\\ndirectly from the training data, with the number of bins and bin edges\\ntypically defined arbitrarily, this approach can ultimately break end-to-end DP\\nguarantees and may not always yield optimal utility.\\n  In this paper, we present an extensive measurement study of four\\ndiscretization strategies in the context of DP marginal generative models. More\\nprecisely, we design DP versions of three discretizers (uniform, quantile, and\\nk-means) and reimplement the PrivTree algorithm. We find that optimizing both\\nthe choice of discretizer and bin count can improve utility, on average, by\\nalmost 30% across six DP marginal models, compared to the default strategy and\\nnumber of bins, with PrivTree being the best-performing discretizer in the\\nmajority of cases. We demonstrate that, while DP generative models with\\nnon-private discretization remain vulnerable to membership inference attacks,\\napplying DP during discretization effectively mitigates this risk. Finally, we\\npropose an optimized approach for automatically selecting the optimal number of\\nbins, achieving high utility while reducing both privacy budget consumption and\\ncomputational overhead.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.LG\", \"published\": \"2025-04-09T14:30:30Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14964v1\", \"title\": \"Evaluating Code Generation of LLMs in Advanced Computer Science Problems\", \"summary\": \"Large Language Models (LLMs), such as GitHub Copilot and ChatGPT have become\\npopular among programming students. Students use LLMs to assist them in\\nprogramming courses, including generating source code. Previous work has\\nevaluated the ability of LLMs in solving introductory-course programming\\nassignments. The results have shown that LLMs are highly effective in\\ngenerating code for introductory Computer Science (CS) courses. However, there\\nis a gap in research on evaluating LLMs' ability to generate code that solves\\nadvanced programming assignments. In this work, we evaluate the ability of four\\nLLM tools to solve programming assignments from advanced CS courses in three\\npopular programming languages, Java, Python, and C. We manually select 12\\nproblems, three problems from introductory courses as the baseline and nine\\nprogramming assignments from second- and third-year CS courses. To evaluate the\\nLLM-generated code, we generate a test suite of 1000 test cases per problem and\\nanalyze the program output. Our evaluation shows that although LLMs are highly\\neffective in generating source code for introductory programming courses,\\nsolving advanced programming assignments is more challenging. Nonetheless, in\\nmany cases, LLMs identify the base problem and provide partial solutions that\\nmay be useful to CS students. Furthermore, our results may provide useful\\nguidance for teachers of advanced programming courses on how to design\\nprogramming assignments.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CY\", \"published\": \"2025-04-21T08:45:23Z\"}"}

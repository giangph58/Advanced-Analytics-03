{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07494v1\", \"title\": \"Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\\n  Inference Serving\", \"summary\": \"Large language model (LLM) inference serving systems are essential to various\\nLLM-based applications. As demand for LLM services continues to grow, scaling\\nthese systems to handle high request rates while meeting latency Service-Level\\nObjectives (SLOs), referred to as effective throughput, becomes critical.\\nHowever, existing systems often struggle to improve effective throughput,\\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\\nattainment. We identify two major causes of this bottleneck: (1)\\nmemory-intensive KV cache that limits batch size expansion under GPU memory\\nconstraints, and (2) rigid batch composition enforced by the default\\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\\nKV cache with a memory-efficient hidden cache for reusable input hidden state\\nvectors, allowing large batch sizes and improving request concurrency. Based on\\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\\nthat dynamically optimizes batch composition. We formally define the adaptive\\nscheduling optimization problem and propose an efficient algorithm with\\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\\nto 8.8x improvement in effective throughput compared to the state-of-the-art\\ninference serving systems.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-10T06:51:23Z\"}"}

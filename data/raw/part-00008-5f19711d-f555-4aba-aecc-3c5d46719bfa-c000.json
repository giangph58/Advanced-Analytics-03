{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12083v1\", \"title\": \"Self-alignment of Large Video Language Models with Refined Regularized\\n  Preference Optimization\", \"summary\": \"Despite recent advances in Large Video Language Models (LVLMs), they still\\nstruggle with fine-grained temporal understanding, hallucinate, and often make\\nsimple mistakes on even simple video question-answering tasks, all of which\\npose significant challenges to their safe and reliable deployment in real-world\\napplications. To address these limitations, we propose a self-alignment\\nframework that enables LVLMs to learn from their own errors. Our proposed\\nframework first obtains a training set of preferred and non-preferred response\\npairs, where non-preferred responses are generated by incorporating common\\nerror patterns that often occur due to inadequate spatio-temporal\\nunderstanding, spurious correlations between co-occurring concepts, and\\nover-reliance on linguistic cues while neglecting the vision modality, among\\nothers. To facilitate self-alignment of LVLMs with the constructed preferred\\nand non-preferred response pairs, we introduce Refined Regularized Preference\\nOptimization (RRPO), a novel preference optimization method that utilizes\\nsub-sequence-level refined rewards and token-wise KL regularization to address\\nthe limitations of Direct Preference Optimization (DPO). We demonstrate that\\nRRPO achieves more precise alignment and more stable training compared to DPO.\\nOur experiments and analysis validate the effectiveness of our approach across\\ndiverse video tasks, including video hallucination, short- and long-video\\nunderstanding, and fine-grained temporal reasoning.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-16T13:43:56Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05355v1\", \"title\": \"Nearly Optimal Sample Complexity for Learning with Label Proportions\", \"summary\": \"We investigate Learning from Label Proportions (LLP), a partial information\\nsetting where examples in a training set are grouped into bags, and only\\naggregate label values in each bag are available. Despite the partial\\nobservability, the goal is still to achieve small regret at the level of\\nindividual examples. We give results on the sample complexity of LLP under\\nsquare loss, showing that our sample complexity is essentially optimal. From an\\nalgorithmic viewpoint, we rely on carefully designed variants of Empirical Risk\\nMinimization, and Stochastic Gradient Descent algorithms, combined with ad hoc\\nvariance reduction techniques. On one hand, our theoretical results improve in\\nimportant ways on the existing literature on LLP, specifically in the way the\\nsample complexity depends on the bag size. On the other hand, we validate our\\nalgorithmic solutions on several datasets, demonstrating improved empirical\\nperformance (better accuracy for less samples) against recent baselines.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-08T15:45:23Z\"}"}

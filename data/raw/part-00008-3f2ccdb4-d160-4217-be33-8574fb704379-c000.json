{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11038v1\", \"title\": \"QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models\", \"summary\": \"In typical multimodal tasks, such as Visual Question Answering (VQA),\\nadversarial attacks targeting a specific image and question can lead large\\nvision-language models (LVLMs) to provide incorrect answers. However, it is\\ncommon for a single image to be associated with multiple questions, and LVLMs\\nmay still answer other questions correctly even for an adversarial image\\nattacked by a specific question. To address this, we introduce the\\nquery-agnostic visual attack (QAVA), which aims to create robust adversarial\\nexamples that generate incorrect responses to unspecified and unknown\\nquestions. Compared to traditional adversarial attacks focused on specific\\nimages and questions, QAVA significantly enhances the effectiveness and\\nefficiency of attacks on images when the question is unknown, achieving\\nperformance comparable to attacks on known target questions. Our research\\nbroadens the scope of visual adversarial attacks on LVLMs in practical\\nsettings, uncovering previously overlooked vulnerabilities, particularly in the\\ncontext of visual adversarial threats. The code is available at\\nhttps://github.com/btzyd/qava.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-15T10:00:01Z\"}"}

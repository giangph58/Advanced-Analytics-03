{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15253v1\", \"title\": \"Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as\\n  Test-Time Scaling Evaluators\", \"summary\": \"Scaling test-time computation, or affording a generator large language model\\n(LLM) extra compute during inference, typically employs the help of external\\nnon-generative evaluators (i.e., reward models). Concurrently, LLM-judges,\\nmodels trained to generate evaluations and critiques (explanations) in natural\\nlanguage, are becoming increasingly popular in automatic evaluation. Despite\\njudge empirical successes, their effectiveness as evaluators in test-time\\nscaling settings is largely unknown. In this paper, we introduce the Judge\\nEvaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge\\nperformance in three domains (math reasoning, code generation, and instruction\\nfollowing) under three task settings: response reranking, step-level beam\\nsearch, and critique-based response refinement. We evaluate 10 different judge\\nmodels (7B-70B parameters) for 8 different base generator models (6.7B-72B\\nparameters). Our benchmark shows that while judges are competitive with outcome\\nreward models in reranking, they are consistently worse than process reward\\nmodels in beam search procedures. Furthermore, though unique to LLM-judges,\\ntheir natural language critiques are currently ineffective in guiding the\\ngenerator towards better responses.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-21T17:33:23Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10458v1\", \"title\": \"GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI\\n  Agents\", \"summary\": \"Existing efforts in building Graphical User Interface (GUI) agents largely\\nrely on the training paradigm of supervised fine-tuning on Large\\nVision-Language Models (LVLMs). However, this approach not only demands\\nextensive amounts of training data but also struggles to effectively understand\\nGUI screenshots and generalize to unseen interfaces. The issue significantly\\nlimits its application in real-world scenarios, especially for high-level\\ntasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models\\n(e.g., DeepSeek-R1), which efficiently enhances the problem-solving\\ncapabilities of large language models in real-world settings, we propose \\\\name,\\nthe first reinforcement learning framework designed to enhance the GUI\\ncapabilities of LVLMs in high-level real-world task scenarios, through unified\\naction space rule modeling. By leveraging a small amount of carefully curated\\nhigh-quality data across multiple platforms (including Windows, Linux, MacOS,\\nAndroid, and Web) and employing policy optimization algorithms such as Group\\nRelative Policy Optimization (GRPO) to update the model, \\\\name achieves\\nsuperior performance using only 0.02\\\\% of the data (3K vs. 13M) compared to\\nprevious state-of-the-art methods like OS-Atlas across eight benchmarks\\nspanning three different platforms (mobile, desktop, and web). These results\\ndemonstrate the immense potential of reinforcement learning based on unified\\naction space rule modeling in improving the execution capabilities of LVLMs for\\nreal-world GUI agent tasks.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.CL,cs.HC\", \"published\": \"2025-04-14T17:45:54Z\"}"}

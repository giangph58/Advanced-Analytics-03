{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15188v1\", \"title\": \"Synergistic Weak-Strong Collaboration by Aligning Preferences\", \"summary\": \"Current Large Language Models (LLMs) excel in general reasoning yet struggle\\nwith specialized tasks requiring proprietary or domain-specific knowledge.\\nFine-tuning large models for every niche application is often infeasible due to\\nblack-box constraints and high computational overhead. To address this, we\\npropose a collaborative framework that pairs a specialized weak model with a\\ngeneral strong model. The weak model, tailored to specific domains, produces\\ninitial drafts and background information, while the strong model leverages its\\nadvanced reasoning to refine these drafts, extending LLMs' capabilities to\\ncritical yet specialized tasks. To optimize this collaboration, we introduce a\\ncollaborative feedback to fine-tunes the weak model, which quantifies the\\ninfluence of the weak model's contributions in the collaboration procedure and\\nestablishes preference pairs to guide preference tuning of the weak model. We\\nvalidate our framework through experiments on three domains. We find that the\\ncollaboration significantly outperforms each model alone by leveraging\\ncomplementary strengths. Moreover, aligning the weak model with the\\ncollaborative preference further enhances overall performance.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-21T15:57:33Z\"}"}

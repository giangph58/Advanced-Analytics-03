{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15941v1\", \"title\": \"FairTranslate: An English-French Dataset for Gender Bias Evaluation in\\n  Machine Translation by Overcoming Gender Binarity\", \"summary\": \"Large Language Models (LLMs) are increasingly leveraged for translation tasks\\nbut often fall short when translating inclusive language -- such as texts\\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\\nprotocols. Because these challenges span both computational and societal\\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\\ntranslation with a well-founded framework.\\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\\ndesigned to evaluate non-binary gender biases in machine translation systems\\nfrom English to French. FairTranslate includes 2418 English-French sentence\\npairs related to occupations, annotated with rich metadata such as the\\nstereotypical alignment of the occupation, grammatical gender indicator\\nambiguity, and the ground-truth gender label (male, female, or inclusive).\\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\\nreveal substantial biases in gender representation across LLMs, highlighting\\npersistent challenges in achieving equitable outcomes in machine translation.\\nThese findings underscore the need for focused strategies and interventions\\naimed at ensuring fair and inclusive language usage in LLM-based translation\\nsystems.\\n  We make the FairTranslate dataset publicly available on Hugging Face, and\\ndisclose the code for all experiments on GitHub.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-22T14:35:16Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17447v1\", \"title\": \"FRAG: Frame Selection Augmented Generation for Long Video and Long\\n  Document Understanding\", \"summary\": \"There has been impressive progress in Large Multimodal Models (LMMs). Recent\\nworks extend these models to long inputs, including multi-page documents and\\nlong videos. However, the model size and performance of these long context\\nmodels are still limited due to the computational cost in both training and\\ninference. In this work, we explore an orthogonal direction and process long\\ninputs without long context LMMs. We propose Frame Selection Augmented\\nGeneration (FRAG), where the model first selects relevant frames within the\\ninput, and then only generates the final outputs based on the selected frames.\\nThe core of the selection process is done by scoring each frame independently,\\nwhich does not require long context processing. The frames with the highest\\nscores are then selected by a simple Top-K selection. We show that this\\nfrustratingly simple framework is applicable to both long videos and multi-page\\ndocuments using existing LMMs without any fine-tuning. We consider two models,\\nLLaVA-OneVision and InternVL2, in our experiments and show that FRAG\\nconsistently improves the performance and achieves state-of-the-art\\nperformances for both long video and long document understanding. For videos,\\nFRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on\\nVideo-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA\\ncompared with recent LMMs specialized in long document understanding. Code is\\navailable at: https://github.com/NVlabs/FRAG\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-24T11:19:18Z\"}"}

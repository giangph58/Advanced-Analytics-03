{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04956v1\", \"title\": \"Graffe: Graph Representation Learning via Diffusion Probabilistic Models\", \"summary\": \"Diffusion probabilistic models (DPMs), widely recognized for their potential\\nto generate high-quality samples, tend to go unnoticed in representation\\nlearning. While recent progress has highlighted their potential for capturing\\nvisual semantics, adapting DPMs to graph representation learning remains in its\\ninfancy. In this paper, we introduce Graffe, a self-supervised diffusion model\\nproposed for graph representation learning. It features a graph encoder that\\ndistills a source graph into a compact representation, which, in turn, serves\\nas the condition to guide the denoising process of the diffusion decoder. To\\nevaluate the effectiveness of our model, we first explore the theoretical\\nfoundations of applying diffusion models to representation learning, proving\\nthat the denoising objective implicitly maximizes the conditional mutual\\ninformation between data and its representation. Specifically, we prove that\\nthe negative logarithm of the denoising score matching loss is a tractable\\nlower bound for the conditional mutual information. Empirically, we conduct a\\nseries of case studies to validate our theoretical insights. In addition,\\nGraffe delivers competitive results under the linear probing setting on node\\nand graph classification tasks, achieving state-of-the-art performance on 9 of\\nthe 11 real-world datasets. These findings indicate that powerful generative\\nmodels, especially diffusion models, serve as an effective tool for graph\\nrepresentation learning.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-05-08T05:38:19Z\"}"}

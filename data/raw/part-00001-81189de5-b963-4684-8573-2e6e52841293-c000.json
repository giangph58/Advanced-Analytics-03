{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07471v1\", \"title\": \"Traversal Learning Coordination For Lossless And Efficient Distributed\\n  Learning\", \"summary\": \"In this paper, we introduce Traversal Learning (TL), a novel approach\\ndesigned to address the problem of decreased quality encountered in popular\\ndistributed learning (DL) paradigms such as Federated Learning (FL), Split\\nLearning (SL), and SplitFed Learning (SFL). Traditional FL experiences from an\\naccuracy drop during aggregation due to its averaging function, while SL and\\nSFL face increased loss due to the independent gradient updates on each split\\nnetwork. TL adopts a unique strategy where the model traverses the nodes during\\nforward propagation (FP) and performs backward propagation (BP) on the\\norchestrator, effectively implementing centralized learning (CL) principles\\nwithin a distributed environment. The orchestrator is tasked with generating\\nvirtual batches and planning the sequential node visits of the model during FP,\\naligning them with the ordered index of the data within these batches. We\\nconducted experiments on six datasets representing diverse characteristics\\nacross various domains. Our evaluation demonstrates that TL is on par with\\nclassic CL approaches in terms of accurate inference, thereby offering a viable\\nand robust solution for DL tasks. TL outperformed other DL methods and improved\\naccuracy by 7.85% for independent and identically distributed (IID) datasets,\\nmacro F1-score by 1.06% for non-IID datasets, accuracy by 2.60% for text\\nclassification, and AUC by 3.88% and 4.54% for medical and financial datasets,\\nrespectively. By effectively preserving data privacy while maintaining\\nperformance, TL represents a significant advancement in DL methodologies.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.DC\", \"published\": \"2025-04-10T05:48:57Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15624v1\", \"title\": \"FaceInsight: A Multimodal Large Language Model for Face Perception\", \"summary\": \"Recent advances in multimodal large language models (MLLMs) have demonstrated\\nstrong capabilities in understanding general visual content. However, these\\ngeneral-domain MLLMs perform poorly in face perception tasks, often producing\\ninaccurate or misleading responses to face-specific queries. To address this\\ngap, we propose FaceInsight, the versatile face perception MLLM that provides\\nfine-grained facial information. Our approach introduces visual-textual\\nalignment of facial knowledge to model both uncertain dependencies and\\ndeterministic relationships among facial information, mitigating the\\nlimitations of language-driven reasoning. Additionally, we incorporate face\\nsegmentation maps as an auxiliary perceptual modality, enriching the visual\\ninput with localized structural cues to enhance semantic understanding.\\nComprehensive experiments and analyses across three face perception tasks\\ndemonstrate that FaceInsight consistently outperforms nine compared MLLMs under\\nboth training-free and fine-tuned settings.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-22T06:31:57Z\"}"}

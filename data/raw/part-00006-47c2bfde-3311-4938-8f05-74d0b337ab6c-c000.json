{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07836v1\", \"title\": \"AerialVG: A Challenging Benchmark for Aerial Visual Grounding by\\n  Exploring Positional Relations\", \"summary\": \"Visual grounding (VG) aims to localize target objects in an image based on\\nnatural language descriptions. In this paper, we propose AerialVG, a new task\\nfocusing on visual grounding from aerial views. Compared to traditional VG,\\nAerialVG poses new challenges, \\\\emph{e.g.}, appearance-based grounding is\\ninsufficient to distinguish among multiple visually similar objects, and\\npositional relations should be emphasized. Besides, existing VG models struggle\\nwhen applied to aerial imagery, where high-resolution images cause significant\\ndifficulties. To address these challenges, we introduce the first AerialVG\\ndataset, consisting of 5K real-world aerial images, 50K manually annotated\\ndescriptions, and 103K objects. Particularly, each annotation in AerialVG\\ndataset contains multiple target objects annotated with relative spatial\\nrelations, requiring models to perform comprehensive spatial reasoning.\\nFurthermore, we propose an innovative model especially for the AerialVG task,\\nwhere a Hierarchical Cross-Attention is devised to focus on target regions, and\\na Relation-Aware Grounding module is designed to infer positional relations.\\nExperimental results validate the effectiveness of our dataset and method,\\nhighlighting the importance of spatial reasoning in aerial visual grounding.\\nThe code and dataset will be released.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-10T15:13:00Z\"}"}

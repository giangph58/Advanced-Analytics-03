{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17768v1\", \"title\": \"The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs\", \"summary\": \"Sparse attention offers a promising strategy to extend long-context\\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\\ntrade-offs, and systematic scaling studies remain unexplored. To address this\\ngap, we perform a careful comparison of training-free sparse attention methods\\nat varying model scales, sequence lengths, and sparsity levels on a diverse\\ncollection of long-sequence tasks-including novel ones that rely on natural\\nlanguage while remaining controllable and easy to evaluate. Based on our\\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\\nreveals that for very long sequences, larger and highly sparse models are\\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\\nstatistically guaranteeing accuracy preservation is higher during decoding than\\nprefilling, and correlates with model size in the former. 3) There is no clear\\nstrategy that performs best across tasks and phases, with different units of\\nsparsification or budget adaptivity needed for different scenarios. Even\\nmoderate sparsity levels often result in significant performance degradation on\\nat least one task, highlighting that sparse attention is not a universal\\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\\nfor sparse attention, providing evidence that our findings are likely to hold\\ntrue beyond our range of experiments. Through these insights, we demonstrate\\nthat sparse attention is a key tool to enhance the capabilities of Transformer\\nLLMs for processing longer sequences, but requires careful evaluation of\\ntrade-offs for performance-sensitive applications.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-24T17:39:25Z\"}"}

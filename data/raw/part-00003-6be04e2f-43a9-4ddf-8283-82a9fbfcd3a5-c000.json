{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20922v1\", \"title\": \"DYNAMAX: Dynamic computing for Transformers and Mamba based\\n  architectures\", \"summary\": \"Early exits (EEs) offer a promising approach to reducing computational costs\\nand latency by dynamically terminating inference once a satisfactory prediction\\nconfidence on a data sample is achieved. Although many works integrate EEs into\\nencoder-only Transformers, their application to decoder-only architectures and,\\nmore importantly, Mamba models, a novel family of state-space architectures in\\nthe LLM realm, remains insufficiently explored. This work introduces DYNAMAX,\\nthe first framework to exploit the unique properties of Mamba architectures for\\nearly exit mechanisms. We not only integrate EEs into Mamba but also repurpose\\nMamba as an efficient EE classifier for both Mamba-based and transformer-based\\nLLMs, showcasing its versatility. Our experiments employ the Mistral 7B\\ntransformer compared to the Codestral 7B Mamba model, using data sets such as\\nTruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and\\nconsistency. The results highlight the adaptability of Mamba as a powerful EE\\nclassifier and its efficiency in balancing computational cost and performance\\nquality across NLP tasks. By leveraging Mamba's inherent design for dynamic\\nprocessing, we open pathways for scalable and efficient inference in embedded\\napplications and resource-constrained environments. This study underscores the\\ntransformative potential of Mamba in redefining dynamic computing paradigms for\\nLLMs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-29T16:38:15Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20932v1\", \"title\": \"Improvements of Dark Experience Replay and Reservoir Sampling towards\\n  Better Balance between Consolidation and Plasticity\", \"summary\": \"Continual learning is the one of the most essential abilities for autonomous\\nagents, which can incrementally learn daily-life skills. For this ultimate\\ngoal, a simple but powerful method, dark experience replay (DER), has been\\nproposed recently. DER mitigates catastrophic forgetting, in which the skills\\nacquired in the past are unintentionally forgotten, by stochastically storing\\nthe streaming data in a reservoir sampling (RS) buffer and by relearning them\\nor retaining the past outputs for them. However, since DER considers multiple\\nobjectives, it will not function properly without appropriate weighting of\\nthem. In addition, the ability to retain past outputs inhibits learning if the\\npast outputs are incorrect due to distribution shift or other effects. This is\\ndue to a tradeoff between memory consolidation and plasticity. The tradeoff is\\nhidden even in the RS buffer, which gradually stops storing new data for new\\nskills in it as data is continuously passed to it. To alleviate the tradeoff\\nand achieve better balance, this paper proposes improvement strategies to each\\nof DER and RS. Specifically, DER is improved with automatic adaptation of\\nweights, block of replaying erroneous data, and correction of past outputs. RS\\nis also improved with generalization of acceptance probability, stratification\\nof plural buffers, and intentional omission of unnecessary data. These\\nimprovements are verified through multiple benchmarks including regression,\\nclassification, and reinforcement learning problems. As a result, the proposed\\nmethods achieve steady improvements in learning performance by balancing the\\nmemory consolidation and plasticity.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-29T16:50:05Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07957v1\", \"title\": \"MM-IFEngine: Towards Multimodal Instruction Following\", \"summary\": \"The Instruction Following (IF) ability measures how well Multi-modal Large\\nLanguage Models (MLLMs) understand exactly what users are telling them and\\nwhether they are doing it right. Existing multimodal instruction following\\ntraining data is scarce, the benchmarks are simple with atomic instructions,\\nand the evaluation strategies are imprecise for tasks demanding exact output\\nconstraints. To address this, we present MM-IFEngine, an effective pipeline to\\ngenerate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields\\nlarge-scale, diverse, and high-quality training data MM-IFInstruct-23k, which\\nis suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for\\nDirect Preference Optimization (DPO). We further introduce MM-IFEval, a\\nchallenging and diverse multi-modal instruction-following benchmark that\\nincludes (1) both compose-level constraints for output responses and\\nperception-level constraints tied to the input images, and (2) a comprehensive\\nevaluation pipeline incorporating both rule-based assessment and judge model.\\nWe conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on\\nMM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF\\nbenchmarks, such as MM-IFEval (+10.2$\\\\%$), MIA (+7.6$\\\\%$), and IFEval\\n(+12.3$\\\\%$). The full data and evaluation code will be released on\\nhttps://github.com/SYuan03/MM-IFEngine.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-10T17:59:12Z\"}"}

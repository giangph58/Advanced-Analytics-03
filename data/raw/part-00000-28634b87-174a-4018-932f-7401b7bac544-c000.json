{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20570v1\", \"title\": \"ReCIT: Reconstructing Full Private Data from Gradient in\\n  Parameter-Efficient Fine-Tuning of Large Language Models\", \"summary\": \"Parameter-efficient fine-tuning (PEFT) has emerged as a practical solution\\nfor adapting large language models (LLMs) to custom datasets with significantly\\nreduced computational cost. When carrying out PEFT under collaborative learning\\nscenarios (e.g., federated learning), it is often required to exchange model\\nupdates (or gradients) across parties. These gradients, even with limited\\ndimensions, can cause severe breach of data privacy. Recent works have shown\\nthat both contextual prefixes and personally identifiable information (PII) can\\nbe exposed through gradients. However, \\\\emph{simultaneously} and\\n\\\\emph{accurately} recovering both components from the same training instance\\nremains infeasible due to the following challenges: 1) limited number of PEFT\\nparameters; 2) high-dimensional token spaces; and 3) large batch sizes. We\\npropose ReCIT, a novel privacy attack that addresses all challenges, and\\nachieves recovery of \\\\emph{full} private data from PEFT gradients with high\\nfidelity. Specifically, ReCIT proposes to enhance the memorization capability\\nof the pre-trained model through malicious fine-tuning with Personal Notes;\\nReCIT also proposes a novel filter-based token extraction technique and a token\\npairing mechanism, to accurately reconstruct tokens from the training sequences\\nwith large batch sizes. Extensive evaluations show that ReCIT consistently\\noutperforms state-of-the-art gradient inversion and memorization-based attacks\\nacross different PEFT paradigms. It achieves up to 10$\\\\times$ higher PII\\nrecovery rates and remains effective across varying batch sizes, especially in\\nsettings where prefix reconstruction is intractable for conventional\\napproaches. These findings highlight an urgent need to reassess the privacy\\nguarantees of PEFT, especially in decentralized or shared training\\nenvironments.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR\", \"published\": \"2025-04-29T09:23:19Z\"}"}

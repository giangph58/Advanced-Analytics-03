{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03209v1\", \"title\": \"DYSTIL: Dynamic Strategy Induction with Large Language Models for\\n  Reinforcement Learning\", \"summary\": \"Reinforcement learning from expert demonstrations has long remained a\\nchallenging research problem, and existing state-of-the-art methods using\\nbehavioral cloning plus further RL training often suffer from poor\\ngeneralization, low sample efficiency, and poor model interpretability.\\nInspired by the strong reasoning abilities of large language models (LLMs), we\\npropose a novel strategy-based reinforcement learning framework integrated with\\nLLMs called DYnamic STrategy Induction with Llms for reinforcement learning\\n(DYSTIL) to overcome these limitations. DYSTIL dynamically queries a\\nstrategy-generating LLM to induce textual strategies based on advantage\\nestimations and expert demonstrations, and gradually internalizes induced\\nstrategies into the RL agent through policy optimization to improve its\\nperformance through boosting policy generalization and enhancing sample\\nefficiency. It also provides a direct textual channel to observe and interpret\\nthe evolution of the policy's underlying strategies during training. We test\\nDYSTIL over challenging RL environments from Minigrid and BabyAI, and\\nempirically demonstrate that DYSTIL significantly outperforms state-of-the-art\\nbaseline methods by 17.75% in average success rate while also enjoying higher\\nsample efficiency during the learning process.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-06T05:53:09Z\"}"}

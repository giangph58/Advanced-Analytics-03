{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10985v1\", \"title\": \"DMPT: Decoupled Modality-aware Prompt Tuning for Multi-modal Object\\n  Re-identification\", \"summary\": \"Current multi-modal object re-identification approaches based on large-scale\\npre-trained backbones (i.e., ViT) have displayed remarkable progress and\\nachieved excellent performance. However, these methods usually adopt the\\nstandard full fine-tuning paradigm, which requires the optimization of\\nconsiderable backbone parameters, causing extensive computational and storage\\nrequirements. In this work, we propose an efficient prompt-tuning framework\\ntailored for multi-modal object re-identification, dubbed DMPT, which freezes\\nthe main backbone and only optimizes several newly added decoupled\\nmodality-aware parameters. Specifically, we explicitly decouple the visual\\nprompts into modality-specific prompts which leverage prior modality knowledge\\nfrom a powerful text encoder and modality-independent semantic prompts which\\nextract semantic information from multi-modal inputs, such as visible,\\nnear-infrared, and thermal-infrared. Built upon the extracted features, we\\nfurther design a Prompt Inverse Bind (PromptIBind) strategy that employs bind\\nprompts as a medium to connect the semantic prompt tokens of different\\nmodalities and facilitates the exchange of complementary multi-modal\\ninformation, boosting final re-identification results. Experimental results on\\nmultiple common benchmarks demonstrate that our DMPT can achieve competitive\\nresults to existing state-of-the-art methods while requiring only 6.5%\\nfine-tuning of the backbone parameters.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-15T08:48:41Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11247v1\", \"title\": \"Next-Future: Sample-Efficient Policy Learning for Robotic-Arm Tasks\", \"summary\": \"Hindsight Experience Replay (HER) is widely regarded as the state-of-the-art\\nalgorithm for achieving sample-efficient multi-goal reinforcement learning (RL)\\nin robotic manipulation tasks with binary rewards. HER facilitates learning\\nfrom failed attempts by replaying trajectories with redefined goals. However,\\nit relies on a heuristic-based replay method that lacks a principled framework.\\nTo address this limitation, we introduce a novel replay strategy,\\n\\\"Next-Future\\\", which focuses on rewarding single-step transitions. This\\napproach significantly enhances sample efficiency and accuracy in learning\\nmulti-goal Markov decision processes (MDPs), particularly under stringent\\naccuracy requirements -- a critical aspect for performing complex and precise\\nrobotic-arm tasks. We demonstrate the efficacy of our method by highlighting\\nhow single-step learning enables improved value approximation within the\\nmulti-goal RL framework. The performance of the proposed replay strategy is\\nevaluated across eight challenging robotic manipulation tasks, using ten random\\nseeds for training. Our results indicate substantial improvements in sample\\nefficiency for seven out of eight tasks and higher success rates in six tasks.\\nFurthermore, real-world experiments validate the practical feasibility of the\\nlearned policies, demonstrating the potential of \\\"Next-Future\\\" in solving\\ncomplex robotic-arm tasks.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.CV,cs.LG\", \"published\": \"2025-04-15T14:45:51Z\"}"}

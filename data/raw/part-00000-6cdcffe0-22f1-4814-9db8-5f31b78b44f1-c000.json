{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24321v1\", \"title\": \"Sample-Optimal Private Regression in Polynomial Time\", \"summary\": \"We consider the task of privately obtaining prediction error guarantees in\\nordinary least-squares regression problems with Gaussian covariates (with\\nunknown covariance structure). We provide the first sample-optimal polynomial\\ntime algorithm for this task under both pure and approximate differential\\nprivacy. We show that any improvement to the sample complexity of our algorithm\\nwould violate either statistical-query or information-theoretic lower bounds.\\nAdditionally, our algorithm is robust to a small fraction of arbitrary outliers\\nand achieves optimal error rates as a function of the fraction of outliers. In\\ncontrast, all prior efficient algorithms either incurred sample complexities\\nwith sub-optimal dimension dependence, scaling with the condition number of the\\ncovariates, or obtained a polynomially worse dependence on the privacy\\nparameters.\\n  Our technical contributions are two-fold: first, we leverage resilience\\nguarantees of Gaussians within the sum-of-squares framework. As a consequence,\\nwe obtain efficient sum-of-squares algorithms for regression with optimal\\nrobustness rates and sample complexity. Second, we generalize the recent\\nrobustness-to-privacy framework [HKMN23, (arXiv:2212.05015)] to account for the\\ngeometry induced by the covariance of the input samples. This framework\\ncrucially relies on the robust estimators to be sum-of-squares algorithms, and\\ncombining the two steps yields a sample-optimal private regression algorithm.\\nWe believe our techniques are of independent interest, and we demonstrate this\\nby obtaining an efficient algorithm for covariance-aware mean estimation, with\\nan optimal dependence on the privacy parameters.\", \"main_category\": \"cs.DS\", \"categories\": \"cs.DS,cs.IT,cs.LG,math.IT,stat.ML\", \"published\": \"2025-03-31T17:08:12Z\"}"}

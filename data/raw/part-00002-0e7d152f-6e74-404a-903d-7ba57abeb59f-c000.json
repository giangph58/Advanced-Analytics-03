{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15219v1\", \"title\": \"EvalAgent: Discovering Implicit Evaluation Criteria from the Web\", \"summary\": \"Evaluation of language model outputs on structured writing tasks is typically\\nconducted with a number of desirable criteria presented to human evaluators or\\nlarge language models (LLMs). For instance, on a prompt like \\\"Help me draft an\\nacademic talk on coffee intake vs research productivity\\\", a model response may\\nbe evaluated for criteria like accuracy and coherence. However, high-quality\\nresponses should do more than just satisfy basic task requirements. An\\neffective response to this query should include quintessential features of an\\nacademic talk, such as a compelling opening, clear research questions, and a\\ntakeaway. To help identify these implicit criteria, we introduce EvalAgent, a\\nnovel framework designed to automatically uncover nuanced and task-specific\\ncriteria. EvalAgent first mines expert-authored online guidance. It then uses\\nthis evidence to propose diverse, long-tail evaluation criteria that are\\ngrounded in reliable external sources. Our experiments demonstrate that the\\ngrounded criteria produced by EvalAgent are often implicit (not directly stated\\nin the user's prompt), yet specific (high degree of lexical precision).\\nFurther, EvalAgent criteria are often not satisfied by initial responses but\\nthey are actionable, such that responses can be refined to satisfy them.\\nFinally, we show that combining LLM-generated and EvalAgent criteria uncovers\\nmore human-valued criteria than using LLMs alone.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-21T16:43:50Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05746v1\", \"title\": \"Exploiting Temporal Audio-Visual Correlation Embedding for Audio-Driven\\n  One-Shot Talking Head Animation\", \"summary\": \"The paramount challenge in audio-driven One-shot Talking Head Animation\\n(ADOS-THA) lies in capturing subtle imperceptible changes between adjacent\\nvideo frames. Inherently, the temporal relationship of adjacent audio clips is\\nhighly correlated with that of the corresponding adjacent video frames,\\noffering supplementary information that can be pivotal for guiding and\\nsupervising talking head animations. In this work, we propose to learn\\naudio-visual correlations and integrate the correlations to help enhance\\nfeature representation and regularize final generation by a novel Temporal\\nAudio-Visual Correlation Embedding (TAVCE) framework. Specifically, it first\\nlearns an audio-visual temporal correlation metric, ensuring the temporal audio\\nrelationships of adjacent clips are aligned with the temporal visual\\nrelationships of corresponding adjacent video frames. Since the temporal audio\\nrelationship contains aligned information about the visual frame, we first\\nintegrate it to guide learning more representative features via a simple yet\\neffective channel attention mechanism. During training, we also use the\\nalignment correlations as an additional objective to supervise generating\\nvisual frames. We conduct extensive experiments on several publicly available\\nbenchmarks (i.e., HDTF, LRW, VoxCeleb1, and VoxCeleb2) to demonstrate its\\nsuperiority over existing leading algorithms.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-08T07:23:28Z\"}"}

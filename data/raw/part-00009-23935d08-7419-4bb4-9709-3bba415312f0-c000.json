{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03320v1\", \"title\": \"Recall with Reasoning: Chain-of-Thought Distillation for Mamba's\\n  Long-Context Memory and Extrapolation\", \"summary\": \"Mamba's theoretical infinite-context potential is limited in practice when\\nsequences far exceed training lengths. This work explores unlocking Mamba's\\nlong-context memory ability by a simple-yet-effective method, Recall with\\nReasoning (RwR), by distilling chain-of-thought (CoT) summarization from a\\nteacher model. Specifically, RwR prepends these summarization as CoT prompts\\nduring fine-tuning, teaching Mamba to actively recall and reason over long\\ncontexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's\\nlong-context performance against comparable Transformer/hybrid baselines under\\nsimilar pretraining conditions, while preserving short-context capabilities,\\nall without architectural changes.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-05-06T08:47:58Z\"}"}

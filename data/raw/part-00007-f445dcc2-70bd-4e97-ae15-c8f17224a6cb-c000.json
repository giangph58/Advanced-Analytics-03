{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11197v1\", \"title\": \"Efficient Distributed Retrieval-Augmented Generation for Enhancing\\n  Language Model Performance\", \"summary\": \"Small language models (SLMs) support efficient deployments on\\nresource-constrained edge devices, but their limited capacity compromises\\ninference performance. Retrieval-augmented generation (RAG) is a promising\\nsolution to enhance model performance by integrating external databases,\\nwithout requiring intensive on-device model retraining. However, large-scale\\npublic databases and user-specific private contextual documents are typically\\nlocated on the cloud and the device separately, while existing RAG\\nimplementations are primarily centralized. To bridge this gap, we propose\\nDRAGON, a distributed RAG framework to enhance on-device SLMs through both\\ngeneral and personal knowledge without the risk of leaking document privacy.\\nSpecifically, DRAGON decomposes multi-document RAG into multiple parallel token\\ngeneration processes performed independently and locally on the cloud and the\\ndevice, and employs a newly designed Speculative Aggregation, a dual-side\\nspeculative algorithm to avoid frequent output synchronization between the\\ncloud and device. A new scheduling algorithm is further introduced to identify\\nthe optimal aggregation side based on real-time network conditions. Evaluations\\non real-world hardware testbed demonstrate a significant performance\\nimprovement of DRAGON-up to 1.9x greater gains over standalone SLM compared to\\nthe centralized RAG, substantial reduction in per-token latency, and negligible\\nTime to First Token (TTFT) overhead.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.DC,cs.IR\", \"published\": \"2025-04-15T13:53:08Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12260v1\", \"title\": \"On resolution of L1-norm minimization via a two-metric adaptive\\n  projection method\", \"summary\": \"The two-metric projection method is a simple yet elegant algorithm proposed\\nby Bertsekas\\n  to address bound/box-constrained optimization problems. The algorithm's low\\nper-iteration\\n  cost and potential for using Hessian information make it a favorable\\ncomputation method\\n  for this problem class. Inspired by this algorithm, we propose a two-metric\\nadaptive projection\\n  method for solving the $\\\\ell_1$-norm regularization problem that inherits\\nthese advantages. We demonstrate that the method is theoretically sound -\\n  it has global convergence. Furthermore, it is capable of manifold\\nidentification and has\\n  superlinear convergence rate under the error bound condition and strict\\ncomplementarity.\\n  Therefore, given sparsity in the solution, the method enjoys superfast\\nconvergence in iteration\\n  while maintaining scalability, making it desirable for large-scale problems.\\nWe also equip\\n  the algorithm with competitive complexity to solve nonconvex problems.\\nNumerical experiments\\n  are conducted to illustrate the advantages of this algorithm implied by the\\ntheory compared\\n  to other competitive methods, especially in large-scale scenarios. In\\ncontrast to the original two-metric projection method, our algorithm directly\\nsolves the $\\\\ell_1$-norm minimization problem without resorting to the\\nintermediate reformulation as a bound-constrained problem, so it circumvents\\nthe issue of numerical instability.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC\", \"published\": \"2025-04-16T17:11:39Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01956v1\", \"title\": \"VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in\\n  One Step\", \"summary\": \"Recovering 3D scenes from sparse views is a challenging task due to its\\ninherent ill-posed problem. Conventional methods have developed specialized\\nsolutions (e.g., geometry regularization or feed-forward deterministic model)\\nto mitigate the issue. However, they still suffer from performance degradation\\nby minimal overlap across input views with insufficient visual information.\\nFortunately, recent video generative models show promise in addressing this\\nchallenge as they are capable of generating video clips with plausible 3D\\nstructures. Powered by large pretrained video diffusion models, some pioneering\\nresearch start to explore the potential of video generative prior and create 3D\\nscenes from sparse views. Despite impressive improvements, they are limited by\\nslow inference time and the lack of 3D constraint, leading to inefficiencies\\nand reconstruction artifacts that do not align with real-world geometry\\nstructure. In this paper, we propose VideoScene to distill the video diffusion\\nmodel to generate 3D scenes in one step, aiming to build an efficient and\\neffective tool to bridge the gap from video to 3D. Specifically, we design a\\n3D-aware leap flow distillation strategy to leap over time-consuming redundant\\ninformation and train a dynamic denoising policy network to adaptively\\ndetermine the optimal leap timestep during inference. Extensive experiments\\ndemonstrate that our VideoScene achieves faster and superior 3D scene\\ngeneration results than previous video diffusion models, highlighting its\\npotential as an efficient tool for future video to 3D applications. Project\\nPage: https://hanyang-21.github.io/VideoScene\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-02T17:59:21Z\"}"}

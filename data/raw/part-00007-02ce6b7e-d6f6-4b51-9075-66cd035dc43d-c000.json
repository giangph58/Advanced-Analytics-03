{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19538v1\", \"title\": \"Towards Faster and More Compact Foundation Models for Molecular Property\\n  Prediction\", \"summary\": \"Advancements in machine learning for molecular property prediction have\\nimproved accuracy but at the expense of higher computational cost and longer\\ntraining times. Recently, the Joint Multi-domain Pre-training (JMP) foundation\\nmodel has demonstrated strong performance across various downstream tasks with\\nreduced training time over previous models. Despite JMP's advantages,\\nfine-tuning it on molecular datasets ranging from small-scale to large-scale\\nrequires considerable time and computational resources. In this work, we\\ninvestigate strategies to enhance efficiency by reducing model size while\\npreserving performance. To better understand the model's efficiency, we analyze\\nthe layer contributions of JMP and find that later interaction blocks provide\\ndiminishing returns, suggesting an opportunity for model compression. We\\nexplore block reduction strategies by pruning the pre-trained model and\\nevaluating its impact on efficiency and accuracy during fine-tuning. Our\\nanalysis reveals that removing two interaction blocks results in a minimal\\nperformance drop, reducing the model size by 32% while increasing inference\\nthroughput by 1.3x. These results suggest that JMP-L is over-parameterized and\\nthat a smaller, more efficient variant can achieve comparable performance with\\nlower computational cost. Our study provides insights for developing lighter,\\nfaster, and more scalable foundation models for molecular and materials\\ndiscovery. The code is publicly available at:\\nhttps://github.com/Yasir-Ghunaim/efficient-jmp.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,q-bio.BM\", \"published\": \"2025-04-28T07:41:03Z\"}"}

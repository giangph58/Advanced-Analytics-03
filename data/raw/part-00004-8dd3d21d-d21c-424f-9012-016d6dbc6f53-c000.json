{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04721v1\", \"title\": \"Bridging the Gap between Continuous and Informative Discrete\\n  Representations by Random Product Quantization\", \"summary\": \"Self-supervised learning has become a core technique in speech processing,\\nbut the high dimensionality of its representations makes discretization\\nessential for improving efficiency. However, existing discretization methods\\nstill suffer from significant information loss, resulting in a notable\\nperformance gap compared to continuous representations. To overcome these\\nlimitations, we propose two quantization-based discretization methods: Product\\nQuantization (PQ) and Random Product Quantization (RPQ). PQ partitions the\\noriginal feature space into multiple subspaces and independently quantizes each\\nsub-vector, producing a fused set of discrete units that retain diverse\\ninformation from different subspaces, thus mitigating the loss associated with\\nsingle-cluster quantization. RPQ further enhances representation diversity by\\nrandomly sampling a fixed proportion of feature dimensions multiple times to\\nconstruct sub-vectors, thereby better capturing the variability in the data\\ndistribution. Theoretical analysis shows that RPQ reduces the correlation\\ncoefficient rho (where 0 <= rho <= 1) between sub-quantizers. Its quantization\\nerror is lower-bounded by the product of rho and epsilon-kms, where epsilon-kms\\ndenotes the quantization error of a single K-means quantizer. Experimental\\nresults on a combined dataset built from LibriSpeech and ML-SUPERB show that PQ\\nand RPQ outperform standard K-means discretization, achieving relative\\nimprovements of 21.8 percent and 20.0 percent in WER on LibriSpeech, and 24.1\\npercent and 19.6 percent in CER on ML-SUPERB, respectively. Moreover, their\\nperformance is competitive with, and in some cases even surpasses, that of\\ncontinuous SSL representations.\", \"main_category\": \"eess.AS\", \"categories\": \"eess.AS\", \"published\": \"2025-04-07T04:18:11Z\"}"}

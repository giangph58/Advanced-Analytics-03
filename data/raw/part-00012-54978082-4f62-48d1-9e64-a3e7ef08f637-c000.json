{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17353v1\", \"title\": \"M-MRE: Extending the Mutual Reinforcement Effect to Multimodal\\n  Information Extraction\", \"summary\": \"Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection\\nof information extraction and model interpretability. MRE aims to leverage the\\nmutual understanding between tasks of different granularities, enhancing the\\nperformance of both coarse-grained and fine-grained tasks through joint\\nmodeling. While MRE has been explored and validated in the textual domain, its\\napplicability to visual and multimodal domains remains unexplored. In this\\nwork, we extend MRE to the multimodal information extraction domain for the\\nfirst time. Specifically, we introduce a new task: Multimodal Mutual\\nReinforcement Effect (M-MRE), and construct a corresponding dataset to support\\nthis task. To address the challenges posed by M-MRE, we further propose a\\nPrompt Format Adapter (PFA) that is fully compatible with various Large\\nVision-Language Models (LVLMs). Experimental results demonstrate that MRE can\\nalso be observed in the M-MRE task, a multimodal text-image understanding\\nscenario. This provides strong evidence that MRE facilitates mutual gains\\nacross three interrelated tasks, confirming its generalizability beyond the\\ntextual domain.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.CV,cs.MM\", \"published\": \"2025-04-24T08:14:36Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04139v1\", \"title\": \"LHT: Statistically-Driven Oblique Decision Trees for Interpretable\\n  Classification\", \"summary\": \"We introduce the Learning Hyperplane Tree (LHT), a novel oblique decision\\ntree model designed for expressive and interpretable classification. LHT\\nfundamentally distinguishes itself through a non-iterative,\\nstatistically-driven approach to constructing splitting hyperplanes. Unlike\\nmethods that rely on iterative optimization or heuristics, LHT directly\\ncomputes the hyperplane parameters, which are derived from feature weights\\nbased on the differences in feature expectations between classes within each\\nnode. This deterministic mechanism enables a direct and well-defined hyperplane\\nconstruction process. Predictions leverage a unique piecewise linear membership\\nfunction within leaf nodes, obtained via local least-squares fitting. We\\nformally analyze the convergence of the LHT splitting process, ensuring that\\neach split yields meaningful, non-empty partitions. Furthermore, we establish\\nthat the time complexity for building an LHT up to depth $d$ is $O(mnd)$,\\ndemonstrating the practical feasibility of constructing trees with powerful\\noblique splits using this methodology. The explicit feature weighting at each\\nsplit provides inherent interpretability. Experimental results on benchmark\\ndatasets demonstrate LHT's competitive accuracy, positioning it as a practical,\\ntheoretically grounded, and interpretable alternative in the landscape of\\ntree-based models. The implementation of the proposed method is available at\\nhttps://github.com/Hongyi-Li-sz/LHT_model.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-07T05:25:44Z\"}"}

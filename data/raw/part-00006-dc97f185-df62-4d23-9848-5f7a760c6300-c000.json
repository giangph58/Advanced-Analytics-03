{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16068v1\", \"title\": \"High-performance training and inference for deep equivariant interatomic\\n  potentials\", \"summary\": \"Machine learning interatomic potentials, particularly those based on deep\\nequivariant neural networks, have demonstrated state-of-the-art accuracy and\\ncomputational efficiency in atomistic modeling tasks like molecular dynamics\\nand high-throughput screening. The size of datasets and demands of downstream\\nworkflows are growing rapidly, making robust and scalable software essential.\\nThis work presents a major overhaul of the NequIP framework focusing on\\nmulti-node parallelism, computational performance, and extensibility. The\\nredesigned framework supports distributed training on large datasets and\\nremoves barriers preventing full utilization of the PyTorch 2.0 compiler at\\ntrain time. We demonstrate this acceleration in a case study by training\\nAllegro models on the SPICE 2 dataset of organic molecular systems. For\\ninference, we introduce the first end-to-end infrastructure that uses the\\nPyTorch Ahead-of-Time Inductor compiler for machine learning interatomic\\npotentials. Additionally, we implement a custom kernel for the Allegro model's\\nmost expensive operation, the tensor product. Together, these advancements\\nspeed up molecular dynamics calculations on system sizes of practical relevance\\nby up to a factor of 18.\", \"main_category\": \"physics.comp-ph\", \"categories\": \"physics.comp-ph,cs.LG,physics.chem-ph\", \"published\": \"2025-04-22T17:47:01Z\"}"}

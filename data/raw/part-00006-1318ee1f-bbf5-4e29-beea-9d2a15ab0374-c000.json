{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21775v1\", \"title\": \"Learning Heterogeneous Performance-Fairness Trade-offs in Federated\\n  Learning\", \"summary\": \"Recent methods leverage a hypernet to handle the performance-fairness\\ntrade-offs in federated learning. This hypernet maps the clients' preferences\\nbetween model performance and fairness to preference-specifc models on the\\ntrade-off curve, known as local Pareto front. However, existing methods\\ntypically adopt a uniform preference sampling distribution to train the\\nhypernet across clients, neglecting the inherent heterogeneity of their local\\nPareto fronts. Meanwhile, from the perspective of generalization, they do not\\nconsider the gap between local and global Pareto fronts on the global dataset.\\nTo address these limitations, we propose HetPFL to effectively learn both local\\nand global Pareto fronts. HetPFL comprises Preference Sampling Adaptation (PSA)\\nand Preference-aware Hypernet Fusion (PHF). PSA adaptively determines the\\noptimal preference sampling distribution for each client to accommodate\\nheterogeneous local Pareto fronts. While PHF performs preference-aware fusion\\nof clients' hypernets to ensure the performance of the global Pareto front. We\\nprove that HetPFL converges linearly with respect to the number of rounds,\\nunder weaker assumptions than existing methods. Extensive experiments on four\\ndatasets show that HetPFL significantly outperforms seven baselines in terms of\\nthe quality of learned local and global Pareto fronts.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-30T16:25:02Z\"}"}

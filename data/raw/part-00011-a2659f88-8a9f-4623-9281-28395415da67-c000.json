{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00582v1\", \"title\": \"Block Circulant Adapter for Large Language Models\", \"summary\": \"Fine-tuning large language models (LLMs) is difficult due to their huge model\\nsize. Recent Fourier domain-based methods show potential for reducing\\nfine-tuning costs. We propose a block circulant matrix-based fine-tuning method\\nwith a stable training heuristic to leverage the properties of circulant\\nmatrices and one-dimensional Fourier transforms to reduce storage and\\ncomputation costs. Experiments show that our method uses $14\\\\times$ less number\\nof parameters than VeRA, $16\\\\times$ smaller than LoRA and $32\\\\times$ less FLOPs\\nthan FourierFT, while maintaining close or better task performance. Our\\napproach presents a promising way in frequency domain to fine-tune large models\\non downstream tasks.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-05-01T15:14:32Z\"}"}

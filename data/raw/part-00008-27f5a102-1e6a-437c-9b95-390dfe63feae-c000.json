{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16677v1\", \"title\": \"A Post-trainer's Guide to Multilingual Training Data: Uncovering\\n  Cross-lingual Transfer Dynamics\", \"summary\": \"In order for large language models to be useful across the globe, they are\\nfine-tuned to follow instructions on multilingual data. Despite the ubiquity of\\nsuch post-training, a clear understanding of the dynamics that enable\\ncross-lingual transfer remains elusive. This study examines cross-lingual\\ntransfer (CLT) dynamics in realistic post-training settings. We study two model\\nfamilies of up to 35B parameters in size trained on carefully controlled\\nmixtures of multilingual data on three generative tasks with varying levels of\\ncomplexity (summarization, instruction following, and mathematical reasoning)\\nin both single-task and multi-task instruction tuning settings. Overall, we\\nfind that the dynamics of cross-lingual transfer and multilingual performance\\ncannot be explained by isolated variables, varying depending on the combination\\nof post-training settings. Finally, we identify the conditions that lead to\\neffective cross-lingual transfer in practice.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-23T12:52:49Z\"}"}

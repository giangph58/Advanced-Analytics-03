{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20446v1\", \"title\": \"FT-MoE: Sustainable-learning Mixture of Experts Model for Fault-Tolerant\\n  Computing with Multiple Tasks\", \"summary\": \"Intelligent fault-tolerant (FT) computing has recently demonstrated\\nsignificant advantages of predicting and diagnosing faults in advance, enabling\\nreliable service delivery. However, due to heterogeneity of fault knowledge and\\ncomplex dependence relationships of time series log data, existing deep\\nlearning-based FT algorithms further improve detection performance relying on\\nsingle neural network model with difficulty. To this end, we propose FT-MoE, a\\nsustainable-learning mixture-of-experts model for fault-tolerant computing with\\nmultiple tasks, which enables different parameters learning distinct fault\\nknowledge to achieve high-reliability for service system. Firstly, we use\\ndecoder-based transformer models to obtain fault prototype vectors of\\ndecoupling long-distance dependencies. Followed by, we present a dual mixture\\nof experts networks for high-accurate prediction for both fault detection and\\nclassification tasks. Then, we design a two-stage optimization scheme of\\noffline training and online tuning, which allows that in operation FT-MoE can\\nalso keep learning to adapt to dynamic service environments. Finally, to verify\\nthe effectiveness of FT-MoE, we conduct extensive experiments on the FT\\nbenchmark. Experimental results show that FT-MoE achieves superior performance\\ncompared to the state-of-the-art methods. Code will be available upon\\npublication.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-29T05:44:59Z\"}"}

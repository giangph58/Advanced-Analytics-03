{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14927v1\", \"title\": \"Multimodal Non-Semantic Feature Fusion for Predicting Segment Access\\n  Frequency in Lecture Archives\", \"summary\": \"This study proposes a multimodal neural network-based approach to predict\\nsegment access frequency in lecture archives. These archives, widely used as\\nsupplementary resources in modern education, often consist of long, unedited\\nrecordings that make it difficult to keep students engaged. Captured directly\\nfrom face-to-face lectures without post-processing, they lack visual appeal.\\nMeanwhile, the increasing volume of recorded material renders manual editing\\nand annotation impractical. Automatically detecting high-engagement segments is\\nthus crucial for improving accessibility and maintaining learning\\neffectiveness. Our research focuses on real classroom lecture archives,\\ncharacterized by unedited footage, no additional hardware (e.g., eye-tracking),\\nand limited student numbers. We approximate student engagement using segment\\naccess frequency as a proxy. Our model integrates multimodal features from\\nteachers' actions (via OpenPose and optical flow), audio spectrograms, and\\nslide page progression. These features are deliberately chosen for their\\nnon-semantic nature, making the approach applicable regardless of lecture\\nlanguage. Experiments show that our best model achieves a Pearson correlation\\nof 0.5143 in 7-fold cross-validation and 69.32 percent average accuracy in a\\ndownstream three-class classification task. The results, obtained with high\\ncomputational efficiency and a small dataset, demonstrate the practical\\nfeasibility of our system in real-world educational contexts.\", \"main_category\": \"cs.HC\", \"categories\": \"cs.HC\", \"published\": \"2025-04-21T07:48:11Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03319v1\", \"title\": \"SD-VSum: A Method and Dataset for Script-Driven Video Summarization\", \"summary\": \"In this work, we introduce the task of script-driven video summarization,\\nwhich aims to produce a summary of the full-length video by selecting the parts\\nthat are most relevant to a user-provided script outlining the visual content\\nof the desired summary. Following, we extend a recently-introduced large-scale\\ndataset for generic video summarization (VideoXum) by producing natural\\nlanguage descriptions of the different human-annotated summaries that are\\navailable per video. In this way we make it compatible with the introduced\\ntask, since the available triplets of ``video, summary and summary\\ndescription'' can be used for training a method that is able to produce\\ndifferent summaries for a given video, driven by the provided script about the\\ncontent of each summary. Finally, we develop a new network architecture for\\nscript-driven video summarization (SD-VSum), that relies on the use of a\\ncross-modal attention mechanism for aligning and fusing information from the\\nvisual and text modalities. Our experimental evaluations demonstrate the\\nadvanced performance of SD-VSum against state-of-the-art approaches for\\nquery-driven and generic (unimodal and multimodal) summarization from the\\nliterature, and document its capacity to produce video summaries that are\\nadapted to each user's needs about their content.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.MM\", \"published\": \"2025-05-06T08:47:14Z\"}"}

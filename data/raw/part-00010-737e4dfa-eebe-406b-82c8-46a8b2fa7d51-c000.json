{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13122v1\", \"title\": \"VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference\\n  Optimization for Large Video Models\", \"summary\": \"Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\\npromise in video understanding but often suffer from misalignment with human\\nintuition and video hallucination issues. To address these challenges, we\\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\\nDirect Preference Optimization. VistaDPO enhances text-video preference\\nalignment across three hierarchical levels: i) Instance Level, aligning overall\\nvideo content with responses; ii) Temporal Level, aligning video temporal\\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\\nobjects with language tokens. Given the lack of datasets for fine-grained\\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\\n7.2K QA pairs annotated with chosen and rejected responses, along with\\nspatial-temporal grounding information such as timestamps, keyframes, and\\nbounding boxes. Extensive experiments on benchmarks such as Video\\nHallucination, Video QA, and Captioning performance tasks demonstrate that\\nVistaDPO significantly improves the performance of existing LVMs, effectively\\nmitigating video-language misalignment and hallucination. The code and data are\\navailable at https://github.com/HaroldChen19/VistaDPO.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG\", \"published\": \"2025-04-17T17:39:41Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05470v1\", \"title\": \"Flow-GRPO: Training Flow Matching Models via Online RL\", \"summary\": \"We propose Flow-GRPO, the first method integrating online reinforcement\\nlearning (RL) into flow matching models. Our approach uses two key strategies:\\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\\n(SDE) that matches the original model's marginal distribution at all timesteps,\\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\\nstrategy that reduces training denoising steps while retaining the original\\ninference timestep number, significantly improving sampling efficiency without\\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\\nperfect object counts, spatial relations, and fine-grained attributes, boosting\\nGenEval accuracy from $63\\\\%$ to $95\\\\%$. In visual text rendering, its accuracy\\nimproves from $59\\\\%$ to $92\\\\%$, significantly enhancing text generation.\\nFlow-GRPO also achieves substantial gains in human preference alignment.\\nNotably, little to no reward hacking occurred, meaning rewards did not increase\\nat the cost of image quality or diversity, and both remained stable in our\\nexperiments.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-05-08T17:58:45Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12972v1\", \"title\": \"Estimating Optimal Context Length for Hybrid Retrieval-augmented\\n  Multi-document Summarization\", \"summary\": \"Recent advances in long-context reasoning abilities of language models led to\\ninteresting applications in large-scale multi-document summarization. However,\\nprior work has shown that these long-context models are not effective at their\\nclaimed context windows. To this end, retrieval-augmented systems provide an\\nefficient and effective alternative. However, their performance can be highly\\nsensitive to the choice of retrieval context length. In this work, we present a\\nhybrid method that combines retrieval-augmented systems with long-context\\nwindows supported by recent language models. Our method first estimates the\\noptimal retrieval length as a function of the retriever, summarizer, and\\ndataset. On a randomly sampled subset of the dataset, we use a panel of LLMs to\\ngenerate a pool of silver references. We use these silver references to\\nestimate the optimal context length for a given RAG system configuration. Our\\nresults on the multi-document summarization task showcase the effectiveness of\\nour method across model classes and sizes. We compare against length estimates\\nfrom strong long-context benchmarks such as RULER and HELMET. Our analysis also\\nhighlights the effectiveness of our estimation method for very long-context LMs\\nand its generalization to new classes of LMs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-17T14:24:51Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07807v1\", \"title\": \"Cluster-Driven Expert Pruning for Mixture-of-Experts Large Language\\n  Models\", \"summary\": \"Mixture-of-Experts (MoE) architectures have emerged as a promising paradigm\\nfor scaling large language models (LLMs) with sparse activation of\\ntask-specific experts. Despite their computational efficiency during inference,\\nthe massive overall parameter footprint of MoE models (e.g., GPT-4) introduces\\ncritical challenges for practical deployment. Current pruning approaches often\\nfail to address two inherent characteristics of MoE systems: 1).intra-layer\\nexpert homogeneity where experts within the same MoE layer exhibit functional\\nredundancy, and 2). inter-layer similarity patterns where deeper layers tend to\\ncontain progressively more homogeneous experts. To tackle these issues, we\\npropose Cluster-driven Expert Pruning (C-Prune), a novel two-stage framework\\nfor adaptive task-specific compression of MoE LLMs. C-Prune operates through\\nlayer-wise expert clustering, which groups functionally similar experts within\\neach MoE layer using parameter similarity metrics, followed by global cluster\\npruning, which eliminates redundant clusters across all layers through a\\nunified importance scoring mechanism that accounts for cross-layer homogeneity.\\nWe validate C-Prune through extensive experiments on multiple MoE models and\\nbenchmarks. The results demonstrate that C-Prune effectively reduces model size\\nwhile outperforming existing MoE pruning methods.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-10T14:46:26Z\"}"}

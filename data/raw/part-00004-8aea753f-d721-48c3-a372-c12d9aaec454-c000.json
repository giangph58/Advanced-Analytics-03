{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17670v1\", \"title\": \"DiMeR: Disentangled Mesh Reconstruction Model\", \"summary\": \"With the advent of large-scale 3D datasets, feed-forward 3D generative\\nmodels, such as the Large Reconstruction Model (LRM), have gained significant\\nattention and achieved remarkable success. However, we observe that RGB images\\noften lead to conflicting training objectives and lack the necessary clarity\\nfor geometry reconstruction. In this paper, we revisit the inductive biases\\nassociated with mesh reconstruction and introduce DiMeR, a novel disentangled\\ndual-stream feed-forward model for sparse-view mesh reconstruction. The key\\nidea is to disentangle both the input and framework into geometry and texture\\nparts, thereby reducing the training difficulty for each part according to the\\nPrinciple of Occam's Razor. Given that normal maps are strictly consistent with\\ngeometry and accurately capture surface variations, we utilize normal maps as\\nexclusive input for the geometry branch to reduce the complexity between the\\nnetwork's input and output. Moreover, we improve the mesh extraction algorithm\\nto introduce 3D ground truth supervision. As for texture branch, we use RGB\\nimages as input to obtain the textured mesh. Overall, DiMeR demonstrates robust\\ncapabilities across various tasks, including sparse-view reconstruction,\\nsingle-image-to-3D, and text-to-3D. Numerous experiments show that DiMeR\\nsignificantly outperforms previous methods, achieving over 30% improvement in\\nChamfer Distance on the GSO and OmniObject3D dataset.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-24T15:39:20Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12680v1\", \"title\": \"Embodied-R: Collaborative Framework for Activating Embodied Spatial\\n  Reasoning in Foundation Models via Reinforcement Learning\", \"summary\": \"Humans can perceive and reason about spatial relationships from sequential\\nvisual observations, such as egocentric video streams. However, how pretrained\\nmodels acquire such abilities, especially high-level reasoning, remains\\nunclear. This paper introduces Embodied-R, a collaborative framework combining\\nlarge-scale Vision-Language Models (VLMs) for perception and small-scale\\nLanguage Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a\\nnovel reward system considering think-answer logical consistency, the model\\nachieves slow-thinking capabilities with limited computational resources. After\\ntraining on only 5k embodied video samples, Embodied-R with a 3B LM matches\\nstate-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on\\nboth in-distribution and out-of-distribution embodied spatial reasoning tasks.\\nEmbodied-R also exhibits emergent thinking patterns such as systematic analysis\\nand contextual integration. We further explore research questions including\\nresponse length, training on VLM, strategies for reward design, and differences\\nin model generalization after SFT (Supervised Fine-Tuning) and RL training.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CV\", \"published\": \"2025-04-17T06:16:11Z\"}"}

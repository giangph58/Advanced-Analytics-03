{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17660v1\", \"title\": \"Effortless, Simulation-Efficient Bayesian Inference using Tabular\\n  Foundation Models\", \"summary\": \"Simulation-based inference (SBI) offers a flexible and general approach to\\nperforming Bayesian inference: In SBI, a neural network is trained on synthetic\\ndata simulated from a model and used to rapidly infer posterior distributions\\nfor observed data. A key goal for SBI is to achieve accurate inference with as\\nfew simulations as possible, especially for expensive simulators. In this work,\\nwe address this challenge by repurposing recent probabilistic foundation models\\nfor tabular data: We show how tabular foundation models -- specifically TabPFN\\n-- can be used as pre-trained autoregressive conditional density estimators for\\nSBI. We propose Neural Posterior Estimation with Prior-data Fitted Networks\\n(NPE-PF) and show that it is competitive with current SBI approaches in terms\\nof accuracy for both benchmark tasks and two complex scientific inverse\\nproblems. Crucially, it often substantially outperforms them in terms of\\nsimulation efficiency, sometimes requiring orders of magnitude fewer\\nsimulations. NPE-PF eliminates the need for inference network selection,\\ntraining, and hyperparameter tuning. We also show that it exhibits superior\\nrobustness to model misspecification and can be scaled to simulation budgets\\nthat exceed the context size limit of TabPFN. NPE-PF provides a new direction\\nfor SBI, where training-free, general-purpose inference models offer efficient,\\neasy-to-use, and flexible solutions for a wide range of stochastic inverse\\nproblems.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-24T15:29:39Z\"}"}

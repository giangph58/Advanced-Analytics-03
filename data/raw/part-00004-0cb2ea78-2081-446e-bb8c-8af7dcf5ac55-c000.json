{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19480v1\", \"title\": \"An Automated Reinforcement Learning Reward Design Framework with Large\\n  Language Model for Cooperative Platoon Coordination\", \"summary\": \"Reinforcement Learning (RL) has demonstrated excellent decision-making\\npotential in platoon coordination problems. However, due to the variability of\\ncoordination goals, the complexity of the decision problem, and the\\ntime-consumption of trial-and-error in manual design, finding a well\\nperformance reward function to guide RL training to solve complex platoon\\ncoordination problems remains challenging. In this paper, we formally define\\nthe Platoon Coordination Reward Design Problem (PCRDP), extending the RL-based\\ncooperative platoon coordination problem to incorporate automated reward\\nfunction generation. To address PCRDP, we propose a Large Language Model\\n(LLM)-based Platoon coordination Reward Design (PCRD) framework, which\\nsystematically automates reward function discovery through LLM-driven\\ninitialization and iterative optimization. In this method, LLM first\\ninitializes reward functions based on environment code and task requirements\\nwith an Analysis and Initial Reward (AIR) module, and then iteratively\\noptimizes them based on training feedback with an evolutionary module. The AIR\\nmodule guides LLM to deepen their understanding of code and tasks through a\\nchain of thought, effectively mitigating hallucination risks in code\\ngeneration. The evolutionary module fine-tunes and reconstructs the reward\\nfunction, achieving a balance between exploration diversity and convergence\\nstability for training. To validate our approach, we establish six challenging\\ncoordination scenarios with varying complexity levels within the Yangtze River\\nDelta transportation network simulation. Comparative experimental results\\ndemonstrate that RL agents utilizing PCRD-generated reward functions\\nconsistently outperform human-engineered reward functions, achieving an average\\nof 10\\\\% higher performance metrics in all scenarios.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-28T04:41:15Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21659v1\", \"title\": \"AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning\\n  Optimization\", \"summary\": \"Recently, long-thought reasoning models achieve strong performance on complex\\nreasoning tasks, but often incur substantial inference overhead, making\\nefficiency a critical concern. Our empirical analysis reveals that the benefit\\nof using Long-CoT varies across problems: while some problems require elaborate\\nreasoning, others show no improvement, or even degraded accuracy. This\\nmotivates adaptive reasoning strategies that tailor reasoning depth to the\\ninput. However, prior work primarily reduces redundancy within long reasoning\\npaths, limiting exploration of more efficient strategies beyond the Long-CoT\\nparadigm. To address this, we propose a novel two-stage framework for adaptive\\nand efficient reasoning. First, we construct a hybrid reasoning model by\\nmerging long and short CoT models to enable diverse reasoning styles. Second,\\nwe apply bi-level preference training to guide the model to select suitable\\nreasoning styles (group-level), and prefer concise and correct reasoning within\\neach style group (instance-level). Experiments demonstrate that our method\\nsignificantly reduces inference costs compared to other baseline approaches,\\nwhile maintaining performance. Notably, on five mathematical datasets, the\\naverage length of reasoning is reduced by more than 50%, highlighting the\\npotential of adaptive strategies to optimize reasoning efficiency in large\\nlanguage models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CL\", \"published\": \"2025-04-30T14:01:45Z\"}"}

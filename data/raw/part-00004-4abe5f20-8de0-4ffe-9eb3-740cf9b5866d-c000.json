{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06166v1\", \"title\": \"Assessing how hyperparameters impact Large Language Models' sarcasm\\n  detection performance\", \"summary\": \"Sarcasm detection is challenging for both humans and machines. This work\\nexplores how model characteristics impact sarcasm detection in OpenAI's GPT,\\nand Meta's Llama-2 models, given their strong natural language understanding,\\nand popularity. We evaluate fine-tuned and zero-shot models across various\\nsizes, releases, and hyperparameters. Experiments were conducted on the\\npolitical and balanced (pol-bal) portion of the popular Self-Annotated Reddit\\nCorpus (SARC2.0) sarcasm dataset. Fine-tuned performance improves monotonically\\nwith model size within a model family, while hyperparameter tuning also impacts\\nperformance. In the fine-tuning scenario, full precision Llama-2-13b achieves\\nstate-of-the-art accuracy and $F_1$-score, both measured at 0.83, comparable to\\naverage human performance. In the zero-shot setting, one GPT-4 model achieves\\ncompetitive performance to prior attempts, yielding an accuracy of 0.70 and an\\n$F_1$-score of 0.75. Furthermore, a model's performance may increase or decline\\nwith each release, highlighting the need to reassess performance after each\\nrelease.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-08T16:05:25Z\"}"}

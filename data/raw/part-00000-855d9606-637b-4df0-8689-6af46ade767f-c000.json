{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05136v1\", \"title\": \"Information Geometry of Exponentiated Gradient: Convergence beyond\\n  L-Smoothness\", \"summary\": \"We study the minimization of smooth, possibly nonconvex functions over the\\npositive orthant, a key setting in Poisson inverse problems, using the\\nexponentiated gradient (EG) method. Interpreting EG as Riemannian gradient\\ndescent (RGD) with the $e$-Exp map from information geometry as a retraction,\\nwe prove global convergence under weak assumptions -- without the need for\\n$L$-smoothness -- and finite termination of Riemannian Armijo line search.\\nNumerical experiments, including an accelerated variant, highlight EG's\\npractical advantages, such as faster convergence compared to RGD based on\\ninterior-point geometry.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC\", \"published\": \"2025-04-07T14:41:26Z\"}"}

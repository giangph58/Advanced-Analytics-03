{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12778v1\", \"title\": \"Towards Lossless Token Pruning in Late-Interaction Retrieval Models\", \"summary\": \"Late interaction neural IR models like ColBERT offer a competitive\\neffectiveness-efficiency trade-off across many benchmarks. However, they\\nrequire a huge memory space to store the contextual representation for all the\\ndocument tokens. Some works have proposed using either heuristics or\\nstatistical-based techniques to prune tokens from each document. This however\\ndoesn't guarantee that the removed tokens have no impact on the retrieval\\nscore. Our work uses a principled approach to define how to prune tokens\\nwithout impacting the score between a document and a query. We introduce three\\nregularization losses, that induce a solution with high pruning ratios, as well\\nas two pruning strategies. We study them experimentally (in and out-domain),\\nshowing that we can preserve ColBERT's performance while using only 30\\\\% of the\\ntokens.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR,cs.AI,cs.CL\", \"published\": \"2025-04-17T09:18:58Z\"}"}

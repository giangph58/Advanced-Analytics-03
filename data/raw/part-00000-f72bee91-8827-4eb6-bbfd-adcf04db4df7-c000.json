{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02252v1\", \"title\": \"Adapting World Models with Latent-State Dynamics Residuals\", \"summary\": \"Simulation-to-reality reinforcement learning (RL) faces the critical\\nchallenge of reconciling discrepancies between simulated and real-world\\ndynamics, which can severely degrade agent performance. A promising approach\\ninvolves learning corrections to simulator forward dynamics represented as a\\nresidual error function, however this operation is impractical with\\nhigh-dimensional states such as images. To overcome this, we propose ReDRAW, a\\nlatent-state autoregressive world model pretrained in simulation and calibrated\\nto target environments through residual corrections of latent-state dynamics\\nrather than of explicit observed states. Using this adapted world model, ReDRAW\\nenables RL agents to be optimized with imagined rollouts under corrected\\ndynamics and then deployed in the real world. In multiple vision-based MuJoCo\\ndomains and a physical robot visual lane-following task, ReDRAW effectively\\nmodels changes to dynamics and avoids overfitting in low data regimes where\\ntraditional transfer methods fail.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.RO\", \"published\": \"2025-04-03T03:41:30Z\"}"}

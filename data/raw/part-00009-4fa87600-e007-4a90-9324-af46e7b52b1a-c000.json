{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12185v1\", \"title\": \"SALAD: Improving Robustness and Generalization through Contrastive\\n  Learning with Structure-Aware and LLM-Driven Augmented Data\", \"summary\": \"In various natural language processing (NLP) tasks, fine-tuning Pre-trained\\nLanguage Models (PLMs) often leads to the issue of spurious correlations, which\\nnegatively impacts performance, particularly when dealing with\\nout-of-distribution data. To address this problem, we propose SALAD}(Structure\\nAware and LLM-driven Augmented Data), a novel approach designed to enhance\\nmodel robustness and generalization by generating structure-aware and\\ncounterfactually augmented data for contrastive learning. Our method leverages\\na tagging-based approach to generate structure-aware positive samples and\\nutilizes large language models (LLMs) to generate counterfactual negative\\nsamples with diverse sentence patterns. By applying contrastive learning, SALAD\\nenables the model to focus on learning the structural relationships between key\\nsentence components while minimizing reliance on spurious correlations. We\\nvalidate our approach through experiments on three tasks: Sentiment\\nClassification, Sexism Detection, and Natural Language Inference. The results\\ndemonstrate that SALAD not only improves model robustness and performance\\nacross different environments but also enhances generalization to\\nout-of-distribution datasets and cross-domain scenarios.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-16T15:40:10Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.09914v1\", \"title\": \"Improving Multimodal Hateful Meme Detection Exploiting LMM-Generated\\n  Knowledge\", \"summary\": \"Memes have become a dominant form of communication in social media in recent\\nyears. Memes are typically humorous and harmless, however there are also memes\\nthat promote hate speech, being in this way harmful to individuals and groups\\nbased on their identity. Therefore, detecting hateful content in memes has\\nemerged as a task of critical importance. The need for understanding the\\ncomplex interactions of images and their embedded text renders the hateful meme\\ndetection a challenging multimodal task. In this paper we propose to address\\nthe aforementioned task leveraging knowledge encoded in powerful Large\\nMultimodal Models (LMM). Specifically, we propose to exploit LMMs in a two-fold\\nmanner. First, by extracting knowledge oriented to the hateful meme detection\\ntask in order to build strong meme representations. Specifically, generic\\nsemantic descriptions and emotions that the images along with their embedded\\ntexts elicit are extracted, which are then used to train a simple\\nclassification head for hateful meme detection. Second, by developing a novel\\nhard mining approach introducing directly LMM-encoded knowledge to the training\\nprocess, providing further improvements. We perform extensive experiments on\\ntwo datasets that validate the effectiveness of the proposed method, achieving\\nstate-of-the-art performance. Our code and trained models are publicly\\navailable at: https://github.com/IDT-ITI/LMM-CLIP-meme.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-14T06:23:44Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10430v1\", \"title\": \"LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety\\n  in Large Language Models\", \"summary\": \"Recent advancements in Large Language Models (LLMs) have enabled them to\\napproach human-level persuasion capabilities. However, such potential also\\nraises concerns about the safety risks of LLM-driven persuasion, particularly\\ntheir potential for unethical influence through manipulation, deception,\\nexploitation of vulnerabilities, and many other harmful tactics. In this work,\\nwe present a systematic investigation of LLM persuasion safety through two\\ncritical aspects: (1) whether LLMs appropriately reject unethical persuasion\\ntasks and avoid unethical strategies during execution, including cases where\\nthe initial persuasion goal appears ethically neutral, and (2) how influencing\\nfactors like personality traits and external pressures affect their behavior.\\nTo this end, we introduce PersuSafety, the first comprehensive framework for\\nthe assessment of persuasion safety which consists of three stages, i.e.,\\npersuasion scene creation, persuasive conversation simulation, and persuasion\\nsafety assessment. PersuSafety covers 6 diverse unethical persuasion topics and\\n15 common unethical strategies. Through extensive experiments across 8 widely\\nused LLMs, we observe significant safety concerns in most LLMs, including\\nfailing to identify harmful persuasion tasks and leveraging various unethical\\npersuasion strategies. Our study calls for more attention to improve safety\\nalignment in progressive and goal-driven conversations such as persuasion.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.HC\", \"published\": \"2025-04-14T17:20:34Z\"}"}

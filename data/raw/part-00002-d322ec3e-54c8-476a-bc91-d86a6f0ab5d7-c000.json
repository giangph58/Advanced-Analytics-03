{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11783v1\", \"title\": \"The Digital Cybersecurity Expert: How Far Have We Come?\", \"summary\": \"The increasing deployment of large language models (LLMs) in the\\ncybersecurity domain underscores the need for effective model selection and\\nevaluation. However, traditional evaluation methods often overlook specific\\ncybersecurity knowledge gaps that contribute to performance limitations. To\\naddress this, we develop CSEBenchmark, a fine-grained cybersecurity evaluation\\nframework based on 345 knowledge points expected of cybersecurity experts.\\nDrawing from cognitive science, these points are categorized into factual,\\nconceptual, and procedural types, enabling the design of 11,050 tailored\\nmultiple-choice questions. We evaluate 12 popular LLMs on CSEBenchmark and find\\nthat even the best-performing model achieves only 85.42% overall accuracy, with\\nparticular knowledge gaps in the use of specialized tools and uncommon\\ncommands. Different LLMs have unique knowledge gaps. Even large models from the\\nsame family may perform poorly on knowledge points where smaller models excel.\\nBy identifying and addressing specific knowledge gaps in each LLM, we achieve\\nup to an 84% improvement in correcting previously incorrect predictions across\\nthree existing benchmarks for two cybersecurity tasks. Furthermore, our\\nassessment of each LLM's knowledge alignment with specific cybersecurity roles\\nreveals that different models align better with different roles, such as GPT-4o\\nfor the Google Senior Intelligence Analyst and Deepseek-V3 for the Amazon\\nPrivacy Engineer. These findings underscore the importance of aligning LLM\\nselection with the specific knowledge requirements of different cybersecurity\\nroles for optimal performance.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR\", \"published\": \"2025-04-16T05:36:28Z\"}"}

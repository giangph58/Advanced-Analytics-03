{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16053v1\", \"title\": \"LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free\\n  Receptive Field Enlargement\", \"summary\": \"State space models (SSMs) have emerged as an efficient alternative to\\nTransformer models for language modeling, offering linear computational\\ncomplexity and constant memory usage as context length increases. However,\\ndespite their efficiency in handling long contexts, recent studies have shown\\nthat SSMs, such as Mamba models, generally underperform compared to\\nTransformers in long-context understanding tasks. To address this significant\\nshortfall and achieve both efficient and accurate long-context understanding,\\nwe propose LongMamba, a training-free technique that significantly enhances the\\nlong-context capabilities of Mamba models. LongMamba builds on our discovery\\nthat the hidden channels in Mamba can be categorized into local and global\\nchannels based on their receptive field lengths, with global channels primarily\\nresponsible for long-context capability. These global channels can become the\\nkey bottleneck as the input context lengthens. Specifically, when input lengths\\nlargely exceed the training sequence length, global channels exhibit\\nlimitations in adaptively extend their receptive fields, leading to Mamba's\\npoor long-context performance. The key idea of LongMamba is to mitigate the\\nhidden state memory decay in these global channels by preventing the\\naccumulation of unimportant tokens in their memory. This is achieved by first\\nidentifying critical tokens in the global channels and then applying token\\nfiltering to accumulate only those critical tokens. Through extensive\\nbenchmarking across synthetic and real-world long-context scenarios, LongMamba\\nsets a new standard for Mamba's long-context performance, significantly\\nextending its operational range without requiring additional training. Our code\\nis available at https://github.com/GATECH-EIC/LongMamba.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-22T17:30:36Z\"}"}

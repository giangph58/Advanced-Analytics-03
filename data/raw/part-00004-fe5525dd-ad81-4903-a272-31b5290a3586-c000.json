{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02329v1\", \"title\": \"Towards Assessing Deep Learning Test Input Generators\", \"summary\": \"Deep Learning (DL) systems are increasingly deployed in safety-critical\\napplications, yet they remain vulnerable to robustness issues that can lead to\\nsignificant failures. While numerous Test Input Generators (TIGs) have been\\ndeveloped to evaluate DL robustness, a comprehensive assessment of their\\neffectiveness across different dimensions is still lacking. This paper presents\\na comprehensive assessment of four state-of-the-art TIGs--DeepHunter,\\nDeepFault, AdvGAN, and SinVAD--across multiple critical aspects:\\nfault-revealing capability, naturalness, diversity, and efficiency. Our\\nempirical study leverages three pre-trained models (LeNet-5, VGG16, and\\nEfficientNetB3) on datasets of varying complexity (MNIST, CIFAR-10, and\\nImageNet-1K) to evaluate TIG performance. Our findings reveal important\\ntrade-offs in robustness revealing capability, variation in test case\\ngeneration, and computational efficiency across TIGs. The results also show\\nthat TIG performance varies significantly with dataset complexity, as tools\\nthat perform well on simpler datasets may struggle with more complex ones. In\\ncontrast, others maintain steadier performance or better scalability. This\\npaper offers practical guidance for selecting appropriate TIGs aligned with\\nspecific objectives and dataset characteristics. Nonetheless, more work is\\nneeded to address TIG limitations and advance TIGs for real-world,\\nsafety-critical systems.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CV,cs.SE\", \"published\": \"2025-04-03T07:06:55Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15958v1\", \"title\": \"FreeGraftor: Training-Free Cross-Image Feature Grafting for\\n  Subject-Driven Text-to-Image Generation\", \"summary\": \"Subject-driven image generation aims to synthesize novel scenes that\\nfaithfully preserve subject identity from reference images while adhering to\\ntextual guidance, yet existing methods struggle with a critical trade-off\\nbetween fidelity and efficiency. Tuning-based approaches rely on time-consuming\\nand resource-intensive subject-specific optimization, while zero-shot methods\\nfail to maintain adequate subject consistency. In this work, we propose\\nFreeGraftor, a training-free framework that addresses these limitations through\\ncross-image feature grafting. Specifically, FreeGraftor employs semantic\\nmatching and position-constrained attention fusion to transfer visual details\\nfrom reference subjects to the generated image. Additionally, our framework\\nincorporates a novel noise initialization strategy to preserve geometry priors\\nof reference subjects for robust feature matching. Extensive qualitative and\\nquantitative experiments demonstrate that our method enables precise subject\\nidentity transfer while maintaining text-aligned scene synthesis. Without\\nrequiring model fine-tuning or additional training, FreeGraftor significantly\\noutperforms existing zero-shot and training-free approaches in both subject\\nfidelity and text alignment. Furthermore, our framework can seamlessly extend\\nto multi-subject generation, making it practical for real-world deployment. Our\\ncode is available at https://github.com/Nihukat/FreeGraftor.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-22T14:55:23Z\"}"}

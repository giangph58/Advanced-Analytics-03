{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20644v1\", \"title\": \"Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified\\n  File Selection\", \"summary\": \"Selecting high-quality pre-training data for large language models (LLMs) is\\ncrucial for enhancing their overall performance under limited computation\\nbudget, improving both training and sample efficiency. Recent advancements in\\nfile selection primarily rely on using an existing or trained proxy model to\\nassess the similarity of samples to a target domain, such as high quality\\nsources BookCorpus and Wikipedia. However, upon revisiting these methods, the\\ndomain-similarity selection criteria demonstrates a diversity dilemma,\\ni.e.dimensional collapse in the feature space, improving performance on the\\ndomain-related tasks but causing severe degradation on generic performance. To\\nprevent collapse and enhance diversity, we propose a DiverSified File selection\\nalgorithm (DiSF), which selects the most decorrelated text files in the feature\\nspace. We approach this with a classical greedy algorithm to achieve more\\nuniform eigenvalues in the feature covariance matrix of the selected texts,\\nanalyzing its approximation to the optimal solution under a formulation of\\n$\\\\gamma$-weakly submodular optimization problem. Empirically, we establish a\\nbenchmark and conduct extensive experiments on the TinyLlama architecture with\\nmodels from 120M to 1.1B parameters. Evaluating across nine tasks from the\\nHarness framework, DiSF demonstrates a significant improvement on overall\\nperformance. Specifically, DiSF saves 98.5% of 590M training files in\\nSlimPajama, outperforming the full-data pre-training within a 50B training\\nbudget, and achieving about 1.5x training efficiency and 5x data efficiency.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-29T11:13:18Z\"}"}

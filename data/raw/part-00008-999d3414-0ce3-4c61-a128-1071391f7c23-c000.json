{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05815v1\", \"title\": \"Parasite: A Steganography-based Backdoor Attack Framework for Diffusion\\n  Models\", \"summary\": \"Recently, the diffusion model has gained significant attention as one of the\\nmost successful image generation models, which can generate high-quality images\\nby iteratively sampling noise. However, recent studies have shown that\\ndiffusion models are vulnerable to backdoor attacks, allowing attackers to\\nenter input data containing triggers to activate the backdoor and generate\\ntheir desired output. Existing backdoor attack methods primarily focused on\\ntarget noise-to-image and text-to-image tasks, with limited work on backdoor\\nattacks in image-to-image tasks. Furthermore, traditional backdoor attacks\\noften rely on a single, conspicuous trigger to generate a fixed target image,\\nlacking concealability and flexibility. To address these limitations, we\\npropose a novel backdoor attack method called \\\"Parasite\\\" for image-to-image\\ntasks in diffusion models, which not only is the first to leverage\\nsteganography for triggers hiding, but also allows attackers to embed the\\ntarget content as a backdoor trigger to achieve a more flexible attack.\\n\\\"Parasite\\\" as a novel attack method effectively bypasses existing detection\\nframeworks to execute backdoor attacks. In our experiments, \\\"Parasite\\\" achieved\\na 0 percent backdoor detection rate against the mainstream defense frameworks.\\nIn addition, in the ablation study, we discuss the influence of different\\nhiding coefficients on the attack results. You can find our code at\\nhttps://anonymous.4open.science/r/Parasite-1715/.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-08T08:53:47Z\"}"}

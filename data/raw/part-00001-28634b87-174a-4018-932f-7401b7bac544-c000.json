{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20571v1\", \"title\": \"Reinforcement Learning for Reasoning in Large Language Models with One\\n  Training Example\", \"summary\": \"We show that reinforcement learning with verifiable reward using one training\\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\\ncapabilities of large language models (LLMs). Applying RLVR to the base model\\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\\nPPO), and different math examples (many of which yield approximately 30% or\\ngreater improvement on MATH500 when employed as a single training example). In\\naddition, we identify some interesting phenomena during 1-shot RLVR, including\\ncross-domain generalization, increased frequency of self-reflection, and\\nsustained test performance improvement even after the training accuracy has\\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\\ngradient loss, distinguishing it from the \\\"grokking\\\" phenomenon. We also show\\nthe critical role of promoting exploration (e.g., by adding entropy loss with\\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\\nthat applying entropy loss alone, without any outcome reward, significantly\\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\\ncan inspire future work on RLVR data efficiency and encourage a re-examination\\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL\", \"published\": \"2025-04-29T09:24:30Z\"}"}

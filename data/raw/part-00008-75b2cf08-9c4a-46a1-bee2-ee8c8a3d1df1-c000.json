{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03467v1\", \"title\": \"Uncertainty-Aware Large Language Models for Explainable Disease\\n  Diagnosis\", \"summary\": \"Explainable disease diagnosis, which leverages patient information (e.g.,\\nsigns and symptoms) and computational models to generate probable diagnoses and\\nreasonings, offers clear clinical values. However, when clinical notes\\nencompass insufficient evidence for a definite diagnosis, such as the absence\\nof definitive symptoms, diagnostic uncertainty usually arises, increasing the\\nrisk of misdiagnosis and adverse outcomes. Although explicitly identifying and\\nexplaining diagnostic uncertainties is essential for trustworthy diagnostic\\nsystems, it remains under-explored. To fill this gap, we introduce ConfiDx, an\\nuncertainty-aware large language model (LLM) created by fine-tuning open-source\\nLLMs with diagnostic criteria. We formalized the task and assembled richly\\nannotated datasets that capture varying degrees of diagnostic ambiguity.\\nEvaluating ConfiDx on real-world datasets demonstrated that it excelled in\\nidentifying diagnostic uncertainties, achieving superior diagnostic\\nperformance, and generating trustworthy explanations for diagnoses and\\nuncertainties. To our knowledge, this is the first study to jointly address\\ndiagnostic uncertainty recognition and explanation, substantially enhancing the\\nreliability of automatic diagnostic systems.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-05-06T12:12:48Z\"}"}

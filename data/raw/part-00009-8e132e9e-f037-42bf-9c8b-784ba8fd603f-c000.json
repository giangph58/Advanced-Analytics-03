{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12860v1\", \"title\": \"When do Random Forests work?\", \"summary\": \"We study the effectiveness of randomizing split-directions in random forests.\\nPrior literature has shown that, on the one hand, randomization can reduce\\nvariance through decorrelation, and, on the other hand, randomization\\nregularizes and works in low signal-to-noise ratio (SNR) environments. First,\\nwe bring together and revisit decorrelation and regularization by presenting a\\nsystematic analysis of out-of-sample mean-squared error (MSE) for different SNR\\nscenarios based on commonly-used data-generating processes. We find that\\nvariance reduction tends to increase with the SNR and forests outperform\\nbagging when the SNR is low because, in low SNR cases, variance dominates bias\\nfor both methods. Second, we show that the effectiveness of randomization is a\\nquestion that goes beyond the SNR. We present a simulation study with fixed and\\nmoderate SNR, in which we examine the effectiveness of randomization for other\\ndata characteristics. In particular, we find that (i) randomization can\\nincrease bias in the presence of fat tails in the distribution of covariates;\\n(ii) in the presence of irrelevant covariates randomization is ineffective\\nbecause bias dominates variance; and (iii) when covariates are mutually\\ncorrelated randomization tends to be effective because variance dominates bias.\\nBeyond randomization, we find that, for both bagging and random forests, bias\\ncan be significantly reduced in the presence of correlated covariates. This\\nlast finding goes beyond the prevailing view that averaging mostly works by\\nvariance reduction. Given that in practice covariates are often correlated, our\\nfindings on correlated covariates could open the way for a better understanding\\nof why random forests work well in many applications.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.LG\", \"published\": \"2025-04-17T11:38:17Z\"}"}

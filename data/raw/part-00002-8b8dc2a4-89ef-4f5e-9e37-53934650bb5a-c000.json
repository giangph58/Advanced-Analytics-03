{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20734v1\", \"title\": \"UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with\\n  Diverse Modalities and Granularities\", \"summary\": \"Retrieval-Augmented Generation (RAG) has shown substantial promise in\\nimproving factual accuracy by grounding model responses with external knowledge\\nrelevant to queries. However, most existing RAG approaches are limited to a\\ntext-only corpus, and while recent efforts have extended RAG to other\\nmodalities such as images and videos, they typically operate over a single\\nmodality-specific corpus. In contrast, real-world queries vary widely in the\\ntype of knowledge they require, which a single type of knowledge source cannot\\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\\ndiverse modalities and granularities. Specifically, motivated by the\\nobservation that forcing all modalities into a unified representation space\\nderived from a single combined corpus causes a modality gap, where the\\nretrieval tends to favor items from the same modality as the query, we propose\\na modality-aware routing mechanism that dynamically identifies the most\\nappropriate modality-specific corpus and performs targeted retrieval within it.\\nAlso, beyond modality, we organize each modality into multiple granularity\\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\\nmodalities, showing its superiority over modality-specific and unified\\nbaselines.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.CV,cs.IR,cs.LG\", \"published\": \"2025-04-29T13:18:58Z\"}"}

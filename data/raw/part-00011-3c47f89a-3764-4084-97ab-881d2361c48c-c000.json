{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10873v1\", \"title\": \"Can Vision-Language Models Understand and Interpret Dynamic Gestures\\n  from Pedestrians? Pilot Datasets and Exploration Towards Instructive\\n  Nonverbal Commands for Cooperative Autonomous Vehicles\", \"summary\": \"In autonomous driving, it is crucial to correctly interpret traffic gestures\\n(TGs), such as those of an authority figure providing orders or instructions,\\nor a pedestrian signaling the driver, to ensure a safe and pleasant traffic\\nenvironment for all road users. This study investigates the capabilities of\\nstate-of-the-art vision-language models (VLMs) in zero-shot interpretation,\\nfocusing on their ability to caption and classify human gestures in traffic\\ncontexts. We create and publicly share two custom datasets with varying formal\\nand informal TGs, such as 'Stop', 'Reverse', 'Hail', etc. The datasets are\\n\\\"Acted TG (ATG)\\\" and \\\"Instructive TG In-The-Wild (ITGI)\\\". They are annotated\\nwith natural language, describing the pedestrian's body position and gesture.\\nWe evaluate models using three methods utilizing expert-generated captions as\\nbaseline and control: (1) caption similarity, (2) gesture classification, and\\n(3) pose sequence reconstruction similarity. Results show that current VLMs\\nstruggle with gesture understanding: sentence similarity averages below 0.59,\\nand classification F1 scores reach only 0.14-0.39, well below the expert\\nbaseline of 0.70. While pose reconstruction shows potential, it requires more\\ndata and refined metrics to be reliable. Our findings reveal that although some\\nSOTA VLMs can interpret zero-shot human traffic gestures, none are accurate and\\nrobust enough to be trustworthy, emphasizing the need for further research in\\nthis domain.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.HC\", \"published\": \"2025-04-15T05:04:25Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00625v1\", \"title\": \"SA-GAT-SR: Self-Adaptable Graph Attention Networks with Symbolic\\n  Regression for high-fidelity material property prediction\", \"summary\": \"Recent advances in machine learning have demonstrated an enormous utility of\\ndeep learning approaches, particularly Graph Neural Networks (GNNs) for\\nmaterials science. These methods have emerged as powerful tools for\\nhigh-throughput prediction of material properties, offering a compelling\\nenhancement and alternative to traditional first-principles calculations. While\\nthe community has predominantly focused on developing increasingly complex and\\nuniversal models to enhance predictive accuracy, such approaches often lack\\nphysical interpretability and insights into materials behavior. Here, we\\nintroduce a novel computational paradigm, Self-Adaptable Graph Attention\\nNetworks integrated with Symbolic Regression (SA-GAT-SR), that synergistically\\ncombines the predictive capability of GNNs with the interpretative power of\\nsymbolic regression. Our framework employs a self-adaptable encoding algorithm\\nthat automatically identifies and adjust attention weights so as to screen\\ncritical features from an expansive 180-dimensional feature space while\\nmaintaining O(n) computational scaling. The integrated SR module subsequently\\ndistills these features into compact analytical expressions that explicitly\\nreveal quantum-mechanically meaningful relationships, achieving 23 times\\nacceleration compared to conventional SR implementations that heavily rely on\\nfirst principle calculations-derived features as input. This work suggests a\\nnew framework in computational materials science, bridging the gap between\\npredictive accuracy and physical interpretability, offering valuable physical\\ninsights into material behavior.\", \"main_category\": \"physics.comp-ph\", \"categories\": \"physics.comp-ph,cond-mat.mtrl-sci,cs.LG\", \"published\": \"2025-05-01T16:05:10Z\"}"}

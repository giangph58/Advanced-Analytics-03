{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15771v1\", \"title\": \"Grounded in Context: Retrieval-Based Method for Hallucination Detection\", \"summary\": \"Despite advancements in grounded content generation, production Large\\nLanguage Models (LLMs) based applications still suffer from hallucinated\\nanswers. We present \\\"Grounded in Context\\\" - Deepchecks' hallucination detection\\nframework, designed for production-scale long-context data and tailored to\\ndiverse use cases, including summarization, data extraction, and RAG. Inspired\\nby RAG architecture, our method integrates retrieval and Natural Language\\nInference (NLI) models to predict factual consistency between premises and\\nhypotheses using an encoder-based model with only a 512-token context window.\\nOur framework identifies unsupported claims with an F1 score of 0.83 in\\nRAGTruth's response-level classification task, matching methods that trained on\\nthe dataset, and outperforming all comparable frameworks using similar-sized\\nmodels.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-22T10:28:23Z\"}"}

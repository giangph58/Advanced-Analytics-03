{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15028v1\", \"title\": \"A Controllable Appearance Representation for Flexible Transfer and\\n  Editing\", \"summary\": \"We present a method that computes an interpretable representation of material\\nappearance within a highly compact, disentangled latent space. This\\nrepresentation is learned in a self-supervised fashion using an adapted\\nFactorVAE. We train our model with a carefully designed unlabeled dataset,\\navoiding possible biases induced by human-generated labels. Our model\\ndemonstrates strong disentanglement and interpretability by effectively\\nencoding material appearance and illumination, despite the absence of explicit\\nsupervision. Then, we use our representation as guidance for training a\\nlightweight IP-Adapter to condition a diffusion pipeline that transfers the\\nappearance of one or more images onto a target geometry, and allows the user to\\nfurther edit the resulting appearance. Our approach offers fine-grained control\\nover the generated results: thanks to the well-structured compact latent space,\\nusers can intuitively manipulate attributes such as hue or glossiness in image\\nspace to achieve the desired final appearance.\", \"main_category\": \"cs.GR\", \"categories\": \"cs.GR,cs.CV\", \"published\": \"2025-04-21T11:29:06Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24298v1\", \"title\": \"Order Matters: On Parameter-Efficient Image-to-Video Probing for\\n  Recognizing Nearly Symmetric Actions\", \"summary\": \"We study parameter-efficient image-to-video probing for the unaddressed\\nchallenge of recognizing nearly symmetric actions - visually similar actions\\nthat unfold in opposite temporal order (e.g., opening vs. closing a bottle).\\nExisting probing mechanisms for image-pretrained models, such as DinoV2 and\\nCLIP, rely on attention mechanism for temporal modeling but are inherently\\npermutation-invariant, leading to identical predictions regardless of frame\\norder. To address this, we introduce Self-attentive Temporal Embedding Probing\\n(STEP), a simple yet effective approach designed to enforce temporal\\nsensitivity in parameter-efficient image-to-video transfer. STEP enhances\\nself-attentive probing with three key modifications: (1) a learnable frame-wise\\npositional encoding, explicitly encoding temporal order; (2) a single global\\nCLS token, for sequence coherence; and (3) a simplified attention mechanism to\\nimprove parameter efficiency. STEP outperforms existing image-to-video probing\\nmechanisms by 3-15% across four activity recognition benchmarks with only 1/3\\nof the learnable parameters. On two datasets, it surpasses all published\\nmethods, including fully fine-tuned models. STEP shows a distinct advantage in\\nrecognizing nearly symmetric actions, surpassing other probing mechanisms by\\n9-19%. and parameter-heavier PEFT-based transfer methods by 5-15%. Code and\\nmodels will be made publicly available.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-31T16:42:38Z\"}"}

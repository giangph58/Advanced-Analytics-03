{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04488v1\", \"title\": \"\\\"I Can See Forever!\\\": Evaluating Real-time VideoLLMs for Assisting\\n  Individuals with Visual Impairments\", \"summary\": \"The visually impaired population, especially the severely visually impaired,\\nis currently large in scale, and daily activities pose significant challenges\\nfor them. Although many studies use large language and vision-language models\\nto assist the blind, most focus on static content and fail to meet real-time\\nperception needs in dynamic and complex environments, such as daily activities.\\nTo provide them with more effective intelligent assistance, it is imperative to\\nincorporate advanced visual understanding technologies. Although real-time\\nvision and speech interaction VideoLLMs demonstrate strong real-time visual\\nunderstanding, no prior work has systematically evaluated their effectiveness\\nin assisting visually impaired individuals. In this work, we conduct the first\\nsuch evaluation. First, we construct a benchmark dataset (VisAssistDaily),\\ncovering three categories of assistive tasks for visually impaired individuals:\\nBasic Skills, Home Life Tasks, and Social Life Tasks. The results show that\\nGPT-4o achieves the highest task success rate. Next, we conduct a user study to\\nevaluate the models in both closed-world and open-world scenarios, further\\nexploring the practical challenges of applying VideoLLMs in assistive contexts.\\nOne key issue we identify is the difficulty current models face in perceiving\\npotential hazards in dynamic environments. To address this, we build an\\nenvironment-awareness dataset named SafeVid and introduce a polling mechanism\\nthat enables the model to proactively detect environmental risks. We hope this\\nwork provides valuable insights and inspiration for future research in this\\nfield.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.HC,cs.MM\", \"published\": \"2025-05-07T15:03:16Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05695v1\", \"title\": \"Architecture independent generalization bounds for overparametrized deep\\n  ReLU networks\", \"summary\": \"We prove that overparametrized neural networks are able to generalize with a\\ntest error that is independent of the level of overparametrization, and\\nindependent of the Vapnik-Chervonenkis (VC) dimension. We prove explicit bounds\\nthat only depend on the metric geometry of the test and training sets, on the\\nregularity properties of the activation function, and on the operator norms of\\nthe weights and norms of biases. For overparametrized deep ReLU networks with a\\ntraining sample size bounded by the input space dimension, we explicitly\\nconstruct zero loss minimizers without use of gradient descent, and prove that\\nthe generalization error is independent of the network architecture.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,math.AP,math.OC,stat.ML\", \"published\": \"2025-04-08T05:37:38Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04623v1\", \"title\": \"EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via\\n  Reinforcement Learning\", \"summary\": \"Multimodal large language models (MLLMs) have advanced perception across\\ntext, vision, and audio, yet they often struggle with structured cross-modal\\nreasoning, particularly when integrating audio and visual signals. We introduce\\nEchoInk-R1, a reinforcement learning framework that enhances such reasoning in\\nMLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group\\nRelative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice\\nquestion answering over synchronized audio-image pairs. To enable this, we\\ncurate AVQA-R1-6K, a dataset pairing such audio-image inputs with\\nmultiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves\\n85.77% accuracy on the validation set, outperforming the base model, which\\nscores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,\\nEchoInk-R1 demonstrates reflective reasoning by revisiting initial\\ninterpretations and refining responses when facing ambiguous multimodal inputs.\\nThese results suggest that lightweight reinforcement learning fine-tuning\\nenhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to\\nunify audio, visual, and textual modalities for general open-world reasoning\\nvia reinforcement learning. Code and data are publicly released to facilitate\\nfurther research.\", \"main_category\": \"eess.AS\", \"categories\": \"eess.AS,cs.AI,cs.CV,cs.MM,cs.SD\", \"published\": \"2025-05-07T17:59:49Z\"}"}

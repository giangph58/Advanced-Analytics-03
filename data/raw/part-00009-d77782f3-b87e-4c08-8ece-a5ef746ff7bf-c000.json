{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20629v1\", \"title\": \"AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized\\n  Speech Generation\", \"summary\": \"In this paper, we address the task of multimodal-to-speech generation, which\\naims to synthesize high-quality speech from multiple input modalities: text,\\nvideo, and reference audio. This task has gained increasing attention due to\\nits wide range of applications, such as film production, dubbing, and virtual\\navatars. Despite recent progress, existing methods still suffer from\\nlimitations in speech intelligibility, audio-video synchronization, speech\\nnaturalness, and voice similarity to the reference speaker. To address these\\nchallenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer\\nthat generates accurate, synchronized, and natural-sounding speech from aligned\\nmultimodal inputs. Built upon the in-context learning capability of the DiT\\narchitecture, AlignDiT explores three effective strategies to align multimodal\\nrepresentations. Furthermore, we introduce a novel multimodal classifier-free\\nguidance mechanism that allows the model to adaptively balance information from\\neach modality during speech synthesis. Extensive experiments demonstrate that\\nAlignDiT significantly outperforms existing methods across multiple benchmarks\\nin terms of quality, synchronization, and speaker similarity. Moreover,\\nAlignDiT exhibits strong generalization capability across various multimodal\\ntasks, such as video-to-speech synthesis and visual forced alignment,\\nconsistently achieving state-of-the-art performance. The demo page is available\\nat https://mm.kaist.ac.kr/projects/AlignDiT .\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.MM\", \"published\": \"2025-04-29T10:56:24Z\"}"}

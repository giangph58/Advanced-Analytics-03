{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11423v1\", \"title\": \"ADT: Tuning Diffusion Models with Adversarial Supervision\", \"summary\": \"Diffusion models have achieved outstanding image generation by reversing a\\nforward noising process to approximate true data distributions. During\\ntraining, these models predict diffusion scores from noised versions of true\\nsamples in a single forward pass, while inference requires iterative denoising\\nstarting from white noise. This training-inference divergences hinder the\\nalignment between inference and training data distributions, due to potential\\nprediction biases and cumulative error accumulation. To address this problem,\\nwe propose an intuitive but effective fine-tuning framework, called Adversarial\\nDiffusion Tuning (ADT), by stimulating the inference process during\\noptimization and aligning the final outputs with training data by adversarial\\nsupervision. Specifically, to achieve robust adversarial training, ADT features\\na siamese-network discriminator with a fixed pre-trained backbone and\\nlightweight trainable parameters, incorporates an image-to-image sampling\\nstrategy to smooth discriminative difficulties, and preserves the original\\ndiffusion loss to prevent discriminator hacking. In addition, we carefully\\nconstrain the backward-flowing path for back-propagating gradients along the\\ninference path without incurring memory overload or gradient explosion.\\nFinally, extensive experiments on Stable Diffusion models (v1.5, XL, and v3),\\ndemonstrate that ADT significantly improves both distribution alignment and\\nimage quality.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-15T17:37:50Z\"}"}

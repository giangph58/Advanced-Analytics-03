{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17735v1\", \"title\": \"EgoCHARM: Resource-Efficient Hierarchical Activity Recognition using an\\n  Egocentric IMU Sensor\", \"summary\": \"Human activity recognition (HAR) on smartglasses has various use cases,\\nincluding health/fitness tracking and input for context-aware AI assistants.\\nHowever, current approaches for egocentric activity recognition suffer from low\\nperformance or are resource-intensive. In this work, we introduce a resource\\n(memory, compute, power, sample) efficient machine learning algorithm,\\nEgoCHARM, for recognizing both high level and low level activities using a\\nsingle egocentric (head-mounted) Inertial Measurement Unit (IMU). Our\\nhierarchical algorithm employs a semi-supervised learning strategy, requiring\\nprimarily high level activity labels for training, to learn generalizable low\\nlevel motion embeddings that can be effectively utilized for low level activity\\nrecognition. We evaluate our method on 9 high level and 3 low level activities\\nachieving 0.826 and 0.855 F1 scores on high level and low level activity\\nrecognition respectively, with just 63k high level and 22k low level model\\nparameters, allowing the low level encoder to be deployed directly on current\\nIMU chips with compute. Lastly, we present results and insights from a\\nsensitivity analysis and highlight the opportunities and limitations of\\nactivity recognition using egocentric IMUs.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG\", \"published\": \"2025-04-24T16:48:45Z\"}"}

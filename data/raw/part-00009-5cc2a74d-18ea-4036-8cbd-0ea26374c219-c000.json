{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05661v1\", \"title\": \"Online Bernstein-von Mises theorem\", \"summary\": \"Online learning is an inferential paradigm in which parameters are updated\\nincrementally from sequentially available data, in contrast to batch learning,\\nwhere the entire dataset is processed at once. In this paper, we assume that\\nmini-batches from the full dataset become available sequentially. The Bayesian\\nframework, which updates beliefs about unknown parameters after observing each\\nmini-batch, is naturally suited for online learning. At each step, we update\\nthe posterior distribution using the current prior and new observations, with\\nthe updated posterior serving as the prior for the next step. However, this\\nrecursive Bayesian updating is rarely computationally tractable unless the\\nmodel and prior are conjugate. When the model is regular, the updated posterior\\ncan be approximated by a normal distribution, as justified by the Bernstein-von\\nMises theorem. We adopt a variational approximation at each step and\\ninvestigate the frequentist properties of the final posterior obtained through\\nthis sequential procedure. Under mild assumptions, we show that the accumulated\\napproximation error becomes negligible once the mini-batch size exceeds a\\nthreshold depending on the parameter dimension. As a result, the sequentially\\nupdated posterior is asymptotically indistinguishable from the full posterior.\", \"main_category\": \"math.ST\", \"categories\": \"math.ST,stat.TH,G.3\", \"published\": \"2025-04-08T04:22:56Z\"}"}

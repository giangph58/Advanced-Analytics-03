{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03261v1\", \"title\": \"DiffVQA: Video Quality Assessment Using Diffusion Feature Extractor\", \"summary\": \"Video Quality Assessment (VQA) aims to evaluate video quality based on\\nperceptual distortions and human preferences. Despite the promising performance\\nof existing methods using Convolutional Neural Networks (CNNs) and Vision\\nTransformers (ViTs), they often struggle to align closely with human\\nperceptions, particularly in diverse real-world scenarios. This challenge is\\nexacerbated by the limited scale and diversity of available datasets. To\\naddress this limitation, we introduce a novel VQA framework, DiffVQA, which\\nharnesses the robust generalization capabilities of diffusion models\\npre-trained on extensive datasets. Our framework adapts these models to\\nreconstruct identical input frames through a control module. The adapted\\ndiffusion model is then used to extract semantic and distortion features from a\\nresizing branch and a cropping branch, respectively. To enhance the model's\\nability to handle long-term temporal dynamics, a parallel Mamba module is\\nintroduced, which extracts temporal coherence augmented features that are\\nmerged with the diffusion features to predict the final score. Experiments\\nacross multiple datasets demonstrate DiffVQA's superior performance on\\nintra-dataset evaluations and its exceptional generalization across datasets.\\nThese results confirm that leveraging a diffusion model as a feature extractor\\ncan offer enhanced VQA performance compared to CNN and ViT backbones.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,eess.IV\", \"published\": \"2025-05-06T07:42:24Z\"}"}

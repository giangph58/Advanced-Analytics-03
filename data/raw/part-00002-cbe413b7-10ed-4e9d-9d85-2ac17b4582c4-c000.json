{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14915v1\", \"title\": \"StableQuant: Layer Adaptive Post-Training Quantization for Speech\\n  Foundation Models\", \"summary\": \"In this paper, we propose StableQuant, a novel adaptive post-training\\nquantization (PTQ) algorithm for widely used speech foundation models (SFMs).\\nWhile PTQ has been successfully employed for compressing large language models\\n(LLMs) due to its ability to bypass additional fine-tuning, directly applying\\nthese techniques to SFMs may not yield optimal results, as SFMs utilize\\ndistinct network architecture for feature extraction. StableQuant demonstrates\\noptimal quantization performance regardless of the network architecture type,\\nas it adaptively determines the quantization range for each layer by analyzing\\nboth the scale distributions and overall performance. We evaluate our algorithm\\non two SFMs, HuBERT and wav2vec2.0, for an automatic speech recognition (ASR)\\ntask, and achieve superior performance compared to traditional PTQ methods.\\nStableQuant successfully reduces the sizes of SFM models to a quarter and\\ndoubles the inference speed while limiting the word error rate (WER)\\nperformance drop to less than 0.3% with 8-bit quantization.\", \"main_category\": \"eess.AS\", \"categories\": \"eess.AS,cs.AI\", \"published\": \"2025-04-21T07:33:27Z\"}"}

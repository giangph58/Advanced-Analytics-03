{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03558v1\", \"title\": \"Multi-Agent Reinforcement Learning Scheduling to Support Low Latency in\\n  Teleoperated Driving\", \"summary\": \"The teleoperated driving (TD) scenario comes with stringent Quality of\\nService (QoS) communication constraints, especially in terms of end-to-end\\n(E2E) latency and reliability. In this context, Predictive Quality of Service\\n(PQoS), possibly combined with Reinforcement Learning (RL) techniques, is a\\npowerful tool to estimate QoS degradation and react accordingly. For example,\\nan intelligent agent can be trained to select the optimal compression\\nconfiguration for automotive data, and reduce the file size whenever QoS\\nconditions deteriorate. However, compression may inevitably compromise data\\nquality, with negative implications for the TD application. An alternative\\nstrategy involves operating at the Radio Access Network (RAN) level to optimize\\nradio parameters based on current network conditions, while preserving data\\nquality. In this paper, we propose Multi-Agent Reinforcement Learning (MARL)\\nscheduling algorithms, based on Proximal Policy Optimization (PPO), to\\ndynamically and intelligently allocate radio resources to minimize E2E latency\\nin a TD scenario. We evaluate two training paradigms, i.e., decentralized\\nlearning with local observations (IPPO) vs. centralized aggregation (MAPPO), in\\nconjunction with two resource allocation strategies, i.e., proportional\\nallocation (PA) and greedy allocation (GA). We prove via ns-3 simulations that\\nMAPPO, combined with GA, achieves the best results in terms of latency,\\nespecially as the number of vehicles increases.\", \"main_category\": \"cs.NI\", \"categories\": \"cs.NI\", \"published\": \"2025-05-06T14:11:21Z\"}"}

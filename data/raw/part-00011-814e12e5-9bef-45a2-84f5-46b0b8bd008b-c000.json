{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15162v1\", \"title\": \"To Offload or Not To Offload: Model-driven Comparison of Edge-native and\\n  On-device Processing\", \"summary\": \"Computational offloading is a promising approach for overcoming resource\\nconstraints on client devices by moving some or all of an application's\\ncomputations to remote servers. With the advent of specialized hardware\\naccelerators, client devices are now able to perform fast local processing of\\nspecific tasks, such as machine learning inference, reducing the need for\\noffloading computations. However, edge servers with accelerators also offer\\nfaster processing for offloaded tasks than was previously possible. In this\\npaper, we present an analytic and experimental comparison of on-device\\nprocessing and edge offloading for a range of accelerator, network, and\\napplication workload scenarios, with the goal of understanding when to use\\nlocal on-device processing and when to offload computations. We present models\\nthat leverage analytical queuing results to capture the effects of dynamic\\nfactors such as the performance gap between the device and edge server, network\\nvariability, server load, and multi-tenancy on the edge server. We\\nexperimentally demonstrate the accuracy of our models for a range of hardware\\nand application scenarios and show that our models achieve a mean absolute\\npercentage error of 2.2% compared to observed latencies. We use our models to\\ndevelop an adaptive resource manager for intelligent offloading and show its\\nefficacy in the presence of variable network conditions and dynamic\\nmulti-tenant edge settings.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC\", \"published\": \"2025-04-21T15:08:01Z\"}"}

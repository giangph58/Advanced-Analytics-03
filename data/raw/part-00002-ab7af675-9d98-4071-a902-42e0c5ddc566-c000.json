{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20609v1\", \"title\": \"WenyanGPT: A Large Language Model for Classical Chinese Tasks\", \"summary\": \"Classical Chinese, as the core carrier of Chinese culture, plays a crucial\\nrole in the inheritance and study of ancient literature. However, existing\\nnatural language processing models primarily optimize for Modern Chinese,\\nresulting in inadequate performance on Classical Chinese. This paper presents a\\ncomprehensive solution for Classical Chinese language processing. By continuing\\npre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we\\nconstruct a large language model, WenyanGPT, which is specifically designed for\\nClassical Chinese tasks. Additionally, we develop an evaluation benchmark\\ndataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that\\nWenyanGPT significantly outperforms current advanced LLMs in various Classical\\nChinese tasks. We make the model's training data, instruction fine-tuning\\ndata\\\\footnote, and evaluation benchmark dataset publicly available to promote\\nfurther research and development in the field of Classical Chinese processing.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-29T10:19:05Z\"}"}

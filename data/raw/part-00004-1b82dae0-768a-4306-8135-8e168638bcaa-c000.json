{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05177v1\", \"title\": \"MARK: Memory Augmented Refinement of Knowledge\", \"summary\": \"Large Language Models (LLMs) assist in specialized tasks but struggle to\\nalign with evolving domain knowledge without costly fine-tuning. Domain\\nknowledge consists of: Knowledge: Immutable facts (e.g., 'A stone is solid')\\nand generally accepted principles (e.g., ethical standards); Refined Memory:\\nEvolving insights shaped by business needs and real-world changes. However, a\\nsignificant gap often exists between a domain expert's deep, nuanced\\nunderstanding and the system's domain knowledge, which can hinder accurate\\ninformation retrieval and application. Our Memory-Augmented Refinement of\\nKnowledge (MARK) framework enables LLMs to continuously learn without\\nretraining by leveraging structured refined memory, inspired by the Society of\\nMind. MARK operates through specialized agents, each serving a distinct role:\\nResidual Refined Memory Agent: Stores and retrieves domain-specific insights to\\nmaintain context over time; User Question Refined Memory Agent: Captures\\nuser-provided facts, abbreviations, and terminology for better comprehension;\\nLLM Response Refined Memory Agent: Extracts key elements from responses for\\nrefinement and personalization. These agents analyse stored refined memory,\\ndetect patterns, resolve contradictions, and improve response accuracy.\\nTemporal factors like recency and frequency prioritize relevant information\\nwhile discarding outdated insights. MARK enhances LLMs in multiple ways: Ground\\nTruth Strategy: Reduces hallucinations by establishing a structured reference;\\nDomain-Specific Adaptation: Essential for fields like healthcare, law, and\\nmanufacturing, where proprietary insights are absent from public datasets;\\nPersonalized AI Assistants: Improves virtual assistants by remembering user\\npreferences, ensuring coherent responses over time.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.LG\", \"published\": \"2025-05-08T12:28:00Z\"}"}

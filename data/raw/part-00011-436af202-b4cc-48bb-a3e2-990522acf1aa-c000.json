{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05295v1\", \"title\": \"Performance Estimation in Binary Classification Using Calibrated\\n  Confidence\", \"summary\": \"Model monitoring is a critical component of the machine learning lifecycle,\\nsafeguarding against undetected drops in the model's performance after\\ndeployment. Traditionally, performance monitoring has required access to ground\\ntruth labels, which are not always readily available. This can result in\\nunacceptable latency or render performance monitoring altogether impossible.\\nRecently, methods designed to estimate the accuracy of classifier models\\nwithout access to labels have shown promising results. However, there are\\nvarious other metrics that might be more suitable for assessing model\\nperformance in many cases. Until now, none of these important metrics has\\nreceived similar interest from the scientific community. In this work, we\\naddress this gap by presenting CBPE, a novel method that can estimate any\\nbinary classification metric defined using the confusion matrix. In particular,\\nwe choose four metrics from this large family: accuracy, precision, recall, and\\nF$_1$, to demonstrate our method. CBPE treats the elements of the confusion\\nmatrix as random variables and leverages calibrated confidence scores of the\\nmodel to estimate their distributions. The desired metric is then also treated\\nas a random variable, whose full probability distribution can be derived from\\nthe estimated confusion matrix. CBPE is shown to produce estimates that come\\nwith strong theoretical guarantees and valid confidence intervals.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,I.2.6\", \"published\": \"2025-05-08T14:34:44Z\"}"}

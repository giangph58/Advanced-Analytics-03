{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17671v1\", \"title\": \"Data-Driven Calibration of Prediction Sets in Large Vision-Language\\n  Models Based on Inductive Conformal Prediction\", \"summary\": \"This study addresses the critical challenge of hallucination mitigation in\\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\\nhigh confidence, posing risks in safety-critical applications. We propose a\\nmodel-agnostic uncertainty quantification method that integrates dynamic\\nthreshold calibration and cross-modal consistency verification. By partitioning\\ndata into calibration and test sets, the framework computes nonconformity\\nscores to construct prediction sets with statistical guarantees under\\nuser-defined risk levels ($\\\\alpha$). Key innovations include: (1) rigorous\\ncontrol of \\\\textbf{marginal coverage} to ensure empirical error rates remain\\nstrictly below $\\\\alpha$; (2) dynamic adjustment of prediction set sizes\\ninversely with $\\\\alpha$, filtering low-confidence outputs; (3) elimination of\\nprior distribution assumptions and retraining requirements. Evaluations on\\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\\ntheoretical guarantees across all $\\\\alpha$ values. The framework achieves\\nstable performance across varying calibration-to-test split ratios,\\nunderscoring its robustness for real-world deployment in healthcare, autonomous\\nsystems, and other safety-sensitive domains. This work bridges the gap between\\ntheoretical reliability and practical applicability in multi-modal AI systems,\\noffering a scalable solution for hallucination detection and uncertainty-aware\\ndecision-making.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-24T15:39:46Z\"}"}

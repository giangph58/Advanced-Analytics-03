{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10434v1\", \"title\": \"Anchor Token Matching: Implicit Structure Locking for Training-free AR\\n  Image Editing\", \"summary\": \"Text-to-image generation has seen groundbreaking advancements with diffusion\\nmodels, enabling high-fidelity synthesis and precise image editing through\\ncross-attention manipulation. Recently, autoregressive (AR) models have\\nre-emerged as powerful alternatives, leveraging next-token generation to match\\ndiffusion models. However, existing editing techniques designed for diffusion\\nmodels fail to translate directly to AR models due to fundamental differences\\nin structural control. Specifically, AR models suffer from spatial poverty of\\nattention maps and sequential accumulation of structural errors during image\\nediting, which disrupt object layouts and global consistency. In this work, we\\nintroduce Implicit Structure Locking (ISLock), the first training-free editing\\nstrategy for AR visual models. Rather than relying on explicit attention\\nmanipulation or fine-tuning, ISLock preserves structural blueprints by\\ndynamically aligning self-attention patterns with reference images through the\\nAnchor Token Matching (ATM) protocol. By implicitly enforcing structural\\nconsistency in latent space, our method ISLock enables structure-aware editing\\nwhile maintaining generative autonomy. Extensive experiments demonstrate that\\nISLock achieves high-quality, structure-consistent edits without additional\\ntraining and is superior or comparable to conventional editing techniques. Our\\nfindings pioneer the way for efficient and flexible AR-based image editing,\\nfurther bridging the performance gap between diffusion and autoregressive\\ngenerative models. The code will be publicly available at\\nhttps://github.com/hutaiHang/ATM\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-14T17:25:19Z\"}"}

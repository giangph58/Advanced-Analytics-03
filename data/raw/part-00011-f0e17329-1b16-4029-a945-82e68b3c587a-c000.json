{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04494v1\", \"title\": \"A Two-Timescale Primal-Dual Framework for Reinforcement Learning via\\n  Online Dual Variable Guidance\", \"summary\": \"We study reinforcement learning by combining recent advances in regularized\\nlinear programming formulations with the classical theory of stochastic\\napproximation. Motivated by the challenge of designing algorithms that leverage\\noff-policy data while maintaining on-policy exploration, we propose PGDA-RL, a\\nnovel primal-dual Projected Gradient Descent-Ascent algorithm for solving\\nregularized Markov Decision Processes (MDPs). PGDA-RL integrates experience\\nreplay-based gradient estimation with a two-timescale decomposition of the\\nunderlying nested optimization problem. The algorithm operates asynchronously,\\ninteracts with the environment through a single trajectory of correlated data,\\nand updates its policy online in response to the dual variable associated with\\nthe occupation measure of the underlying MDP. We prove that PGDA-RL converges\\nalmost surely to the optimal value function and policy of the regularized MDP.\\nOur convergence analysis relies on tools from stochastic approximation theory\\nand holds under weaker assumptions than those required by existing primal-dual\\nRL approaches, notably removing the need for a simulator or a fixed behavioral\\npolicy.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC,cs.LG\", \"published\": \"2025-05-07T15:18:43Z\"}"}

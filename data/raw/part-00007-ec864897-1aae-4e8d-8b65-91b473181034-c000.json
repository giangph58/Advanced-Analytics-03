{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20566v1\", \"title\": \"Inclusive Training Separation and Implicit Knowledge Interaction for\\n  Balanced Online Class-Incremental Learning\", \"summary\": \"Online class-incremental learning (OCIL) focuses on gradually learning new\\nclasses (called plasticity) from a stream of data in a single-pass, while\\nconcurrently preserving knowledge of previously learned classes (called\\nstability). The primary challenge in OCIL lies in maintaining a good balance\\nbetween the knowledge of old and new classes within the continually updated\\nmodel. Most existing methods rely on explicit knowledge interaction through\\nexperience replay, and often employ exclusive training separation to address\\nbias problems. Nevertheless, it still remains a big challenge to achieve a\\nwell-balanced learner, as these methods often exhibit either reduced plasticity\\nor limited stability due to difficulties in continually integrating knowledge\\nin the OCIL setting. In this paper, we propose a novel replay-based method,\\ncalled Balanced Online Incremental Learning (BOIL), which can achieve both high\\nplasticity and stability, thus ensuring more balanced performance in OCIL. Our\\nBOIL method proposes an inclusive training separation strategy using dual\\nclassifiers so that knowledge from both old and new classes can effectively be\\nintegrated into the model, while introducing implicit approaches for\\ntransferring knowledge across the two classifiers. Extensive experimental\\nevaluations over three widely-used OCIL benchmark datasets demonstrate the\\nsuperiority of BOIL, showing more balanced yet better performance compared to\\nstate-of-the-art replay-based OCIL methods.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-29T09:13:00Z\"}"}

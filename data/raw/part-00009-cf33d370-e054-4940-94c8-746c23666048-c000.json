{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12845v1\", \"title\": \"Can LLMs reason over extended multilingual contexts? Towards\\n  long-context evaluation beyond retrieval and haystacks\", \"summary\": \"Existing multilingual long-context benchmarks, often based on the popular\\nneedle-in-a-haystack test, primarily evaluate a model's ability to locate\\nspecific information buried within irrelevant texts. However, such a\\nretrieval-centric approach is myopic and inherently limited, as successful\\nrecall alone does not indicate a model's capacity to reason over extended\\ncontexts. Moreover, these benchmarks are susceptible to data leakage,\\nshort-circuiting, and risk making the evaluation a priori identifiable. To\\naddress these limitations, we introduce MLRBench, a new synthetic benchmark for\\nmultilingual long-context reasoning. Unlike existing benchmarks, MLRBench goes\\nbeyond surface-level retrieval by including tasks that assess multi-hop\\ninference, aggregation, and epistemic reasoning. Spanning seven languages,\\nMLRBench is designed to be parallel, resistant to leakage, and scalable to\\narbitrary context lengths. Our extensive experiments with an open-weight large\\nlanguage model (LLM) reveal a pronounced gap between high- and low-resource\\nlanguages, particularly for tasks requiring the model to aggregate multiple\\nfacts or predict the absence of information. We also find that, in multilingual\\nsettings, LLMs effectively utilize less than 30% of their claimed context\\nlength. Although off-the-shelf Retrieval Augmented Generation helps alleviate\\nthis to a certain extent, it does not solve the long-context problem. We\\nopen-source MLRBench to enable future research in improved evaluation and\\ntraining of multilingual LLMs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-17T11:02:35Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21307v1\", \"title\": \"The Dual Power of Interpretable Token Embeddings: Jailbreaking Attacks\\n  and Defenses for Diffusion Model Unlearning\", \"summary\": \"Despite the remarkable generalization capabilities of diffusion models,\\nrecent studies have shown that these models can memorize and generate harmful\\ncontent when prompted with specific text instructions. Although fine-tuning\\napproaches have been developed to mitigate this issue by unlearning harmful\\nconcepts, these methods can be easily circumvented through jailbreaking\\nattacks. This indicates that the harmful concept has not been fully erased from\\nthe model. However, existing attack methods, while effective, lack\\ninterpretability regarding why unlearned models still retain the concept,\\nthereby hindering the development of defense strategies. In this work, we\\naddress these limitations by proposing an attack method that learns an\\northogonal set of interpretable attack token embeddings. The attack token\\nembeddings can be decomposed into human-interpretable textual elements,\\nrevealing that unlearned models still retain the target concept through\\nimplicit textual components. Furthermore, these attack token embeddings are\\nrobust and transferable across text prompts, initial noises, and unlearned\\nmodels. Finally, leveraging this diverse set of embeddings, we design a defense\\nmethod applicable to both our proposed attack and existing attack methods.\\nExperimental results demonstrate the effectiveness of both our attack and\\ndefense strategies.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-30T04:33:43Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05291v1\", \"title\": \"Benchmarking Ophthalmology Foundation Models for Clinically Significant\\n  Age Macular Degeneration Detection\", \"summary\": \"Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to\\nlearn robust representations from large-scale natural image datasets, enhancing\\ntheir generalization across domains. In retinal imaging, foundation models\\npretrained on either natural or ophthalmic data have shown promise, but the\\nbenefits of in-domain pretraining remain uncertain. To investigate this, we\\nbenchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets\\ntotaling 70,000 expert-annotated images for the task of moderate-to-late\\nage-related macular degeneration (AMD) identification. Our results show that\\niBOT pretrained on natural images achieves the highest out-of-distribution\\ngeneralization, with AUROCs of 0.80-0.97, outperforming domain-specific models,\\nwhich achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining,\\nwhich achieved AUROCs of 0.68-0.91. These findings highlight the value of\\nfoundation models in improving AMD identification and challenge the assumption\\nthat in-domain pretraining is necessary. Furthermore, we release BRAMD, an\\nopen-access dataset (n=587) of DFIs with AMD labels from Brazil.\", \"main_category\": \"eess.IV\", \"categories\": \"eess.IV,cs.AI,cs.CV,q-bio.TO\", \"published\": \"2025-05-08T14:31:02Z\"}"}

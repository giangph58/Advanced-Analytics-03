{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12264v1\", \"title\": \"Towards Learning to Complete Anything in Lidar\", \"summary\": \"We propose CAL (Complete Anything in Lidar) for Lidar-based shape-completion\\nin-the-wild. This is closely related to Lidar-based semantic/panoptic scene\\ncompletion. However, contemporary methods can only complete and recognize\\nobjects from a closed vocabulary labeled in existing Lidar datasets. Different\\nto that, our zero-shot approach leverages the temporal context from multi-modal\\nsensor sequences to mine object shapes and semantic features of observed\\nobjects. These are then distilled into a Lidar-only instance-level completion\\nand recognition model. Although we only mine partial shape completions, we find\\nthat our distilled model learns to infer full object shapes from multiple such\\npartial observations across the dataset. We show that our model can be prompted\\non standard benchmarks for Semantic and Panoptic Scene Completion, localize\\nobjects as (amodal) 3D bounding boxes, and recognize objects beyond fixed class\\nvocabularies. Our project page is\\nhttps://research.nvidia.com/labs/dvl/projects/complete-anything-lidar\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-16T17:21:55Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11936v1\", \"title\": \"Mind2Matter: Creating 3D Models from EEG Signals\", \"summary\": \"The reconstruction of 3D objects from brain signals has gained significant\\nattention in brain-computer interface (BCI) research. Current research\\npredominantly utilizes functional magnetic resonance imaging (fMRI) for 3D\\nreconstruction tasks due to its excellent spatial resolution. Nevertheless, the\\nclinical utility of fMRI is limited by its prohibitive costs and inability to\\nsupport real-time operations. In comparison, electroencephalography (EEG)\\npresents distinct advantages as an affordable, non-invasive, and mobile\\nsolution for real-time brain-computer interaction systems. While recent\\nadvances in deep learning have enabled remarkable progress in image generation\\nfrom neural data, decoding EEG signals into structured 3D representations\\nremains largely unexplored. In this paper, we propose a novel framework that\\ntranslates EEG recordings into 3D object reconstructions by leveraging neural\\ndecoding techniques and generative models. Our approach involves training an\\nEEG encoder to extract spatiotemporal visual features, fine-tuning a large\\nlanguage model to interpret these features into descriptive multimodal outputs,\\nand leveraging generative 3D Gaussians with layout-guided control to synthesize\\nthe final 3D structures. Experiments demonstrate that our model captures\\nsalient geometric and semantic features, paving the way for applications in\\nbrain-computer interfaces (BCIs), virtual reality, and neuroprosthetics.Our\\ncode is available in https://github.com/sddwwww/Mind2Matter.\", \"main_category\": \"cs.GR\", \"categories\": \"cs.GR,cs.HC,eess.SP\", \"published\": \"2025-04-16T10:16:03Z\"}"}

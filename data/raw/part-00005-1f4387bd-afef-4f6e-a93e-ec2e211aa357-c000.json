{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16763v1\", \"title\": \"Noise-Tolerant Coreset-Based Class Incremental Continual Learning\", \"summary\": \"Many applications of computer vision require the ability to adapt to novel\\ndata distributions after deployment. Adaptation requires algorithms capable of\\ncontinual learning (CL). Continual learners must be plastic to adapt to novel\\ntasks while minimizing forgetting of previous tasks.However, CL opens up\\navenues for noise to enter the training pipeline and disrupt the CL. This work\\nfocuses on label noise and instance noise in the context of class-incremental\\nlearning (CIL), where new classes are added to a classifier over time, and\\nthere is no access to external data from past classes. We aim to understand the\\nsensitivity of CL methods that work by replaying items from a memory\\nconstructed using the idea of Coresets. We derive a new bound for the\\nrobustness of such a method to uncorrelated instance noise under a general\\nadditive noise threat model, revealing several insights. Putting the theory\\ninto practice, we create two continual learning algorithms to construct\\nnoise-tolerant replay buffers. We empirically compare the effectiveness of\\nprior memory-based continual learners and the proposed algorithms under label\\nand uncorrelated instance noise on five diverse datasets. We show that existing\\nmemory-based CL are not robust whereas the proposed methods exhibit significant\\nimprovements in maximizing classification accuracy and minimizing forgetting in\\nthe noisy CIL setting.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CV,cs.NE\", \"published\": \"2025-04-23T14:34:20Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11195v1\", \"title\": \"R-TPT: Improving Adversarial Robustness of Vision-Language Models\\n  through Test-Time Prompt Tuning\", \"summary\": \"Vision-language models (VLMs), such as CLIP, have gained significant\\npopularity as foundation models, with numerous fine-tuning methods developed to\\nenhance performance on downstream tasks. However, due to their inherent\\nvulnerability and the common practice of selecting from a limited set of\\nopen-source models, VLMs suffer from a higher risk of adversarial attacks than\\ntraditional vision models. Existing defense techniques typically rely on\\nadversarial fine-tuning during training, which requires labeled data and lacks\\nof flexibility for downstream tasks. To address these limitations, we propose\\nrobust test-time prompt tuning (R-TPT), which mitigates the impact of\\nadversarial attacks during the inference stage. We first reformulate the\\nclassic marginal entropy objective by eliminating the term that introduces\\nconflicts under adversarial conditions, retaining only the pointwise entropy\\nminimization. Furthermore, we introduce a plug-and-play reliability-based\\nweighted ensembling strategy, which aggregates useful information from reliable\\naugmented views to strengthen the defense. R-TPT enhances defense against\\nadversarial attacks without requiring labeled training data while offering high\\nflexibility for inference tasks. Extensive experiments on widely used\\nbenchmarks with various attacks demonstrate the effectiveness of R-TPT. The\\ncode is available in https://github.com/TomSheng21/R-TPT.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CR,cs.CV\", \"published\": \"2025-04-15T13:49:31Z\"}"}

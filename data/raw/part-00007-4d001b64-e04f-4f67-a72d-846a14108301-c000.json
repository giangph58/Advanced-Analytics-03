{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05185v1\", \"title\": \"Concise Reasoning via Reinforcement Learning\", \"summary\": \"Despite significant advancements in large language models (LLMs), a major\\ndrawback of reasoning models is their enormous token usage, which increases\\ncomputational cost, resource requirements, and response time. In this work, we\\nrevisit the core principles of reinforcement learning (RL) and, through\\nmathematical analysis, demonstrate that the tendency to generate lengthy\\nresponses arises inherently from RL-based optimization during training. This\\nfinding questions the prevailing assumption that longer responses inherently\\nimprove reasoning accuracy. Instead, we uncover a natural correlation between\\nconciseness and accuracy that has been largely overlooked. Moreover, we show\\nthat introducing a secondary phase of RL post-training, using a small set of\\nproblems and limited resources, can significantly reduce a model's chain of\\nthought while maintaining or even enhancing accuracy. Finally, we validate our\\nconclusions through extensive experimental results.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-07T15:35:54Z\"}"}

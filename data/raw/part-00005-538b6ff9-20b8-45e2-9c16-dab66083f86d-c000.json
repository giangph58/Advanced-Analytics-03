{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02546v1\", \"title\": \"GPG: A Simple and Strong Reinforcement Learning Baseline for Model\\n  Reasoning\", \"summary\": \"Reinforcement Learning (RL) can directly enhance the reasoning capabilities\\nof large language models without extensive reliance on Supervised Fine-Tuning\\n(SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism\\nand propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike\\nconventional methods, GPG directly optimize the original RL objective, thus\\nobviating the need for surrogate loss functions. As illustrated in our paper,\\nby eliminating both the critic and reference models, and avoiding KL divergence\\nconstraints, our approach significantly simplifies the training process when\\ncompared to Group Relative Policy Optimization (GRPO). Our approach achieves\\nsuperior performance without relying on auxiliary techniques or adjustments.\\nExtensive experiments demonstrate that our method not only reduces\\ncomputational costs but also consistently outperforms GRPO across various\\nunimodal and multimodal tasks. Our code is available at\\nhttps://github.com/AMAP-ML/GPG.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-03T12:53:41Z\"}"}

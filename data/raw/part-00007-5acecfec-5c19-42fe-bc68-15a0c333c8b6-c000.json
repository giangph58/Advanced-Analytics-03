{"value":"{\"aid\": \"http://arxiv.org/abs/2504.09946v1\", \"title\": \"Assessing Judging Bias in Large Reasoning Models: An Empirical Study\", \"summary\": \"Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have\\ndemonstrated remarkable reasoning capabilities, raising important questions\\nabout their biases in LLM-as-a-judge settings. We present a comprehensive\\nbenchmark comparing judging biases between LLMs and LRMs across both subjective\\npreference-alignment datasets and objective fact-based datasets. Through\\ninvestigation of bandwagon, authority, position, and distraction biases, we\\nuncover four key findings: (1) despite their advanced reasoning capabilities,\\nLRMs remain susceptible to the above biases; (2) LRMs demonstrate better\\nrobustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit\\nnotable position bias, preferring options in later positions; and (4) we\\nidentify a novel \\\"superficial reflection bias\\\" where phrases mimicking\\nreasoning (e.g., \\\"wait, let me think...\\\") significantly influence model\\njudgments. To address these biases, we design and evaluate three mitigation\\nstrategies: specialized system prompts that reduce judging biases by up to 19\\\\%\\nin preference alignment datasets and 14\\\\% in fact-related datasets, in-context\\nlearning that provides up to 27\\\\% improvement on preference tasks but shows\\ninconsistent results on factual tasks, and a self-reflection mechanism that\\nreduces biases by up to 10\\\\% in preference datasets and 16\\\\% in fact-related\\ndatasets, with self-reflection proving particularly effective for LRMs. Our\\nwork provides crucial insights for developing more reliable LLM-as-a-Judge\\nframeworks, especially as LRMs become increasingly deployed as automated\\njudges.\", \"main_category\": \"cs.CY\", \"categories\": \"cs.CY,cs.CL\", \"published\": \"2025-04-14T07:14:27Z\"}"}

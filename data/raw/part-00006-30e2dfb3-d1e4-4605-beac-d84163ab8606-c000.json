{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03710v1\", \"title\": \"Actor-Critics Can Achieve Optimal Sample Efficiency\", \"summary\": \"Actor-critic algorithms have become a cornerstone in reinforcement learning\\n(RL), leveraging the strengths of both policy-based and value-based methods.\\nDespite recent progress in understanding their statistical efficiency, no\\nexisting work has successfully learned an $\\\\epsilon$-optimal policy with a\\nsample complexity of $O(1/\\\\epsilon^2)$ trajectories with general function\\napproximation when strategic exploration is necessary.\\n  We address this open problem by introducing a novel actor-critic algorithm\\nthat attains a sample-complexity of $O(dH^5 \\\\log|\\\\mathcal{A}|/\\\\epsilon^2 + d\\nH^4 \\\\log|\\\\mathcal{F}|/ \\\\epsilon^2)$ trajectories, and accompanying $\\\\sqrt{T}$\\nregret when the Bellman eluder dimension $d$ does not increase with $T$ at more\\nthan a $\\\\log T$ rate.\\n  Here, $\\\\mathcal{F}$ is the critic function class, $\\\\mathcal{A}$ is the action\\nspace, and $H$ is the horizon in the finite horizon MDP setting. Our algorithm\\nintegrates optimism, off-policy critic estimation targeting the optimal\\nQ-function, and rare-switching policy resets.\\n  We extend this to the setting of Hybrid RL, showing that initializing the\\ncritic with offline data yields sample efficiency gains compared to purely\\noffline or online RL. Further, utilizing access to offline data, we provide a\\n\\\\textit{non-optimistic} provably efficient actor-critic algorithm that only\\nadditionally requires $N_{\\\\text{off}} \\\\geq c_{\\\\text{off}}^*dH^4/\\\\epsilon^2$ in\\nexchange for omitting optimism, where $c_{\\\\text{off}}^*$ is the single-policy\\nconcentrability coefficient and $N_{\\\\text{off}}$ is the number of offline\\nsamples. This addresses another open problem in the literature. We further\\nprovide numerical experiments to support our theoretical findings.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.AI,cs.LG\", \"published\": \"2025-05-06T17:32:39Z\"}"}

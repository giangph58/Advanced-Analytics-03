{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23762v1\", \"title\": \"UniSep: Universal Target Audio Separation with Language Models at Scale\", \"summary\": \"We propose Universal target audio Separation (UniSep), addressing the\\nseparation task on arbitrary mixtures of different types of audio.\\nDistinguished from previous studies, UniSep is performed on unlimited source\\ndomains and unlimited source numbers. We formulate the separation task as a\\nsequence-to-sequence problem, and a large language model (LLM) is used to model\\nthe audio sequence in the discrete latent space, leveraging the power of LLM in\\nhandling complex mixture audios with large-scale data. Moreover, a novel\\npre-training strategy is proposed to utilize audio-only data, which reduces the\\nefforts of large-scale data simulation and enhances the ability of LLMs to\\nunderstand the consistency and correlation of information within audio\\nsequences. We also demonstrate the effectiveness of scaling datasets in an\\naudio separation task: we use large-scale data (36.5k hours), including speech,\\nmusic, and sound, to train a universal target audio separation model that is\\nnot limited to a specific domain. Experiments show that UniSep achieves\\ncompetitive subjective and objective evaluation results compared with\\nsingle-task models.\", \"main_category\": \"cs.SD\", \"categories\": \"cs.SD,eess.AS\", \"published\": \"2025-03-31T06:27:37Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17379v1\", \"title\": \"A Spatially-Aware Multiple Instance Learning Framework for Digital\\n  Pathology\", \"summary\": \"Multiple instance learning (MIL) is a promising approach for weakly\\nsupervised classification in pathology using whole slide images (WSIs).\\nHowever, conventional MIL methods such as Attention-Based Deep Multiple\\nInstance Learning (ABMIL) typically disregard spatial interactions among\\npatches that are crucial to pathological diagnosis. Recent advancements, such\\nas Transformer based MIL (TransMIL), have incorporated spatial context and\\ninter-patch relationships. However, it remains unclear whether explicitly\\nmodeling patch relationships yields similar performance gains in ABMIL, which\\nrelies solely on Multi-Layer Perceptrons (MLPs). In contrast, TransMIL employs\\nTransformer-based layers, introducing a fundamental architectural shift at the\\ncost of substantially increased computational complexity. In this work, we\\nenhance the ABMIL framework by integrating interaction-aware representations to\\naddress this question. Our proposed model, Global ABMIL (GABMIL), explicitly\\ncaptures inter-instance dependencies while preserving computational efficiency.\\nExperimental results on two publicly available datasets for tumor subtyping in\\nbreast and lung cancers demonstrate that GABMIL achieves up to a 7 percentage\\npoint improvement in AUPRC and a 5 percentage point increase in the Kappa score\\nover ABMIL, with minimal or no additional computational overhead. These\\nfindings underscore the importance of incorporating patch interactions within\\nMIL frameworks.\", \"main_category\": \"eess.IV\", \"categories\": \"eess.IV,cs.CV\", \"published\": \"2025-04-24T08:53:46Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13151v1\", \"title\": \"MIB: A Mechanistic Interpretability Benchmark\", \"summary\": \"How can we know whether new mechanistic interpretability methods achieve real\\nimprovements? In pursuit of meaningful and lasting evaluation standards, we\\npropose MIB, a benchmark with two tracks spanning four tasks and five models.\\nMIB favors methods that precisely and concisely recover relevant causal\\npathways or specific causal variables in neural language models. The circuit\\nlocalization track compares methods that locate the model components - and\\nconnections between them - most important for performing a task (e.g.,\\nattribution patching or information flow routes). The causal variable\\nlocalization track compares methods that featurize a hidden vector, e.g.,\\nsparse autoencoders (SAEs) or distributed alignment search (DAS), and locate\\nmodel features for a causal variable relevant to the task. Using MIB, we find\\nthat attribution and mask optimization methods perform best on circuit\\nlocalization. For causal variable localization, we find that the supervised DAS\\nmethod performs best, while SAE features are not better than neurons, i.e.,\\nstandard dimensions of hidden vectors. These findings illustrate that MIB\\nenables meaningful comparisons of methods, and increases our confidence that\\nthere has been real progress in the field.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL\", \"published\": \"2025-04-17T17:55:45Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16020v1\", \"title\": \"AlphaGrad: Non-Linear Gradient Normalization Optimizer\", \"summary\": \"We introduce AlphaGrad, a memory-efficient, conditionally stateless optimizer\\naddressing the memory overhead and hyperparameter complexity of adaptive\\nmethods like Adam. AlphaGrad enforces scale invariance via tensor-wise L2\\ngradient normalization followed by a smooth hyperbolic tangent transformation,\\n$g' = \\\\tanh(\\\\alpha \\\\cdot \\\\tilde{g})$, controlled by a single steepness\\nparameter $\\\\alpha$. Our contributions include: (1) the AlphaGrad algorithm\\nformulation; (2) a formal non-convex convergence analysis guaranteeing\\nstationarity; (3) extensive empirical evaluation on diverse RL benchmarks (DQN,\\nTD3, PPO). Compared to Adam, AlphaGrad demonstrates a highly context-dependent\\nperformance profile. While exhibiting instability in off-policy DQN, it\\nprovides enhanced training stability with competitive results in TD3 (requiring\\ncareful $\\\\alpha$ tuning) and achieves substantially superior performance in\\non-policy PPO. These results underscore the critical importance of empirical\\n$\\\\alpha$ selection, revealing strong interactions between the optimizer's\\ndynamics and the underlying RL algorithm. AlphaGrad presents a compelling\\nalternative optimizer for memory-constrained scenarios and shows significant\\npromise for on-policy learning regimes where its stability and efficiency\\nadvantages can be particularly impactful.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.NE,stat.ML\", \"published\": \"2025-04-22T16:33:14Z\"}"}

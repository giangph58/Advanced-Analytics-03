{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21334v1\", \"title\": \"Simple Visual Artifact Detection in Sora-Generated Videos\", \"summary\": \"The December 2024 release of OpenAI's Sora, a powerful video generation model\\ndriven by natural language prompts, highlights a growing convergence between\\nlarge language models (LLMs) and video synthesis. As these multimodal systems\\nevolve into video-enabled LLMs (VidLLMs), capable of interpreting, generating,\\nand interacting with visual content, understanding their limitations and\\nensuring their safe deployment becomes essential. This study investigates\\nvisual artifacts frequently found and reported in Sora-generated videos, which\\ncan compromise quality, mislead viewers, or propagate disinformation. We\\npropose a multi-label classification framework targeting four common artifact\\nlabel types: label 1: boundary / edge defects, label 2: texture / noise issues,\\nlabel 3: movement / joint anomalies, and label 4: object mismatches /\\ndisappearances. Using a dataset of 300 manually annotated frames extracted from\\n15 Sora-generated videos, we trained multiple 2D CNN architectures (ResNet-50,\\nEfficientNet-B3 / B4, ViT-Base). The best-performing model trained by ResNet-50\\nachieved an average multi-label classification accuracy of 94.14%. This work\\nsupports the broader development of VidLLMs by contributing to (1) the creation\\nof datasets for video quality evaluation, (2) interpretable artifact-based\\nanalysis beyond language metrics, and (3) the identification of visual risks\\nrelevant to factuality and safety.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-30T05:41:43Z\"}"}

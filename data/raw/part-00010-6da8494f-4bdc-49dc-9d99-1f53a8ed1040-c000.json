{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07754v1\", \"title\": \"Efficient Tuning of Large Language Models for Knowledge-Grounded\\n  Dialogue Generation\", \"summary\": \"Large language models (LLMs) demonstrate remarkable text comprehension and\\ngeneration capabilities but often lack the ability to utilize up-to-date or\\ndomain-specific knowledge not included in their training data. To address this\\ngap, we introduce KEDiT, an efficient method for fine-tuning LLMs for\\nknowledge-grounded dialogue generation. KEDiT operates in two main phases:\\nfirst, it employs an information bottleneck to compress retrieved knowledge\\ninto learnable parameters, retaining essential information while minimizing\\ncomputational overhead. Second, a lightweight knowledge-aware adapter\\nintegrates these compressed knowledge vectors into the LLM during fine-tuning,\\nupdating less than 2\\\\% of the model parameters. The experimental results on the\\nWizard of Wikipedia and a newly constructed PubMed-Dialog dataset demonstrate\\nthat KEDiT excels in generating contextually relevant and informative\\nresponses, outperforming competitive baselines in automatic, LLM-based, and\\nhuman evaluations. This approach effectively combines the strengths of\\npretrained LLMs with the adaptability needed for incorporating dynamic\\nknowledge, presenting a scalable solution for fields such as medicine.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-10T13:54:36Z\"}"}

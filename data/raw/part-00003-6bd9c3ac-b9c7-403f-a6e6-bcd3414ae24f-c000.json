{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19903v1\", \"title\": \"Convergence Analysis of Asynchronous Federated Learning with Gradient\\n  Compression for Non-Convex Optimization\", \"summary\": \"Gradient compression is an effective technique for reducing communication\\ncosts in federated learning (FL), and error feedback (EF) is usually adopted to\\nremedy the compression errors. However, there remains a lack of systematic\\nstudy on these techniques in asynchronous FL. In this paper, we fill this gap\\nby analyzing the convergence behaviors of FL under different frameworks. We\\nfirstly consider a basic asynchronous FL framework AsynFL, and provide an\\nimproved convergence analysis that relies on fewer assumptions and yields a\\nsuperior convergence rate than prior studies. Then, we consider a variant\\nframework with gradient compression, AsynFLC. We show sufficient conditions for\\nits convergence to the optimum, indicating the interaction between asynchronous\\ndelay and compression rate. Our analysis also demonstrates that asynchronous\\ndelay amplifies the variance caused by compression, thereby hindering\\nconvergence, and such an impact is exacerbated by high data heterogeneity.\\nFurthermore, we study the convergence of AsynFLC-EF, the framework that further\\nintegrates EF. We prove that EF can effectively reduce the variance of gradient\\nestimation despite asynchronous delay, which enables AsynFLC-EF to match the\\nconvergence rate of AsynFL. We also show that the impact of asynchronous delay\\non EF is limited to slowing down the higher-order convergence term.\\nExperimental results substantiate our analytical findings very well.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-28T15:35:34Z\"}"}

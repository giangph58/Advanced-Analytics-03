{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02264v1\", \"title\": \"MMTL-UniAD: A Unified Framework for Multimodal and Multi-Task Learning\\n  in Assistive Driving Perception\", \"summary\": \"Advanced driver assistance systems require a comprehensive understanding of\\nthe driver's mental/physical state and traffic context but existing works often\\nneglect the potential benefits of joint learning between these tasks. This\\npaper proposes MMTL-UniAD, a unified multi-modal multi-task learning framework\\nthat simultaneously recognizes driver behavior (e.g., looking around, talking),\\ndriver emotion (e.g., anxiety, happiness), vehicle behavior (e.g., parking,\\nturning), and traffic context (e.g., traffic jam, traffic smooth). A key\\nchallenge is avoiding negative transfer between tasks, which can impair\\nlearning performance. To address this, we introduce two key components into the\\nframework: one is the multi-axis region attention network to extract global\\ncontext-sensitive features, and the other is the dual-branch multimodal\\nembedding to learn multimodal embeddings from both task-shared and\\ntask-specific features. The former uses a multi-attention mechanism to extract\\ntask-relevant features, mitigating negative transfer caused by task-unrelated\\nfeatures. The latter employs a dual-branch structure to adaptively adjust\\ntask-shared and task-specific parameters, enhancing cross-task knowledge\\ntransfer while reducing task conflicts. We assess MMTL-UniAD on the AIDE\\ndataset, using a series of ablation studies, and show that it outperforms\\nstate-of-the-art methods across all four tasks. The code is available on\\nhttps://github.com/Wenzhuo-Liu/MMTL-UniAD.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-03T04:23:27Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04368v1\", \"title\": \"Pipelining Split Learning in Multi-hop Edge Networks\", \"summary\": \"To support large-scale model training, split learning (SL) enables multiple\\nedge devices/servers to share the intensive training workload. However, most\\nexisting works on SL focus solely on two-tier model splitting. Moreover, while\\nsome recent works have investigated the model splitting and placement problems\\nfor multi-hop SL, these solutions fail to overcome the resource idleness issue,\\nresulting in significant network idle time. In this work, we propose a\\npipelined SL scheme by addressing the joint optimization problem of model\\nsplitting and placement (MSP) in multi-hop edge networks. By applying pipeline\\nparallelism to SL, we identify that the MSP problem can be mapped to a problem\\nof minimizing the weighted sum of a bottleneck cost function (min-max) and a\\nlinear cost function (min-sum). Based on graph theory, we devise a\\nbottleneck-aware shortest-path algorithm to obtain the optimal solution.\\nBesides, given the MSP outcomes, we also derive the closed-form solution to the\\nmicro-batch size in the pipeline. Finally, we develop an alternating\\noptimization algorithm of MSP and micro-batch size to solve the joint\\noptimization problem to minimize the end-to-end training latency. Extensive\\nsimulations have demonstrated the significant advantages of our algorithm\\ncompared to existing benchmarks without pipeline parallelism.\", \"main_category\": \"cs.NI\", \"categories\": \"cs.NI\", \"published\": \"2025-05-07T12:36:24Z\"}"}

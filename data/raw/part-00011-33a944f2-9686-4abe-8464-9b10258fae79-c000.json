{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10462v1\", \"title\": \"The Scalability of Simplicity: Empirical Analysis of Vision-Language\\n  Learning with a Single Transformer\", \"summary\": \"This paper introduces SAIL, a single transformer unified multimodal large\\nlanguage model (MLLM) that integrates raw pixel encoding and language decoding\\nwithin a singular architecture. Unlike existing modular MLLMs, which rely on a\\npre-trained vision transformer (ViT), SAIL eliminates the need for a separate\\nvision encoder, presenting a more minimalist architecture design. Instead of\\nintroducing novel architectural components, SAIL adapts mix-attention\\nmechanisms and multimodal positional encodings to better align with the\\ndistinct characteristics of visual and textual modalities. We systematically\\ncompare SAIL's properties-including scalability, cross-modal information flow\\npatterns, and visual representation capabilities-with those of modular MLLMs.\\nBy scaling both training data and model size, SAIL achieves performance\\ncomparable to modular MLLMs. Notably, the removal of pretrained ViT components\\nenhances SAIL's scalability and results in significantly different cross-modal\\ninformation flow patterns. Moreover, SAIL demonstrates strong visual\\nrepresentation capabilities, achieving results on par with ViT-22B in vision\\ntasks such as semantic segmentation. Code and models are available at\\nhttps://github.com/bytedance/SAIL.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-14T17:50:20Z\"}"}

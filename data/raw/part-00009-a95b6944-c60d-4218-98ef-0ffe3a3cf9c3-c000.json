{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10141v1\", \"title\": \"The Impact of Model Zoo Size and Composition on Weight Space Learning\", \"summary\": \"Re-using trained neural network models is a common strategy to reduce\\ntraining cost and transfer knowledge. Weight space learning - using the weights\\nof trained models as data modality - is a promising new field to re-use\\npopulations of pre-trained models for future tasks. Approaches in this field\\nhave demonstrated high performance both on model analysis and weight generation\\ntasks. However, until now their learning setup requires homogeneous model zoos\\nwhere all models share the same exact architecture, limiting their capability\\nto generalize beyond the population of models they saw during training. In this\\nwork, we remove this constraint and propose a modification to a common weight\\nspace learning method to accommodate training on heterogeneous populations of\\nmodels. We further investigate the resulting impact of model diversity on\\ngenerating unseen neural network model weights for zero-shot knowledge\\ntransfer. Our extensive experimental evaluation shows that including models\\nwith varying underlying image datasets has a high impact on performance and\\ngeneralization, for both in- and out-of-distribution settings. Code is\\navailable on github.com/HSG-AIML/MultiZoo-SANE.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-14T11:54:06Z\"}"}

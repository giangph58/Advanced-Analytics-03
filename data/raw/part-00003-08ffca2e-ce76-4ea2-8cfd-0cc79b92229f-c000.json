{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11368v1\", \"title\": \"From Gaze to Insight: Bridging Human Visual Attention and Vision\\n  Language Model Explanation for Weakly-Supervised Medical Image Segmentation\", \"summary\": \"Medical image segmentation remains challenging due to the high cost of\\npixel-level annotations for training. In the context of weak supervision,\\nclinician gaze data captures regions of diagnostic interest; however, its\\nsparsity limits its use for segmentation. In contrast, vision-language models\\n(VLMs) provide semantic context through textual descriptions but lack the\\nexplanation precision required. Recognizing that neither source alone suffices,\\nwe propose a teacher-student framework that integrates both gaze and language\\nsupervision, leveraging their complementary strengths. Our key insight is that\\ngaze data indicates where clinicians focus during diagnosis, while VLMs explain\\nwhy those regions are significant. To implement this, the teacher model first\\nlearns from gaze points enhanced by VLM-generated descriptions of lesion\\nmorphology, establishing a foundation for guiding the student model. The\\nteacher then directs the student through three strategies: (1) Multi-scale\\nfeature alignment to fuse visual cues with textual semantics; (2)\\nConfidence-weighted consistency constraints to focus on reliable predictions;\\n(3) Adaptive masking to limit error propagation in uncertain areas. Experiments\\non the Kvasir-SEG, NCI-ISBI, and ISIC datasets show that our method achieves\\nDice scores of 80.78%, 80.53%, and 84.22%, respectively-improving 3-5% over\\ngaze baselines without increasing the annotation burden. By preserving\\ncorrelations among predictions, gaze data, and lesion descriptions, our\\nframework also maintains clinical interpretability. This work illustrates how\\nintegrating human visual attention with AI-generated semantic context can\\neffectively overcome the limitations of individual weak supervision signals,\\nthereby advancing the development of deployable, annotation-efficient medical\\nAI systems. Code is available at: https://github.com/jingkunchen/FGI.git.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-15T16:32:15Z\"}"}

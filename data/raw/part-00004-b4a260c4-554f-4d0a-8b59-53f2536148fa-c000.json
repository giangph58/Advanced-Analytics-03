{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21798v1\", \"title\": \"SWE-smith: Scaling Data for Software Engineering Agents\", \"summary\": \"Despite recent progress in Language Models (LMs) for software engineering,\\ncollecting training data remains a significant pain point. Existing datasets\\nare small, with at most 1,000s of training instances from 11 or fewer GitHub\\nrepositories. The procedures to curate such datasets are often complex,\\nnecessitating hundreds of hours of human labor; companion execution\\nenvironments also take up several terabytes of storage, severely limiting their\\nscalability and usability. To address this pain point, we introduce SWE-smith,\\na novel pipeline for generating software engineering training data at scale.\\nGiven any Python codebase, SWE-smith constructs a corresponding execution\\nenvironment, then automatically synthesizes 100s to 1,000s of task instances\\nthat break existing test(s) in the codebase. Using SWE-smith, we create a\\ndataset of 50k instances sourced from 128 GitHub repositories, an order of\\nmagnitude larger than all previous works. We train SWE-agent-LM-32B, achieving\\n40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art\\namong open source models. We open source SWE-smith (collection procedure, task\\ninstances, trajectories, models) to lower the barrier of entry for research in\\nLM systems for automated software engineering. All assets available at\\nhttps://swesmith.com.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE,cs.AI,cs.CL\", \"published\": \"2025-04-30T16:56:06Z\"}"}

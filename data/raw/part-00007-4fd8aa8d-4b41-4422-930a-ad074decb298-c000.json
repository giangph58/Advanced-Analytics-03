{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10068v1\", \"title\": \"Mavors: Multi-granularity Video Representation for Multimodal Large\\n  Language Model\", \"summary\": \"Long-context video understanding in multimodal large language models (MLLMs)\\nfaces a critical challenge: balancing computational efficiency with the\\nretention of fine-grained spatio-temporal patterns. Existing approaches (e.g.,\\nsparse sampling, dense sampling with low resolution, and token compression)\\nsuffer from significant information loss in temporal dynamics, spatial details,\\nor subtle interactions, particularly in videos with complex motion or varying\\nresolutions. To address this, we propose $\\\\mathbf{Mavors}$, a novel framework\\nthat introduces $\\\\mathbf{M}$ulti-gr$\\\\mathbf{a}$nularity\\n$\\\\mathbf{v}$ide$\\\\mathbf{o}$ $\\\\mathbf{r}$epre$\\\\mathbf{s}$entation for holistic\\nlong-video modeling. Specifically, Mavors directly encodes raw video content\\ninto latent representations through two core components: 1) an Intra-chunk\\nVision Encoder (IVE) that preserves high-resolution spatial features via 3D\\nconvolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator\\n(IFA) that establishes temporal coherence across chunks using transformer-based\\ndependency modeling with chunk-level rotary position encodings. Moreover, the\\nframework unifies image and video understanding by treating images as\\nsingle-frame videos via sub-image decomposition. Experiments across diverse\\nbenchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity\\nand temporal continuity, significantly outperforming existing methods in tasks\\nrequiring fine-grained spatio-temporal reasoning.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.CL\", \"published\": \"2025-04-14T10:14:44Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20859v1\", \"title\": \"X-Cross: Dynamic Integration of Language Models for Cross-Domain\\n  Sequential Recommendation\", \"summary\": \"As new products are emerging daily, recommendation systems are required to\\nquickly adapt to possible new domains without needing extensive retraining.\\nThis work presents ``X-Cross'' -- a novel cross-domain\\nsequential-recommendation model that recommends products in new domains by\\nintegrating several domain-specific language models; each model is fine-tuned\\nwith low-rank adapters (LoRA). Given a recommendation prompt, operating layer\\nby layer, X-Cross dynamically refines the representation of each source\\nlanguage model by integrating knowledge from all other models. These refined\\nrepresentations are propagated from one layer to the next, leveraging the\\nactivations from each domain adapter to ensure domain-specific nuances are\\npreserved while enabling adaptability across domains. Using Amazon datasets for\\nsequential recommendation, X-Cross achieves performance comparable to a model\\nthat is fine-tuned with LoRA, while using only 25% of the additional\\nparameters. In cross-domain tasks, such as adapting from Toys domain to Tools,\\nElectronics or Sports, X-Cross demonstrates robust performance, while requiring\\nabout 50%-75% less fine-tuning data than LoRA to make fine-tuning effective.\\nFurthermore, X-Cross achieves significant improvement in accuracy over\\nalternative cross-domain baselines. Overall, X-Cross enables scalable and\\nadaptive cross-domain recommendations, reducing computational overhead and\\nproviding an efficient solution for data-constrained environments.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR,cs.AI,cs.CL,cs.LG\", \"published\": \"2025-04-29T15:33:20Z\"}"}

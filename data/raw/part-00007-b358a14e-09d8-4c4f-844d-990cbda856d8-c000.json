{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16030v1\", \"title\": \"LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale\", \"summary\": \"Recent video large language models (Video LLMs) often depend on costly human\\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\\nwhich limits their training at scale. In this paper, we explore large-scale\\ntraining for Video LLM with cheap automatic speech recognition (ASR)\\ntranscripts. Specifically, we propose a novel streaming training approach that\\ndensely interleaves the ASR words and video frames according to their\\ntimestamps. Compared to previous studies in vision-language representation with\\nASR, our method naturally fits the streaming characteristics of ASR, thus\\nenabling the model to learn temporally-aligned, fine-grained vision-language\\nmodeling. To support the training algorithm, we introduce a data production\\npipeline to process YouTube videos and their closed captions (CC, same as ASR),\\nresulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset\\nfor high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,\\nthe ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general\\nvideo QA performance and exhibits a new capability in real-time video\\ncommentary. To evaluate this, we carefully design a new LiveSports-3K\\nbenchmark, using LLM-as-a-judge to measure the free-form commentary.\\nExperiments show our final LiveCC-7B-Instruct model can surpass advanced 72B\\nmodels (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even\\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\\nthe 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,\\ndemonstrating the broad generalizability of our approach. All resources of this\\npaper have been released at https://showlab.github.io/livecc.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-22T16:52:09Z\"}"}

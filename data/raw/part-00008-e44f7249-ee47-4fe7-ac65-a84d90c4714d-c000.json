{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05070v1\", \"title\": \"Performance Evaluation of Large Language Models in Bangla Consumer\\n  Health Query Summarization\", \"summary\": \"Consumer Health Queries (CHQs) in Bengali (Bangla), a low-resource language,\\noften contain extraneous details, complicating efficient medical responses.\\nThis study investigates the zero-shot performance of nine advanced large\\nlanguage models (LLMs): GPT-3.5-Turbo, GPT-4, Claude-3.5-Sonnet,\\nLlama3-70b-Instruct, Mixtral-8x22b-Instruct, Gemini-1.5-Pro,\\nQwen2-72b-Instruct, Gemma-2-27b, and Athene-70B, in summarizing Bangla CHQs.\\nUsing the BanglaCHQ-Summ dataset comprising 2,350 annotated query-summary\\npairs, we benchmarked these LLMs using ROUGE metrics against Bangla T5, a\\nfine-tuned state-of-the-art model. Mixtral-8x22b-Instruct emerged as the top\\nperforming model in ROUGE-1 and ROUGE-L, while Bangla T5 excelled in ROUGE-2.\\nThe results demonstrate that zero-shot LLMs can rival fine-tuned models,\\nachieving high-quality summaries even without task-specific training. This work\\nunderscores the potential of LLMs in addressing challenges in low-resource\\nlanguages, providing scalable solutions for healthcare query summarization.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-05-08T09:06:28Z\"}"}

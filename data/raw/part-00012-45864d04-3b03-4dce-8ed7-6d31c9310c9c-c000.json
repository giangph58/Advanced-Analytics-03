{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03507v1\", \"title\": \"Modality-Guided Dynamic Graph Fusion and Temporal Diffusion for\\n  Self-Supervised RGB-T Tracking\", \"summary\": \"To reduce the reliance on large-scale annotations, self-supervised RGB-T\\ntracking approaches have garnered significant attention. However, the omission\\nof the object region by erroneous pseudo-label or the introduction of\\nbackground noise affects the efficiency of modality fusion, while pseudo-label\\nnoise triggered by similar object noise can further affect the tracking\\nperformance. In this paper, we propose GDSTrack, a novel approach that\\nintroduces dynamic graph fusion and temporal diffusion to address the above\\nchallenges in self-supervised RGB-T tracking. GDSTrack dynamically fuses the\\nmodalities of neighboring frames, treats them as distractor noise, and\\nleverages the denoising capability of a generative model. Specifically, by\\nconstructing an adjacency matrix via an Adjacency Matrix Generator (AMG), the\\nproposed Modality-guided Dynamic Graph Fusion (MDGF) module uses a dynamic\\nadjacency matrix to guide graph attention, focusing on and fusing the object's\\ncoherent regions. Temporal Graph-Informed Diffusion (TGID) models MDGF features\\nfrom neighboring frames as interference, and thus improving robustness against\\nsimilar-object noise. Extensive experiments conducted on four public RGB-T\\ntracking datasets demonstrate that GDSTrack outperforms the existing\\nstate-of-the-art methods. The source code is available at\\nhttps://github.com/LiShenglana/GDSTrack.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-06T13:15:34Z\"}"}

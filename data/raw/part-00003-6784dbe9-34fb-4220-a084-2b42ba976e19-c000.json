{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21561v1\", \"title\": \"Iterative Trajectory Exploration for Multimodal Agents\", \"summary\": \"Multimodal agents, which integrate a controller (e.g., a large language\\nmodel) with external tools, have demonstrated remarkable capabilities in\\ntackling complex tasks. However, existing agents need to collect a large number\\nof expert data for fine-tuning to adapt to new environments. In this paper, we\\npropose an online self-exploration method for multimodal agents, namely SPORT,\\nvia step-wise preference optimization to refine the trajectories of agents,\\nwhich automatically generates tasks and learns from solving the generated\\ntasks, without any expert annotation. SPORT operates through four iterative\\ncomponents: task synthesis, step sampling, step verification, and preference\\ntuning. First, we synthesize multi-modal tasks using language models. Then, we\\nintroduce a novel search scheme, where step sampling and step verification are\\nexecuted alternately to solve each generated task. We employ a verifier to\\nprovide AI feedback to construct step-wise preference data. The data is\\nsubsequently used to update the controller's policy through preference tuning,\\nproducing a SPORT Agent. By interacting with real environments, the SPORT Agent\\nevolves into a more refined and capable system. Evaluation in the GTA and GAIA\\nbenchmarks show that the SPORT Agent achieves 6.41\\\\% and 3.64\\\\% improvements,\\nunderscoring the generalization and effectiveness introduced by our method. The\\nproject page is https://SPORT-Agents.github.io.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-30T12:01:27Z\"}"}

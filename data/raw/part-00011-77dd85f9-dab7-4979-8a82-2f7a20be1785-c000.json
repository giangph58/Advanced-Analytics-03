{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17665v1\", \"title\": \"Evaluating Grounded Reasoning by Code-Assisted Large Language Models for\\n  Mathematics\", \"summary\": \"Assisting LLMs with code generation improved their performance on\\nmathematical reasoning tasks. However, the evaluation of code-assisted LLMs is\\ngenerally restricted to execution correctness, lacking a rigorous evaluation of\\ntheir generated programs. In this work, we bridge this gap by conducting an\\nin-depth analysis of code-assisted LLMs' generated programs in response to math\\nreasoning tasks. Our evaluation focuses on the extent to which LLMs ground\\ntheir programs to math rules, and how that affects their end performance. For\\nthis purpose, we assess the generations of five different LLMs, on two\\ndifferent math datasets, both manually and automatically. Our results reveal\\nthat the distribution of grounding depends on LLMs' capabilities and the\\ndifficulty of math problems. Furthermore, mathematical grounding is more\\neffective for closed-source models, while open-source models fail to employ\\nmath rules in their solutions correctly. On MATH500, the percentage of grounded\\nprograms decreased to half, while the ungrounded generations doubled in\\ncomparison to ASDiv grade-school problems. Our work highlights the need for\\nin-depth evaluation beyond execution accuracy metrics, toward a better\\nunderstanding of code-assisted LLMs' capabilities and limits in the math\\ndomain.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-24T15:34:24Z\"}"}

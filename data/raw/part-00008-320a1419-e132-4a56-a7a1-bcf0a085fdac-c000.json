{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00592v1\", \"title\": \"Uncertainty-Aware Multi-Expert Knowledge Distillation for Imbalanced\\n  Disease Grading\", \"summary\": \"Automatic disease image grading is a significant application of artificial\\nintelligence for healthcare, enabling faster and more accurate patient\\nassessments. However, domain shifts, which are exacerbated by data imbalance,\\nintroduce bias into the model, posing deployment difficulties in clinical\\napplications. To address the problem, we propose a novel\\n\\\\textbf{U}ncertainty-aware \\\\textbf{M}ulti-experts \\\\textbf{K}nowledge\\n\\\\textbf{D}istillation (UMKD) framework to transfer knowledge from multiple\\nexpert models to a single student model. Specifically, to extract\\ndiscriminative features, UMKD decouples task-agnostic and task-specific\\nfeatures with shallow and compact feature alignment in the feature space. At\\nthe output space, an uncertainty-aware decoupled distillation (UDD) mechanism\\ndynamically adjusts knowledge transfer weights based on expert model\\nuncertainties, ensuring robust and reliable distillation. Additionally, UMKD\\nalso tackles the problems of model architecture heterogeneity and distribution\\ndiscrepancies between source and target domains, which are inadequately tackled\\nby previous KD approaches. Extensive experiments on histology prostate grading\\n(\\\\textit{SICAPv2}) and fundus image grading (\\\\textit{APTOS}) demonstrate that\\nUMKD achieves a new state-of-the-art in both source-imbalanced and\\ntarget-imbalanced scenarios, offering a robust and practical solution for\\nreal-world disease image grading.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG\", \"published\": \"2025-05-01T15:26:23Z\"}"}

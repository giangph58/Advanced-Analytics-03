{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11004v1\", \"title\": \"Dynamic Compressing Prompts for Efficient Inference of Large Language\\n  Models\", \"summary\": \"Large Language Models (LLMs) have shown outstanding performance across a\\nvariety of tasks, partly due to advanced prompting techniques. However, these\\ntechniques often require lengthy prompts, which increase computational costs\\nand can hinder performance because of the limited context windows of LLMs.\\nWhile prompt compression is a straightforward solution, existing methods\\nconfront the challenges of retaining essential information, adapting to context\\nchanges, and remaining effective across different tasks. To tackle these\\nissues, we propose a task-agnostic method called Dynamic Compressing Prompts\\n(LLM-DCP). Our method reduces the number of prompt tokens while aiming to\\npreserve the performance as much as possible. We model prompt compression as a\\nMarkov Decision Process (MDP), enabling the DCP-Agent to sequentially remove\\nredundant tokens by adapting to dynamic contexts and retaining crucial content.\\nWe develop a reward function for training the DCP-Agent that balances the\\ncompression rate, the quality of the LLM output, and the retention of key\\ninformation. This allows for prompt token reduction without needing an external\\nblack-box LLM. Inspired by the progressive difficulty adjustment in curriculum\\nlearning, we introduce a Hierarchical Prompt Compression (HPC) training\\nstrategy that gradually increases the compression difficulty, enabling the\\nDCP-Agent to learn an effective compression method that maintains information\\nintegrity. Experiments demonstrate that our method outperforms state-of-the-art\\ntechniques, especially at higher compression rates. The code for our approach\\nwill be available at https://github.com/Fhujinwu/DCP.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-15T09:20:45Z\"}"}

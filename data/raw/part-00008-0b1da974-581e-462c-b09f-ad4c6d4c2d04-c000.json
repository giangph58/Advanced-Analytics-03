{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14875v1\", \"title\": \"ReSpec: Relevance and Specificity Grounded Online Filtering for Learning\\n  on Video-Text Data Streams\", \"summary\": \"The rapid growth of video-text data presents challenges in storage and\\ncomputation during training. Online learning, which processes streaming data in\\nreal-time, offers a promising solution to these issues while also allowing\\nswift adaptations in scenarios demanding real-time responsiveness. One strategy\\nto enhance the efficiency and effectiveness of learning involves identifying\\nand prioritizing data that enhances performance on target downstream tasks. We\\npropose Relevance and Specificity-based online filtering framework (ReSpec)\\nthat selects data based on four criteria: (i) modality alignment for clean\\ndata, (ii) task relevance for target focused data, (iii) specificity for\\ninformative and detailed data, and (iv) efficiency for low-latency processing.\\nRelevance is determined by the probabilistic alignment of incoming data with\\ndownstream tasks, while specificity employs the distance to a root embedding\\nrepresenting the least specific data as an efficient proxy for informativeness.\\nBy establishing reference points from target task data, ReSpec filters incoming\\ndata in real-time, eliminating the need for extensive storage and compute.\\nEvaluating on large-scale datasets WebVid2M and VideoCC3M, ReSpec attains\\nstate-of-the-art performance on five zeroshot video retrieval tasks, using as\\nlittle as 5% of the data while incurring minimal compute. The source code is\\navailable at https://github.com/cdjkim/ReSpec.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.LG\", \"published\": \"2025-04-21T06:02:03Z\"}"}

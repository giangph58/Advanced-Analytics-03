{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20685v1\", \"title\": \"Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion\", \"summary\": \"Generating realistic listener facial motions in dyadic conversations remains\\nchallenging due to the high-dimensional action space and temporal dependency\\nrequirements. Existing approaches usually consider extracting 3D Morphable\\nModel (3DMM) coefficients and modeling in the 3DMM space. However, this makes\\nthe computational speed of the 3DMM a bottleneck, making it difficult to\\nachieve real-time interactive responses. To tackle this problem, we propose\\nFacial Action Diffusion (FAD), which introduces the diffusion methods from the\\nfield of image generation to achieve efficient facial action generation. We\\nfurther build the Efficient Listener Network (ELNet) specially designed to\\naccommodate both the visual and audio information of the speaker as input.\\nConsidering of FAD and ELNet, the proposed method learns effective listener\\nfacial motion representations and leads to improvements of performance over the\\nstate-of-the-art methods while reducing 99% computational time.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.HC\", \"published\": \"2025-04-29T12:08:02Z\"}"}

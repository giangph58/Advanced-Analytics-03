{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12098v1\", \"title\": \"Gauging Overprecision in LLMs: An Empirical Study\", \"summary\": \"Recently, overconfidence in large language models (LLMs) has garnered\\nconsiderable attention due to its fundamental importance in quantifying the\\ntrustworthiness of LLM generation. However, existing approaches prompt the\\n\\\\textit{black box LLMs} to produce their confidence (\\\\textit{verbalized\\nconfidence}), which can be subject to many biases and hallucinations. Inspired\\nby a different aspect of overconfidence in cognitive science called\\n\\\\textit{overprecision}, we designed a framework for its study in black box\\nLLMs. This framework contains three main phases: 1) generation, 2) refinement\\nand 3) evaluation. In the generation phase we prompt the LLM to generate\\nanswers to numerical questions in the form of intervals with a certain level of\\nconfidence. This confidence level is imposed in the prompt and not required for\\nthe LLM to generate as in previous approaches. We use various prompting\\ntechniques and use the same prompt multiple times to gauge the effects of\\nrandomness in the generation process. In the refinement phase, answers from the\\nprevious phase are refined to generate better answers. The LLM answers are\\nevaluated and studied in the evaluation phase to understand its internal\\nworkings. This study allowed us to gain various insights into LLM\\noverprecision: 1) LLMs are highly uncalibrated for numerical tasks 2)\\n{\\\\color{blue}there is no correlation between the length of the interval and the\\nimposed confidence level, which can be symptomatic of a a) lack of\\nunderstanding of the concept of confidence or b) inability to adjust\\nself-confidence by following instructions}, {\\\\color{blue}3)} LLM numerical\\nprecision differs depending on the task, scale of answer and prompting\\ntechnique {\\\\color{blue}4) Refinement of answers doesn't improve precision in\\nmost cases}. We believe this study offers new perspectives on LLM\\noverconfidence and serves as a strong baseline for overprecision in LLMs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-16T14:02:21Z\"}"}

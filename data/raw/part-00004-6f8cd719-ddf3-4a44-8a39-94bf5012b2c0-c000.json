{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15667v1\", \"title\": \"Performance Estimation for Supervised Medical Image Segmentation Models\\n  on Unlabeled Data Using UniverSeg\", \"summary\": \"The performance of medical image segmentation models is usually evaluated\\nusing metrics like the Dice score and Hausdorff distance, which compare\\npredicted masks to ground truth annotations. However, when applying the model\\nto unseen data, such as in clinical settings, it is often impractical to\\nannotate all the data, making the model's performance uncertain. To address\\nthis challenge, we propose the Segmentation Performance Evaluator (SPE), a\\nframework for estimating segmentation models' performance on unlabeled data.\\nThis framework is adaptable to various evaluation metrics and model\\narchitectures. Experiments on six publicly available datasets across six\\nevaluation metrics including pixel-based metrics such as Dice score and\\ndistance-based metrics like HD95, demonstrated the versatility and\\neffectiveness of our approach, achieving a high correlation (0.956$\\\\pm$0.046)\\nand low MAE (0.025$\\\\pm$0.019) compare with real Dice score on the independent\\ntest set. These results highlight its ability to reliably estimate model\\nperformance without requiring annotations. The SPE framework integrates\\nseamlessly into any model training process without adding training overhead,\\nenabling performance estimation and facilitating the real-world application of\\nmedical image segmentation algorithms. The source code is publicly available\", \"main_category\": \"eess.IV\", \"categories\": \"eess.IV,cs.CV\", \"published\": \"2025-04-22T07:42:48Z\"}"}

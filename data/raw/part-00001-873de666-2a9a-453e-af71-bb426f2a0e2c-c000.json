{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11199v1\", \"title\": \"Video Summarization with Large Language Models\", \"summary\": \"The exponential increase in video content poses significant challenges in\\nterms of efficient navigation, search, and retrieval, thus requiring advanced\\nvideo summarization techniques. Existing video summarization methods, which\\nheavily rely on visual features and temporal dynamics, often fail to capture\\nthe semantics of video content, resulting in incomplete or incoherent\\nsummaries. To tackle the challenge, we propose a new video summarization\\nframework that leverages the capabilities of recent Large Language Models\\n(LLMs), expecting that the knowledge learned from massive data enables LLMs to\\nevaluate video frames in a manner that better aligns with diverse semantics and\\nhuman judgments, effectively addressing the inherent subjectivity in defining\\nkeyframes. Our method, dubbed LLM-based Video Summarization (LLMVS), translates\\nvideo frames into a sequence of captions using a Muti-modal Large Language\\nModel (M-LLM) and then assesses the importance of each frame using an LLM,\\nbased on the captions in its local context. These local importance scores are\\nrefined through a global attention mechanism in the entire context of video\\ncaptions, ensuring that our summaries effectively reflect both the details and\\nthe overarching narrative. Our experimental results demonstrate the superiority\\nof the proposed method over existing ones in standard benchmarks, highlighting\\nthe potential of LLMs in the processing of multimedia content.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-15T13:56:14Z\"}"}

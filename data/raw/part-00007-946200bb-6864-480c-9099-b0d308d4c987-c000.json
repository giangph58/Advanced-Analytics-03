{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04907v1\", \"title\": \"Video-Bench: Human-Aligned Video Generation Benchmark\", \"summary\": \"Video generation assessment is essential for ensuring that generative models\\nproduce visually realistic, high-quality videos while aligning with human\\nexpectations. Current video generation benchmarks fall into two main\\ncategories: traditional benchmarks, which use metrics and embeddings to\\nevaluate generated video quality across multiple dimensions but often lack\\nalignment with human judgments; and large language model (LLM)-based\\nbenchmarks, though capable of human-like reasoning, are constrained by a\\nlimited understanding of video quality metrics and cross-modal consistency. To\\naddress these challenges and establish a benchmark that better aligns with\\nhuman preferences, this paper introduces Video-Bench, a comprehensive benchmark\\nfeaturing a rich prompt suite and extensive evaluation dimensions. This\\nbenchmark represents the first attempt to systematically leverage MLLMs across\\nall dimensions relevant to video generation assessment in generative models. By\\nincorporating few-shot scoring and chain-of-query techniques, Video-Bench\\nprovides a structured, scalable approach to generated video evaluation.\\nExperiments on advanced models including Sora demonstrate that Video-Bench\\nachieves superior alignment with human preferences across all dimensions.\\nMoreover, in instances where our framework's assessments diverge from human\\nevaluations, it consistently offers more objective and accurate insights,\\nsuggesting an even greater potential advantage over traditional human judgment.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-07T10:32:42Z\"}"}

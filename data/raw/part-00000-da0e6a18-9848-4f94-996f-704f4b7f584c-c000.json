{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01357v1\", \"title\": \"Age-Aware Partial Gradient Update Strategy for Federated Learning Over\\n  the Air\", \"summary\": \"We propose an age-aware strategy to update gradients in an over-the-air\\nfederated learning system. The system comprises an edge server and multiple\\nclients, collaborating to minimize a global loss function. In each\\ncommunication round, clients perform local training, modulate their gradient\\nupdates onto a set of shared orthogonal waveforms, and simultaneously transmit\\nthe analog signals to the edge server. The edge server then extracts a noisy\\naggregated gradient from the received radio signal, updates the global model,\\nand broadcasts it to the clients for the next round of local computing. Despite\\nenabling all clients to upload information in every communication round, the\\nsystem is constrained by the limited number of available waveform carriers,\\nallowing only a subset of gradient parameters to be transmitted. To address\\nthis issue, our method maintains an age vector on the edge server, tracking the\\ntime elapsed since each coordinate of the global model was last updated. The\\nserver leverages this information to prioritize gradient entries for\\ntransmission, ensuring that outdated yet significant parameters are updated\\nmore frequently. We derive the convergence rate of the proposed algorithm to\\nquantify its effectiveness. Furthermore, experimental evaluations on the MNIST\\nand CIFAR-10 datasets demonstrate that our approach achieves higher accuracy\\nand more stable convergence performance compared to baseline methods,\\nhighlighting its potential for improving communication efficiency in\\nover-the-air federated learning systems.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC\", \"published\": \"2025-04-02T05:01:53Z\"}"}

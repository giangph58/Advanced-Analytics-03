{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05273v1\", \"title\": \"A Connection Between Learning to Reject and Bhattacharyya Divergences\", \"summary\": \"Learning to reject provide a learning paradigm which allows for our models to\\nabstain from making predictions. One way to learn the rejector is to learn an\\nideal marginal distribution (w.r.t. the input domain) - which characterizes a\\nhypothetical best marginal distribution - and compares it to the true marginal\\ndistribution via a density ratio. In this paper, we consider learning a joint\\nideal distribution over both inputs and labels; and develop a link between\\nrejection and thresholding different statistical divergences. We further find\\nthat when one considers a variant of the log-loss, the rejector obtained by\\nconsidering the joint ideal distribution corresponds to the thresholding of the\\nskewed Bhattacharyya divergence between class-probabilities. This is in\\ncontrast to the marginal case - that is equivalent to a typical\\ncharacterization of optimal rejection, Chow's Rule - which corresponds to a\\nthresholding of the Kullback-Leibler divergence. In general, we find that\\nrejecting via a Bhattacharyya divergence is less aggressive than Chow's Rule.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.IT,cs.LG,math.IT\", \"published\": \"2025-05-08T14:18:42Z\"}"}

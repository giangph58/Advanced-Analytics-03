{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11299v1\", \"title\": \"Efficient and Stable Multi-Dimensional Kolmogorov-Smirnov Distance\", \"summary\": \"We revisit extending the Kolmogorov-Smirnov distance between probability\\ndistributions to the multidimensional setting and make new arguments about the\\nproper way to approach this generalization. Our proposed formulation maximizes\\nthe difference over orthogonal dominating rectangular ranges (d-sided\\nrectangles in R^d), and is an integral probability metric. We also prove that\\nthe distance between a distribution and a sample from the distribution\\nconverges to 0 as the sample size grows, and bound this rate. Moreover, we show\\nthat one can, up to this same approximation error, compute the distance\\nefficiently in 4 or fewer dimensions; specifically the runtime is near-linear\\nin the size of the sample needed for that error. With this, we derive a\\ndelta-precision two-sample hypothesis test using this distance. Finally, we\\nshow these metric and approximation properties do not hold for other popular\\nvariants.\", \"main_category\": \"stat.CO\", \"categories\": \"stat.CO,cs.CG,cs.LG\", \"published\": \"2025-04-15T15:42:49Z\"}"}

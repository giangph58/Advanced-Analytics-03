{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11281v1\", \"title\": \"The Obvious Invisible Threat: LLM-Powered GUI Agents' Vulnerability to\\n  Fine-Print Injections\", \"summary\": \"A Large Language Model (LLM) powered GUI agent is a specialized autonomous\\nsystem that performs tasks on the user's behalf according to high-level\\ninstructions. It does so by perceiving and interpreting the graphical user\\ninterfaces (GUIs) of relevant apps, often visually, inferring necessary\\nsequences of actions, and then interacting with GUIs by executing the actions\\nsuch as clicking, typing, and tapping. To complete real-world tasks, such as\\nfilling forms or booking services, GUI agents often need to process and act on\\nsensitive user data. However, this autonomy introduces new privacy and security\\nrisks. Adversaries can inject malicious content into the GUIs that alters agent\\nbehaviors or induces unintended disclosures of private information. These\\nattacks often exploit the discrepancy between visual saliency for agents and\\nhuman users, or the agent's limited ability to detect violations of contextual\\nintegrity in task automation. In this paper, we characterized six types of such\\nattacks, and conducted an experimental study to test these attacks with six\\nstate-of-the-art GUI agents, 234 adversarial webpages, and 39 human\\nparticipants. Our findings suggest that GUI agents are highly vulnerable,\\nparticularly to contextually embedded threats. Moreover, human users are also\\nsusceptible to many of these attacks, indicating that simple human oversight\\nmay not reliably prevent failures. This misalignment highlights the need for\\nprivacy-aware agent design. We propose practical defense strategies to inform\\nthe development of safer and more reliable GUI agents.\", \"main_category\": \"cs.HC\", \"categories\": \"cs.HC,cs.CL,cs.CR\", \"published\": \"2025-04-15T15:21:09Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11082v1\", \"title\": \"DeepMLF: Multimodal language model with learnable tokens for deep fusion\\n  in sentiment analysis\", \"summary\": \"While multimodal fusion has been extensively studied in Multimodal Sentiment\\nAnalysis (MSA), the role of fusion depth and multimodal capacity allocation\\nremains underexplored. In this work, we position fusion depth, scalability, and\\ndedicated multimodal capacity as primary factors for effective fusion. We\\nintroduce DeepMLF, a novel multimodal language model (LM) with learnable tokens\\ntailored toward deep fusion. DeepMLF leverages an audiovisual encoder and a\\npretrained decoder LM augmented with multimodal information across its layers.\\nWe append learnable tokens to the LM that: 1) capture modality interactions in\\na controlled fashion and 2) preserve independent information flow for each\\nmodality. These fusion tokens gather linguistic information via causal\\nself-attention in LM Blocks and integrate with audiovisual information through\\ncross-attention MM Blocks. Serving as dedicated multimodal capacity, this\\ndesign enables progressive fusion across multiple layers, providing depth in\\nthe fusion process. Our training recipe combines modality-specific losses and\\nlanguage modelling loss, with the decoder LM tasked to predict ground truth\\npolarity. Across three MSA benchmarks with varying dataset characteristics,\\nDeepMLF achieves state-of-the-art performance. Our results confirm that deeper\\nfusion leads to better performance, with optimal fusion depths (5-7) exceeding\\nthose of existing approaches. Additionally, our analysis on the number of\\nfusion tokens reveals that small token sets ($\\\\sim$20) achieve optimal\\nperformance. We examine the importance of representation learning order (fusion\\ncurriculum) through audiovisual encoder initialization experiments. Our\\nablation studies demonstrate the superiority of the proposed fusion design and\\ngating while providing a holistic examination of DeepMLF's scalability to LLMs,\\nand the impact of each training objective and embedding regularization.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-15T11:28:02Z\"}"}

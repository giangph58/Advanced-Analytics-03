{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21850v1\", \"title\": \"COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning\", \"summary\": \"Multimodal Large Language Models (MLLMs) excel at simple vision-language\\ntasks but struggle when faced with complex tasks that require multiple\\ncapabilities, such as simultaneously recognizing objects, counting them, and\\nunderstanding their spatial relationships. This might be partially the result\\nof the fact that Visual Instruction Tuning (VIT), a critical training step for\\nMLLMs, has traditionally focused on scaling data volume, but not the\\ncompositional complexity of training examples. We propose COMPACT\\n(COMPositional Atomic-to-complex visual Capability Tuning), which generates a\\ntraining dataset explicitly controlling for the compositional complexity of the\\ntraining examples. The data from COMPACT allows MLLMs to train on combinations\\nof atomic capabilities to learn complex capabilities more efficiently. Across\\nall benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT\\nwhile using less than 10% of its data budget, and even outperforms it on\\nseveral, especially those involving complex multi-capability tasks. For\\nexample, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0%\\nimprovement on MM-Vet compared to the full-scale VIT on particularly complex\\nquestions that require four or more atomic capabilities. COMPACT offers a\\nscalable, data-efficient, visual compositional tuning recipe to improve on\\ncomplex visual-language tasks.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-30T17:57:22Z\"}"}

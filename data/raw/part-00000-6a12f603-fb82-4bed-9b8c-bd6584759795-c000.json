{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23751v1\", \"title\": \"Decoupled Distillation to Erase: A General Unlearning Method for Any\\n  Class-centric Tasks\", \"summary\": \"In this work, we present DEcoupLEd Distillation To Erase (DELETE), a general\\nand strong unlearning method for any class-centric tasks. To derive this, we\\nfirst propose a theoretical framework to analyze the general form of unlearning\\nloss and decompose it into forgetting and retention terms. Through the\\ntheoretical framework, we point out that a class of previous methods could be\\nmainly formulated as a loss that implicitly optimizes the forgetting term while\\nlacking supervision for the retention term, disturbing the distribution of\\npre-trained model and struggling to adequately preserve knowledge of the\\nremaining classes. To address it, we refine the retention term using \\\"dark\\nknowledge\\\" and propose a mask distillation unlearning method. By applying a\\nmask to separate forgetting logits from retention logits, our approach\\noptimizes both the forgetting and refined retention components simultaneously,\\nretaining knowledge of the remaining classes while ensuring thorough forgetting\\nof the target class. Without access to the remaining data or intervention\\n(i.e., used in some works), we achieve state-of-the-art performance across\\nvarious benchmarks. What's more, DELETE is a general solution that can be\\napplied to various downstream tasks, including face recognition, backdoor\\ndefense, and semantic segmentation with great performance.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-31T06:02:27Z\"}"}

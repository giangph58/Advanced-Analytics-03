{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05638v1\", \"title\": \"TAGC: Optimizing Gradient Communication in Distributed Transformer\\n  Training\", \"summary\": \"The increasing complexity of large language models (LLMs) necessitates\\nefficient training strategies to mitigate the high computational costs\\nassociated with distributed training. A significant bottleneck in this process\\nis gradient synchronization across multiple GPUs, particularly in the\\nzero-redundancy parallelism mode. In this paper, we introduce Transformer-Aware\\nGradient Compression (TAGC), an optimized gradient compression algorithm\\ndesigned specifically for transformer-based models. TAGC extends the lossless\\nhomomorphic compression method by adapting it for sharded models and\\nincorporating transformer-specific optimizations, such as layer-selective\\ncompression and dynamic sparsification. Our experimental results demonstrate\\nthat TAGC accelerates training by up to 15% compared to the standard Fully\\nSharded Data Parallel (FSDP) approach, with minimal impact on model quality. We\\nintegrate TAGC into the PyTorch FSDP framework, the implementation is publicly\\navailable at https://github.com/ipolyakov/TAGC.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.DC\", \"published\": \"2025-04-08T03:33:39Z\"}"}

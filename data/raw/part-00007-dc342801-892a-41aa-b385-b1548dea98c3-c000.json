{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15582v1\", \"title\": \"Smooth Calibration and Decision Making\", \"summary\": \"Calibration requires predictor outputs to be consistent with their Bayesian\\nposteriors. For machine learning predictors that do not distinguish between\\nsmall perturbations, calibration errors are continuous in predictions, e.g.,\\nsmooth calibration error (Foster and Hart, 2018), Distance to Calibration\\n(Blasiok et al., 2023a). On the contrary, decision-makers who use predictions\\nmake optimal decisions discontinuously in probabilistic space, experiencing\\nloss from miscalibration discontinuously. Calibration errors for\\ndecision-making are thus discontinuous, e.g., Expected Calibration Error\\n(Foster and Vohra, 1997), and Calibration Decision Loss (Hu and Wu, 2024).\\nThus, predictors with a low calibration error for machine learning may suffer a\\nhigh calibration error for decision-making, i.e., they may not be trustworthy\\nfor decision-makers optimizing assuming their predictions are correct. It is\\nnatural to ask if post-processing a predictor with a low calibration error for\\nmachine learning is without loss to achieve a low calibration error for\\ndecision-making. In our paper, we show that post-processing an online predictor\\nwith $\\\\epsilon$ distance to calibration achieves $O(\\\\sqrt{\\\\epsilon})$ ECE and\\nCDL, which is asymptotically optimal. The post-processing algorithm adds noise\\nto make predictions differentially private. The optimal bound from low distance\\nto calibration predictors from post-processing is non-optimal compared with\\nexisting online calibration algorithms that directly optimize for ECE and CDL.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.DS,stat.ML\", \"published\": \"2025-04-22T04:55:41Z\"}"}

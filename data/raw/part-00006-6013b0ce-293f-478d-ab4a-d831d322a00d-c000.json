{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00503v1\", \"title\": \"Variational OOD State Correction for Offline Reinforcement Learning\", \"summary\": \"The performance of Offline reinforcement learning is significantly impacted\\nby the issue of state distributional shift, and out-of-distribution (OOD) state\\ncorrection is a popular approach to address this problem. In this paper, we\\npropose a novel method named Density-Aware Safety Perception (DASP) for OOD\\nstate correction. Specifically, our method encourages the agent to prioritize\\nactions that lead to outcomes with higher data density, thereby promoting its\\noperation within or the return to in-distribution (safe) regions. To achieve\\nthis, we optimize the objective within a variational framework that\\nconcurrently considers both the potential outcomes of decision-making and their\\ndensity, thus providing crucial contextual information for safe\\ndecision-making. Finally, we validate the effectiveness and feasibility of our\\nproposed method through extensive experimental evaluations on the offline\\nMuJoCo and AntMaze suites.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.RO\", \"published\": \"2025-05-01T13:14:07Z\"}"}

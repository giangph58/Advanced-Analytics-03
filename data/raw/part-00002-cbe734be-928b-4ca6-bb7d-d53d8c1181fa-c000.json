{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16433v1\", \"title\": \"FrogDogNet: Fourier frequency Retained visual prompt Output Guidance for\\n  Domain Generalization of CLIP in Remote Sensing\", \"summary\": \"In recent years, large-scale vision-language models (VLMs) like CLIP have\\ngained attention for their zero-shot inference using instructional text\\nprompts. While these models excel in general computer vision, their potential\\nfor domain generalization in remote sensing (RS) remains underexplored.\\nExisting approaches enhance prompt learning by generating visual prompt tokens\\nbut rely on full-image features, introducing noise and background artifacts\\nthat vary within a class, causing misclassification. To address this, we\\npropose FrogDogNet, a novel prompt learning framework integrating Fourier\\nfrequency filtering and self-attention to improve RS scene classification and\\ndomain generalization. FrogDogNet selectively retains invariant low-frequency\\ncomponents while eliminating noise and irrelevant backgrounds, ensuring robust\\nfeature representation across domains. The model first extracts significant\\nfeatures via projection and self-attention, then applies frequency-based\\nfiltering to preserve essential structural information for prompt learning.\\nExtensive experiments on four RS datasets and three domain generalization tasks\\nshow that FrogDogNet consistently outperforms state-of-the-art prompt learning\\nmethods, demonstrating superior adaptability across domain shifts. Our findings\\nhighlight the effectiveness of frequency-based invariant feature retention in\\ngeneralization, paving the way for broader applications. Our code is available\\nat https://github.com/HariseetharamG/FrogDogNet\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-23T05:35:59Z\"}"}

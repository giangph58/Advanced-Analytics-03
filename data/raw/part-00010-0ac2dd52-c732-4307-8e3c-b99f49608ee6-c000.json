{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05693v1\", \"title\": \"STRIVE: A Think & Improve Approach with Iterative Refinement for\\n  Enhancing Question Quality Estimation\", \"summary\": \"Automatically assessing question quality is crucial for educators as it saves\\ntime, ensures consistency, and provides immediate feedback for refining\\nteaching materials. We propose a novel methodology called STRIVE (Structured\\nThinking and Refinement with multiLLMs for Improving Verified Question\\nEstimation) using a series of Large Language Models (LLMs) for automatic\\nquestion evaluation. This approach aims to improve the accuracy and depth of\\nquestion quality assessment, ultimately supporting diverse learners and\\nenhancing educational practices. The method estimates question quality in an\\nautomated manner by generating multiple evaluations based on the strengths and\\nweaknesses of the provided question and then choosing the best solution\\ngenerated by the LLM. Then the process is improved by iterative review and\\nresponse with another LLM until the evaluation metric values converge. This\\nsophisticated method of evaluating question quality improves the estimation of\\nquestion quality by automating the task of question quality evaluation.\\nCorrelation scores show that using this proposed method helps to improve\\ncorrelation with human judgments compared to the baseline method. Error\\nanalysis shows that metrics like relevance and appropriateness improve\\nsignificantly relative to human judgments by using STRIVE.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-08T05:34:38Z\"}"}

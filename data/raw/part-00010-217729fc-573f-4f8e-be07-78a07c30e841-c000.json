{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11336v1\", \"title\": \"Looking beyond the next token\", \"summary\": \"The structure of causal language model training assumes that each token can\\nbe accurately predicted from the previous context. This contrasts with humans'\\nnatural writing and reasoning process, where goals are typically known before\\nthe exact argument or phrasings. While this mismatch has been well studied in\\nthe literature, the working assumption has been that architectural changes are\\nneeded to address this mismatch. We argue that rearranging and processing the\\ntraining data sequences can allow models to more accurately imitate the true\\ndata-generating process, and does not require any other changes to the\\narchitecture or training infrastructure. We demonstrate that this technique,\\nTrelawney, and the inference algorithms derived from it allow us to improve\\nperformance on several key benchmarks that span planning, algorithmic\\nreasoning, and story generation tasks. Finally, our method naturally enables\\nthe generation of long-term goals at no additional cost. We investigate how\\nusing the model's goal-generation capability can further improve planning and\\nreasoning. Additionally, we believe Trelawney could potentially open doors to\\nnew capabilities beyond the current language modeling paradigm.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL\", \"published\": \"2025-04-15T16:09:06Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10885v1\", \"title\": \"PuzzleBench: A Fully Dynamic Evaluation Framework for Large Multimodal\\n  Models on Puzzle Solving\", \"summary\": \"Large Multimodal Models (LMMs) have demonstrated impressive capabilities\\nacross a wide range of multimodal tasks, achieving ever-increasing performance\\non various evaluation benchmarks. However, existing benchmarks are typically\\nstatic and often overlap with pre-training datasets, leading to fixed\\ncomplexity constraints and substantial data contamination issues. Meanwhile,\\nmanually annotated datasets are labor-intensive, time-consuming, and subject to\\nhuman bias and inconsistency, leading to reliability and reproducibility\\nissues. To address these problems, we propose a fully dynamic multimodal\\nevaluation framework, named Open-ended Visual Puzzle Generation (OVPG), which\\naims to generate fresh, diverse, and verifiable evaluation data automatically\\nin puzzle-solving tasks. Specifically, the OVPG pipeline consists of a raw\\nmaterial sampling module, a visual content generation module, and a puzzle rule\\ndesign module, which ensures that each evaluation instance is primitive, highly\\nrandomized, and uniquely solvable, enabling continual adaptation to the\\nevolving capabilities of LMMs. Built upon OVPG, we construct PuzzleBench, a\\ndynamic and scalable benchmark comprising 11,840 VQA samples. It features six\\ncarefully designed puzzle tasks targeting three core LMM competencies, visual\\nrecognition, logical reasoning, and context understanding. PuzzleBench differs\\nfrom static benchmarks that quickly become outdated. It enables ongoing dataset\\nrefreshing through OVPG and a rich set of open-ended puzzle designs, allowing\\nseamless adaptation to the evolving capabilities of LMMs.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-15T05:29:31Z\"}"}

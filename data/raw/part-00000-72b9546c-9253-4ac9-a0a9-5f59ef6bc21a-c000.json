{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20869v1\", \"title\": \"Quantifying the Noise of Structural Perturbations on Graph Adversarial\\n  Attacks\", \"summary\": \"Graph neural networks have been widely utilized to solve graph-related tasks\\nbecause of their strong learning power in utilizing the local information of\\nneighbors. However, recent studies on graph adversarial attacks have proven\\nthat current graph neural networks are not robust against malicious attacks.\\nYet much of the existing work has focused on the optimization objective based\\non attack performance to obtain (near) optimal perturbations, but paid less\\nattention to the strength quantification of each perturbation such as the\\ninjection of a particular node/link, which makes the choice of perturbations a\\nblack-box model that lacks interpretability. In this work, we propose the\\nconcept of noise to quantify the attack strength of each adversarial link.\\nFurthermore, we propose three attack strategies based on the defined noise and\\nclassification margins in terms of single and multiple steps optimization.\\nExtensive experiments conducted on benchmark datasets against three\\nrepresentative graph neural networks demonstrate the effectiveness of the\\nproposed attack strategies. Particularly, we also investigate the preferred\\npatterns of effective adversarial perturbations by analyzing the corresponding\\nproperties of the selected perturbation nodes.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CR\", \"published\": \"2025-04-29T15:42:56Z\"}"}

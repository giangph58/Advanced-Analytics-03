{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13110v1\", \"title\": \"Propagation of Chaos in One-hidden-layer Neural Networks beyond\\n  Logarithmic Time\", \"summary\": \"We study the approximation gap between the dynamics of a polynomial-width\\nneural network and its infinite-width counterpart, both trained using projected\\ngradient descent in the mean-field scaling regime. We demonstrate how to\\ntightly bound this approximation gap through a differential equation governed\\nby the mean-field dynamics. A key factor influencing the growth of this ODE is\\nthe local Hessian of each particle, defined as the derivative of the particle's\\nvelocity in the mean-field dynamics with respect to its position. We apply our\\nresults to the canonical feature learning problem of estimating a\\nwell-specified single-index model; we permit the information exponent to be\\narbitrarily large, leading to convergence times that grow polynomially in the\\nambient dimension $d$. We show that, due to a certain ``self-concordance''\\nproperty in these problems -- where the local Hessian of a particle is bounded\\nby a constant times the particle's velocity -- polynomially many neurons are\\nsufficient to closely approximate the mean-field dynamics throughout training.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.LG\", \"published\": \"2025-04-17T17:24:38Z\"}"}

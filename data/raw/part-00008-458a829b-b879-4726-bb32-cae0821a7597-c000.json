{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03392v1\", \"title\": \"Automatic Calibration for Membership Inference Attack on Large Language\\n  Models\", \"summary\": \"Membership Inference Attacks (MIAs) have recently been employed to determine\\nwhether a specific text was part of the pre-training data of Large Language\\nModels (LLMs). However, existing methods often misinfer non-members as members,\\nleading to a high false positive rate, or depend on additional reference models\\nfor probability calibration, which limits their practicality. To overcome these\\nchallenges, we introduce a novel framework called Automatic Calibration\\nMembership Inference Attack (ACMIA), which utilizes a tunable temperature to\\ncalibrate output probabilities effectively. This approach is inspired by our\\ntheoretical insights into maximum likelihood estimation during the pre-training\\nof LLMs. We introduce ACMIA in three configurations designed to accommodate\\ndifferent levels of model access and increase the probability gap between\\nmembers and non-members, improving the reliability and robustness of membership\\ninference. Extensive experiments on various open-source LLMs demonstrate that\\nour proposed attack is highly effective, robust, and generalizable, surpassing\\nstate-of-the-art baselines across three widely used benchmarks. Our code is\\navailable at:\\n\\\\href{https://github.com/Salehzz/ACMIA}{\\\\textcolor{blue}{Github}}.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-05-06T10:15:05Z\"}"}

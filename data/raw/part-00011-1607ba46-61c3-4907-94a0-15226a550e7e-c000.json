{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11386v1\", \"title\": \"Trajectory Encoding Temporal Graph Networks\", \"summary\": \"Temporal Graph Networks (TGNs) have demonstrated significant success in\\ndynamic graph tasks such as link prediction and node classification. Both tasks\\ncomprise transductive settings, where the model predicts links among known\\nnodes, and in inductive settings, where it generalises learned patterns to\\npreviously unseen nodes. Existing TGN designs face a dilemma under these dual\\nscenarios. Anonymous TGNs, which rely solely on temporal and structural\\ninformation, offer strong inductive generalisation but struggle to distinguish\\nknown nodes. In contrast, non-anonymous TGNs leverage node features to excel in\\ntransductive tasks yet fail to adapt to new nodes. To address this challenge,\\nwe propose Trajectory Encoding TGN (TETGN). Our approach introduces\\nautomatically expandable node identifiers (IDs) as learnable temporal\\npositional features and performs message passing over these IDs to capture each\\nnode's historical context. By integrating this trajectory-aware module with a\\nstandard TGN using multi-head attention, TETGN effectively balances\\ntransductive accuracy with inductive generalisation. Experimental results on\\nthree real-world datasets show that TETGN significantly outperforms strong\\nbaselines on both link prediction and node classification tasks, demonstrating\\nits ability to unify the advantages of anonymous and non-anonymous models for\\ndynamic graph learning.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-15T16:57:09Z\"}"}

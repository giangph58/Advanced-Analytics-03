{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10076v1\", \"title\": \"Towards Scalable Bayesian Optimization via Gradient-Informed Bayesian\\n  Neural Networks\", \"summary\": \"Bayesian optimization (BO) is a widely used method for data-driven\\noptimization that generally relies on zeroth-order data of objective function\\nto construct probabilistic surrogate models. These surrogates guide the\\nexploration-exploitation process toward finding global optimum. While Gaussian\\nprocesses (GPs) are commonly employed as surrogates of the unknown objective\\nfunction, recent studies have highlighted the potential of Bayesian neural\\nnetworks (BNNs) as scalable and flexible alternatives. Moreover, incorporating\\ngradient observations into GPs, when available, has been shown to improve BO\\nperformance. However, the use of gradients within BNN surrogates remains\\nunexplored. By leveraging automatic differentiation, gradient information can\\nbe seamlessly integrated into BNN training, resulting in more informative\\nsurrogates for BO. We propose a gradient-informed loss function for BNN\\ntraining, effectively augmenting function observations with local gradient\\ninformation. The effectiveness of this approach is demonstrated on well-known\\nbenchmarks in terms of improved BNN predictions and faster BO convergence as\\nthe number of decision variables increases.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,stat.ML\", \"published\": \"2025-04-14T10:21:08Z\"}"}

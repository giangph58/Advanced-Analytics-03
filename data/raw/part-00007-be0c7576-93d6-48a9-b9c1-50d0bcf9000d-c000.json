{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02671v1\", \"title\": \"LLM for Complex Reasoning Task: An Exploratory Study in Fermi Problems\", \"summary\": \"Fermi Problems (FPs) are mathematical reasoning tasks that require human-like\\nlogic and numerical reasoning. Unlike other reasoning questions, FPs often\\ninvolve real-world impracticalities or ambiguous concepts, making them\\nchallenging even for humans to solve. Despite advancements in AI, particularly\\nwith large language models (LLMs) in various reasoning tasks, FPs remain\\nrelatively under-explored. This work conducted an exploratory study to examine\\nthe capabilities and limitations of LLMs in solving FPs. We first evaluated the\\noverall performance of three advanced LLMs using a publicly available FP\\ndataset. We designed prompts according to the recently proposed TELeR taxonomy,\\nincluding a zero-shot scenario. Results indicated that all three LLMs achieved\\na fp_score (range between 0 - 1) below 0.5, underscoring the inherent\\ndifficulty of these reasoning tasks. To further investigate, we categorized FPs\\ninto standard and specific questions, hypothesizing that LLMs would perform\\nbetter on standard questions, which are characterized by clarity and\\nconciseness, than on specific ones. Comparative experiments confirmed this\\nhypothesis, demonstrating that LLMs performed better on standard FPs in terms\\nof both accuracy and efficiency.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-03T15:13:36Z\"}"}

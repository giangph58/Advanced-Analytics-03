{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14941v1\", \"title\": \"Vector Embedding, Retrieval-Augmented Generation, CPU-NPU Collaboration,\\n  Heterogeneous Computing\", \"summary\": \"Retrieval-Augmented Generation is a technology that enhances large language\\nmodels by integrating information retrieval. In the industry, inference\\nservices based on LLMs are highly sensitive to cost-performance ratio,\\nprompting the need for improving hardware resource utilization in the inference\\nservice. Specifically, vector embedding and retrieval processes take up to 20%\\nof the total latency. Therefore, optimizing the utilization of computational\\nresources in vector embeddings is crucial for enhancing the cost-performance\\nratio of inference processes, which in turn boosts their product\\ncompetitiveness.In this paper, we analyze the deployment costs of vector\\nembedding technology in inference services, propose a theoretical formula, and\\ndetermine through the mathematical expression that increasing the capacity to\\nprocess concurrent queries is the key to reducing the deployment costs of\\nvector embeddings. Therefore, in this paper, we focus on improving the\\nproduct's capability to process concurrent queries. To optimize concurrency\\nwithout sacrificing performance, we have designed a queue manager that adeptly\\noffloads CPU peak queries. This manager utilizes a linear regression model to\\nascertain the optimal queue depths, a critical parameter that significantly\\ninfluences the efficacy of the system. We further develop a system named WindVE\\nthat uses a CPU-NPU heterogeneous architecture to offload peak concurrent\\nqueries, which leverages the performance differences between the two processors\\nto effectively manage traffic surges. Through experiments, we compare WindVE to\\nthe state-of-the-art vector embedding framework FlagEmbedding, and achieve a\\nconcurrency level up to 22.3% higher than the scheme without offloading.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC\", \"published\": \"2025-04-21T08:02:25Z\"}"}

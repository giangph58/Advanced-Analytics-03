{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23923v1\", \"title\": \"What the F*ck Is Artificial General Intelligence?\", \"summary\": \"Artificial general intelligence (AGI) is an established field of research.\\nYet Melanie Mitchell and others have questioned if the term still has meaning.\\nAGI has been subject to so much hype and speculation it has become something of\\na Rorschach test. Mitchell points out that the debate will only be settled\\nthrough long term, scientific investigation. To that end here is a short,\\naccessible and provocative overview of AGI. I compare definitions of\\nintelligence, settling on intelligence in terms of adaptation and AGI as an\\nartificial scientist. Taking my queue from Sutton's Bitter Lesson I describe\\ntwo foundational tools used to build adaptive systems: search and\\napproximation. I compare pros, cons, hybrids and architectures like o3,\\nAlphaGo, AERA, NARS and Hyperon. I then discuss overall meta-approaches to\\nmaking systems behave more intelligently. I divide them into scale-maxing,\\nsimp-maxing, w-maxing based on the Bitter Lesson, Ockham's and Bennett's\\nRazors. These maximise resources, simplicity of form, and the weakness of\\nconstraints on functionality. I discuss examples including AIXI, the free\\nenergy principle and The Embiggening of language models. I conclude that though\\nscale-maxed approximation dominates, AGI will be a fusion of tools and\\nmeta-approaches. The Embiggening was enabled by improvements in hardware. Now\\nthe bottlenecks are sample and energy efficiency.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-03-31T10:15:37Z\"}"}

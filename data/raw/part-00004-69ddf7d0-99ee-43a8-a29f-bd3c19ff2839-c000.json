{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03303v1\", \"title\": \"Comparative Analysis of Lightweight Deep Learning Models for\\n  Memory-Constrained Devices\", \"summary\": \"This paper presents a comprehensive evaluation of lightweight deep learning\\nmodels for image classification, emphasizing their suitability for deployment\\nin resource-constrained environments such as low-memory devices. Five\\nstate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,\\nEfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse\\ndatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using\\nfour key performance metrics: classification accuracy, inference time,\\nfloating-point operations (FLOPs), and model size. Additionally, we investigate\\nthe impact of hyperparameter tuning, data augmentation, and training paradigms\\nby comparing pretrained models with scratch-trained counterparts, focusing on\\nMobileNetV3 Small. Our findings reveal that transfer learning significantly\\nenhances model accuracy and computational efficiency, particularly for complex\\ndatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest\\naccuracy, while MobileNetV3 offers the best balance between accuracy and\\nefficiency, and SqueezeNet excels in inference speed and compactness. This\\nstudy highlights critical trade-offs between accuracy and efficiency, offering\\nactionable insights for deploying lightweight models in real-world applications\\nwhere computational resources are limited. By addressing these challenges, this\\nresearch contributes to optimizing deep learning systems for edge computing and\\nmobile platforms.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-05-06T08:36:01Z\"}"}

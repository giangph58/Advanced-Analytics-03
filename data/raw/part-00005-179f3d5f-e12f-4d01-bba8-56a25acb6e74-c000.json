{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13069v1\", \"title\": \"Early Accessibility: Automating Alt-Text Generation for UI Icons During\\n  App Development\", \"summary\": \"Alt-text is essential for mobile app accessibility, yet UI icons often lack\\nmeaningful descriptions, limiting accessibility for screen reader users.\\nExisting approaches either require extensive labeled datasets, struggle with\\npartial UI contexts, or operate post-development, increasing technical debt. We\\nfirst conduct a formative study to determine when and how developers prefer to\\ngenerate icon alt-text. We then explore the ALTICON approach for generating\\nalt-text for UI icons during development using two fine-tuned models: a\\ntext-only large language model that processes extracted UI metadata and a\\nmulti-modal model that jointly analyzes icon images and textual context. To\\nimprove accuracy, the method extracts relevant UI information from the DOM\\ntree, retrieves in-icon text via OCR, and applies structured prompts for\\nalt-text generation. Our empirical evaluation with the most closely related\\ndeep-learning and vision-language models shows that ALTICON generates alt-text\\nthat is of higher quality while not requiring a full-screen input.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE,cs.HC\", \"published\": \"2025-04-17T16:31:05Z\"}"}

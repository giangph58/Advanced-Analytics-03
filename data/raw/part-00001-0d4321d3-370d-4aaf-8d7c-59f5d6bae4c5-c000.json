{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07640v1\", \"title\": \"Enhancing Large Language Models through Neuro-Symbolic Integration and\\n  Ontological Reasoning\", \"summary\": \"Large Language Models (LLMs) demonstrate impressive capabilities in natural\\nlanguage processing but suffer from inaccuracies and logical inconsistencies\\nknown as hallucinations. This compromises their reliability, especially in\\ndomains requiring factual accuracy. We propose a neuro-symbolic approach\\nintegrating symbolic ontological reasoning and machine learning methods to\\nenhance the consistency and reliability of LLM outputs. Our workflow utilizes\\nOWL ontologies, a symbolic reasoner (e.g., HermiT) for consistency checking,\\nand a lightweight machine learning model (logistic regression) for mapping\\nnatural language statements into logical forms compatible with the ontology.\\nWhen inconsistencies between LLM outputs and the ontology are detected, the\\nsystem generates explanatory feedback to guide the LLM towards a corrected,\\nlogically coherent response in an iterative refinement loop. We present a\\nworking Python prototype demonstrating this pipeline. Experimental results in a\\ndefined domain suggest significant improvements in semantic coherence and\\nfactual accuracy of LLM outputs, showcasing the potential of combining LLM\\nfluency with the rigor of formal semantics.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-10T10:39:24Z\"}"}

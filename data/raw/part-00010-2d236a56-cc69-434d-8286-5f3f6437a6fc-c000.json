{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12939v1\", \"title\": \"Disentangling Polysemantic Channels in Convolutional Neural Networks\", \"summary\": \"Mechanistic interpretability is concerned with analyzing individual\\ncomponents in a (convolutional) neural network (CNN) and how they form larger\\ncircuits representing decision mechanisms. These investigations are challenging\\nsince CNNs frequently learn polysemantic channels that encode distinct\\nconcepts, making them hard to interpret. To address this, we propose an\\nalgorithm to disentangle a specific kind of polysemantic channel into multiple\\nchannels, each responding to a single concept. Our approach restructures\\nweights in a CNN, utilizing that different concepts within the same channel\\nexhibit distinct activation patterns in the previous layer. By disentangling\\nthese polysemantic features, we enhance the interpretability of CNNs,\\nultimately improving explanatory techniques such as feature visualizations.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG\", \"published\": \"2025-04-17T13:37:47Z\"}"}

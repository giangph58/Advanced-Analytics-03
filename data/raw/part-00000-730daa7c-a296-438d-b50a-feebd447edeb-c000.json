{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20000v1\", \"title\": \"Knowledge Distillation of Domain-adapted LLMs for Question-Answering in\\n  Telecom\", \"summary\": \"Knowledge Distillation (KD) is one of the approaches to reduce the size of\\nLarge Language Models (LLMs). A LLM with smaller number of model parameters\\n(student) is trained to mimic the performance of a LLM of a larger size\\n(teacher model) on a specific task. For domain-specific tasks, it is not clear\\nif teacher or student model, or both, must be considered for domain adaptation.\\nIn this work, we study this problem from perspective of telecom domain\\nQuestion-Answering (QA) task. We systematically experiment with Supervised\\nFine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to\\nKD. We design experiments to study the impact of vocabulary (same and\\ndifferent) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the\\ndistilled model. Multi-faceted evaluation of the distillation using 14\\ndifferent metrics (N-gram, embedding and LLM-based metrics) is considered.\\nExperimental results show that SFT of teacher improves performance of distilled\\nmodel when both models have same vocabulary, irrespective of algorithm and\\nmetrics. Overall, SFT of both teacher and student results in better performance\\nacross all metrics, although the statistical significance of the same depends\\non the vocabulary of the teacher models.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.IR,cs.LG,I.2.7\", \"published\": \"2025-04-28T17:19:25Z\"}"}

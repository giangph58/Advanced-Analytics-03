{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10344v1\", \"title\": \"ALMTokenizer: A Low-bitrate and Semantic-rich Audio Codec Tokenizer for\\n  Audio Language Modeling\", \"summary\": \"Recent advancements in audio language models have underscored the pivotal\\nrole of audio tokenization, which converts audio signals into discrete tokens,\\nthereby facilitating the application of language model architectures to the\\naudio domain. In this study, we introduce ALMTokenizer, a novel low-bitrate and\\nsemantically rich audio codec tokenizer for audio language models. Prior\\nmethods, such as Encodec, typically encode individual audio frames into\\ndiscrete tokens without considering the use of context information across\\nframes. Unlike these methods, we introduce a novel query-based compression\\nstrategy to capture holistic information with a set of learnable query tokens\\nby explicitly modeling the context information across frames. This design not\\nonly enables the codec model to capture more semantic information but also\\nencodes the audio signal with fewer token sequences. Additionally, to enhance\\nthe semantic information in audio codec models, we introduce the following: (1)\\nA masked autoencoder (MAE) loss, (2) Vector quantization based on semantic\\npriors, and (3) An autoregressive (AR) prediction loss. As a result,\\nALMTokenizer achieves competitive reconstruction performance relative to\\nstate-of-the-art approaches while operating at a lower bitrate. Within the same\\naudio language model framework, ALMTokenizer outperforms previous tokenizers in\\naudio understanding and generation tasks.\", \"main_category\": \"cs.SD\", \"categories\": \"cs.SD\", \"published\": \"2025-04-14T15:51:56Z\"}"}

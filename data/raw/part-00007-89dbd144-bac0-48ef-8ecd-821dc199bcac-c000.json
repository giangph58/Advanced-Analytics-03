{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05764v1\", \"title\": \"Layer-Aware Embedding Fusion for LLMs in Text Classifications\", \"summary\": \"Embedding fusion has emerged as an effective approach for enhancing\\nperformance across various NLP tasks. However, systematic guidelines for\\nselecting optimal layers and developing effective fusion strategies for the\\nintegration of LLMs remain underexplored. In this study, we propose a\\nlayer-aware embedding selection method and investigate how to quantitatively\\nevaluate different layers to identify the most important ones for downstream\\nNLP tasks, showing that the critical layers vary depending on the dataset. We\\nalso explore how combining embeddings from multiple LLMs, without requiring\\nmodel fine-tuning, can improve performance. Experiments on four English text\\nclassification datasets (SST-2, MR, R8, and R52) demonstrate that different\\nlayers in LLMs exhibit varying degrees of representational strength for\\nclassification, and that combining embeddings from different models can enhance\\nperformance if the models exhibit complementary characteristics. Additionally,\\nwe discuss resources overhead (memory and inference time) to provide a balanced\\nperspective on the real world feasibility of embedding fusion. Future work will\\nexplore multilingual and domain specific datasets, as well as techniques for\\nautomating layer selection, to improve both performance and scalability.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-08T07:45:50Z\"}"}

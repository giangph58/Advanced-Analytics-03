{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05409v1\", \"title\": \"Hide & Seek: Transformer Symmetries Obscure Sharpness & Riemannian\\n  Geometry Finds It\", \"summary\": \"The concept of sharpness has been successfully applied to traditional\\narchitectures like MLPs and CNNs to predict their generalization. For\\ntransformers, however, recent work reported weak correlation between flatness\\nand generalization. We argue that existing sharpness measures fail for\\ntransformers, because they have much richer symmetries in their attention\\nmechanism that induce directions in parameter space along which the network or\\nits loss remain identical. We posit that sharpness must account fully for these\\nsymmetries, and thus we redefine it on a quotient manifold that results from\\nquotienting out the transformer symmetries, thereby removing their ambiguities.\\nLeveraging tools from Riemannian geometry, we propose a fully general notion of\\nsharpness, in terms of a geodesic ball on the symmetry-corrected quotient\\nmanifold. In practice, we need to resort to approximating the geodesics. Doing\\nso up to first order yields existing adaptive sharpness measures, and we\\ndemonstrate that including higher-order terms is crucial to recover correlation\\nwith generalization. We present results on diagonal networks with synthetic\\ndata, and show that our geodesic sharpness reveals strong correlation for\\nreal-world transformers on both text and image classification tasks.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-08T16:51:03Z\"}"}

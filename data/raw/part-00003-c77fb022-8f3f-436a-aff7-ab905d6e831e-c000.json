{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17480v1\", \"title\": \"Unified Attacks to Large Language Model Watermarks: Spoofing and\\n  Scrubbing in Unauthorized Knowledge Distillation\", \"summary\": \"Watermarking has emerged as a critical technique for combating misinformation\\nand protecting intellectual property in large language models (LLMs). A recent\\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\\nteacher models can be inherited by student models through knowledge\\ndistillation. On the positive side, this inheritance allows for the detection\\nof unauthorized knowledge distillation by identifying watermark traces in\\nstudent models. However, the robustness of watermarks against scrubbing attacks\\nand their unforgeability in the face of spoofing attacks under unauthorized\\nknowledge distillation remain largely unexplored. Existing watermark attack\\nmethods either assume access to model internals or fail to simultaneously\\nsupport both scrubbing and spoofing attacks. In this work, we propose\\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\\nframework that enables bidirectional attacks under unauthorized knowledge\\ndistillation. Our approach employs contrastive decoding to extract corrupted or\\namplified watermark texts via comparing outputs from the student model and\\nweakly watermarked references, followed by bidirectional distillation to train\\nnew student models capable of watermark removal and watermark forgery,\\nrespectively. Extensive experiments show that CDG-KD effectively performs\\nattacks while preserving the general performance of the distilled model. Our\\nfindings underscore critical need for developing watermarking schemes that are\\nrobust and unforgeable.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-24T12:15:46Z\"}"}

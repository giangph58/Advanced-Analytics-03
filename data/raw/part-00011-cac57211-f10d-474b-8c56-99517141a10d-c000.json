{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19565v1\", \"title\": \"m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation\\n  Framework for Biomedical Large Language Models Training\", \"summary\": \"The rapid progress of large language models (LLMs) in biomedical research has\\nunderscored the limitations of existing open-source annotated scientific\\ncorpora, which are often insufficient in quantity and quality. Addressing the\\nchallenge posed by the complex hierarchy of biomedical knowledge, we propose a\\nknowledge-driven, multi-agent framework for scientific corpus distillation\\ntailored for LLM training in the biomedical domain. Central to our approach is\\na collaborative multi-agent architecture, where specialized agents, each guided\\nby the Medical Subject Headings (MeSH) hierarchy, work in concert to\\nautonomously extract, synthesize, and self-evaluate high-quality textual data\\nfrom vast scientific literature. These agents collectively generate and refine\\ndomain-specific question-answer pairs, ensuring comprehensive coverage and\\nconsistency with biomedical ontologies while minimizing manual involvement.\\nExtensive experimental results show that language models trained on our\\nmulti-agent distilled datasets achieve notable improvements in biomedical\\nquestion-answering tasks, outperforming both strong life sciences LLM baselines\\nand advanced proprietary models. Notably, our AI-Ready dataset enables\\nLlama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger\\nscale. Detailed ablation studies and case analyses further validate the\\neffectiveness and synergy of each agent within the framework, highlighting the\\npotential of multi-agent collaboration in biomedical LLM training.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,q-bio.QM\", \"published\": \"2025-04-28T08:18:24Z\"}"}

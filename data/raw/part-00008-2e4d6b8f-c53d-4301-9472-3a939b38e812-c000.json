{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21380v1\", \"title\": \"Sparse-to-Sparse Training of Diffusion Models\", \"summary\": \"Diffusion models (DMs) are a powerful type of generative models that have\\nachieved state-of-the-art results in various image synthesis tasks and have\\nshown potential in other domains, such as natural language processing and\\ntemporal data modeling. Despite their stable training dynamics and ability to\\nproduce diverse high-quality samples, DMs are notorious for requiring\\nsignificant computational resources, both in the training and inference stages.\\nPrevious work has focused mostly on increasing the efficiency of model\\ninference. This paper introduces, for the first time, the paradigm of\\nsparse-to-sparse training to DMs, with the aim of improving both training and\\ninference efficiency. We focus on unconditional generation and train sparse DMs\\nfrom scratch (Latent Diffusion and ChiroDiff) on six datasets using three\\ndifferent methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of\\nsparsity in model performance. Our experiments show that sparse DMs are able to\\nmatch and often outperform their Dense counterparts, while substantially\\nreducing the number of trainable parameters and FLOPs. We also identify safe\\nand effective values to perform sparse-to-sparse training of DMs.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CV\", \"published\": \"2025-04-30T07:28:11Z\"}"}

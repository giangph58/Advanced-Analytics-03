{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04410v1\", \"title\": \"DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception\", \"summary\": \"Dense visual prediction tasks have been constrained by their reliance on\\npredefined categories, limiting their applicability in real-world scenarios\\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\\nCLIP have shown promise in open-vocabulary tasks, their direct application to\\ndense prediction often leads to suboptimal performance due to limitations in\\nlocal feature representation. In this work, we present our observation that\\nCLIP's image tokens struggle to effectively aggregate information from\\nspatially or semantically related regions, resulting in features that lack\\nlocal discriminability and spatial consistency. To address this issue, we\\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\\nself-attention module to obtain ``content'' and ``context'' features\\nrespectively. The ``content'' features are aligned with image crop\\nrepresentations to improve local discriminability, while ``context'' features\\nlearn to retain the spatial correlations under the guidance of vision\\nfoundation models, such as DINO. Extensive experiments demonstrate that DeCLIP\\nsignificantly outperforms existing methods across multiple open-vocabulary\\ndense prediction tasks, including object detection and semantic segmentation.\\nCode is available at \\\\textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-07T13:46:34Z\"}"}

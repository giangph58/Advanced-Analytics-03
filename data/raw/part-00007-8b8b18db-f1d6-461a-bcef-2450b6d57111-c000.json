{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10081v1\", \"title\": \"RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning\\n  Capability\", \"summary\": \"Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have been\\nrapidly progressing and achieving breakthrough performance on complex reasoning\\ntasks such as mathematics and coding. However, the open-source R1 models have\\nraised safety concerns in wide applications, such as the tendency to comply\\nwith malicious queries, which greatly impacts the utility of these powerful\\nmodels in their applications. In this paper, we introduce RealSafe-R1 as\\nsafety-aligned versions of DeepSeek-R1 distilled models. To train these models,\\nwe construct a dataset of 15k safety-aware reasoning trajectories generated by\\nDeepSeek-R1, under explicit instructions for expected refusal behavior. Both\\nquantitative experiments and qualitative case studies demonstrate the models'\\nimprovements, which are shown in their safety guardrails against both harmful\\nqueries and jailbreak attacks. Importantly, unlike prior safety alignment\\nefforts that often compromise reasoning performance, our method preserves the\\nmodels' reasoning capabilities by maintaining the training data within the\\noriginal distribution of generation. Model weights of RealSafe-R1 are\\nopen-source at https://huggingface.co/RealSafe.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CL\", \"published\": \"2025-04-14T10:26:37Z\"}"}

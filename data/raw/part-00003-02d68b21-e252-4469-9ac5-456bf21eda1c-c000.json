{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04155v1\", \"title\": \"An Adaptive Mixed Precision and Dynamically Scaled Preconditioned\\n  Conjugate Gradient Algorithm\", \"summary\": \"We propose an adaptive mixed precision and dynamically scaled preconditioned\\nconjugate gradient algorithm (AMP-PCG). It dynamically adjusts the precision\\nfor storing vectors and computing, exploiting low precision when appropriate,\\nwhile maintaining a convergence rate and accuracy comparable to that of double\\nprecision PCG. Our mixed precision strategy consists of three main components:\\n(1) The residual and matrix-vector product are initially computed in double\\nprecision, and the algorithm switches these to single precision based on the\\nchosen convergence tolerance and an estimate of the residual gap. (2) Depending\\non the eigenvalue distribution, the preconditioned residual and search\\ndirection are either in half precision throughout the iterations or initially\\nin double precision and then stepwise reduced to single and half precision. (3)\\nA dynamically scaled residual is used at every iteration to mitigate underflow\\nin half precision. We provide theoretical support for our estimates and we\\ndemonstrate the effectiveness of AMP-PCG through numerical experiments,\\nhighlighting both its robustness and the significant performance gains (1.63x\\nspeedup) achieved compared to double precision PCG on a GPU.\", \"main_category\": \"math.NA\", \"categories\": \"math.NA,cs.NA\", \"published\": \"2025-05-07T06:11:49Z\"}"}

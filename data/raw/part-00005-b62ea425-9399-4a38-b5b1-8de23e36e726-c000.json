{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05413v1\", \"title\": \"DPQ-HD: Post-Training Compression for Ultra-Low Power Hyperdimensional\\n  Computing\", \"summary\": \"Hyperdimensional Computing (HDC) is emerging as a promising approach for edge\\nAI, offering a balance between accuracy and efficiency. However, current\\nHDC-based applications often rely on high-precision models and/or encoding\\nmatrices to achieve competitive performance, which imposes significant\\ncomputational and memory demands, especially for ultra-low power devices. While\\nrecent efforts use techniques like precision reduction and pruning to increase\\nthe efficiency, most require retraining to maintain performance, making them\\nexpensive and impractical. To address this issue, we propose a novel Post\\nTraining Compression algorithm, Decomposition-Pruning-Quantization (DPQ-HD),\\nwhich aims at compressing the end-to-end HDC system, achieving near floating\\npoint performance without the need of retraining. DPQ-HD reduces computational\\nand memory overhead by uniquely combining the above three compression\\ntechniques and efficiently adapts to hardware constraints. Additionally, we\\nintroduce an energy-efficient inference approach that progressively evaluates\\nsimilarity scores such as cosine similarity and performs early exit to reduce\\nthe computation, accelerating prediction inference while maintaining accuracy.\\nWe demonstrate that DPQ-HD achieves up to 20-100x reduction in memory for image\\nand graph classification tasks with only a 1-2% drop in accuracy compared to\\nuncompressed workloads. Lastly, we show that DPQ-HD outperforms the existing\\npost-training compression methods and performs better or at par with\\nretraining-based state-of-the-art techniques, requiring significantly less\\noverall optimization time (up to 100x) and faster inference (up to 56x) on a\\nmicrocontroller\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-08T16:54:48Z\"}"}

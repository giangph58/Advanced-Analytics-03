{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06593v1\", \"title\": \"A Multi-Modal Interaction Framework for Efficient Human-Robot\\n  Collaborative Shelf Picking\", \"summary\": \"The growing presence of service robots in human-centric environments, such as\\nwarehouses, demands seamless and intuitive human-robot collaboration. In this\\npaper, we propose a collaborative shelf-picking framework that combines\\nmultimodal interaction, physics-based reasoning, and task division for enhanced\\nhuman-robot teamwork.\\n  The framework enables the robot to recognize human pointing gestures,\\ninterpret verbal cues and voice commands, and communicate through visual and\\nauditory feedback. Moreover, it is powered by a Large Language Model (LLM)\\nwhich utilizes Chain of Thought (CoT) and a physics-based simulation engine for\\nsafely retrieving cluttered stacks of boxes on shelves, relationship graph for\\nsub-task generation, extraction sequence planning and decision making.\\nFurthermore, we validate the framework through real-world shelf picking\\nexperiments such as 1) Gesture-Guided Box Extraction, 2) Collaborative Shelf\\nClearing and 3) Collaborative Stability Assistance.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.HC\", \"published\": \"2025-04-09T05:42:33Z\"}"}

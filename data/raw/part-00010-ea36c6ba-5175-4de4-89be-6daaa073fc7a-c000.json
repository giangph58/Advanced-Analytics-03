{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05122v1\", \"title\": \"Text2Cypher: Data Pruning using Hard Example Selection\", \"summary\": \"Database query languages such as SQL for relational databases and Cypher for\\ngraph databases have been widely adopted. Recent advancements in large language\\nmodels (LLMs) enable natural language interactions with databases through\\nmodels like Text2SQL and Text2Cypher. Fine-tuning these models typically\\nrequires large, diverse datasets containing non-trivial examples. However, as\\ndataset size increases, the cost of fine-tuning also rises. This makes smaller,\\nhigh-quality datasets essential for reducing costs for the same or better\\nperformance. In this paper, we propose five hard-example selection techniques\\nfor pruning the Text2Cypher dataset, aiming to preserve or improve performance\\nwhile reducing resource usage. Our results show that these hard-example\\nselection approaches can halve training time and costs with minimal impact on\\nperformance, and demonstrates that hard-example selection provides a\\ncost-effective solution.\", \"main_category\": \"cs.DB\", \"categories\": \"cs.DB,cs.LG\", \"published\": \"2025-05-08T10:51:13Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03194v1\", \"title\": \"Convergence Of Consistency Model With Multistep Sampling Under General\\n  Data Assumptions\", \"summary\": \"Diffusion models accomplish remarkable success in data generation tasks\\nacross various domains. However, the iterative sampling process is\\ncomputationally expensive. Consistency models are proposed to learn consistency\\nfunctions to map from noise to data directly, which allows one-step fast data\\ngeneration and multistep sampling to improve sample quality. In this paper, we\\nstudy the convergence of consistency models when the self-consistency property\\nholds approximately under the training distribution. Our analysis requires only\\nmild data assumption and applies to a family of forward processes. When the\\ntarget data distribution has bounded support or has tails that decay\\nsufficiently fast, we show that the samples generated by the consistency model\\nare close to the target distribution in Wasserstein distance; when the target\\ndistribution satisfies some smoothness assumption, we show that with an\\nadditional perturbation step for smoothing, the generated samples are close to\\nthe target distribution in total variation distance. We provide two case\\nstudies with commonly chosen forward processes to demonstrate the benefit of\\nmultistep sampling.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-06T05:31:10Z\"}"}

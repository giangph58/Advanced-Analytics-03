{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17528v1\", \"title\": \"TACO: Tackling Over-correction in Federated Learning with Tailored\\n  Adaptive Correction\", \"summary\": \"Non-independent and identically distributed (Non-IID) data across edge\\nclients have long posed significant challenges to federated learning (FL)\\ntraining in edge computing environments. Prior works have proposed various\\nmethods to mitigate this statistical heterogeneity. While these works can\\nachieve good theoretical performance, in this work we provide the first\\ninvestigation into a hidden over-correction phenomenon brought by the uniform\\nmodel correction coefficients across clients adopted by existing methods. Such\\nover-correction could degrade model performance and even cause failures in\\nmodel convergence. To address this, we propose TACO, a novel algorithm that\\naddresses the non-IID nature of clients' data by implementing fine-grained,\\nclient-specific gradient correction and model aggregation, steering local\\nmodels towards a more accurate global optimum. Moreover, we verify that leading\\nFL algorithms generally have better model accuracy in terms of communication\\nrounds rather than wall-clock time, resulting from their extra computation\\noverhead imposed on clients. To enhance the training efficiency, TACO deploys a\\nlightweight model correction and tailored aggregation approach that requires\\nminimum computation overhead and no extra information beyond the synchronized\\nmodel parameters. To validate TACO's effectiveness, we present the first FL\\nconvergence analysis that reveals the root cause of over-correction. Extensive\\nexperiments across various datasets confirm TACO's superior and stable\\nperformance in practice.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,I.2.6\", \"published\": \"2025-04-24T13:16:21Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14960v1\", \"title\": \"MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient\\n  Large-Scale MoE Model Training with Megatron Core\", \"summary\": \"Mixture of Experts (MoE) models enhance neural network scalability by\\ndynamically selecting relevant experts per input token, enabling larger model\\nsizes while maintaining manageable computation costs. However, efficient\\ntraining of large-scale MoE models across thousands of GPUs presents\\nsignificant challenges due to limitations in existing parallelism strategies.\\nWe introduce an end-to-end training framework for large-scale MoE models that\\nutilizes five-dimensional hybrid parallelism: Tensor Parallelism, Expert\\nParallelism, Context Parallelism, Data Parallelism, and Pipeline Parallelism.\\nCentral to our approach is MoE Parallel Folding, a novel strategy that\\ndecouples the parallelization of attention and MoE layers in Transformer\\nmodels, allowing each layer type to adopt optimal parallel configurations.\\nAdditionally, we develop a flexible token-level dispatcher that supports both\\ntoken-dropping and token-dropless MoE training across all five dimensions of\\nparallelism. This dispatcher accommodates dynamic tensor shapes and coordinates\\ndifferent parallelism schemes for Attention and MoE layers, facilitating\\ncomplex parallelism implementations. Our experiments demonstrate significant\\nimprovements in training efficiency and scalability. We achieve up to 49.3%\\nModel Flops Utilization (MFU) for the Mixtral 8x22B model and 39.0% MFU for the\\nQwen2-57B-A14B model on H100 GPUs, outperforming existing methods. The\\nframework scales efficiently up to 1,024 GPUs and maintains high performance\\nwith sequence lengths up to 128K tokens, validating its effectiveness for\\nlarge-scale MoE model training. The code is available in Megatron-Core.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.DC\", \"published\": \"2025-04-21T08:39:47Z\"}"}

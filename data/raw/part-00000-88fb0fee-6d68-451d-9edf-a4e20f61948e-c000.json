{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23924v1\", \"title\": \"Model Hemorrhage and the Robustness Limits of Large Language Models\", \"summary\": \"Large language models (LLMs) demonstrate strong performance across natural\\nlanguage processing tasks, yet undergo significant performance degradation when\\nmodified for deployment through quantization, pruning, or decoding strategy\\nadjustments. We define this phenomenon as model hemorrhage - performance\\ndecline caused by parameter alterations and architectural changes. Through\\nsystematic analysis of various LLM frameworks, we identify key vulnerability\\npatterns: layer expansion frequently disrupts attention mechanisms, compression\\ntechniques induce information loss cascades, and decoding adjustments amplify\\nprediction divergences. Our investigation reveals transformer architectures\\nexhibit inherent robustness thresholds that determine hemorrhage severity\\nacross modification types. We propose three mitigation strategies:\\ngradient-aware pruning preserves critical weight pathways, dynamic quantization\\nscaling maintains activation integrity, and decoding calibration aligns\\ngeneration trajectories with original model distributions. This work\\nestablishes foundational metrics for evaluating model stability during\\nadaptation, providing practical guidelines for maintaining performance while\\nenabling efficient LLM deployment. Our findings advance understanding of neural\\nnetwork resilience under architectural transformations, particularly for\\nlarge-scale language models.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-03-31T10:16:03Z\"}"}

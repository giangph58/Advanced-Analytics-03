{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03254v1\", \"title\": \"PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for\\n  Efficient CNNs\", \"summary\": \"Convolutional neural networks (CNNs) are crucial for computer vision tasks on\\nresource-constrained devices. Quantization effectively compresses these models,\\nreducing storage size and energy cost. However, in modern depthwise-separable\\narchitectures, the computational cost is distributed unevenly across its\\ncomponents, with pointwise operations being the most expensive. By applying a\\ngeneral quantization scheme to this imbalanced cost distribution, existing\\nquantization approaches fail to fully exploit potential efficiency gains. To\\nthis end, we introduce PROM, a straightforward approach for quantizing modern\\ndepthwise-separable convolutional networks by selectively using two distinct\\nbit-widths. Specifically, pointwise convolutions are quantized to ternary\\nweights, while the remaining modules use 8-bit weights, which is achieved\\nthrough a simple quantization-aware training procedure. Additionally, by\\nquantizing activations to 8-bit, our method transforms pointwise convolutions\\nwith ternary weights into int8 additions, which enjoy broad support across\\nhardware platforms and effectively eliminates the need for expensive\\nmultiplications. Applying PROM to MobileNetV2 reduces the model's energy cost\\nby more than an order of magnitude (23.9x) and its storage size by 2.7x\\ncompared to the float16 baseline while retaining similar classification\\nperformance on ImageNet. Our method advances the Pareto frontier for energy\\nconsumption vs. top-1 accuracy for quantized convolutional models on ImageNet.\\nPROM addresses the challenges of quantizing depthwise-separable convolutional\\nnetworks to both ternary and 8-bit weights, offering a simple way to reduce\\nenergy cost and storage size.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG\", \"published\": \"2025-05-06T07:32:24Z\"}"}

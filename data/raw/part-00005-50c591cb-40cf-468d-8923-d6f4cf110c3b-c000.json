{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16921v1\", \"title\": \"IberBench: LLM Evaluation on Iberian Languages\", \"summary\": \"Large Language Models (LLMs) remain difficult to evaluate comprehensively,\\nparticularly for languages other than English, where high-quality data is often\\nlimited. Existing benchmarks and leaderboards are predominantly\\nEnglish-centric, with only a few addressing other languages. These benchmarks\\nfall short in several key areas: they overlook the diversity of language\\nvarieties, prioritize fundamental Natural Language Processing (NLP)\\ncapabilities over tasks of industrial relevance, and are static. With these\\naspects in mind, we present IberBench, a comprehensive and extensible benchmark\\ndesigned to assess LLM performance on both fundamental and industry-relevant\\nNLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.\\nIberBench integrates 101 datasets from evaluation campaigns and recent\\nbenchmarks, covering 22 task categories such as sentiment and emotion analysis,\\ntoxicity detection, and summarization. The benchmark addresses key limitations\\nin current evaluation practices, such as the lack of linguistic diversity and\\nstatic evaluation setups by enabling continual updates and community-driven\\nmodel and dataset submissions moderated by a committee of experts. We evaluate\\n23 LLMs ranging from 100 million to 14 billion parameters and provide empirical\\ninsights into their strengths and limitations. Our findings indicate that (i)\\nLLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)\\nperformance is on average lower for Galician and Basque, (iii) some tasks show\\nresults close to random, and (iv) in other tasks LLMs perform above random but\\nbelow shared task systems. IberBench offers open-source implementations for the\\nentire evaluation pipeline, including dataset normalization and hosting,\\nincremental evaluation of LLMs, and a publicly accessible leaderboard.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-23T17:48:25Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16054v1\", \"title\": \"$\\u03c0_{0.5}$: a Vision-Language-Action Model with Open-World\\n  Generalization\", \"summary\": \"In order for robots to be useful, they must perform practically relevant\\ntasks in the real world, outside of the lab. While vision-language-action (VLA)\\nmodels have demonstrated impressive results for end-to-end robot control, it\\nremains an open question how far such models can generalize in the wild. We\\ndescribe $\\\\pi_{0.5}$, a new model based on $\\\\pi_{0}$ that uses co-training on\\nheterogeneous tasks to enable broad generalization. $\\\\pi_{0.5}$\\\\ uses data from\\nmultiple robots, high-level semantic prediction, web data, and other sources to\\nenable broadly generalizable real-world robotic manipulation. Our system uses a\\ncombination of co-training and hybrid multi-modal examples that combine image\\nobservations, language commands, object detections, semantic subtask\\nprediction, and low-level actions. Our experiments show that this kind of\\nknowledge transfer is essential for effective generalization, and we\\ndemonstrate for the first time that an end-to-end learning-enabled robotic\\nsystem can perform long-horizon and dexterous manipulation skills, such as\\ncleaning a kitchen or bedroom, in entirely new homes.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.RO\", \"published\": \"2025-04-22T17:31:29Z\"}"}

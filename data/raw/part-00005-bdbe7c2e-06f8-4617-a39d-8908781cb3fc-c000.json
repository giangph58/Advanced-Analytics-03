{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10329v1\", \"title\": \"InstructEngine: Instruction-driven Text-to-Image Alignment\", \"summary\": \"Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF) has been\\nextensively utilized for preference alignment of text-to-image models. Existing\\nmethods face certain limitations in terms of both data and algorithm. For\\ntraining data, most approaches rely on manual annotated preference data, either\\nby directly fine-tuning the generators or by training reward models to provide\\ntraining signals. However, the high annotation cost makes them difficult to\\nscale up, the reward model consumes extra computation and cannot guarantee\\naccuracy. From an algorithmic perspective, most methods neglect the value of\\ntext and only take the image feedback as a comparative signal, which is\\ninefficient and sparse. To alleviate these drawbacks, we propose the\\nInstructEngine framework. Regarding annotation cost, we first construct a\\ntaxonomy for text-to-image generation, then develop an automated data\\nconstruction pipeline based on it. Leveraging advanced large multimodal models\\nand human-defined rules, we generate 25K text-image preference pairs. Finally,\\nwe introduce cross-validation alignment method, which refines data efficiency\\nby organizing semantically analogous samples into mutually comparable pairs.\\nEvaluations on DrawBench demonstrate that InstructEngine improves SD v1.5 and\\nSDXL's performance by 10.53% and 5.30%, outperforming state-of-the-art\\nbaselines, with ablation study confirming the benefits of InstructEngine's all\\ncomponents. A win rate of over 50% in human reviews also proves that\\nInstructEngine better aligns with human preferences.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-14T15:36:28Z\"}"}

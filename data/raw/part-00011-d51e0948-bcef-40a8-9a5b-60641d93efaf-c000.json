{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06962v1\", \"title\": \"Efficient Self-Supervised Learning for Earth Observation via Dynamic\\n  Dataset Curation\", \"summary\": \"Self-supervised learning (SSL) has enabled the development of vision\\nfoundation models for Earth Observation (EO), demonstrating strong\\ntransferability across diverse remote sensing tasks. While prior work has\\nfocused on network architectures and training strategies, the role of dataset\\ncuration, especially in balancing and diversifying pre-training datasets,\\nremains underexplored. In EO, this challenge is amplified by the redundancy and\\nheavy-tailed distributions common in satellite imagery, which can lead to\\nbiased representations and inefficient training.\\n  In this work, we propose a dynamic dataset pruning strategy designed to\\nimprove SSL pre-training by maximizing dataset diversity and balance. Our\\nmethod iteratively refines the training set without requiring a pre-existing\\nfeature extractor, making it well-suited for domains where curated datasets are\\nlimited or unavailable. We demonstrate our approach on the Sentinel-1 Wave Mode\\n(WV) Synthetic Aperture Radar (SAR) archive, a challenging dataset dominated by\\nocean observations. We train models from scratch on the entire Sentinel-1 WV\\narchive spanning 10 years. Across three downstream tasks, our results show that\\ndynamic pruning improves both computational efficiency and representation\\nquality, leading to stronger transferability.\\n  We also release the weights of Nereus-SAR-1, the first model in the Nereus\\nfamily, a series of foundation models for ocean observation and analysis using\\nSAR imagery, at github.com/galeio-research/nereus-sar-models/.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-09T15:13:26Z\"}"}

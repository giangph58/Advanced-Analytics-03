{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05341v1\", \"title\": \"Robust Online Learning with Private Information\", \"summary\": \"This paper investigates the robustness of online learning algorithms when\\nlearners possess private information. No-external-regret algorithms, prevalent\\nin machine learning, are vulnerable to strategic manipulation, allowing an\\nadaptive opponent to extract full surplus. Even standard\\nno-weak-external-regret algorithms, designed for optimal learning in stationary\\nenvironments, exhibit similar vulnerabilities. This raises a fundamental\\nquestion: can a learner simultaneously prevent full surplus extraction by\\nadaptive opponents while maintaining optimal performance in well-behaved\\nenvironments? To address this, we model the problem as a two-player repeated\\ngame, where the learner with private information plays against the environment,\\nfacing ambiguity about the environment's types: stationary or adaptive. We\\nintroduce \\\\emph{partial safety} as a key design criterion for online learning\\nalgorithms to prevent full surplus extraction. We then propose the\\n\\\\emph{Explore-Exploit-Punish} (\\\\textsf{EEP}) algorithm and prove that it\\nsatisfies partial safety while achieving optimal learning in stationary\\nenvironments, and has a variant that delivers improved welfare performance. Our\\nfindings highlight the risks of applying standard online learning algorithms in\\nstrategic settings with adverse selection. We advocate for a shift toward\\nonline learning algorithms that explicitly incorporate safeguards against\\nstrategic manipulation while ensuring strong learning performance.\", \"main_category\": \"econ.TH\", \"categories\": \"econ.TH,cs.GT\", \"published\": \"2025-05-08T15:29:06Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20860v1\", \"title\": \"FedMVP: Federated Multi-modal Visual Prompt Tuning for Vision-Language\\n  Models\", \"summary\": \"Textual prompt tuning adapts Vision-Language Models (e.g., CLIP) in federated\\nlearning by tuning lightweight input tokens (or prompts) on local client data,\\nwhile keeping network weights frozen. Post training, only the prompts are\\nshared by the clients with the central server for aggregation. However, textual\\nprompt tuning often struggles with overfitting to known concepts and may be\\noverly reliant on memorized text features, limiting its adaptability to unseen\\nconcepts. To address this limitation, we propose Federated Multimodal Visual\\nPrompt Tuning (FedMVP) that conditions the prompts on comprehensive contextual\\ninformation -- image-conditioned features and textual attribute features of a\\nclass -- that is multimodal in nature. At the core of FedMVP is a PromptFormer\\nmodule that synergistically aligns textual and visual features through\\ncross-attention, enabling richer contexual integration. The dynamically\\ngenerated multimodal visual prompts are then input to the frozen vision encoder\\nof CLIP, and trained with a combination of CLIP similarity loss and a\\nconsistency loss. Extensive evaluation on 20 datasets spanning three\\ngeneralization settings demonstrates that FedMVP not only preserves performance\\non in-distribution classes and domains, but also displays higher\\ngeneralizability to unseen classes and domains when compared to\\nstate-of-the-art methods. Codes will be released upon acceptance.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-29T15:36:51Z\"}"}

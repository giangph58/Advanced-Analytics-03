{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03603v1\", \"title\": \"PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model\", \"summary\": \"Audio-driven human animation technology is widely used in human-computer\\ninteraction, and the emergence of diffusion models has further advanced its\\ndevelopment. Currently, most methods rely on multi-stage generation and\\nintermediate representations, resulting in long inference time and issues with\\ngeneration quality in specific foreground regions and audio-motion consistency.\\nThese shortcomings are primarily due to the lack of localized fine-grained\\nsupervised guidance. To address above challenges, we propose PAHA, an\\nend-to-end audio-driven upper-body human animation framework with diffusion\\nmodel. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts\\nConsistency Enhancement (PCE). PAR dynamically adjusts regional training loss\\nweights based on pose confidence scores, effectively improving visual quality.\\nPCE constructs and trains diffusion-based regional audio-visual classifiers to\\nimprove the consistency of motion and co-speech audio. Afterwards, we design\\ntwo novel inference guidance methods for the foregoing classifiers, Sequential\\nGuidance (SG) and Differential Guidance (DG), to balance efficiency and quality\\nrespectively. Additionally, we build CNAS, the first public Chinese News Anchor\\nSpeech dataset, to advance research and validation in this field. Extensive\\nexperimental results and user studies demonstrate that PAHA significantly\\noutperforms existing methods in audio-motion alignment and video-related\\nevaluations. The codes and CNAS dataset will be released upon acceptance.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.MM\", \"published\": \"2025-05-06T15:03:58Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11429v1\", \"title\": \"Improving Statistical Privacy by Subsampling\", \"summary\": \"Differential privacy (DP) considers a scenario, where an adversary has almost\\ncomplete information about the entries of a database This worst-case assumption\\nis likely to overestimate the privacy thread for an individual in real life.\\nStatistical privacy (SP) denotes a setting where only the distribution of the\\ndatabase entries is known to an adversary, but not their exact values. In this\\ncase one has to analyze the interaction between noiseless privacy based on the\\nentropy of distributions and privacy mechanisms that distort the answers of\\nqueries, which can be quite complex.\\n  A privacy mechanism often used is to take samples of the data for answering a\\nquery. This paper proves precise bounds how much different methods of sampling\\nincrease privacy in the statistical setting with respect to database size and\\nsampling rate. They allow us to deduce when and how much sampling provides an\\nimprovement and how far this depends on the privacy parameter {\\\\epsilon}. To\\nperform these investigations we develop a framework to model sampling\\ntechniques.\\n  For the DP setting tradeoff functions have been proposed as a finer measure\\nfor privacy compared to ({\\\\epsilon},{\\\\delta})-pairs. We apply these tools to\\nstatistical privacy with subsampling to get a comparable characterization\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR\", \"published\": \"2025-04-15T17:40:45Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15956v1\", \"title\": \"Universal Approximation with Softmax Attention\", \"summary\": \"We prove that with linear transformations, both (i) two-layer self-attention\\nand (ii) one-layer self-attention followed by a softmax function are universal\\napproximators for continuous sequence-to-sequence functions on compact domains.\\nOur main technique is a new interpolation-based method for analyzing\\nattention's internal mechanism. This leads to our key insight: self-attention\\nis able to approximate a generalized version of ReLU to arbitrary precision,\\nand hence subsumes many known universal approximators. Building on these, we\\nshow that two-layer multi-head attention alone suffices as a\\nsequence-to-sequence universal approximator. In contrast, prior works rely on\\nfeed-forward networks to establish universal approximation in Transformers.\\nFurthermore, we extend our techniques to show that, (softmax-)attention-only\\nlayers are capable of approximating various statistical models in-context. We\\nbelieve these techniques hold independent interest.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,stat.ML\", \"published\": \"2025-04-22T14:51:33Z\"}"}

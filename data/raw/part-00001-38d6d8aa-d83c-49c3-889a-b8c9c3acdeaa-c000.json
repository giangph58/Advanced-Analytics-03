{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11426v1\", \"title\": \"A Dual-Space Framework for General Knowledge Distillation of Large\\n  Language Models\", \"summary\": \"Knowledge distillation (KD) is a promising solution to compress large\\nlanguage models (LLMs) by transferring their knowledge to smaller models.\\nDuring this process, white-box KD methods usually minimize the distance between\\nthe output distributions of the teacher model and the student model to transfer\\nmore information. However, we reveal that the current white-box KD framework\\nexhibits two limitations: a) bridging probability distributions from different\\noutput spaces will limit the similarity between the teacher model and the\\nstudent model; b) this framework cannot be applied to LLMs with different\\nvocabularies. One of the root causes for these limitations is that the\\ndistributions from the teacher and the student for KD are output by different\\nprediction heads, which yield distributions in different output spaces and\\ndimensions. Therefore, in this paper, we propose a dual-space knowledge\\ndistillation (DSKD) framework that unifies the prediction heads of the teacher\\nand the student models for KD. Specifically, we first introduce two projectors\\nwith ideal initialization to project the teacher/student hidden states into the\\nstudent/teacher representation spaces. After this, the hidden states from\\ndifferent models can share the same head and unify the output spaces of the\\ndistributions. Furthermore, we develop an exact token alignment (ETA) algorithm\\nto align the same tokens in two differently-tokenized sequences. Based on the\\nabove, our DSKD framework is a general KD framework that supports both\\noff-policy and on-policy KD, and KD between any two LLMs regardless of their\\nvocabularies. Extensive experiments on instruction-following, mathematical\\nreasoning, and code generation benchmarks show that DSKD significantly\\noutperforms existing methods based on the current white-box KD framework and\\nsurpasses other cross-tokenizer KD methods for LLMs with different\\nvocabularies.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-15T17:38:47Z\"}"}

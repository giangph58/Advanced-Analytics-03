{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16379v1\", \"title\": \"SplitReason: Learning To Offload Reasoning\", \"summary\": \"Reasoning in large language models (LLMs) tends to produce substantially\\nlonger token generation sequences than simpler language modeling tasks. This\\nextended generation length reflects the multi-step, compositional nature of\\nreasoning and is often correlated with higher solution accuracy. From an\\nefficiency perspective, longer token generation exacerbates the inherently\\nsequential and memory-bound decoding phase of LLMs. However, not all parts of\\nthis expensive reasoning process are equally difficult to generate. We leverage\\nthis observation by offloading only the most challenging parts of the reasoning\\nprocess to a larger, more capable model, while performing most of the\\ngeneration with a smaller, more efficient model; furthermore, we teach the\\nsmaller model to identify these difficult segments and independently trigger\\noffloading when needed. To enable this behavior, we annotate difficult segments\\nacross 18k reasoning traces from the OpenR1-Math-220k chain-of-thought (CoT)\\ndataset. We then apply supervised fine-tuning (SFT) and reinforcement learning\\nfine-tuning (RLFT) to a 1.5B-parameter reasoning model, training it to learn to\\noffload the most challenging parts of its own reasoning process to a larger\\nmodel. This approach improves AIME24 reasoning accuracy by 24% and 28.3% while\\noffloading 1.35% and 5% of the generated tokens respectively. We open-source\\nour SplitReason model, data, code and logs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-23T03:00:02Z\"}"}

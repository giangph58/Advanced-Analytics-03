{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05445v1\", \"title\": \"clem:todd: A Framework for the Systematic Benchmarking of LLM-Based\\n  Task-Oriented Dialogue System Realisations\", \"summary\": \"The emergence of instruction-tuned large language models (LLMs) has advanced\\nthe field of dialogue systems, enabling both realistic user simulations and\\nrobust multi-turn conversational agents. However, existing research often\\nevaluates these components in isolation-either focusing on a single user\\nsimulator or a specific system design-limiting the generalisability of insights\\nacross architectures and configurations. In this work, we propose clem todd\\n(chat-optimized LLMs for task-oriented dialogue systems development), a\\nflexible framework for systematically evaluating dialogue systems under\\nconsistent conditions. clem todd enables detailed benchmarking across\\ncombinations of user simulators and dialogue systems, whether existing models\\nfrom literature or newly developed ones. It supports plug-and-play integration\\nand ensures uniform datasets, evaluation metrics, and computational\\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\\ntask-oriented dialogue systems within this unified setup and integrating three\\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\\nprovide actionable insights into how architecture, scale, and prompting\\nstrategies affect dialogue performance, offering practical guidance for\\nbuilding efficient and effective conversational AI systems.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-05-08T17:36:36Z\"}"}

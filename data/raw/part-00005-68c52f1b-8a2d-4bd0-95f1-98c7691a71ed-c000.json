{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20800v1\", \"title\": \"Adept: Annotation-Denoising Auxiliary Tasks with Discrete Cosine\\n  Transform Map and Keypoint for Human-Centric Pretraining\", \"summary\": \"Human-centric perception is the core of diverse computer vision tasks and has\\nbeen a long-standing research focus. However, previous research studied these\\nhuman-centric tasks individually, whose performance is largely limited to the\\nsize of the public task-specific datasets. Recent human-centric methods\\nleverage the additional modalities, e.g., depth, to learn fine-grained semantic\\ninformation, which limits the benefit of pretraining models due to their\\nsensitivity to camera views and the scarcity of RGB-D data on the Internet.\\nThis paper improves the data scalability of human-centric pretraining methods\\nby discarding depth information and exploring semantic information of RGB\\nimages in the frequency space by Discrete Cosine Transform (DCT). We further\\npropose new annotation denoising auxiliary tasks with keypoints and DCT maps to\\nenforce the RGB image extractor to learn fine-grained semantic information of\\nhuman bodies. Our extensive experiments show that when pretrained on\\nlarge-scale datasets (COCO and AIC datasets) without depth annotation, our\\nmodel achieves better performance than state-of-the-art methods by +0.5 mAP on\\nCOCO, +1.4 PCKh on MPII and -0.51 EPE on Human3.6M for pose estimation, by\\n+4.50 mIoU on Human3.6M for human parsing, by -3.14 MAE on SHA and -0.07 MAE on\\nSHB for crowd counting, by +1.1 F1 score on SHA and +0.8 F1 score on SHA for\\ncrowd localization, and by +0.1 mAP on Market1501 and +0.8 mAP on MSMT for\\nperson ReID. We also validate the effectiveness of our method on MPII+NTURGBD\\ndatasets\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-29T14:14:29Z\"}"}

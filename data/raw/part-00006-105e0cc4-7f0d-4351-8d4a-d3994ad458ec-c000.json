{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05475v1\", \"title\": \"SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with\\n  Video Diffusion and Data Augmentation\", \"summary\": \"Creating high-quality animatable 3D human avatars from a single image remains\\na significant challenge in computer vision due to the inherent difficulty of\\nreconstructing complete 3D information from a single viewpoint. Current\\napproaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods\\nproduce high-quality results but require multiple views or video sequences,\\nwhile video diffusion models can generate animations from single images but\\nstruggle with consistency and identity preservation. We present SVAD, a novel\\napproach that addresses these limitations by leveraging complementary strengths\\nof existing techniques. Our method generates synthetic training data through\\nvideo diffusion, enhances it with identity preservation and image restoration\\nmodules, and utilizes this refined data to train 3DGS avatars. Comprehensive\\nevaluations demonstrate that SVAD outperforms state-of-the-art (SOTA)\\nsingle-image methods in maintaining identity consistency and fine details\\nacross novel poses and viewpoints, while enabling real-time rendering\\ncapabilities. Through our data augmentation pipeline, we overcome the\\ndependency on dense monocular or multi-view training data typically required by\\ntraditional 3DGS approaches. Extensive quantitative, qualitative comparisons\\nshow our method achieves superior performance across multiple metrics against\\nbaseline models. By effectively combining the generative power of diffusion\\nmodels with both the high-quality results and rendering efficiency of 3DGS, our\\nwork establishes a new approach for high-fidelity avatar generation from a\\nsingle image input.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-08T17:59:58Z\"}"}

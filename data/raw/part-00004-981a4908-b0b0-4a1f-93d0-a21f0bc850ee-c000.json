{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11454v1\", \"title\": \"Elucidating the Design Space of Multimodal Protein Language Models\", \"summary\": \"Multimodal protein language models (PLMs) integrate sequence and token-based\\nstructural information, serving as a powerful foundation for protein modeling,\\ngeneration, and design. However, the reliance on tokenizing 3D structures into\\ndiscrete tokens causes substantial loss of fidelity about fine-grained\\nstructural details and correlations. In this paper, we systematically elucidate\\nthe design space of multimodal PLMs to overcome their limitations. We identify\\ntokenization loss and inaccurate structure token predictions by the PLMs as\\nmajor bottlenecks. To address these, our proposed design space covers improved\\ngenerative modeling, structure-aware architectures and representation learning,\\nand data exploration. Our advancements approach finer-grained supervision,\\ndemonstrating that token-based multimodal PLMs can achieve robust structural\\nmodeling. The effective design methods dramatically improve the structure\\ngeneration diversity, and notably, folding abilities of our 650M model by\\nreducing the RMSD from 5.52 to 2.36 on PDB testset, even outperforming 3B\\nbaselines and on par with the specialized folding models.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,q-bio.QM\", \"published\": \"2025-04-15T17:59:43Z\"}"}

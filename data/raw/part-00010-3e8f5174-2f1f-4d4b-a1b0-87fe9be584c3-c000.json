{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07866v1\", \"title\": \"Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend\\n  NPUs\", \"summary\": \"We present Pangu Ultra, a Large Language Model (LLM) with 135 billion\\nparameters and dense Transformer modules trained on Ascend Neural Processing\\nUnits (NPUs). Although the field of LLM has been witnessing unprecedented\\nadvances in pushing the scale and capability of LLM in recent years, training\\nsuch a large-scale model still involves significant optimization and system\\nchallenges. To stabilize the training process, we propose depth-scaled sandwich\\nnormalization, which effectively eliminates loss spikes during the training\\nprocess of deep models. We pre-train our model on 13.2 trillion diverse and\\nhigh-quality tokens and further enhance its reasoning capabilities during\\npost-training. To perform such large-scale training efficiently, we utilize\\n8,192 Ascend NPUs with a series of system optimizations. Evaluations on\\nmultiple diverse benchmarks indicate that Pangu Ultra significantly advances\\nthe state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral\\nLarge 2, and even achieves competitive results with DeepSeek-R1, whose sparse\\nmodel structure contains much more parameters. Our exploration demonstrates\\nthat Ascend NPUs are capable of efficiently and effectively training dense\\nmodels with more than 100 billion parameters. Our model and system will be\\navailable for our commercial customers.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-10T15:41:51Z\"}"}

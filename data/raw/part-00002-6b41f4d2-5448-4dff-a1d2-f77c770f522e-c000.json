{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01399v1\", \"title\": \"Leveraging Generalizability of Image-to-Image Translation for Enhanced\\n  Adversarial Defense\", \"summary\": \"In the rapidly evolving field of artificial intelligence, machine learning\\nemerges as a key technology characterized by its vast potential and inherent\\nrisks. The stability and reliability of these models are important, as they are\\nfrequent targets of security threats. Adversarial attacks, first rigorously\\ndefined by Ian Goodfellow et al. in 2013, highlight a critical vulnerability:\\nthey can trick machine learning models into making incorrect predictions by\\napplying nearly invisible perturbations to images. Although many studies have\\nfocused on constructing sophisticated defensive mechanisms to mitigate such\\nattacks, they often overlook the substantial time and computational costs of\\ntraining and maintaining these models. Ideally, a defense method should be able\\nto generalize across various, even unseen, adversarial attacks with minimal\\noverhead. Building on our previous work on image-to-image translation-based\\ndefenses, this study introduces an improved model that incorporates residual\\nblocks to enhance generalizability. The proposed method requires training only\\na single model, effectively defends against diverse attack types, and is\\nwell-transferable between different target models. Experiments show that our\\nmodel can restore the classification accuracy from near zero to an average of\\n72\\\\% while maintaining competitive performance compared to state-of-the-art\\nmethods.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG\", \"published\": \"2025-04-02T06:38:28Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15724v1\", \"title\": \"Collaborative Split Federated Learning with Parallel Training and\\n  Aggregation\", \"summary\": \"Federated learning (FL) operates based on model exchanges between the server\\nand the clients, and it suffers from significant client-side computation and\\ncommunication burden. Split federated learning (SFL) arises a promising\\nsolution by splitting the model into two parts, that are trained sequentially:\\nthe clients train the first part of the model (client-side model) and transmit\\nit to the server that trains the second (server-side model). Existing SFL\\nschemes though still exhibit long training delays and significant communication\\noverhead, especially when clients of different computing capability\\nparticipate. Thus, we propose Collaborative-Split Federated Learning~(C-SFL), a\\nnovel scheme that splits the model into three parts, namely the model parts\\ntrained at the computationally weak clients, the ones trained at the\\ncomputationally strong clients, and the ones at the server. Unlike existing\\nworks, C-SFL enables parallel training and aggregation of model's parts at the\\nclients and at the server, resulting in reduced training delays and\\ncommmunication overhead while improving the model's accuracy. Experiments\\nverify the multiple gains of C-SFL against the existing schemes.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC,cs.AI\", \"published\": \"2025-04-22T09:18:57Z\"}"}

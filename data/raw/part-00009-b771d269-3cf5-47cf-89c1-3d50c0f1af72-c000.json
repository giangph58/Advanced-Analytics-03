{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15051v1\", \"title\": \"VeLU: Variance-enhanced Learning Unit for Deep Neural Networks\", \"summary\": \"Activation functions are fundamental in deep neural networks and directly\\nimpact gradient flow, optimization stability, and generalization. Although ReLU\\nremains standard because of its simplicity, it suffers from vanishing gradients\\nand lacks adaptability. Alternatives like Swish and GELU introduce smooth\\ntransitions, but fail to dynamically adjust to input statistics. We propose\\nVeLU, a Variance-enhanced Learning Unit as an activation function that\\ndynamically scales based on input variance by integrating ArcTan-Sin\\ntransformations and Wasserstein-2 regularization, effectively mitigating\\ncovariate shifts and stabilizing optimization. Extensive experiments on\\nViT_B16, VGG19, ResNet50, DenseNet121, MobileNetV2, and EfficientNetB3 confirm\\nVeLU's superiority over ReLU, ReLU6, Swish, and GELU on six vision benchmarks.\\nThe codes of VeLU are publicly available on GitHub.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CV\", \"published\": \"2025-04-21T12:20:46Z\"}"}

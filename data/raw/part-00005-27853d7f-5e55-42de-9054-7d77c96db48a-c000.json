{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12644v1\", \"title\": \"Quantum Computing Supported Adversarial Attack-Resilient Autonomous\\n  Vehicle Perception Module for Traffic Sign Classification\", \"summary\": \"Deep learning (DL)-based image classification models are essential for\\nautonomous vehicle (AV) perception modules since incorrect categorization might\\nhave severe repercussions. Adversarial attacks are widely studied cyberattacks\\nthat can lead DL models to predict inaccurate output, such as incorrectly\\nclassified traffic signs by the perception module of an autonomous vehicle. In\\nthis study, we create and compare hybrid classical-quantum deep learning\\n(HCQ-DL) models with classical deep learning (C-DL) models to demonstrate\\nrobustness against adversarial attacks for perception modules. Before feeding\\nthem into the quantum system, we used transfer learning models, alexnet and\\nvgg-16, as feature extractors. We tested over 1000 quantum circuits in our\\nHCQ-DL models for projected gradient descent (PGD), fast gradient sign attack\\n(FGSA), and gradient attack (GA), which are three well-known untargeted\\nadversarial approaches. We evaluated the performance of all models during\\nadversarial attacks and no-attack scenarios. Our HCQ-DL models maintain\\naccuracy above 95\\\\% during a no-attack scenario and above 91\\\\% for GA and FGSA\\nattacks, which is higher than C-DL models. During the PGD attack, our\\nalexnet-based HCQ-DL model maintained an accuracy of 85\\\\% compared to C-DL\\nmodels that achieved accuracies below 21\\\\%. Our results highlight that the\\nHCQ-DL models provide improved accuracy for traffic sign classification under\\nadversarial settings compared to their classical counterparts.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CR,cs.CV,cs.ET\", \"published\": \"2025-04-17T05:08:08Z\"}"}

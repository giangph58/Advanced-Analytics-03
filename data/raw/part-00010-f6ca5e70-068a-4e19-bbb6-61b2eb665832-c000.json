{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05343v1\", \"title\": \"Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound\\n  Source Localization\", \"summary\": \"Large-scale vision-language models demonstrate strong multimodal alignment\\nand generalization across diverse tasks. Among them, CLIP stands out as one of\\nthe most successful approaches. In this work, we extend the application of CLIP\\nto sound source localization, proposing a self-supervised method operates\\nwithout explicit text input. We introduce a framework that maps audios into\\ntokens compatible with CLIP's text encoder, producing audio-driven embeddings.\\nThese embeddings are used to generate sounding region masks, from which visual\\nfeatures are extracted and aligned with the audio embeddings through a\\ncontrastive audio-visual correspondence objective. Our findings show that\\nalignment knowledge of pre-trained multimodal foundation model enables our\\nmethod to generate more complete and compact localization for sounding objects.\\nWe further propose an LLM-guided extension that distills object-aware\\naudio-visual scene understanding into the model during training to enhance\\nalignment. Extensive experiments across five diverse tasks demonstrate that our\\nmethod, in all variants, outperforms state-of-the-art approaches and achieves\\nstrong generalization in zero-shot settings.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.SD,eess.AS\", \"published\": \"2025-05-08T15:32:04Z\"}"}

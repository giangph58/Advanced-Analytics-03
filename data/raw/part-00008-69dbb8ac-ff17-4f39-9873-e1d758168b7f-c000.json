{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16073v1\", \"title\": \"Guiding VLM Agents with Process Rewards at Inference Time for GUI\\n  Navigation\", \"summary\": \"Recent advancements in visual language models (VLMs) have notably enhanced\\ntheir capabilities in handling complex Graphical User Interface (GUI)\\ninteraction tasks. Despite these improvements, current frameworks often\\nstruggle to generate correct actions in challenging GUI environments.\\nState-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source\\nVLMs for GUI tasks requires significant resources. Additionally, existing\\ntrajectory-level evaluation and refinement techniques frequently fall short due\\nto delayed feedback and local optimization issues. To address these challenges,\\nwe propose an approach that guides VLM agents with process supervision by a\\nreward model during GUI navigation and control at inference time. This guidance\\nallows the VLM agent to optimize actions at each inference step, thereby\\nimproving performance in both static and dynamic environments. In particular,\\nour method demonstrates significant performance gains in three GUI navigation\\ntasks, achieving a 3.4% improvement in single step action accuracy for static\\nenvironments, along with a around 33% increase in task success rate in one\\ndynamic environment. With further integration of trajectory reflection and\\nretry mechanisms, we also demonstrate even greater enhancement in task success.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-22T17:52:42Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10479v1\", \"title\": \"InternVL3: Exploring Advanced Training and Test-Time Recipes for\\n  Open-Source Multimodal Models\", \"summary\": \"We introduce InternVL3, a significant advancement in the InternVL series\\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\\ntext-only large language model (LLM) into a multimodal large language model\\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\\nduring a single pre-training stage. This unified training paradigm effectively\\naddresses the complexities and alignment challenges commonly encountered in\\nconventional post-hoc training pipelines for MLLMs. To further improve\\nperformance and scalability, InternVL3 incorporates variable visual position\\nencoding (V2PE) to support extended multimodal contexts, employs advanced\\npost-training techniques such as supervised fine-tuning (SFT) and mixed\\npreference optimization (MPO), and adopts test-time scaling strategies\\nalongside an optimized training infrastructure. Extensive empirical evaluations\\ndemonstrate that InternVL3 delivers superior performance across a wide range of\\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\\ncapabilities remain highly competitive with leading proprietary models,\\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\\nmaintaining strong pure-language proficiency. In pursuit of open-science\\nprinciples, we will publicly release both the training data and model weights\\nto foster further research and development in next-generation MLLMs.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-14T17:59:25Z\"}"}

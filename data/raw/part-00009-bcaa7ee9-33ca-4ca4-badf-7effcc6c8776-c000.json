{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15610v1\", \"title\": \"A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in\\n  Resource-Constrained Settings\", \"summary\": \"The current study describes a cost-effective method for adapting large\\nlanguage models (LLMs) for academic advising with study-abroad contexts in mind\\nand for application in low-resource methods for acculturation. With the\\nMistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and\\na 4-bit quantization method, the model underwent training in two distinct\\nstages related to this study's purpose to enhance domain specificity while\\nmaintaining computational efficiency. In Phase 1, the model was conditioned\\nwith a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained\\nwith manually curated datasets from the StudyAbroadGPT project to achieve\\nenhanced, contextualized responses. Technical innovations entailed\\nmemory-efficient quantization, parameter-efficient adaptation, and continuous\\ntraining analytics via Weights & Biases. After training, this study\\ndemonstrated a reduction in training loss by 52.7%, 92% accuracy in\\ndomain-specific recommendations, achieved 95% markdown-based formatting\\nsupport, and a median run-rate of 100 samples per second on off-the-shelf GPU\\nequipment. These findings support the effective application of\\ninstruction-tuned LLMs within educational advisers, especially in low-resource\\ninstitutional scenarios. Limitations included decreased generalizability and\\nthe application of a synthetically generated dataset, but this framework is\\nscalable for adding new multilingual-augmented and real-time academic advising\\nprocesses. Future directions may include plans for the integration of\\nretrieval-augmented generation, applying dynamic quantization routines, and\\nconnecting to real-time academic databases to increase adaptability and\\naccuracy.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-22T06:08:13Z\"}"}

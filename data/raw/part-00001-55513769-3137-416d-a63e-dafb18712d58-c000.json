{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15564v1\", \"title\": \"A Large-scale Class-level Benchmark Dataset for Code Generation with\\n  LLMs\", \"summary\": \"Recent advancements in large language models (LLMs) have demonstrated\\npromising capabilities in code generation tasks. However, most existing\\nbenchmarks focus on isolated functions and fail to capture the complexity of\\nreal-world, class-level software structures. To address this gap, we introduce\\na large-scale, Python class-level dataset curated from $13{,}174$ real-world\\nopen-source projects. The dataset contains over 842,000 class skeletons, each\\nincluding class and method signatures, along with associated docstrings when\\navailable. We preserve structural and contextual dependencies critical to\\nrealistic software development scenarios and enrich the dataset with static\\ncode metrics to support downstream analysis. To evaluate the usefulness of this\\ndataset, we use extracted class skeletons as prompts for GPT-4 to generate full\\nclass implementations. Results show that the LLM-generated classes exhibit\\nstrong lexical and structural similarity to human-written counterparts, with\\naverage ROUGE@L, BLEU, and TSED scores of 0.80, 0.59, and 0.73, respectively.\\nThese findings confirm that well-structured prompts derived from real-world\\nclass skeletons significantly enhance LLM performance in class-level code\\ngeneration. This dataset offers a valuable resource for benchmarking, training,\\nand improving LLMs in realistic software engineering contexts.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE,cs.AI,cs.LG\", \"published\": \"2025-04-22T03:33:57Z\"}"}

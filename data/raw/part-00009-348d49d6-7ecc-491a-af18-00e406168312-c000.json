{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21668v1\", \"title\": \"Traceback of Poisoning Attacks to Retrieval-Augmented Generation\", \"summary\": \"Large language models (LLMs) integrated with retrieval-augmented generation\\n(RAG) systems improve accuracy by leveraging external knowledge sources.\\nHowever, recent research has revealed RAG's susceptibility to poisoning\\nattacks, where the attacker injects poisoned texts into the knowledge database,\\nleading to attacker-desired responses. Existing defenses, which predominantly\\nfocus on inference-time mitigation, have proven insufficient against\\nsophisticated attacks. In this paper, we introduce RAGForensics, the first\\ntraceback system for RAG, designed to identify poisoned texts within the\\nknowledge database that are responsible for the attacks. RAGForensics operates\\niteratively, first retrieving a subset of texts from the database and then\\nutilizing a specially crafted prompt to guide an LLM in detecting potential\\npoisoning texts. Empirical evaluations across multiple datasets demonstrate the\\neffectiveness of RAGForensics against state-of-the-art poisoning attacks. This\\nwork pioneers the traceback of poisoned texts in RAG systems, providing a\\npractical and promising defense mechanism to enhance their security.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.IR,cs.LG\", \"published\": \"2025-04-30T14:10:02Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17672v1\", \"title\": \"Cross-region Model Training with Communication-Computation Overlapping\\n  and Delay Compensation\", \"summary\": \"Training large language models (LLMs) requires massive computational\\nresources, often necessitating the aggregation of geographically distributed\\ndata centers (\\\\ie, cross-region training). However, the high communication\\nlatency in wide-area networks severely degrades the efficiency of traditional\\ndistributed training. While methods like DiLoCo reduce communication frequency,\\nthey suffer from blocking synchronization. Streaming DiLoCo alleviates this\\nissue via communication-computation overlapping but introduces update staleness\\nand model inconsistency due to delayed global updates and partial\\nsynchronization. These factors impair convergence, especially when aggressive\\noverlap is needed to mask high latency. We propose CoCoDC, a novel distributed\\ntraining framework with communication-computation overlapping and delay\\ncompensation, to explicitly tackle these challenges. Within the CoCoDC\\nframework, we specifically develop a novel Delay Compensation strategy based on\\nTaylor expansion to effectively mitigate the staleness and an Adaptive\\nTransmission strategy that dynamically schedules model fragment synchronization\\nto optimize bandwidth usage and accelerate convergence. Extensive experiments\\nhighlight the superior performance of CoCoDC over both DiLoCo and Streaming\\nDiLoCo regarding final accuracy and training speed. Specifically, CoCoDC\\nreduces the training steps needed to reach a comparable perplexity by up to\\n21.0% compared to Streaming DiLoCo. Our work provides an effective solution for\\nscalable and efficient cross-region LLM training.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC\", \"published\": \"2025-04-24T15:40:17Z\"}"}

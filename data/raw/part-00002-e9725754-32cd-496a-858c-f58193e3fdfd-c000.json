{"value":"{\"aid\": \"http://arxiv.org/abs/2504.09936v1\", \"title\": \"KeepKV: Eliminating Output Perturbation in KV Cache Compression for\\n  Efficient LLMs Inference\", \"summary\": \"Efficient inference of large language models (LLMs) is hindered by an\\never-growing key-value (KV) cache, making KV cache compression a critical\\nresearch direction. Traditional methods selectively evict less important KV\\ncache entries based on attention scores or position heuristics, which leads to\\ninformation loss and hallucinations. Recently, merging-based strategies have\\nbeen explored to retain more information by merging KV pairs that would be\\ndiscarded; however, these existing approaches inevitably introduce\\ninconsistencies in attention distributions before and after merging, causing\\noutput perturbation and degraded generation quality. To overcome this\\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\\nto eliminate output perturbation while preserving performance under strict\\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\\nrecords merging history and adaptively adjusts attention scores. Moreover, it\\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\\nattention consistency and compensating for attention loss resulting from cache\\nmerging. KeepKV successfully retains essential context information within a\\nsignificantly compressed cache. Extensive experiments on various benchmarks and\\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\\nenhances inference throughput by more than 2x and keeps superior generation\\nquality even with 10% KV cache budgets.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL\", \"published\": \"2025-04-14T06:58:00Z\"}"}

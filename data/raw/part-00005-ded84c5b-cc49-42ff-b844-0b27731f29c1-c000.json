{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03439v1\", \"title\": \"The Steganographic Potentials of Language Models\", \"summary\": \"The potential for large language models (LLMs) to hide messages within plain\\ntext (steganography) poses a challenge to detection and thwarting of unaligned\\nAI agents, and undermines faithfulness of LLMs reasoning. We explore the\\nsteganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)\\nto: (1) develop covert encoding schemes, (2) engage in steganography when\\nprompted, and (3) utilize steganography in realistic scenarios where hidden\\nreasoning is likely, but not prompted. In these scenarios, we detect the\\nintention of LLMs to hide their reasoning as well as their steganography\\nperformance. Our findings in the fine-tuning experiments as well as in\\nbehavioral non fine-tuning evaluations reveal that while current models exhibit\\nrudimentary steganographic abilities in terms of security and capacity,\\nexplicit algorithmic guidance markedly enhances their capacity for information\\nconcealment.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CR,cs.LG\", \"published\": \"2025-05-06T11:25:52Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12605v1\", \"title\": \"AdaQual-Diff: Diffusion-Based Image Restoration via Adaptive Quality\\n  Prompting\", \"summary\": \"Restoring images afflicted by complex real-world degradations remains\\nchallenging, as conventional methods often fail to adapt to the unique mixture\\nand severity of artifacts present. This stems from a reliance on indirect cues\\nwhich poorly capture the true perceptual quality deficit. To address this\\nfundamental limitation, we introduce AdaQual-Diff, a diffusion-based framework\\nthat integrates perceptual quality assessment directly into the generative\\nrestoration process. Our approach establishes a mathematical relationship\\nbetween regional quality scores from DeQAScore and optimal guidance complexity,\\nimplemented through an Adaptive Quality Prompting mechanism. This mechanism\\nsystematically modulates prompt structure according to measured degradation\\nseverity: regions with lower perceptual quality receive computationally\\nintensive, structurally complex prompts with precise restoration directives,\\nwhile higher quality regions receive minimal prompts focused on preservation\\nrather than intervention. The technical core of our method lies in the dynamic\\nallocation of computational resources proportional to degradation severity,\\ncreating a spatially-varying guidance field that directs the diffusion process\\nwith mathematical precision. By combining this quality-guided approach with\\ncontent-specific conditioning, our framework achieves fine-grained control over\\nregional restoration intensity without requiring additional parameters or\\ninference iterations. Experimental results demonstrate that AdaQual-Diff\\nachieves visually superior restorations across diverse synthetic and real-world\\ndatasets.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-17T03:08:27Z\"}"}

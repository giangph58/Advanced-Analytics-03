{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15270v1\", \"title\": \"An LMM for Efficient Video Understanding via Reinforced Compression of\\n  Video Cubes\", \"summary\": \"Large Multimodal Models (LMMs) uniformly perceive video frames, creating\\ncomputational inefficiency for videos with inherently varying temporal\\ninformation density. This paper present \\\\textbf{Quicksviewer}, an LMM with new\\nperceiving paradigm that partitions a video of nonuniform density into varying\\ncubes using Gumbel Softmax, followed by a unified resampling for each cube to\\nachieve efficient video understanding. This simple and intuitive approach\\ndynamically compress video online based on its temporal density, significantly\\nreducing spatiotemporal redundancy (overall 45$\\\\times$ compression rate), while\\nenabling efficient training with large receptive field. We train the model from\\na language backbone through three progressive stages, each incorporating\\nlengthy videos on average of 420s/1fps thanks to the perceiving efficiency.\\nWith only 0.8M total video-text samples for training, our model outperforms the\\ndirect baseline employing a fixed partitioning strategy by a maximum of 8.72 in\\naccuracy, demonstrating the effectiveness in performance. On Video-MME,\\nQuicksviewer achieves SOTA under modest sequence lengths using just up to 5\\\\%\\nof tokens per frame required by baselines. With this paradigm, scaling up the\\nnumber of input frames reveals a clear power law of the model capabilities. It\\nis also empirically verified that the segments generated by the cubing network\\ncan help for analyzing continuous events in videos.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.CL\", \"published\": \"2025-04-21T17:57:21Z\"}"}

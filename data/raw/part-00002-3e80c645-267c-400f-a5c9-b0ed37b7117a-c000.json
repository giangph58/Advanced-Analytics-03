{"value":"{\"aid\": \"http://arxiv.org/abs/2504.09951v1\", \"title\": \"Towards Weaker Variance Assumptions for Stochastic Optimization\", \"summary\": \"We revisit a classical assumption for analyzing stochastic gradient\\nalgorithms where the squared norm of the stochastic subgradient (or the\\nvariance for smooth problems) is allowed to grow as fast as the squared norm of\\nthe optimization variable. We contextualize this assumption in view of its\\ninception in the 1960s, its seemingly independent appearance in the recent\\nliterature, its relationship to weakest-known variance assumptions for\\nanalyzing stochastic gradient algorithms, and its relevance in deterministic\\nproblems for non-Lipschitz nonsmooth convex optimization. We build on and\\nextend a connection recently made between this assumption and the Halpern\\niteration. For convex nonsmooth, and potentially stochastic, optimization, we\\nanalyze horizon-free, anytime algorithms with last-iterate rates. For problems\\nbeyond simple constrained optimization, such as convex problems with functional\\nconstraints or regularized convex-concave min-max problems, we obtain rates for\\noptimality measures that do not require boundedness of the feasible set.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC,cs.LG,stat.ML\", \"published\": \"2025-04-14T07:26:34Z\"}"}

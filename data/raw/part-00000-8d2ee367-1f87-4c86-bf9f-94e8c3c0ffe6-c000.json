{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16856v1\", \"title\": \"Emo Pillars: Knowledge Distillation to Support Fine-Grained\\n  Context-Aware and Context-Less Emotion Classification\", \"summary\": \"Most datasets for sentiment analysis lack context in which an opinion was\\nexpressed, often crucial for emotion understanding, and are mainly limited by a\\nfew emotion categories. Foundation large language models (LLMs) like GPT-4\\nsuffer from over-predicting emotions and are too resource-intensive. We design\\nan LLM-based data synthesis pipeline and leverage a large model, Mistral-7b,\\nfor the generation of training examples for more accessible, lightweight\\nBERT-type encoder models. We focus on enlarging the semantic diversity of\\nexamples and propose grounding the generation into a corpus of narratives to\\nproduce non-repetitive story-character-centered utterances with unique contexts\\nover 28 emotion classes. By running 700K inferences in 450 GPU hours, we\\ncontribute with the dataset of 100K contextual and also 300K context-less\\nexamples to cover both scenarios. We use it for fine-tuning pre-trained\\nencoders, which results in several Emo Pillars models. We show that Emo Pillars\\nmodels are highly adaptive to new domains when tuned to specific tasks such as\\nGoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on\\nthe first three. We also validate our dataset, conducting statistical analysis\\nand human evaluation, and confirm the success of our measures in utterance\\ndiversification (although less for the neutral class) and context\\npersonalization, while pointing out the need for improved handling of\\nout-of-taxonomy labels within the pipeline.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-23T16:23:17Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16047v1\", \"title\": \"Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive\\n  Analysis\", \"summary\": \"Foundation models, trained on vast amounts of data using self-supervised\\ntechniques, have emerged as a promising frontier for advancing artificial\\nintelligence (AI) applications in medicine. This study evaluates three\\ndifferent vision-language foundation models (RAD-DINO, CheXagent, and\\nBiomedCLIP) on their ability to capture fine-grained imaging features for\\nradiology tasks. The models were assessed across classification, segmentation,\\nand regression tasks for pneumothorax and cardiomegaly on chest radiographs.\\nSelf-supervised RAD-DINO consistently excelled in segmentation tasks, while\\ntext-supervised CheXagent demonstrated superior classification performance.\\nBiomedCLIP showed inconsistent performance across tasks. A custom segmentation\\nmodel that integrates global and local features substantially improved\\nperformance for all foundation models, particularly for challenging\\npneumothorax segmentation. The findings highlight that pre-training methodology\\nsignificantly influences model performance on specific downstream tasks. For\\nfine-grained segmentation tasks, models trained without text supervision\\nperformed better, while text-supervised models offered advantages in\\nclassification and interpretability. These insights provide guidance for\\nselecting foundation models based on specific clinical applications in\\nradiology.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-22T17:20:34Z\"}"}

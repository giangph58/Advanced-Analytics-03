{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04486v1\", \"title\": \"Efficient Flow Matching using Latent Variables\", \"summary\": \"Flow matching models have shown great potential in image generation tasks\\namong probabilistic generative models. Building upon the ideas of continuous\\nnormalizing flows, flow matching models generalize the transport path of the\\ndiffusion models from a simple prior distribution to the data. Most flow\\nmatching models in the literature do not explicitly model the underlying\\nstructure/manifold in the target data when learning the flow from a simple\\nsource distribution like the standard Gaussian. This leads to inefficient\\nlearning, especially for many high-dimensional real-world datasets, which often\\nreside in a low-dimensional manifold. Existing strategies of incorporating\\nmanifolds, including data with underlying multi-modal distribution, often\\nrequire expensive training and hence frequently lead to suboptimal performance.\\nTo this end, we present \\\\texttt{Latent-CFM}, which provides simplified\\ntraining/inference strategies to incorporate multi-modal data structures using\\npretrained deep latent variable models. Through experiments on multi-modal\\nsynthetic data and widely used image benchmark datasets, we show that\\n\\\\texttt{Latent-CFM} exhibits improved generation quality with significantly\\nless training ($\\\\sim 50\\\\%$ less in some cases) and computation than\\nstate-of-the-art flow matching models. Using a 2d Darcy flow dataset, we\\ndemonstrate that our approach generates more physically accurate samples than\\ncompetitive approaches. In addition, through latent space analysis, we\\ndemonstrate that our approach can be used for conditional image generation\\nconditioned on latent features.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.LG\", \"published\": \"2025-05-07T14:59:23Z\"}"}

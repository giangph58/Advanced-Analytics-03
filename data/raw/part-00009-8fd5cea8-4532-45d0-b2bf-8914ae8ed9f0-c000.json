{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00482v1\", \"title\": \"JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers\", \"summary\": \"We present JointDiT, a diffusion transformer that models the joint\\ndistribution of RGB and depth. By leveraging the architectural benefit and\\noutstanding image prior of the state-of-the-art diffusion transformer, JointDiT\\nnot only generates high-fidelity images but also produces geometrically\\nplausible and accurate depth maps. This solid joint distribution modeling is\\nachieved through two simple yet effective techniques that we propose, i.e.,\\nadaptive scheduling weights, which depend on the noise levels of each modality,\\nand the unbalanced timestep sampling strategy. With these techniques, we train\\nour model across all noise levels for each modality, enabling JointDiT to\\nnaturally handle various combinatorial generation tasks, including joint\\ngeneration, depth estimation, and depth-conditioned image generation by simply\\ncontrolling the timestep of each branch. JointDiT demonstrates outstanding\\njoint generation performance. Furthermore, it achieves comparable results in\\ndepth estimation and depth-conditioned image generation, suggesting that joint\\ndistribution modeling can serve as a replaceable alternative to conditional\\ngeneration. The project page is available at\\nhttps://byungki-k.github.io/JointDiT/.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-05-01T12:21:23Z\"}"}

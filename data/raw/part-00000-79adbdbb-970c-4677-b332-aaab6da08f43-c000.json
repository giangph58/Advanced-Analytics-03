{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12717v1\", \"title\": \"Post-pre-training for Modality Alignment in Vision-Language Foundation\\n  Models\", \"summary\": \"Contrastive language image pre-training (CLIP) is an essential component of\\nbuilding modern vision-language foundation models. While CLIP demonstrates\\nremarkable zero-shot performance on downstream tasks, the multi-modal feature\\nspaces still suffer from a modality gap, which is a gap between image and text\\nfeature clusters and limits downstream task performance. Although existing\\nworks attempt to address the modality gap by modifying pre-training or\\nfine-tuning, they struggle with heavy training costs with large datasets or\\ndegradations of zero-shot performance. This paper presents CLIP-Refine, a\\npost-pre-training method for CLIP models at a phase between pre-training and\\nfine-tuning. CLIP-Refine aims to align the feature space with 1 epoch training\\non small image-text datasets without zero-shot performance degradations. To\\nthis end, we introduce two techniques: random feature alignment (RaFA) and\\nhybrid contrastive-distillation (HyCD). RaFA aligns the image and text features\\nto follow a shared prior distribution by minimizing the distance to random\\nreference vectors sampled from the prior. HyCD updates the model with hybrid\\nsoft labels generated by combining ground-truth image-text pair labels and\\noutputs from the pre-trained CLIP model. This contributes to achieving both\\nmaintaining the past knowledge and learning new knowledge to align features.\\nOur extensive experiments with multiple classification and retrieval tasks show\\nthat CLIP-Refine succeeds in mitigating the modality gap and improving the\\nzero-shot performance.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.LG\", \"published\": \"2025-04-17T07:46:19Z\"}"}

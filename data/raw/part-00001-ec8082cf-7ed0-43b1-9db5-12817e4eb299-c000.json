{"value":"{\"aid\": \"http://arxiv.org/abs/2503.21677v1\", \"title\": \"A tale of two goals: leveraging sequentiality in multi-goal scenarios\", \"summary\": \"Several hierarchical reinforcement learning methods leverage planning to\\ncreate a graph or sequences of intermediate goals, guiding a lower-level\\ngoal-conditioned (GC) policy to reach some final goals. The low-level policy is\\ntypically conditioned on the current goal, with the aim of reaching it as\\nquickly as possible. However, this approach can fail when an intermediate goal\\ncan be reached in multiple ways, some of which may make it impossible to\\ncontinue toward subsequent goals. To address this issue, we introduce two\\ninstances of Markov Decision Process (MDP) where the optimization objective\\nfavors policies that not only reach the current goal but also subsequent ones.\\nIn the first, the agent is conditioned on both the current and final goals,\\nwhile in the second, it is conditioned on the next two goals in the sequence.\\nWe conduct a series of experiments on navigation and pole-balancing tasks in\\nwhich sequences of intermediate goals are given. By evaluating policies trained\\nwith TD3+HER on both the standard GC-MDP and our proposed MDPs, we show that,\\nin most cases, conditioning on the next two goals improves stability and sample\\nefficiency over other approaches.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-03-27T16:47:46Z\"}"}

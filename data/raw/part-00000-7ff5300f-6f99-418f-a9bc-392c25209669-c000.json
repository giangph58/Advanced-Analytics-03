{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17243v1\", \"title\": \"NeuralGrok: Accelerate Grokking by Neural Gradient Transformation\", \"summary\": \"Grokking is proposed and widely studied as an intricate phenomenon in which\\ngeneralization is achieved after a long-lasting period of overfitting. In this\\nwork, we propose NeuralGrok, a novel gradient-based approach that learns an\\noptimal gradient transformation to accelerate the generalization of\\ntransformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary\\nmodule (e.g., an MLP block) in conjunction with the base model. This module\\ndynamically modulates the influence of individual gradient components based on\\ntheir contribution to generalization, guided by a bilevel optimization\\nalgorithm. Our extensive experiments demonstrate that NeuralGrok significantly\\naccelerates generalization, particularly in challenging arithmetic tasks. We\\nalso show that NeuralGrok promotes a more stable training paradigm, constantly\\nreducing the model's complexity, while traditional regularization methods, such\\nas weight decay, can introduce substantial instability and impede\\ngeneralization. We further investigate the intrinsic model complexity\\nleveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that\\nNeuralGrok effectively facilitates generalization by reducing the model\\ncomplexity. We offer valuable insights on the grokking phenomenon of\\nTransformer models, which encourages a deeper understanding of the fundamental\\nprinciples governing generalization ability.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-24T04:41:35Z\"}"}

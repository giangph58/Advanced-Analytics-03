{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12268v1\", \"title\": \"HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level\\n  Synthesis Design Tasks\", \"summary\": \"The rapid scaling of large language model (LLM) training and inference has\\ndriven their adoption in semiconductor design across academia and industry.\\nWhile most prior work evaluates LLMs on hardware description language (HDL)\\ntasks, particularly Verilog, designers are increasingly using high-level\\nsynthesis (HLS) to build domain-specific accelerators and complex hardware\\nsystems. However, benchmarks and tooling to comprehensively evaluate LLMs for\\nHLS design tasks remain scarce.\\n  To address this, we introduce HLS-Eval, the first complete benchmark and\\nevaluation framework for LLM-driven HLS design. HLS-Eval targets two core\\ntasks: (1) generating HLS code from natural language descriptions, and (2)\\nperforming HLS-specific code edits to optimize performance and hardware\\nefficiency. The benchmark includes 94 unique designs drawn from standard HLS\\nbenchmarks and novel sources. Each case is prepared via a semi-automated flow\\nthat produces a natural language description and a paired testbench for\\nC-simulation and synthesis validation, ensuring each task is \\\"LLM-ready.\\\"\\n  Beyond the benchmark, HLS-Eval offers a modular Python framework for\\nautomated, parallel evaluation of both local and hosted LLMs. It includes a\\nparallel evaluation engine, direct HLS tool integration, and abstractions for\\nto support different LLM interaction paradigms, enabling rapid prototyping of\\nnew benchmarks, tasks, and LLM methods.\\n  We demonstrate HLS-Eval through baseline evaluations of open-source LLMs on\\nVitis HLS, measuring outputs across four key metrics - parseability,\\ncompilability, runnability, and synthesizability - reflecting the iterative HLS\\ndesign cycle. We also report pass@k metrics, establishing clear baselines and\\nreusable infrastructure for the broader LLM-for-hardware community.\\n  All benchmarks, framework code, and results are open-sourced at\\nhttps://github.com/stefanpie/hls-eval.\", \"main_category\": \"cs.AR\", \"categories\": \"cs.AR,cs.AI\", \"published\": \"2025-04-16T17:30:36Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05774v1\", \"title\": \"Transferable Mask Transformer: Cross-domain Semantic Segmentation with\\n  Region-adaptive Transferability Estimation\", \"summary\": \"Recent advances in Vision Transformers (ViTs) have set new benchmarks in\\nsemantic segmentation. However, when adapting pretrained ViTs to new target\\ndomains, significant performance degradation often occurs due to distribution\\nshifts, resulting in suboptimal global attention. Since self-attention\\nmechanisms are inherently data-driven, they may fail to effectively attend to\\nkey objects when source and target domains exhibit differences in texture,\\nscale, or object co-occurrence patterns. While global and patch-level domain\\nadaptation methods provide partial solutions, region-level adaptation with\\ndynamically shaped regions is crucial due to spatial heterogeneity in\\ntransferability across different image areas. We present Transferable Mask\\nTransformer (TMT), a novel region-level adaptation framework for semantic\\nsegmentation that aligns cross-domain representations through spatial\\ntransferability analysis. TMT consists of two key components: (1) An Adaptive\\nCluster-based Transferability Estimator (ACTE) that dynamically segments images\\ninto structurally and semantically coherent regions for localized\\ntransferability assessment, and (2) A Transferable Masked Attention (TMA)\\nmodule that integrates region-specific transferability maps into ViTs'\\nattention mechanisms, prioritizing adaptation in regions with low\\ntransferability and high semantic uncertainty. Comprehensive evaluations across\\n20 cross-domain pairs demonstrate TMT's superiority, achieving an average 2%\\nMIoU improvement over vanilla fine-tuning and a 1.28% increase compared to\\nstate-of-the-art baselines. The source code will be publicly available.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-08T07:53:51Z\"}"}

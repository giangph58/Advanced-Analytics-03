{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12211v1\", \"title\": \"Creating benchmarkable components to measure the quality ofAI-enhanced\\n  developer tools\", \"summary\": \"In the AI community, benchmarks to evaluate model quality are well\\nestablished, but an equivalent approach to benchmarking products built upon\\ngenerative AI models is still missing. This has had two consequences. First, it\\nhas made teams focus on model quality over the developer experience, while\\nsuccessful products combine both. Second, product team have struggled to answer\\nquestions about their products in relation to their competitors.\\n  In this case study, we share: (1) our process to create robust,\\nenterprise-grade and modular components to support the benchmarking of the\\ndeveloper experience (DX) dimensions of our team's AI for code offerings, and\\n(2) the components we have created to do so, including demographics and\\nattitudes towards AI surveys, a benchmarkable task, and task and feature\\nsurveys. By doing so, we hope to lower the barrier to the DX benchmarking of\\ngenAI-enhanced code products.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE,cs.HC\", \"published\": \"2025-04-16T15:58:33Z\"}"}

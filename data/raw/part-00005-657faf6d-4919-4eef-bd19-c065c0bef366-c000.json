{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15032v1\", \"title\": \"DyST-XL: Dynamic Layout Planning and Content Control for Compositional\\n  Text-to-Video Generation\", \"summary\": \"Compositional text-to-video generation, which requires synthesizing dynamic\\nscenes with multiple interacting entities and precise spatial-temporal\\nrelationships, remains a critical challenge for diffusion-based models.\\nExisting methods struggle with layout discontinuity, entity identity drift, and\\nimplausible interaction dynamics due to unconstrained cross-attention\\nmechanisms and inadequate physics-aware reasoning. To address these\\nlimitations, we propose DyST-XL, a \\\\textbf{training-free} framework that\\nenhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through\\nframe-aware control. DyST-XL integrates three key innovations: (1) A Dynamic\\nLayout Planner that leverages large language models (LLMs) to parse input\\nprompts into entity-attribute graphs and generates physics-aware keyframe\\nlayouts, with intermediate frames interpolated via trajectory optimization; (2)\\nA Dual-Prompt Controlled Attention Mechanism that enforces localized text-video\\nalignment through frame-aware attention masking, achieving the precise control\\nover individual entities; and (3) An Entity-Consistency Constraint strategy\\nthat propagates first-frame feature embeddings to subsequent frames during\\ndenoising, preserving object identity without manual annotation. Experiments\\ndemonstrate that DyST-XL excels in compositional text-to-video generation,\\nsignificantly improving performance on complex prompts and bridging a crucial\\ngap in training-free video synthesis.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-21T11:41:22Z\"}"}

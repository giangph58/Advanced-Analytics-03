{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16925v1\", \"title\": \"Latent Diffusion Planning for Imitation Learning\", \"summary\": \"Recent progress in imitation learning has been enabled by policy\\narchitectures that scale to complex visuomotor tasks, multimodal distributions,\\nand large datasets. However, these methods often rely on learning from large\\namount of expert demonstrations. To address these shortcomings, we propose\\nLatent Diffusion Planning (LDP), a modular approach consisting of a planner\\nwhich can leverage action-free demonstrations, and an inverse dynamics model\\nwhich can leverage suboptimal data, that both operate over a learned latent\\nspace. First, we learn a compact latent space through a variational\\nautoencoder, enabling effective forecasting of future states in image-based\\ndomains. Then, we train a planner and an inverse dynamics model with diffusion\\nobjectives. By separating planning from action prediction, LDP can benefit from\\nthe denser supervision signals of suboptimal and action-free data. On simulated\\nvisual robotic manipulation tasks, LDP outperforms state-of-the-art imitation\\nlearning approaches, as they cannot leverage such additional data.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.AI\", \"published\": \"2025-04-23T17:53:34Z\"}"}

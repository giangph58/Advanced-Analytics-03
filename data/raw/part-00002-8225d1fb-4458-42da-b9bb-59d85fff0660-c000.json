{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05262v1\", \"title\": \"Enhancing Cooperative Multi-Agent Reinforcement Learning with State\\n  Modelling and Adversarial Exploration\", \"summary\": \"Learning to cooperate in distributed partially observable environments with\\nno communication abilities poses significant challenges for multi-agent deep\\nreinforcement learning (MARL). This paper addresses key concerns in this\\ndomain, focusing on inferring state representations from individual agent\\nobservations and leveraging these representations to enhance agents'\\nexploration and collaborative task execution policies. To this end, we propose\\na novel state modelling framework for cooperative MARL, where agents infer\\nmeaningful belief representations of the non-observable state, with respect to\\noptimizing their own policies, while filtering redundant and less informative\\njoint state information. Building upon this framework, we propose the MARL SMPE\\nalgorithm. In SMPE, agents enhance their own policy's discriminative abilities\\nunder partial observability, explicitly by incorporating their beliefs into the\\npolicy network, and implicitly by adopting an adversarial type of exploration\\npolicies which encourages agents to discover novel, high-value states while\\nimproving the discriminative abilities of others. Experimentally, we show that\\nSMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative\\ntasks from the MPE, LBF, and RWARE benchmarks.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.MA\", \"published\": \"2025-05-08T14:07:20Z\"}"}

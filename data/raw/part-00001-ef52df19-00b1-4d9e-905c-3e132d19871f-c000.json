{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20039v1\", \"title\": \"AutoJudge: Judge Decoding Without Manual Annotation\", \"summary\": \"We introduce AutoJudge, a framework that accelerates large language model\\n(LLM) inference with task-specific lossy speculative decoding. Instead of\\nmatching the original model output distribution token-by-token, we identify\\nwhich of the generated tokens affect the downstream quality of the generated\\nresponse, relaxing the guarantee so that the \\\"unimportant\\\" tokens can be\\ngenerated faster. Our approach relies on a semi-greedy search algorithm to test\\nwhich of the mismatches between target and draft model should be corrected to\\npreserve quality, and which ones may be skipped. We then train a lightweight\\nclassifier based on existing LLM embeddings to predict, at inference time,\\nwhich mismatching tokens can be safely accepted without compromising the final\\nanswer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B\\n(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more\\naccepted tokens per verification cycle with under 1% degradation in answer\\naccuracy compared to standard speculative decoding and over 2x with small loss\\nin accuracy. When applied to the LiveCodeBench benchmark, our approach\\nautomatically detects other, programming-specific important tokens and shows\\nsimilar speedups, demonstrating its ability to generalize across tasks.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-28T17:59:28Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10900v1\", \"title\": \"Bridging Distribution Gaps in Time Series Foundation Model Pretraining\\n  with Prototype-Guided Normalization\", \"summary\": \"Foundation models have achieved remarkable success across diverse\\nmachine-learning domains through large-scale pretraining on large, diverse\\ndatasets. However, pretraining on such datasets introduces significant\\nchallenges due to substantial mismatches in data distributions, a problem\\nparticularly pronounced with time series data. In this paper, we tackle this\\nissue by proposing a domain-aware adaptive normalization strategy within the\\nTransformer architecture. Specifically, we replace the traditional LayerNorm\\nwith a prototype-guided dynamic normalization mechanism (ProtoNorm), where\\nlearned prototypes encapsulate distinct data distributions, and\\nsample-to-prototype affinity determines the appropriate normalization layer.\\nThis mechanism effectively captures the heterogeneity of time series\\ncharacteristics, aligning pretrained representations with downstream tasks.\\nThrough comprehensive empirical evaluation, we demonstrate that our method\\nsignificantly outperforms conventional pretraining techniques across both\\nclassification and forecasting tasks, while effectively mitigating the adverse\\neffects of distribution shifts during pretraining. Incorporating ProtoNorm is\\nas simple as replacing a single line of code. Extensive experiments on diverse\\nreal-world time series benchmarks validate the robustness and generalizability\\nof our approach, advancing the development of more versatile time series\\nfoundation models.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-15T06:23:00Z\"}"}

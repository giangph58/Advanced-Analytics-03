{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04744v1\", \"title\": \"Grounding 3D Object Affordance with Language Instructions, Visual\\n  Observations and Interactions\", \"summary\": \"Grounding 3D object affordance is a task that locates objects in 3D space\\nwhere they can be manipulated, which links perception and action for embodied\\nintelligence. For example, for an intelligent robot, it is necessary to\\naccurately ground the affordance of an object and grasp it according to human\\ninstructions. In this paper, we introduce a novel task that grounds 3D object\\naffordance based on language instructions, visual observations and\\ninteractions, which is inspired by cognitive science. We collect an Affordance\\nGrounding dataset with Points, Images and Language instructions (AGPIL) to\\nsupport the proposed task. In the 3D physical world, due to observation\\norientation, object rotation, or spatial occlusion, we can only get a partial\\nobservation of the object. So this dataset includes affordance estimations of\\nobjects from full-view, partial-view, and rotation-view perspectives. To\\naccomplish this task, we propose LMAffordance3D, the first multi-modal,\\nlanguage-guided 3D affordance grounding network, which applies a\\nvision-language model to fuse 2D and 3D spatial features with semantic\\nfeatures. Comprehensive experiments on AGPIL demonstrate the effectiveness and\\nsuperiority of our method on this task, even in unseen experimental settings.\\nOur project is available at https://sites.google.com/view/lmaffordance3d.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.RO\", \"published\": \"2025-04-07T05:38:23Z\"}"}

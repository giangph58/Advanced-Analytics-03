{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17690v1\", \"title\": \"On the Generalization of Adversarially Trained Quantum Classifiers\", \"summary\": \"Quantum classifiers are vulnerable to adversarial attacks that manipulate\\ntheir input classical or quantum data. A promising countermeasure is\\nadversarial training, where quantum classifiers are trained by using an\\nattack-aware, adversarial loss function. This work establishes novel bounds on\\nthe generalization error of adversarially trained quantum classifiers when\\ntested in the presence of perturbation-constrained adversaries. The bounds\\nquantify the excess generalization error incurred to ensure robustness to\\nadversarial attacks as scaling with the training sample size $m$ as\\n$1/\\\\sqrt{m}$, while yielding insights into the impact of the quantum embedding.\\nFor quantum binary classifiers employing \\\\textit{rotation embedding}, we find\\nthat, in the presence of adversarial attacks on classical inputs $\\\\mathbf{x}$,\\nthe increase in sample complexity due to adversarial training over conventional\\ntraining vanishes in the limit of high dimensional inputs $\\\\mathbf{x}$. In\\ncontrast, when the adversary can directly attack the quantum state\\n$\\\\rho(\\\\mathbf{x})$ encoding the input $\\\\mathbf{x}$, the excess generalization\\nerror depends on the choice of embedding only through its Hilbert space\\ndimension. The results are also extended to multi-class classifiers. We\\nvalidate our theoretical findings with numerical experiments.\", \"main_category\": \"quant-ph\", \"categories\": \"quant-ph,cs.LG\", \"published\": \"2025-04-24T15:59:55Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15843v1\", \"title\": \"Pre-DPO: Improving Data Utilization in Direct Preference Optimization\\n  Using a Guiding Reference Model\", \"summary\": \"Direct Preference Optimization (DPO) simplifies reinforcement learning from\\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\\nhuman preferences without an explicit reward model. We find that during DPO\\ntraining, the reference model plays the role of a data weight adjuster.\\nHowever, the common practice of initializing the policy and reference models\\nidentically in DPO can lead to inefficient data utilization and impose a\\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\\nPreference Optimization (SimPO) reduces training robustness and necessitates\\nstricter conditions to prevent catastrophic forgetting. In this work, we\\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\\nenhances preference optimization performance by leveraging a guiding reference\\nmodel. This reference model provides foresight into the optimal policy state\\nachievable through the training preference data, serving as a guiding mechanism\\nthat adaptively assigns higher weights to samples more suitable for the model\\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\\nimproves the performance of both DPO and SimPO, without relying on external\\nmodels or additional data.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-22T12:39:30Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10403v1\", \"title\": \"Satellite Federated Fine-Tuning for Foundation Models in Space Computing\\n  Power Networks\", \"summary\": \"Advancements in artificial intelligence (AI) and low-earth orbit (LEO)\\nsatellites have promoted the application of large remote sensing foundation\\nmodels for various downstream tasks. However, direct downloading of these\\nmodels for fine-tuning on the ground is impeded by privacy concerns and limited\\nbandwidth. Satellite federated learning (FL) offers a solution by enabling\\nmodel fine-tuning directly on-board satellites and aggregating model updates\\nwithout data downloading. Nevertheless, for large foundation models, the\\ncomputational capacity of satellites is insufficient to support effective\\non-board fine-tuning in traditional satellite FL frameworks. To address these\\nchallenges, we propose a satellite-ground collaborative federated fine-tuning\\nframework. The key of the framework lies in how to reasonably decompose and\\nallocate model components to alleviate insufficient on-board computation\\ncapabilities. During fine-tuning, satellites exchange intermediate results with\\nground stations or other satellites for forward propagation and back\\npropagation, which brings communication challenges due to the special\\ncommunication topology of space transmission networks, such as intermittent\\nsatellite-ground communication, short duration of satellite-ground\\ncommunication windows, and unstable inter-orbit inter-satellite links (ISLs).\\nTo reduce transmission delays, we further introduce tailored communication\\nstrategies that integrate both communication and computing resources.\\nSpecifically, we propose a parallel intra-orbit communication strategy, a\\ntopology-aware satellite-ground communication strategy, and a\\nlatency-minimalization inter-orbit communication strategy to reduce space\\ncommunication costs. Simulation results demonstrate significant reductions in\\ntraining time with improvements of approximately 33%.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.DC,cs.NI\", \"published\": \"2025-04-14T16:52:34Z\"}"}

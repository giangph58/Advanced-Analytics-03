{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17397v1\", \"title\": \"Fine-tune Smarter, Not Harder: Parameter-Efficient Fine-Tuning for\\n  Geospatial Foundation Models\", \"summary\": \"Earth observation (EO) is crucial for monitoring environmental changes,\\nresponding to disasters, and managing natural resources. In this context,\\nfoundation models facilitate remote sensing image analysis to retrieve relevant\\ngeoinformation accurately and efficiently. However, as these models grow in\\nsize, fine-tuning becomes increasingly challenging due to the associated\\ncomputational resources and costs, limiting their accessibility and\\nscalability. Furthermore, full fine-tuning can lead to forgetting pre-trained\\nfeatures and even degrade model generalization. To address this,\\nParameter-Efficient Fine-Tuning (PEFT) techniques offer a promising solution.\\nIn this paper, we conduct extensive experiments with various foundation model\\narchitectures and PEFT techniques to evaluate their effectiveness on five\\ndifferent EO datasets. Our results provide a comprehensive comparison, offering\\ninsights into when and how PEFT methods support the adaptation of pre-trained\\ngeospatial models. We demonstrate that PEFT techniques match or even exceed\\nfull fine-tuning performance and enhance model generalisation to unseen\\ngeographic regions, while reducing training time and memory requirements.\\nAdditional experiments investigate the effect of architecture choices such as\\nthe decoder type or the use of metadata, suggesting UNet decoders and\\nfine-tuning without metadata as the recommended configuration. We have\\nintegrated all evaluated foundation models and techniques into the open-source\\npackage TerraTorch to support quick, scalable, and cost-effective model\\nadaptation.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-24T09:37:02Z\"}"}

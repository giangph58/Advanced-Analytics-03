{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03189v1\", \"title\": \"Patterns and Mechanisms of Contrastive Activation Engineering\", \"summary\": \"Controlling the behavior of Large Language Models (LLMs) remains a\\nsignificant challenge due to their inherent complexity and opacity. While\\ntechniques like fine-tuning can modify model behavior, they typically require\\nextensive computational resources. Recent work has introduced a class of\\ncontrastive activation engineering (CAE) techniques as promising approaches for\\nsteering LLM outputs through targeted modifications to their internal\\nrepresentations. Applied at inference-time with zero cost, CAE has the\\npotential to introduce a new paradigm of flexible, task-specific LLM behavior\\ntuning. We analyze the performance of CAE in in-distribution,\\nout-of-distribution settings, evaluate drawbacks, and begin to develop\\ncomprehensive guidelines for its effective deployment. We find that 1. CAE is\\nonly reliably effective when applied to in-distribution contexts. 2. Increasing\\nthe number of samples used to generate steering vectors has diminishing returns\\nat around 80 samples. 3. Steering vectors are susceptible to adversarial inputs\\nthat reverses the behavior that is steered for. 4. Steering vectors harm the\\noverall model perplexity. 5. Larger models are more resistant to\\nsteering-induced degradation.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.HC\", \"published\": \"2025-05-06T05:15:12Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19516v1\", \"title\": \"Bullet: Boosting GPU Utilization for LLM Serving via Dynamic\\n  Spatial-Temporal Orchestration\", \"summary\": \"Modern LLM serving systems confront inefficient GPU utilization due to the\\nfundamental mismatch between compute-intensive prefill and memory-bound decode\\nphases. While current practices attempt to address this by organizing these\\nphases into hybrid batches, such solutions create an inefficient tradeoff that\\nsacrifices either throughput or latency, leaving substantial GPU resources\\nunderutilized. We identify two key root causes: 1) the prefill phase suffers\\nfrom suboptimal compute utilization due to wave quantization and attention\\nbottlenecks. 2) hybrid batches disproportionately prioritize latency over\\nthroughput, resulting in wasted compute and memory bandwidth. To mitigate the\\nissues, we present Bullet, a novel spatial-temporal orchestration system that\\neliminates these inefficiencies through precise phase coordination. Bullet\\nenables concurrent execution of prefill and decode phases, while dynamically\\nprovisioning GPU resources using real-time performance modeling. By integrating\\nSLO-aware scheduling and adaptive resource allocation, Bullet maximizes\\nutilization without compromising latency targets. Experimental evaluations on\\nreal-world workloads demonstrate that Bullet delivers 1.26x average throughput\\ngains (up to 1.55x) over state-of-the-arts, while consistently meeting latency\\nconstraints.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC\", \"published\": \"2025-04-28T06:26:21Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01757v1\", \"title\": \"KD$^{2}$M: An unifying framework for feature knowledge distillation\", \"summary\": \"Knowledge Distillation (KD) seeks to transfer the knowledge of a teacher,\\ntowards a student neural net. This process is often done by matching the\\nnetworks' predictions (i.e., their output), but, recently several works have\\nproposed to match the distributions of neural nets' activations (i.e., their\\nfeatures), a process known as \\\\emph{distribution matching}. In this paper, we\\npropose an unifying framework, Knowledge Distillation through Distribution\\nMatching (KD$^{2}$M), which formalizes this strategy. Our contributions are\\nthreefold. We i) provide an overview of distribution metrics used in\\ndistribution matching, ii) benchmark on computer vision datasets, and iii)\\nderive new theoretical results for KD.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.LG\", \"published\": \"2025-04-02T14:14:46Z\"}"}

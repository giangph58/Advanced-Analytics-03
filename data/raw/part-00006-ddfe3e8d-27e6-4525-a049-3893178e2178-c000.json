{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16761v1\", \"title\": \"Tri-FusionNet: Enhancing Image Description Generation with\\n  Transformer-based Fusion Network and Dual Attention Mechanism\", \"summary\": \"Image description generation is essential for accessibility and AI\\nunderstanding of visual content. Recent advancements in deep learning have\\nsignificantly improved natural language processing and computer vision. In this\\nwork, we propose Tri-FusionNet, a novel image description generation model that\\nintegrates transformer modules: a Vision Transformer (ViT) encoder module with\\ndual-attention mechanism, a Robustly Optimized BERT Approach (RoBERTa) decoder\\nmodule, and a Contrastive Language-Image Pre-Training (CLIP) integrating\\nmodule. The ViT encoder, enhanced with dual attention, focuses on relevant\\nspatial regions and linguistic context, improving image feature extraction. The\\nRoBERTa decoder is employed to generate precise textual descriptions. CLIP's\\nintegrating module aligns visual and textual data through contrastive learning,\\nensuring effective combination of both modalities. This fusion of ViT, RoBERTa,\\nand CLIP, along with dual attention, enables the model to produce more\\naccurate, contextually rich, and flexible descriptions. The proposed framework\\ndemonstrated competitive performance on the Flickr30k and Flickr8k datasets,\\nwith BLEU scores ranging from 0.767 to 0.456 and 0.784 to 0.479, CIDEr scores\\nof 1.679 and 1.483, METEOR scores of 0.478 and 0.358, and ROUGE-L scores of\\n0.567 and 0.789, respectively. On MS-COCO, the framework obtained BLEU scores\\nof 0.893 (B-1), 0.821 (B-2), 0.794 (B-3), and 0.725 (B-4). The results\\ndemonstrate the effectiveness of Tri-FusionNet in generating high-quality image\\ndescriptions.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-23T14:33:29Z\"}"}

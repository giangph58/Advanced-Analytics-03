{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10002v1\", \"title\": \"FLoRA: Sample-Efficient Preference-based RL via Low-Rank Style\\n  Adaptation of Reward Functions\", \"summary\": \"Preference-based reinforcement learning (PbRL) is a suitable approach for\\nstyle adaptation of pre-trained robotic behavior: adapting the robot's policy\\nto follow human user preferences while still being able to perform the original\\ntask. However, collecting preferences for the adaptation process in robotics is\\noften challenging and time-consuming. In this work we explore the adaptation of\\npre-trained robots in the low-preference-data regime. We show that, in this\\nregime, recent adaptation approaches suffer from catastrophic reward forgetting\\n(CRF), where the updated reward model overfits to the new preferences, leading\\nthe agent to become unable to perform the original task. To mitigate CRF, we\\npropose to enhance the original reward model with a small number of parameters\\n(low-rank matrices) responsible for modeling the preference adaptation. Our\\nevaluation shows that our method can efficiently and effectively adjust robotic\\nbehavior to human preferences across simulation benchmark tasks and multiple\\nreal-world robotic tasks.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.LG\", \"published\": \"2025-04-14T09:04:14Z\"}"}

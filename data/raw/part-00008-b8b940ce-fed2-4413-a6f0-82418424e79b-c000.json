{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01540v1\", \"title\": \"From Sm\\u00f8r-re-br\\u00f8d to Subwords: Training LLMs on Danish, One\\n  Morpheme at a Time\", \"summary\": \"The best performing transformer-based language models use subword\\ntokenization techniques, such as Byte-Pair-Encoding (BPE). However, these\\napproaches often overlook linguistic principles, such as morphological\\nsegmentation, which we believe is fundamental for understanding\\nlanguage-specific word structure. In this study, we leverage an annotated\\nDanish morphological dataset to train a semisupervised model for morphological\\nsegmentation, enabling the development of tokenizers optimized for Danish\\nmorphology. We evaluate four distinct tokenizers, including two custom\\nmorphological tokenizers, by analyzing their performance in morphologically\\nsegmenting Danish words. Additionally, we train two generative transformer\\nmodels, \\\\textit{CerebrasGPT-111M} and \\\\textit{LLaMA-3.2 1B}, using these\\ntokenizers and evaluate their downstream performance. Our findings reveal that\\nour custom-developed tokenizers substantially enhance morphological\\nsegmentation, achieving an F1 score of 58.84, compared to 39.28 achieved by a\\nDanish BPE tokenizer. In downstream tasks, models trained with our\\nmorphological tokenizers outperform those using BPE tokenizers across different\\nevaluation metrics. These results highlight that incorporating Danish\\nmorphological segmentation strategies into tokenizers leads to improved\\nperformance in generative transformer models on Danish language\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-02T09:26:02Z\"}"}

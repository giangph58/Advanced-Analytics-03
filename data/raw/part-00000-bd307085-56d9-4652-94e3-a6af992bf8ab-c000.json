{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14928v1\", \"title\": \"EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent\\n  Dialogue Framework\", \"summary\": \"Large language models (LLMs) increasingly serve as educational tools, yet\\nevaluating their teaching capabilities remains challenging due to the\\nresource-intensive, context-dependent, and methodologically complex nature of\\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\\nframework that efficiently assesses teaching capabilities through simulated\\ndynamic educational scenarios, featuring specialized agents for teaching,\\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\\nnot correlate linearly with model scale or general reasoning capabilities -\\nwith some smaller open-source models outperforming larger commercial\\ncounterparts in teaching contexts. This finding highlights a critical gap in\\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\\nanalysis and expert case studies, identifies distinct pedagogical strengths\\nemployed by top-performing models (e.g., sophisticated questioning strategies,\\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\\nour automated qualitative analysis of effective teaching behaviors, validating\\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\\nspecialized optimization beyond simple scaling, suggesting next-generation\\neducational AI prioritize targeted enhancement of specific pedagogical\\neffectiveness.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CE,cs.CL,cs.CY,cs.HC\", \"published\": \"2025-04-21T07:48:20Z\"}"}

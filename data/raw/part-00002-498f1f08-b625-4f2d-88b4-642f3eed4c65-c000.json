{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17520v1\", \"title\": \"Communication-Efficient Personalized Distributed Learning with Data and\\n  Node Heterogeneity\", \"summary\": \"To jointly tackle the challenges of data and node heterogeneity in\\ndecentralized learning, we propose a distributed strong lottery ticket\\nhypothesis (DSLTH), based on which a communication-efficient personalized\\nlearning algorithm is developed. In the proposed method, each local model is\\nrepresented as the Hadamard product of global real-valued parameters and a\\npersonalized binary mask for pruning. The local model is learned by updating\\nand fusing the personalized binary masks while the real-valued parameters are\\nfixed among different agents. To further reduce the complexity of hardware\\nimplementation, we incorporate a group sparse regularization term in the loss\\nfunction, enabling the learned local model to achieve structured sparsity.\\nThen, a binary mask aggregation algorithm is designed by introducing an\\nintermediate aggregation tensor and adding a personalized fine-tuning step in\\neach iteration, which constrains model updates towards the local data\\ndistribution. The proposed method effectively leverages the relativity among\\nagents while meeting personalized requirements in heterogeneous node\\nconditions. We also provide a theoretical proof for the DSLTH, establishing it\\nas the foundation of the proposed method. Numerical simulations confirm the\\nvalidity of the DSLTH and demonstrate the effectiveness of the proposed\\nalgorithm.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.DC,cs.MA\", \"published\": \"2025-04-24T13:02:54Z\"}"}

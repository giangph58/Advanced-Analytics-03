{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04575v1\", \"title\": \"Componential Prompt-Knowledge Alignment for Domain Incremental Learning\", \"summary\": \"Domain Incremental Learning (DIL) aims to learn from non-stationary data\\nstreams across domains while retaining and utilizing past knowledge. Although\\nprompt-based methods effectively store multi-domain knowledge in prompt\\nparameters and obtain advanced performance through cross-domain prompt fusion,\\nwe reveal an intrinsic limitation: component-wise misalignment between\\ndomain-specific prompts leads to conflicting knowledge integration and degraded\\npredictions. This arises from the random positioning of knowledge components\\nwithin prompts, where irrelevant component fusion introduces interference.To\\naddress this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a\\nnovel prompt-based DIL method that introduces component-aware prompt-knowledge\\nalignment during training, significantly improving both the learning and\\ninference capacity of the model. KA-Prompt operates in two phases: (1) Initial\\nComponential Structure Configuring, where a set of old prompts containing\\nknowledge relevant to the new domain are mined via greedy search, which is then\\nexploited to initialize new prompts to achieve reusable knowledge transfer and\\nestablish intrinsic alignment between new and old prompts. (2) Online Alignment\\nPreservation, which dynamically identifies the target old prompts and applies\\nadaptive componential consistency constraints as new prompts evolve. Extensive\\nexperiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.\\nOur source code is available at\\nhttps://github.com/zhoujiahuan1991/ICML2025-KA-Prompt\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG\", \"published\": \"2025-05-07T17:12:15Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01850v1\", \"title\": \"Code Red! On the Harmfulness of Applying Off-the-shelf Large Language\\n  Models to Programming Tasks\", \"summary\": \"Nowadays, developers increasingly rely on solutions powered by Large Language\\nModels (LLM) to assist them with their coding tasks. This makes it crucial to\\nalign these tools with human values to prevent malicious misuse. In this paper,\\nwe propose a comprehensive framework for assessing the potential harmfulness of\\nLLMs within the software engineering domain. We begin by developing a taxonomy\\nof potentially harmful software engineering scenarios and subsequently, create\\na dataset of prompts based on this taxonomy. To systematically assess the\\nresponses, we design and validate an automatic evaluator that classifies the\\noutputs of a variety of LLMs both open-source and closed-source models, as well\\nas general-purpose and code-specific LLMs. Furthermore, we investigate the\\nimpact of models size, architecture family, and alignment strategies on their\\ntendency to generate harmful content. The results show significant disparities\\nin the alignment of various LLMs for harmlessness. We find that some models and\\nmodel families, such as Openhermes, are more harmful than others and that\\ncode-specific models do not perform better than their general-purpose\\ncounterparts. Notably, some fine-tuned models perform significantly worse than\\ntheir base-models due to their design choices. On the other side, we find that\\nlarger models tend to be more helpful and are less likely to respond with\\nharmful information. These results highlight the importance of targeted\\nalignment strategies tailored to the unique challenges of software engineering\\ntasks and provide a foundation for future work in this critical area.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE,cs.AI\", \"published\": \"2025-04-02T16:00:14Z\"}"}

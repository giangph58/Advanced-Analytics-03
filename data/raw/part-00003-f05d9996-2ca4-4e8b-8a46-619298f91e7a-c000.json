{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04980v1\", \"title\": \"LVLM-MPC Collaboration for Autonomous Driving: A Safety-Aware and\\n  Task-Scalable Control Architecture\", \"summary\": \"This paper proposes a novel Large Vision-Language Model (LVLM) and Model\\nPredictive Control (MPC) integration framework that delivers both task\\nscalability and safety for Autonomous Driving (AD). LVLMs excel at high-level\\ntask planning across diverse driving scenarios. However, since these foundation\\nmodels are not specifically designed for driving and their reasoning is not\\nconsistent with the feasibility of low-level motion planning, concerns remain\\nregarding safety and smooth task switching. This paper integrates LVLMs with\\nMPC Builder, which automatically generates MPCs on demand, based on symbolic\\ntask commands generated by the LVLM, while ensuring optimality and safety. The\\ngenerated MPCs can strongly assist the execution or rejection of LVLM-driven\\ntask switching by providing feedback on the feasibility of the given tasks and\\ngenerating task-switching-aware MPCs. Our approach provides a safe, flexible,\\nand adaptable control framework, bridging the gap between cutting-edge\\nfoundation models and reliable vehicle operation. We demonstrate the\\neffectiveness of our approach through a simulation experiment, showing that our\\nsystem can safely and effectively handle highway driving while maintaining the\\nflexibility and adaptability of LVLMs.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.SY,eess.SY\", \"published\": \"2025-05-08T06:35:30Z\"}"}

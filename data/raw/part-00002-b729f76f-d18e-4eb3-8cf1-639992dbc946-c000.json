{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03473v1\", \"title\": \"Evaluation of LLMs on Long-tail Entity Linking in Historical Documents\", \"summary\": \"Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)\\napplications, enabling the disambiguation of entity mentions by linking them to\\ntheir corresponding entries in a reference knowledge base (KB). Thanks to their\\ndeep contextual understanding capabilities, LLMs offer a new perspective to\\ntackle EL, promising better results than traditional methods. Despite the\\nimpressive generalization capabilities of LLMs, linking less popular, long-tail\\nentities remains challenging as these entities are often underrepresented in\\ntraining data and knowledge bases. Furthermore, the long-tail EL task is an\\nunderstudied problem, and limited studies address it with LLMs. In the present\\nwork, we assess the performance of two popular LLMs, GPT and LLama3, in a\\nlong-tail entity linking scenario. Using MHERCL v0.1, a manually annotated\\nbenchmark of sentences from domain-specific historical texts, we quantitatively\\ncompare the performance of LLMs in identifying and linking entities to their\\ncorresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity\\nLinking and Relation Extraction framework. Our preliminary experiments reveal\\nthat LLMs perform encouragingly well in long-tail EL, indicating that this\\ntechnology can be a valuable adjunct in filling the gap between head and\\nlong-tail EL.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-05-06T12:25:15Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01752v1\", \"title\": \"A Two-Timescale Approach for Wireless Federated Learning with Parameter\\n  Freezing and Power Control\", \"summary\": \"Federated learning (FL) enables distributed devices to train a shared machine\\nlearning (ML) model collaboratively while protecting their data privacy.\\nHowever, the resource-limited mobile devices suffer from intensive\\ncomputation-and-communication costs of model parameters. In this paper, we\\nobserve the phenomenon that the model parameters tend to be stabilized long\\nbefore convergence during training process. Based on this observation, we\\npropose a two-timescale FL framework by joint optimization of freezing\\nstabilized parameters and controlling transmit power for the unstable\\nparameters to balance the energy consumption and convergence. First, we analyze\\nthe impact of model parameter freezing and unreliable transmission on the\\nconvergence rate. Next, we formulate a two-timescale optimization problem of\\nparameter freezing percentage and transmit power to minimize the model\\nconvergence error subject to the energy budget. To solve this problem, we\\ndecompose it into parallel sub-problems and decompose each sub-problem into two\\ndifferent timescales problems using the Lyapunov optimization method. The\\noptimal parameter freezing and power control strategies are derived in an\\nonline fashion. Experimental results demonstrate the superiority of the\\nproposed scheme compared with the benchmark schemes.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-02T14:05:45Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05017v1\", \"title\": \"Scalable Multi-Stage Influence Function for Large Language Models via\\n  Eigenvalue-Corrected Kronecker-Factored Parameterization\", \"summary\": \"Pre-trained large language models (LLMs) are commonly fine-tuned to adapt to\\ndownstream tasks. Since the majority of knowledge is acquired during\\npre-training, attributing the predictions of fine-tuned LLMs to their\\npre-training data may provide valuable insights. Influence functions have been\\nproposed as a means to explain model predictions based on training data.\\nHowever, existing approaches fail to compute ``multi-stage'' influence and lack\\nscalability to billion-scale LLMs.\\n  In this paper, we propose the multi-stage influence function to attribute the\\ndownstream predictions of fine-tuned LLMs to pre-training data under the\\nfull-parameter fine-tuning paradigm. To enhance the efficiency and practicality\\nof our multi-stage influence function, we leverage Eigenvalue-corrected\\nKronecker-Factored (EK-FAC) parameterization for efficient approximation.\\nEmpirical results validate the superior scalability of EK-FAC approximation and\\nthe effectiveness of our multi-stage influence function. Additionally, case\\nstudies on a real-world LLM, dolly-v2-3b, demonstrate its interpretive power,\\nwith exemplars illustrating insights provided by multi-stage influence\\nestimates. Our code is public at\\nhttps://github.com/colored-dye/multi_stage_influence_function.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-05-08T07:43:44Z\"}"}

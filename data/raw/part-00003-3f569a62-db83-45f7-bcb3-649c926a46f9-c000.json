{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16005v1\", \"title\": \"CAPO: Cost-Aware Prompt Optimization\", \"summary\": \"Large language models (LLMs) have revolutionized natural language processing\\nby solving a wide range of tasks simply guided by a prompt. Yet their\\nperformance is highly sensitive to prompt formulation. While automated prompt\\noptimization addresses this challenge by finding optimal prompts, current\\nmethods require a substantial number of LLM calls and input tokens, making\\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\\nOptimization), an algorithm that enhances prompt optimization efficiency by\\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\\noperators, incorporating racing to save evaluations and multi-objective\\noptimization to balance performance with prompt length. It jointly optimizes\\ninstructions and few-shot examples while leveraging task descriptions for\\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\\nbetter performances already with smaller budgets, saves evaluations through\\nracing, and decreases average prompt length via a length penalty, making it\\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\\noutperforms its competitors and generally remains robust to initial prompts.\\nCAPO represents an important step toward making prompt optimization more\\npowerful and accessible by improving cost-efficiency.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.NE,stat.ML\", \"published\": \"2025-04-22T16:14:31Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05215v1\", \"title\": \"Diffusion Model Quantization: A Review\", \"summary\": \"Recent success of large text-to-image models has empirically underscored the\\nexceptional performance of diffusion models in generative tasks. To facilitate\\ntheir efficient deployment on resource-constrained edge devices, model\\nquantization has emerged as a pivotal technique for both compression and\\nacceleration. This survey offers a thorough review of the latest advancements\\nin diffusion model quantization, encapsulating and analyzing the current state\\nof the art in this rapidly advancing domain. First, we provide an overview of\\nthe key challenges encountered in the quantization of diffusion models,\\nincluding those based on U-Net architectures and Diffusion Transformers (DiT).\\nWe then present a comprehensive taxonomy of prevalent quantization techniques,\\nengaging in an in-depth discussion of their underlying principles.\\nSubsequently, we perform a meticulous analysis of representative diffusion\\nmodel quantization schemes from both qualitative and quantitative perspectives.\\nFrom a quantitative standpoint, we rigorously benchmark a variety of methods\\nusing widely recognized datasets, delivering an extensive evaluation of the\\nmost recent and impactful research in the field. From a qualitative standpoint,\\nwe categorize and synthesize the effects of quantization errors, elucidating\\nthese impacts through both visual analysis and trajectory examination. In\\nconclusion, we outline prospective avenues for future research, proposing novel\\ndirections for the quantization of generative models in practical applications.\\nThe list of related papers, corresponding codes, pre-trained models and\\ncomparison results are publicly available at the survey project homepage\\nhttps://github.com/TaylorJocelyn/Diffusion-Model-Quantization.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-08T13:09:34Z\"}"}

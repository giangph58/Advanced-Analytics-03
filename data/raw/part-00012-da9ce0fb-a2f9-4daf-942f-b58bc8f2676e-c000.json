{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04831v1\", \"title\": \"SMF: Template-free and Rig-free Animation Transfer using Kinetic Codes\", \"summary\": \"Animation retargeting involves applying a sparse motion description (e.g.,\\n2D/3D keypoint sequences) to a given character mesh to produce a semantically\\nplausible and temporally coherent full-body motion. Existing approaches come\\nwith a mix of restrictions - they require annotated training data, assume\\naccess to template-based shape priors or artist-designed deformation rigs,\\nsuffer from limited generalization to unseen motion and/or shapes, or exhibit\\nmotion jitter. We propose Self-supervised Motion Fields (SMF) as a\\nself-supervised framework that can be robustly trained with sparse motion\\nrepresentations, without requiring dataset specific annotations, templates, or\\nrigs. At the heart of our method are Kinetic Codes, a novel autoencoder-based\\nsparse motion encoding, that exposes a semantically rich latent space\\nsimplifying large-scale training. Our architecture comprises dedicated spatial\\nand temporal gradient predictors, which are trained end-to-end. The resultant\\nnetwork, regularized by the Kinetic Codes's latent space, has good\\ngeneralization across shapes and motions. We evaluated our method on unseen\\nmotion sampled from AMASS, D4D, Mixamo, and raw monocular video for animation\\ntransfer on various characters with varying shapes and topology. We report a\\nnew SoTA on the AMASS dataset in the context of generalization to unseen\\nmotion. Project webpage at https://motionfields.github.io/\", \"main_category\": \"cs.GR\", \"categories\": \"cs.GR,cs.CV\", \"published\": \"2025-04-07T08:42:52Z\"}"}

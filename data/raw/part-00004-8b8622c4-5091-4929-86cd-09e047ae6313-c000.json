{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00389v1\", \"title\": \"CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a\\n  Single Forward Pass\", \"summary\": \"As a fundamental task in Information Retrieval and Computational Linguistics,\\nsentence representation has profound implications for a wide range of practical\\napplications such as text clustering, content analysis, question-answering\\nsystems, and web search. Recent advances in pre-trained language models (PLMs)\\nhave driven remarkable progress in this field, particularly through\\nunsupervised embedding derivation methods centered on discriminative PLMs like\\nBERT. However, due to time and computational constraints, few efforts have\\nattempted to integrate unsupervised sentence representation with generative\\nPLMs, which typically possess much larger parameter sizes. Given that\\nstate-of-the-art models in both academia and industry are predominantly based\\non generative architectures, there is a pressing need for an efficient\\nunsupervised text representation framework tailored to decoder-only PLMs. To\\naddress this concern, we propose CSE-SFP, an innovative method that exploits\\nthe structural characteristics of generative models. Compared to existing\\nstrategies, CSE-SFP requires only a single forward pass to perform effective\\nunsupervised contrastive learning. Rigorous experimentation demonstrates that\\nCSE-SFP not only produces higher-quality embeddings but also significantly\\nreduces both training time and memory consumption. Furthermore, we introduce\\ntwo ratio metrics that jointly assess alignment and uniformity, thereby\\nproviding a more robust means for evaluating the semantic spatial properties of\\nencoding models.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-05-01T08:27:14Z\"}"}

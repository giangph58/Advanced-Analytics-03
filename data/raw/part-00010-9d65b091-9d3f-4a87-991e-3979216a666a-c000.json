{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21793v1\", \"title\": \"Reconciling Discrete-Time Mixed Policies and Continuous-Time Relaxed\\n  Controls in Reinforcement Learning and Stochastic Control\", \"summary\": \"Reinforcement learning (RL) is currently one of the most popular methods,\\nwith breakthrough results in a variety of fields. The framework relies on the\\nconcept of Markov decision process (MDP), which corresponds to a discrete time\\noptimal control problem. In the RL literature, such problems are usually\\nformulated with mixed policies, from which a random action is sampled at each\\ntime step. Recently, the optimal control community has studied continuous-time\\nversions of RL algorithms, replacing MDPs with mixed policies by continuous\\ntime stochastic processes with relaxed controls. In this work, we rigorously\\nconnect the two problems: we prove the strong convergence of the former towards\\nthe latter when the time discretization goes to $0$.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC\", \"published\": \"2025-04-30T16:50:52Z\"}"}

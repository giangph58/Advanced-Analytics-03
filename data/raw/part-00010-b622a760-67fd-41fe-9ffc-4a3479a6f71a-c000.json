{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05803v1\", \"title\": \"SE4Lip: Speech-Lip Encoder for Talking Head Synthesis to Solve\\n  Phoneme-Viseme Alignment Ambiguity\", \"summary\": \"Speech-driven talking head synthesis tasks commonly use general acoustic\\nfeatures (such as HuBERT and DeepSpeech) as guided speech features. However, we\\ndiscovered that these features suffer from phoneme-viseme alignment ambiguity,\\nwhich refers to the uncertainty and imprecision in matching phonemes (speech)\\nwith visemes (lip). To address this issue, we propose the Speech Encoder for\\nLip (SE4Lip) to encode lip features from speech directly, aligning speech and\\nlip features in the joint embedding space by a cross-modal alignment framework.\\nThe STFT spectrogram with the GRU-based model is designed in SE4Lip to preserve\\nthe fine-grained speech features. Experimental results show that SE4Lip\\nachieves state-of-the-art performance in both NeRF and 3DGS rendering models.\\nIts lip sync accuracy improves by 13.7% and 14.2% compared to the best baseline\\nand produces results close to the ground truth videos.\", \"main_category\": \"cs.GR\", \"categories\": \"cs.GR,cs.CV\", \"published\": \"2025-04-08T08:35:59Z\"}"}

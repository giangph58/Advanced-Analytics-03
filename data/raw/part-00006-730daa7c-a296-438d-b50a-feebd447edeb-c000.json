{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20006v1\", \"title\": \"Chatbot Arena Meets Nuggets: Towards Explanations and Diagnostics in the\\n  Evaluation of LLM Responses\", \"summary\": \"Battles, or side-by-side comparisons in so called arenas that elicit human\\npreferences, have emerged as a popular approach to assessing the output quality\\nof LLMs. Recently, this idea has been extended to retrieval-augmented\\ngeneration (RAG) systems. While undoubtedly representing an advance in\\nevaluation, battles have at least two drawbacks, particularly in the context of\\ncomplex information-seeking queries: they are neither explanatory nor\\ndiagnostic. Recently, the nugget evaluation methodology has emerged as a\\npromising approach to evaluate the quality of RAG answers. Nuggets decompose\\nlong-form LLM-generated answers into atomic facts, highlighting important\\npieces of information necessary in a \\\"good\\\" response. In this work, we apply\\nour AutoNuggetizer framework to analyze data from roughly 7K Search Arena\\nbattles provided by LMArena in a fully automatic manner. Our results show a\\nsignificant correlation between nugget scores and human preferences, showcasing\\npromise in our approach to explainable and diagnostic system evaluations.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR\", \"published\": \"2025-04-28T17:24:36Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19718v1\", \"title\": \"Pixels2Points: Fusing 2D and 3D Features for Facial Skin Segmentation\", \"summary\": \"Face registration deforms a template mesh to closely fit a 3D face scan, the\\nquality of which commonly degrades in non-skin regions (e.g., hair, beard,\\naccessories), because the optimized template-to-scan distance pulls the\\ntemplate mesh towards the noisy scan surface. Improving registration quality\\nrequires a clean separation of skin and non-skin regions on the scan mesh.\\nExisting image-based (2D) or scan-based (3D) segmentation methods however\\nperform poorly. Image-based segmentation outputs multi-view inconsistent masks,\\nand they cannot account for scan inaccuracies or scan-image misalignment, while\\nscan-based methods suffer from lower spatial resolution compared to images. In\\nthis work, we introduce a novel method that accurately separates skin from\\nnon-skin geometry on 3D human head scans. For this, our method extracts\\nfeatures from multi-view images using a frozen image foundation model and\\naggregates these features in 3D. These lifted 2D features are then fused with\\n3D geometric features extracted from the scan mesh, to then predict a\\nsegmentation mask directly on the scan mesh. We show that our segmentations\\nimprove the registration accuracy over pure 2D or 3D segmentation methods by\\n8.89% and 14.3%, respectively. Although trained only on synthetic data, our\\nmodel generalizes well to real data.\", \"main_category\": \"cs.GR\", \"categories\": \"cs.GR,cs.CV\", \"published\": \"2025-04-28T12:13:12Z\"}"}

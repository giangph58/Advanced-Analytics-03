{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11997v1\", \"title\": \"A Computationally Efficient Algorithm for Infinite-Horizon\\n  Average-Reward Linear MDPs\", \"summary\": \"We study reinforcement learning in infinite-horizon average-reward settings\\nwith linear MDPs. Previous work addresses this problem by approximating the\\naverage-reward setting by discounted setting and employing a value\\niteration-based algorithm that uses clipping to constrain the span of the value\\nfunction for improved statistical efficiency. However, the clipping procedure\\nrequires computing the minimum of the value function over the entire state\\nspace, which is prohibitive since the state space in linear MDP setting can be\\nlarge or even infinite. In this paper, we introduce a value iteration method\\nwith efficient clipping operation that only requires computing the minimum of\\nvalue functions over the set of states visited by the algorithm. Our algorithm\\nenjoys the same regret bound as the previous work while being computationally\\nefficient, with computational complexity that is independent of the size of the\\nstate space.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-16T11:47:41Z\"}"}

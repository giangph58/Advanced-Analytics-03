{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20437v1\", \"title\": \"GaLore 2: Large-Scale LLM Pre-Training by Gradient Low-Rank Projection\", \"summary\": \"Large language models (LLMs) have revolutionized natural language\\nunderstanding and generation but face significant memory bottlenecks during\\ntraining. GaLore, Gradient Low-Rank Projection, addresses this issue by\\nleveraging the inherent low-rank structure of weight gradients, enabling\\nsubstantial memory savings without sacrificing performance. Recent works\\nfurther extend GaLore from various aspects, including low-bit quantization and\\nhigher-order tensor structures. However, there are several remaining challenges\\nfor GaLore, such as the computational overhead of SVD for subspace updates and\\nthe integration with state-of-the-art training parallelization strategies\\n(e.g., FSDP). In this paper, we present GaLore 2, an efficient and scalable\\nGaLore framework that addresses these challenges and incorporates recent\\nadvancements. In addition, we demonstrate the scalability of GaLore 2 by\\npre-training Llama 7B from scratch using up to 500 billion training tokens,\\nhighlighting its potential impact on real LLM pre-training scenarios.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-29T05:27:02Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04147v1\", \"title\": \"R^3-VQA: \\\"Read the Room\\\" by Video Social Reasoning\", \"summary\": \"\\\"Read the room\\\" is a significant social reasoning capability in human daily\\nlife. Humans can infer others' mental states from subtle social cues. Previous\\nsocial reasoning tasks and datasets lack complexity (e.g., simple scenes, basic\\ninteractions, incomplete mental state variables, single-step reasoning, etc.)\\nand fall far short of the challenges present in real-life social interactions.\\nIn this paper, we contribute a valuable, high-quality, and comprehensive video\\ndataset named R^3-VQA with precise and fine-grained annotations of social\\nevents and mental states (i.e., belief, intent, desire, and emotion) as well as\\ncorresponding social causal chains in complex social scenarios. Moreover, we\\ninclude human-annotated and model-generated QAs. Our task R^3-VQA includes\\nthree aspects: Social Event Understanding, Mental State Estimation, and Social\\nCausal Reasoning. As a benchmark, we comprehensively evaluate the social\\nreasoning capabilities and consistencies of current state-of-the-art large\\nvision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs\\nare still far from human-level consistent social reasoning in complex social\\nscenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on\\nsocial reasoning tasks. We provide some of our dataset and codes in\\nsupplementary material and will release our full dataset and codes upon\\nacceptance.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-05-07T05:55:45Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16585v1\", \"title\": \"Enhancing Variable Selection in Large-scale Logistic Regression:\\n  Leveraging Manual Labeling with Beneficial Noise\", \"summary\": \"In large-scale supervised learning, penalized logistic regression (PLR)\\neffectively addresses the overfitting problem by introducing regularization\\nterms yet its performance still depends on efficient variable selection\\nstrategies. This paper theoretically demonstrates that label noise stemming\\nfrom manual labeling, which is solely related to classification difficulty,\\nrepresents a type of beneficial noise for variable selection in PLR. This\\nbenefit is reflected in a more accurate estimation of the selected non-zero\\ncoefficients when compared with the case where only truth labels are used.\\nUnder large-scale settings, the sample size for PLR can become very large,\\nmaking it infeasible to store on a single machine. In such cases, distributed\\ncomputing methods are required to handle PLR model with manual labeling. This\\npaper presents a partition-insensitive parallel algorithm founded on the ADMM\\n(alternating direction method of multipliers) algorithm to address PLR by\\nincorporating manual labeling. The partition insensitivity of the proposed\\nalgorithm refers to the fact that the solutions obtained by the algorithm will\\nnot change with the distributed storage of data. In addition, the algorithm has\\nglobal convergence and a sublinear convergence rate. Experimental results\\nindicate that, as compared with traditional variable selection classification\\ntechniques, the PLR with manually-labeled noisy data achieves higher estimation\\nand classification accuracy across multiple large-scale datasets.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,stat.CO,stat.ML\", \"published\": \"2025-04-23T10:05:54Z\"}"}

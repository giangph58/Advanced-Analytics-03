{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06011v1\", \"title\": \"Llama-3-Nanda-10B-Chat: An Open Generative Large Language Model for\\n  Hindi\", \"summary\": \"Developing high-quality large language models (LLMs) for moderately resourced\\nlanguages presents unique challenges in data availability, model adaptation,\\nand evaluation. We introduce Llama-3-Nanda-10B-Chat, or Nanda for short, a\\nstate-of-the-art Hindi-centric instruction-tuned generative LLM, designed to\\npush the boundaries of open-source Hindi language models. Built upon\\nLlama-3-8B, Nanda incorporates continuous pre-training with expanded\\ntransformer blocks, leveraging the Llama Pro methodology. A key challenge was\\nthe limited availability of high-quality Hindi text data; we addressed this\\nthrough rigorous data curation, augmentation, and strategic bilingual training,\\nbalancing Hindi and English corpora to optimize cross-linguistic knowledge\\ntransfer. With 10 billion parameters, Nanda stands among the top-performing\\nopen-source Hindi and multilingual models of similar scale, demonstrating\\nsignificant advantages over many existing models. We provide an in-depth\\ndiscussion of training strategies, fine-tuning techniques, safety alignment,\\nand evaluation metrics, demonstrating how these approaches enabled Nanda to\\nachieve state-of-the-art results. By open-sourcing Nanda, we aim to advance\\nresearch in Hindi LLMs and support a wide range of real-world applications\\nacross academia, industry, and public services.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-08T13:16:54Z\"}"}

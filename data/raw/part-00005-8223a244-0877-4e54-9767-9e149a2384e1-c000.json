{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04341v1\", \"title\": \"PAC-Bayesian risk bounds for fully connected deep neural network with\\n  Gaussian priors\", \"summary\": \"Deep neural networks (DNNs) have emerged as a powerful methodology with\\nsignificant practical successes in fields such as computer vision and natural\\nlanguage processing. Recent works have demonstrated that sparsely connected\\nDNNs with carefully designed architectures can achieve minimax estimation rates\\nunder classical smoothness assumptions. However, subsequent studies revealed\\nthat simple fully connected DNNs can achieve comparable convergence rates,\\nchallenging the necessity of sparsity. Theoretical advances in Bayesian neural\\nnetworks (BNNs) have been more fragmented. Much of those work has concentrated\\non sparse networks, leaving the theoretical properties of fully connected BNNs\\nunderexplored. In this paper, we address this gap by investigating fully\\nconnected Bayesian DNNs with Gaussian prior using PAC-Bayes bounds. We\\nestablish upper bounds on the prediction risk for a probabilistic deep neural\\nnetwork method, showing that these bounds match (up to logarithmic factors) the\\nminimax-optimal rates in Besov space, for both nonparametric regression and\\nbinary classification with logistic loss. Importantly, our results hold for a\\nbroad class of practical activation functions that are Lipschitz continuous.\", \"main_category\": \"math.ST\", \"categories\": \"math.ST,stat.ML,stat.TH\", \"published\": \"2025-05-07T11:42:18Z\"}"}

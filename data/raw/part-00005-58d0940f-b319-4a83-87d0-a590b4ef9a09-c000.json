{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06791v1\", \"title\": \"Beware of \\\"Explanations\\\" of AI\", \"summary\": \"Understanding the decisions made and actions taken by increasingly complex AI\\nsystem remains a key challenge. This has led to an expanding field of research\\nin explainable artificial intelligence (XAI), highlighting the potential of\\nexplanations to enhance trust, support adoption, and meet regulatory standards.\\nHowever, the question of what constitutes a \\\"good\\\" explanation is dependent on\\nthe goals, stakeholders, and context. At a high level, psychological insights\\nsuch as the concept of mental model alignment can offer guidance, but success\\nin practice is challenging due to social and technical factors. As a result of\\nthis ill-defined nature of the problem, explanations can be of poor quality\\n(e.g. unfaithful, irrelevant, or incoherent), potentially leading to\\nsubstantial risks. Instead of fostering trust and safety, poorly designed\\nexplanations can actually cause harm, including wrong decisions, privacy\\nviolations, manipulation, and even reduced AI adoption. Therefore, we caution\\nstakeholders to beware of explanations of AI: while they can be vital, they are\\nnot automatically a remedy for transparency or responsible AI adoption, and\\ntheir misuse or limitations can exacerbate harm. Attention to these caveats can\\nhelp guide future research to improve the quality and impact of AI\\nexplanations.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-09T11:31:08Z\"}"}

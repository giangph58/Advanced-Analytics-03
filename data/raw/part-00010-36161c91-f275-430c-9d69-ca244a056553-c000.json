{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04999v1\", \"title\": \"CLAM: Continuous Latent Action Models for Robot Learning from Unlabeled\\n  Demonstrations\", \"summary\": \"Learning robot policies using imitation learning requires collecting large\\namounts of costly action-labeled expert demonstrations, which fundamentally\\nlimits the scale of training data. A promising approach to address this\\nbottleneck is to harness the abundance of unlabeled observations-e.g., from\\nvideo demonstrations-to learn latent action labels in an unsupervised way.\\nHowever, we find that existing methods struggle when applied to complex robot\\ntasks requiring fine-grained motions. We design continuous latent action models\\n(CLAM) which incorporate two key ingredients we find necessary for learning to\\nsolve complex continuous control tasks from unlabeled observation data: (a)\\nusing continuous latent action labels instead of discrete representations, and\\n(b) jointly training an action decoder to ensure that the latent action space\\ncan be easily grounded to real actions with relatively few labeled examples.\\nImportantly, the labeled examples can be collected from non-optimal play data,\\nenabling CLAM to learn performant policies without access to any action-labeled\\nexpert data. We demonstrate on continuous control benchmarks in DMControl\\n(locomotion) and MetaWorld (manipulation), as well as on a real WidowX robot\\narm that CLAM significantly outperforms prior state-of-the-art methods,\\nremarkably with a 2-3x improvement in task success rate compared to the best\\nbaseline. Videos and code can be found at clamrobot.github.io.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.LG\", \"published\": \"2025-05-08T07:07:58Z\"}"}

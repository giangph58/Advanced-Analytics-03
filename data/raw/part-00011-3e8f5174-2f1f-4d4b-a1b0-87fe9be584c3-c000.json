{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07867v1\", \"title\": \"SAMJAM: Zero-Shot Video Scene Graph Generation for Egocentric Kitchen\\n  Videos\", \"summary\": \"Video Scene Graph Generation (VidSGG) is an important topic in understanding\\ndynamic kitchen environments. Current models for VidSGG require extensive\\ntraining to produce scene graphs. Recently, Vision Language Models (VLM) and\\nVision Foundation Models (VFM) have demonstrated impressive zero-shot\\ncapabilities in a variety of tasks. However, VLMs like Gemini struggle with the\\ndynamics for VidSGG, failing to maintain stable object identities across\\nframes. To overcome this limitation, we propose SAMJAM, a zero-shot pipeline\\nthat combines SAM2's temporal tracking with Gemini's semantic understanding.\\nSAM2 also improves upon Gemini's object grounding by producing more accurate\\nbounding boxes. In our method, we first prompt Gemini to generate a frame-level\\nscene graph. Then, we employ a matching algorithm to map each object in the\\nscene graph with a SAM2-generated or SAM2-propagated mask, producing a\\ntemporally-consistent scene graph in dynamic environments. Finally, we repeat\\nthis process again in each of the following frames. We empirically demonstrate\\nthat SAMJAM outperforms Gemini by 8.33% in mean recall on the EPIC-KITCHENS and\\nEPIC-KITCHENS-100 datasets.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-10T15:43:10Z\"}"}

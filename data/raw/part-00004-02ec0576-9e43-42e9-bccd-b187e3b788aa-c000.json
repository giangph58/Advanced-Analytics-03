{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05214v1\", \"title\": \"Post-Training Language Models for Continual Relation Extraction\", \"summary\": \"Real-world data, such as news articles, social media posts, and chatbot\\nconversations, is inherently dynamic and non-stationary, presenting significant\\nchallenges for constructing real-time structured representations through\\nknowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG\\ncreation, often struggles to adapt to evolving data when traditional models\\nrely on static, outdated datasets. Continual Relation Extraction (CRE) methods\\ntackle this issue by incrementally learning new relations while preserving\\npreviously acquired knowledge. This study investigates the application of\\npre-trained language models (PLMs), specifically large language models (LLMs),\\nto CRE, with a focus on leveraging memory replay to address catastrophic\\nforgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and\\nencoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets.\\nTask-incremental fine-tuning of LLMs demonstrates superior performance over\\nearlier approaches using encoder-only models like BERT on TACRED, excelling in\\nseen-task accuracy and overall performance (measured by whole and average\\naccuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel\\nare similarly promising, achieving second place in whole and average accuracy\\nmetrics. This work underscores critical factors in knowledge transfer, language\\nmodel architecture, and KG completeness, advancing CRE with LLMs and memory\\nreplay for dynamic, real-time relation extraction.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-07T16:01:22Z\"}"}

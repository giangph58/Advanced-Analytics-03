{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00603v1\", \"title\": \"Can LLMs Help Improve Analogical Reasoning For Strategic Decisions?\\n  Experimental Evidence from Humans and GPT-4\", \"summary\": \"This study investigates whether large language models, specifically GPT4, can\\nmatch human capabilities in analogical reasoning within strategic decision\\nmaking contexts. Using a novel experimental design involving source to target\\nmatching, we find that GPT4 achieves high recall by retrieving all plausible\\nanalogies but suffers from low precision, frequently applying incorrect\\nanalogies based on superficial similarities. In contrast, human participants\\nexhibit high precision but low recall, selecting fewer analogies yet with\\nstronger causal alignment. These findings advance theory by identifying\\nmatching, the evaluative phase of analogical reasoning, as a distinct step that\\nrequires accurate causal mapping beyond simple retrieval. While current LLMs\\nare proficient in generating candidate analogies, humans maintain a comparative\\nadvantage in recognizing deep structural similarities across domains. Error\\nanalysis reveals that AI errors arise from surface level matching, whereas\\nhuman errors stem from misinterpretations of causal structure. Taken together,\\nthe results suggest a productive division of labor in AI assisted\\norganizational decision making where LLMs may serve as broad analogy\\ngenerators, while humans act as critical evaluators, applying the most\\ncontextually appropriate analogies to strategic problems.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.HC\", \"published\": \"2025-05-01T15:35:01Z\"}"}

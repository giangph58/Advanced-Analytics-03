{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24129v1\", \"title\": \"It's a (Blind) Match! Towards Vision-Language Correspondence without\\n  Parallel Data\", \"summary\": \"The platonic representation hypothesis suggests that vision and language\\nembeddings become more homogeneous as model and dataset sizes increase. In\\nparticular, pairwise distances within each modality become more similar. This\\nsuggests that as foundation models mature, it may become possible to match\\nvision and language embeddings in a fully unsupervised fashion, i.e. without\\nparallel data. We present the first feasibility study, and investigate\\nconformity of existing vision and language foundation models in the context of\\nunsupervised, or \\\"blind\\\", matching. First, we formulate unsupervised matching\\nas a quadratic assignment problem and introduce a novel heuristic that\\noutperforms previous solvers. We also develop a technique to find optimal\\nmatching problems, for which a non-trivial match is very likely. Second, we\\nconduct an extensive study deploying a range of vision and language models on\\nfour datasets. Our analysis reveals that for many problem instances, vision and\\nlanguage representations can be indeed matched without supervision. This\\nfinding opens up the exciting possibility of embedding semantic knowledge into\\nother modalities virtually annotation-free. As a proof of concept, we showcase\\nan unsupervised classifier, which achieves non-trivial classification accuracy\\nwithout any image-text annotation.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG\", \"published\": \"2025-03-31T14:14:25Z\"}"}

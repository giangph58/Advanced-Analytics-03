{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15163v1\", \"title\": \"Survey of Loss Augmented Knowledge Tracing\", \"summary\": \"The training of artificial neural networks is heavily dependent on the\\ncareful selection of an appropriate loss function. While commonly used loss\\nfunctions, such as cross-entropy and mean squared error (MSE), generally\\nsuffice for a broad range of tasks, challenges often emerge due to limitations\\nin data quality or inefficiencies within the learning process. In such\\ncircumstances, the integration of supplementary terms into the loss function\\ncan serve to address these challenges, enhancing both model performance and\\nrobustness. Two prominent techniques, loss regularization and contrastive\\nlearning, have been identified as effective strategies for augmenting the\\ncapacity of loss functions in artificial neural networks.\\n  Knowledge tracing is a compelling area of research that leverages predictive\\nartificial intelligence to facilitate the automation of personalized and\\nefficient educational experiences for students. In this paper, we provide a\\ncomprehensive review of the deep learning-based knowledge tracing (DKT)\\nalgorithms trained using advanced loss functions and discuss their improvements\\nover prior techniques. We discuss contrastive knowledge tracing algorithms,\\nsuch as Bi-CLKT, CL4KT, SP-CLKT, CoSKT, and prediction-consistent DKT,\\nproviding performance benchmarks and insights into real-world deployment\\nchallenges. The survey concludes with future research directions, including\\nhybrid loss strategies and context-aware modeling.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-21T15:09:40Z\"}"}

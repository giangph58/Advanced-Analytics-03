{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14966v1\", \"title\": \"SLO-Aware Scheduling for Large Language Model Inferences\", \"summary\": \"Large language models (LLMs) have revolutionized applications such as code\\ncompletion, chatbots, and online classification. To elevate user experiences,\\nservice level objectives (SLOs) serve as crucial benchmarks for assessing\\ninference services capabilities. In practice, an inference service processes\\nmultiple types of tasks, each with its own distinct SLO. To ensure satisfactory\\nuser experiences, each request's distinct SLOs should be considered in\\nscheduling. However, existing designs lack this consideration, leading to\\ninsufficient hardware utility and suboptimal performance.\\n  This paper analyzes scenarios to process tasks with varying SLOs, and\\nintroduces a simulated annealing-based scheduler to decide request priority\\nsequence based on a request's SLO, input lengths, and possible output lengths.\\nAs the first specialized scheduler for multi-SLO scenarios, this work improves\\nSLO attainment by up to 5x and reduces average latency by 31.6% on\\nPython-Code-23k-ShareGPT and ShareGPT_Vicuna_unfiltered datasets, compared to\\ncurrent state-of-the-art framework vLLM and a new framework LMDeploy.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC\", \"published\": \"2025-04-21T08:48:48Z\"}"}

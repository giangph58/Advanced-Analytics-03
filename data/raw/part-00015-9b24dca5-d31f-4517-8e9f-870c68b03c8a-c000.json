{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03174v1\", \"title\": \"Automated Data Curation Using GPS & NLP to Generate Instruction-Action\\n  Pairs for Autonomous Vehicle Vision-Language Navigation Datasets\", \"summary\": \"Instruction-Action (IA) data pairs are valuable for training robotic systems,\\nespecially autonomous vehicles (AVs), but having humans manually annotate this\\ndata is costly and time-inefficient. This paper explores the potential of using\\nmobile application Global Positioning System (GPS) references and Natural\\nLanguage Processing (NLP) to automatically generate large volumes of IA\\ncommands and responses without having a human generate or retroactively tag the\\ndata. In our pilot data collection, by driving to various destinations and\\ncollecting voice instructions from GPS applications, we demonstrate a means to\\ncollect and categorize the diverse sets of instructions, further accompanied by\\nvideo data to form complete vision-language-action triads. We provide details\\non our completely automated data collection prototype system, ADVLAT-Engine. We\\ncharacterize collected GPS voice instructions into eight different\\nclassifications, highlighting the breadth of commands and referentialities\\navailable for curation from freely available mobile applications. Through\\nresearch and exploration into the automation of IA data pairs using GPS\\nreferences, the potential to increase the speed and volume at which\\nhigh-quality IA datasets are created, while minimizing cost, can pave the way\\nfor robust vision-language-action (VLA) models to serve tasks in\\nvision-language navigation (VLN) and human-interactive autonomous systems.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.CV,cs.LG\", \"published\": \"2025-05-06T04:38:41Z\"}"}

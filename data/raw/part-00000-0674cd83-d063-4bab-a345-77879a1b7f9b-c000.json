{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23877v1\", \"title\": \"ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos\", \"summary\": \"Many recent advances in robotic manipulation have come through imitation\\nlearning, yet these rely largely on mimicking a particularly hard-to-acquire\\nform of demonstrations: those collected on the same robot in the same room with\\nthe same objects as the trained policy must handle at test time. In contrast,\\nlarge pre-recorded human video datasets demonstrating manipulation skills\\nin-the-wild already exist, which contain valuable information for robots. Is it\\npossible to distill a repository of useful robotic skill policies out of such\\ndata without any additional requirements on robot-specific demonstrations or\\nexploration? We present the first such system ZeroMimic, that generates\\nimmediately deployable image goal-conditioned skill policies for several common\\ncategories of manipulation tasks (opening, closing, pouring, pick&place,\\ncutting, and stirring) each capable of acting upon diverse objects and across\\ndiverse unseen task setups. ZeroMimic is carefully designed to exploit recent\\nadvances in semantic and geometric visual understanding of human videos,\\ntogether with modern grasp affordance detectors and imitation policy classes.\\nAfter training ZeroMimic on the popular EpicKitchens dataset of ego-centric\\nhuman videos, we evaluate its out-of-the-box performance in varied real-world\\nand simulated kitchen settings with two different robot embodiments,\\ndemonstrating its impressive abilities to handle these varied tasks. To enable\\nplug-and-play reuse of ZeroMimic policies on other task setups and robots, we\\nrelease software and policy checkpoints of our skill policies.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.CV,cs.LG\", \"published\": \"2025-03-31T09:27:00Z\"}"}

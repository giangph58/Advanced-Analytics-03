{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17728v1\", \"title\": \"CasualHDRSplat: Robust High Dynamic Range 3D Gaussian Splatting from\\n  Casually Captured Videos\", \"summary\": \"Recently, photo-realistic novel view synthesis from multi-view images, such\\nas neural radiance field (NeRF) and 3D Gaussian Splatting (3DGS), have garnered\\nwidespread attention due to their superior performance. However, most works\\nrely on low dynamic range (LDR) images, which limits the capturing of richer\\nscene details. Some prior works have focused on high dynamic range (HDR) scene\\nreconstruction, typically require capturing of multi-view sharp images with\\ndifferent exposure times at fixed camera positions during exposure times, which\\nis time-consuming and challenging in practice. For a more flexible data\\nacquisition, we propose a one-stage method: \\\\textbf{CasualHDRSplat} to easily\\nand robustly reconstruct the 3D HDR scene from casually captured videos with\\nauto-exposure enabled, even in the presence of severe motion blur and varying\\nunknown exposure time. \\\\textbf{CasualHDRSplat} contains a unified\\ndifferentiable physical imaging model which first applies continuous-time\\ntrajectory constraint to imaging process so that we can jointly optimize\\nexposure time, camera response function (CRF), camera poses, and sharp 3D HDR\\nscene. Extensive experiments demonstrate that our approach outperforms existing\\nmethods in terms of robustness and rendering quality. Our source code will be\\navailable at https://github.com/WU-CVGL/CasualHDRSplat\", \"main_category\": \"cs.GR\", \"categories\": \"cs.GR,cs.CV,cs.MM\", \"published\": \"2025-04-24T16:42:37Z\"}"}

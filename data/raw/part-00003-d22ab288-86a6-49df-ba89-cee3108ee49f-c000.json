{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05736v1\", \"title\": \"Rank-Then-Score: Enhancing Large Language Models for Automated Essay\\n  Scoring\", \"summary\": \"In recent years, large language models (LLMs) achieve remarkable success\\nacross a variety of tasks. However, their potential in the domain of Automated\\nEssay Scoring (AES) remains largely underexplored. Moreover, compared to\\nEnglish data, the methods for Chinese AES is not well developed. In this paper,\\nwe propose Rank-Then-Score (RTS), a fine-tuning framework based on large\\nlanguage models to enhance their essay scoring capabilities. Specifically, we\\nfine-tune the ranking model (Ranker) with feature-enriched data, and then feed\\nthe output of the ranking model, in the form of a candidate score set, with the\\nessay content into the scoring model (Scorer) to produce the final score.\\nExperimental results on two benchmark datasets, HSK and ASAP, demonstrate that\\nRTS consistently outperforms the direct prompting (Vanilla) method in terms of\\naverage QWK across all LLMs and datasets, and achieves the best performance on\\nChinese essay scoring using the HSK dataset.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-08T07:10:51Z\"}"}

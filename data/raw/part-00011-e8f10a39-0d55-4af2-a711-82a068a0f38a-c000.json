{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19602v1\", \"title\": \"Soft-Label Caching and Sharpening for Communication-Efficient Federated\\n  Distillation\", \"summary\": \"Federated Learning (FL) enables collaborative model training across\\ndecentralized clients, enhancing privacy by keeping data local. Yet\\nconventional FL, relying on frequent parameter-sharing, suffers from high\\ncommunication overhead and limited model heterogeneity. Distillation-based FL\\napproaches address these issues by sharing predictions (soft-labels) instead,\\nbut they often involve redundant transmissions across communication rounds,\\nreducing efficiency. We propose SCARLET, a novel framework integrating\\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\\ncached soft-labels, achieving up to 50% reduction in communication costs\\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\\nperformance in diverse client scenarios. Experimental evaluations demonstrate\\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\\nmethods in terms of accuracy and communication efficiency. The implementation\\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-28T09:04:30Z\"}"}

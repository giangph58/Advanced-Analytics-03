{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05195v1\", \"title\": \"Concept-Based Unsupervised Domain Adaptation\", \"summary\": \"Concept Bottleneck Models (CBMs) enhance interpretability by explaining\\npredictions through human-understandable concepts but typically assume that\\ntraining and test data share the same distribution. This assumption often fails\\nunder domain shifts, leading to degraded performance and poor generalization.\\nTo address these limitations and improve the robustness of CBMs, we propose the\\nConcept-based Unsupervised Domain Adaptation (CUDA) framework. CUDA is designed\\nto: (1) align concept representations across domains using adversarial\\ntraining, (2) introduce a relaxation threshold to allow minor domain-specific\\ndifferences in concept distributions, thereby preventing performance drop due\\nto over-constraints of these distributions, (3) infer concepts directly in the\\ntarget domain without requiring labeled concept data, enabling CBMs to adapt to\\ndiverse domains, and (4) integrate concept learning into conventional domain\\nadaptation (DA) with theoretical guarantees, improving interpretability and\\nestablishing new benchmarks for DA. Experiments demonstrate that our approach\\nsignificantly outperforms the state-of-the-art CBM and DA methods on real-world\\ndatasets.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CV\", \"published\": \"2025-05-08T12:52:02Z\"}"}

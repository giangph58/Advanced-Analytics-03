{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10921v1\", \"title\": \"MSCRS: Multi-modal Semantic Graph Prompt Learning Framework for\\n  Conversational Recommender Systems\", \"summary\": \"Conversational Recommender Systems (CRSs) aim to provide personalized\\nrecommendations by interacting with users through conversations. Most existing\\nstudies of CRS focus on extracting user preferences from conversational\\ncontexts. However, due to the short and sparse nature of conversational\\ncontexts, it is difficult to fully capture user preferences by conversational\\ncontexts only. We argue that multi-modal semantic information can enrich user\\npreference expressions from diverse dimensions (e.g., a user preference for a\\ncertain movie may stem from its magnificent visual effects and compelling\\nstoryline). In this paper, we propose a multi-modal semantic graph prompt\\nlearning framework for CRS, named MSCRS. First, we extract textual and image\\nfeatures of items mentioned in the conversational contexts. Second, we capture\\nhigher-order semantic associations within different semantic modalities\\n(collaborative, textual, and image) by constructing modality-specific graph\\nstructures. Finally, we propose an innovative integration of multi-modal\\nsemantic graphs with prompt learning, harnessing the power of large language\\nmodels to comprehensively explore high-dimensional semantic relationships.\\nExperimental results demonstrate that our proposed method significantly\\nimproves accuracy in item recommendation, as well as generates more natural and\\ncontextually relevant content in response generation. We have released the code\\nand the expanded multi-modal CRS datasets to facilitate further exploration in\\nrelated research\\\\footnote{https://github.com/BIAOBIAO12138/MSCRS-main}.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR\", \"published\": \"2025-04-15T07:05:22Z\"}"}

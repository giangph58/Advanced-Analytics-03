{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11168v1\", \"title\": \"Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails\", \"summary\": \"Large Language Models (LLMs) guardrail systems are designed to protect\\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\\ninjection and jailbreak detection systems via traditional character injection\\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\\nThrough testing against six prominent protection systems, including Microsoft's\\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\\nused to evade detection while maintaining adversarial utility achieving in some\\ninstances up to 100% evasion success. Furthermore, we demonstrate that\\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\\nleveraging word importance ranking computed by offline white-box models. Our\\nfindings reveal vulnerabilities within current LLM protection mechanisms and\\nhighlight the need for more robust guardrail systems.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.AI,cs.LG,I.2.7\", \"published\": \"2025-04-15T13:16:02Z\"}"}

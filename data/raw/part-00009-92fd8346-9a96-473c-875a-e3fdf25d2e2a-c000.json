{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11250v1\", \"title\": \"A Rollout-Based Algorithm and Reward Function for Efficient Resource\\n  Allocation in Business Processes\", \"summary\": \"Resource allocation plays a critical role in minimizing cycle time and\\nimproving the efficiency of business processes. Recently, Deep Reinforcement\\nLearning (DRL) has emerged as a powerful tool to optimize resource allocation\\npolicies in business processes. In the DRL framework, an agent learns a policy\\nthrough interaction with the environment, guided solely by reward signals that\\nindicate the quality of its decisions. However, existing algorithms are not\\nsuitable for dynamic environments such as business processes. Furthermore,\\nexisting DRL-based methods rely on engineered reward functions that approximate\\nthe desired objective, but a misalignment between reward and objective can lead\\nto undesired decisions or suboptimal policies. To address these issues, we\\npropose a rollout-based DRL algorithm and a reward function to optimize the\\nobjective directly. Our algorithm iteratively improves the policy by evaluating\\nexecution trajectories following different actions. Our reward function\\ndirectly decomposes the objective function of minimizing the mean cycle time.\\nMaximizing our reward function guarantees that the objective function is\\nminimized without requiring extensive reward engineering. The results show that\\nour method consistently learns the optimal policy in all six evaluated business\\nprocesses, outperforming the state-of-the-art algorithm that can only learn the\\noptimal policy in two of the evaluated processes.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-15T14:46:58Z\"}"}

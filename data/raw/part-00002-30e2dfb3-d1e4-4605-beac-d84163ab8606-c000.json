{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03706v1\", \"title\": \"Policy Gradient Adaptive Control for the LQR: Indirect and Direct\\n  Approaches\", \"summary\": \"Motivated by recent advances of reinforcement learning and direct data-driven\\ncontrol, we propose policy gradient adaptive control (PGAC) for the linear\\nquadratic regulator (LQR), which uses online closed-loop data to improve the\\ncontrol policy while maintaining stability. Our method adaptively updates the\\npolicy in feedback by descending the gradient of the LQR cost and is\\ncategorized as indirect, when gradients are computed via an estimated model,\\nversus direct, when gradients are derived from data using sample covariance\\nparameterization. Beyond the vanilla gradient, we also showcase the merits of\\nthe natural gradient and Gauss-Newton methods for the policy update. Notably,\\nnatural gradient descent bridges the indirect and direct PGAC, and the\\nGauss-Newton method of the indirect PGAC leads to an adaptive version of the\\ncelebrated Hewer's algorithm. To account for the uncertainty from noise, we\\npropose a regularization method for both indirect and direct PGAC. For all the\\nconsidered PGAC approaches, we show closed-loop stability and convergence of\\nthe policy to the optimal LQR gain. Simulations validate our theoretical\\nfindings and demonstrate the robustness and computational efficiency of PGAC.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC,cs.SY,eess.SY\", \"published\": \"2025-05-06T17:26:04Z\"}"}

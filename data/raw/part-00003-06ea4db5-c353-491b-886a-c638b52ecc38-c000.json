{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04745v1\", \"title\": \"Can LLMs Interpret and Leverage Structured Linguistic Representations? A\\n  Case Study with AMRs\", \"summary\": \"This paper evaluates the ability of Large Language Models (LLMs) to leverage\\ncontextual information in the form of structured linguistic representations.\\nSpecifically, we examine the impact of encoding both short and long contexts\\nusing Abstract Meaning Representation (AMR) structures across a diverse set of\\nlanguage tasks. We perform our analysis using 8-bit quantized and\\ninstruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our\\nresults indicate that, for tasks involving short contexts, augmenting the\\nprompt with the AMR of the original language context often degrades the\\nperformance of the underlying LLM. However, for tasks that involve long\\ncontexts, such as dialogue summarization in the SAMSum dataset, this\\nenhancement improves LLM performance, for example, by increasing the zero-shot\\ncosine similarity score of Llama 3.1 from 66.2% to 76%. This improvement is\\nmore evident in the newer and larger LLMs, but does not extend to the older or\\nsmaller ones. In addition, we observe that LLMs can effectively reconstruct the\\noriginal text from a linearized AMR, achieving a cosine similarity of 81.3% in\\nthe best-case scenario.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-07T05:38:40Z\"}"}

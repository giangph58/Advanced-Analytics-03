{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20699v1\", \"title\": \"Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine\\n  Translation?\", \"summary\": \"A frequently observed problem with LLMs is their tendency to generate output\\nthat is nonsensical, illogical, or factually incorrect, often referred to\\nbroadly as hallucination. Building on the recently proposed HalluciGen task for\\nhallucination detection and generation, we evaluate a suite of open-access LLMs\\non their ability to detect intrinsic hallucinations in two conditional\\ngeneration tasks: translation and paraphrasing. We study how model performance\\nvaries across tasks and language and we investigate the impact of model size,\\ninstruction tuning, and prompt choice. We find that performance varies across\\nmodels but is consistent across prompts. Finally, we find that NLI models\\nperform comparably well, suggesting that LLM-based detectors are not the only\\nviable option for this specific task.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-29T12:30:05Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04601v1\", \"title\": \"OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision\\n  Encoders for Multimodal Learning\", \"summary\": \"OpenAI's CLIP, released in early 2021, have long been the go-to choice of\\nvision encoder for building multimodal foundation models. Although recent\\nalternatives such as SigLIP have begun to challenge this status quo, to our\\nknowledge none are fully open: their training data remains proprietary and/or\\ntheir training recipes are not released. This paper fills this gap with\\nOpenVision, a fully-open, cost-effective family of vision encoders that match\\nor surpass the performance of OpenAI's CLIP when integrated into multimodal\\nframeworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for\\ntraining framework and Recap-DataComp-1B for training data -- while revealing\\nmultiple key insights in enhancing encoder quality and showcasing practical\\nbenefits in advancing multimodal models. By releasing vision encoders spanning\\nfrom 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible\\ntrade-off between capacity and efficiency in building multimodal models: larger\\nmodels deliver enhanced multimodal performance, while smaller versions enable\\nlightweight, edge-ready multimodal deployments.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-07T17:48:35Z\"}"}

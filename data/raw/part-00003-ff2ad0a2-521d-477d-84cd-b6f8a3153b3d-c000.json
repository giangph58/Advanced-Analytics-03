{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13139v1\", \"title\": \"Syntactic and Semantic Control of Large Language Models via Sequential\\n  Monte Carlo\", \"summary\": \"A wide range of LM applications require generating text that conforms to\\nsyntactic or semantic constraints. Imposing such constraints can be naturally\\nframed as probabilistic conditioning, but exact generation from the resulting\\ndistribution -- which can differ substantially from the LM's base distribution\\n-- is generally intractable. In this work, we develop an architecture for\\ncontrolled LM generation based on sequential Monte Carlo (SMC). Our SMC\\nframework allows us to flexibly incorporate domain- and problem-specific\\nconstraints at inference time, and efficiently reallocate computational\\nresources in light of new information during the course of generation. By\\ncomparing to a number of alternatives and ablations on four challenging domains\\n-- Python code generation for data science, text-to-SQL, goal inference, and\\nmolecule synthesis -- we demonstrate that, with little overhead, our approach\\nallows small open-source language models to outperform models over 8x larger,\\nas well as closed-source, fine-tuned ones. In support of the probabilistic\\nperspective, we show that these performance improvements are driven by better\\napproximation to the posterior distribution. Our system builds on the framework\\nof Lew et al. (2023) and integrates with its language model probabilistic\\nprogramming language, giving users a simple, programmable way to apply SMC to a\\nbroad variety of controlled generation problems.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-17T17:49:40Z\"}"}

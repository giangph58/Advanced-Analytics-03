{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03181v1\", \"title\": \"VLM Q-Learning: Aligning Vision-Language Models for Interactive\\n  Decision-Making\", \"summary\": \"Recent research looks to harness the general knowledge and reasoning of large\\nlanguage models (LLMs) into agents that accomplish user-specified goals in\\ninteractive environments. Vision-language models (VLMs) extend LLMs to\\nmulti-modal data and provide agents with the visual reasoning necessary for new\\napplications in areas such as computer automation. However, agent tasks\\nemphasize skills where accessible open-weight VLMs lag behind their LLM\\nequivalents. For example, VLMs are less capable of following an environment's\\nstrict output syntax requirements and are more focused on open-ended question\\nanswering. Overcoming these limitations requires supervised fine-tuning (SFT)\\non task-specific expert demonstrations. Our work approaches these challenges\\nfrom an offline-to-online reinforcement learning (RL) perspective. RL lets us\\nfine-tune VLMs to agent tasks while learning from the unsuccessful decisions of\\nour own model or more capable (larger) models. We explore an off-policy RL\\nsolution that retains the stability and simplicity of the widely used SFT\\nworkflow while allowing our agent to self-improve and learn from low-quality\\ndatasets. We demonstrate this technique with two open-weight VLMs across three\\nmulti-modal agent domains.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-06T04:51:57Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19739v1\", \"title\": \"Contrastive Language-Image Learning with Augmented Textual Prompts for\\n  3D/4D FER Using Vision-Language Model\", \"summary\": \"In this paper, we introduce AffectVLM, a vision-language model designed to\\nintegrate multiviews for a semantically rich and visually comprehensive\\nunderstanding of facial emotions from 3D/4D data. To effectively capture visual\\nfeatures, we propose a joint representation learning framework paired with a\\nnovel gradient-friendly loss function that accelerates model convergence\\ntowards optimal feature representation. Additionally, we introduce augmented\\ntextual prompts to enhance the model's linguistic capabilities and employ mixed\\nview augmentation to expand the visual dataset. We also develop a Streamlit app\\nfor a real-time interactive inference and enable the model for distributed\\nlearning. Extensive experiments validate the superior performance of AffectVLM\\nacross multiple benchmarks.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-28T12:36:14Z\"}"}

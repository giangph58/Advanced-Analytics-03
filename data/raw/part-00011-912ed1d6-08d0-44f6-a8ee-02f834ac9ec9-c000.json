{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20482v1\", \"title\": \"Group Relative Knowledge Distillation: Learning from Teacher's\\n  Relational Inductive Bias\", \"summary\": \"Knowledge distillation typically transfers knowledge from a teacher model to\\na student model by minimizing differences between their output distributions.\\nHowever, existing distillation approaches largely focus on mimicking absolute\\nprobabilities and neglect the valuable relational inductive biases embedded in\\nthe teacher's relative predictions, leading to exposure bias. In this paper, we\\npropose Group Relative Knowledge Distillation (GRKD), a novel framework that\\ndistills teacher knowledge by learning the relative ranking among classes,\\nrather than directly fitting the absolute distribution. Specifically, we\\nintroduce a group relative loss that encourages the student model to preserve\\nthe pairwise preference orderings provided by the teacher's outputs. Extensive\\nexperiments on classification benchmarks demonstrate that GRKD achieves\\nsuperior generalization compared to existing methods, especially in tasks\\nrequiring fine-grained class differentiation. Our method provides a new\\nperspective on exploiting teacher knowledge, focusing on relational structure\\nrather than absolute likelihood.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-29T07:23:22Z\"}"}

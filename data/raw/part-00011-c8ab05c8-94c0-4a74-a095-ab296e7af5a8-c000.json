{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11879v1\", \"title\": \"Learning Compatible Multi-Prize Subnetworks for Asymmetric Retrieval\", \"summary\": \"Asymmetric retrieval is a typical scenario in real-world retrieval systems,\\nwhere compatible models of varying capacities are deployed on platforms with\\ndifferent resource configurations. Existing methods generally train pre-defined\\nnetworks or subnetworks with capacities specifically designed for\\npre-determined platforms, using compatible learning. Nevertheless, these\\nmethods suffer from limited flexibility for multi-platform deployment. For\\nexample, when introducing a new platform into the retrieval systems, developers\\nhave to train an additional model at an appropriate capacity that is compatible\\nwith existing models via backward-compatible learning. In this paper, we\\npropose a Prunable Network with self-compatibility, which allows developers to\\ngenerate compatible subnetworks at any desired capacity through post-training\\npruning. Thus it allows the creation of a sparse subnetwork matching the\\nresources of the new platform without additional training. Specifically, we\\noptimize both the architecture and weight of subnetworks at different\\ncapacities within a dense network in compatible learning. We also design a\\nconflict-aware gradient integration scheme to handle the gradient conflicts\\nbetween the dense network and subnetworks during compatible learning. Extensive\\nexperiments on diverse benchmarks and visual backbones demonstrate the\\neffectiveness of our method. Our code and model are available at\\nhttps://github.com/Bunny-Black/PrunNet.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-16T08:59:47Z\"}"}

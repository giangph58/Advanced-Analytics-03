{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23696v1\", \"title\": \"Pinalites: Optical properties and Quantum Magnetism of Heteroanionic\\n  A$_3$MO$_5$X$_2$ Compounds\", \"summary\": \"Heteroanionic compounds, which contain two or more types of anions, have\\nemerged as a promising class of materials with diverse properties and\\nfunctionalities. In this paper, I review the experimental findings on\\nCa3ReO5Cl2 and related com-pounds that exhibit remarkable pleochroism and novel\\nquantum magnetism. I discuss how the heteroanionic coordination affects the\\noptical and magnetic properties by modulating the d-orbital states of the\\ntransition metal ions and then compare these materials with other heteroanionic\\nand monoanionic compounds and highlight the potential of A3MO5X2 materials for\\nfuture exploration of materials and phenomena.\", \"main_category\": \"cond-mat.mtrl-sci\", \"categories\": \"cond-mat.mtrl-sci,cond-mat.str-el\", \"published\": \"2025-03-31T03:51:35Z\"}"}
{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23714v1\", \"title\": \"Building Instruction-Tuning Datasets from Human-Written Instructions\\n  with Open-Weight Large Language Models\", \"summary\": \"Instruction tuning is crucial for enabling Large Language Models (LLMs) to\\nsolve real-world tasks. Prior work has shown the effectiveness of\\ninstruction-tuning data synthesized solely from LLMs, raising a fundamental\\nquestion: Do we still need human-originated signals for instruction tuning?\\nThis work answers the question affirmatively: we build state-of-the-art\\ninstruction-tuning datasets sourced from human-written instructions, by simply\\npairing them with LLM-generated responses. LLMs fine-tuned on our datasets\\nconsistently outperform those fine-tuned on existing ones. Our data\\nconstruction approach can be easily adapted to other languages; we build\\ndatasets for Japanese and confirm that LLMs tuned with our data reach\\nstate-of-the-art performance. Analyses suggest that instruction-tuning in a new\\nlanguage allows LLMs to follow instructions, while the tuned models exhibit a\\nnotable lack of culture-specific knowledge in that language. The datasets and\\nfine-tuned models will be publicly available. Our datasets, synthesized with\\nopen-weight LLMs, are openly distributed under permissive licenses, allowing\\nfor diverse use cases.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-03-31T04:28:38Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01771v1\", \"title\": \"Enhancing Interpretability in Generative AI Through Search-Based Data\\n  Influence Analysis\", \"summary\": \"Generative AI models offer powerful capabilities but often lack transparency,\\nmaking it difficult to interpret their output. This is critical in cases\\ninvolving artistic or copyrighted content. This work introduces a\\nsearch-inspired approach to improve the interpretability of these models by\\nanalysing the influence of training data on their outputs. Our method provides\\nobservational interpretability by focusing on a model's output rather than on\\nits internal state. We consider both raw data and latent-space embeddings when\\nsearching for the influence of data items in generated content. We evaluate our\\nmethod by retraining models locally and by demonstrating the method's ability\\nto uncover influential subsets in the training data. This work lays the\\ngroundwork for future extensions, including user-based evaluations with domain\\nexperts, which is expected to improve observational interpretability further.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.LG\", \"published\": \"2025-04-02T14:29:37Z\"}"}

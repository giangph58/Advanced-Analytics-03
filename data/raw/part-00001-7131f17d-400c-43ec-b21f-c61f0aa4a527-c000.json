{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00275v1\", \"title\": \"AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor\\n  Long-Term Medication Adherence and Care\", \"summary\": \"Chronic diseases, including diabetes, hypertension, asthma, HIV-AIDS,\\nepilepsy, and tuberculosis, necessitate rigorous adherence to medication to\\navert disease progression, manage symptoms, and decrease mortality rates.\\nAdherence is frequently undermined by factors including patient behavior,\\ncaregiver support, elevated medical costs, and insufficient healthcare\\ninfrastructure. We propose AdCare-VLM, a specialized Video-LLaVA-based\\nmultimodal large vision language model (LVLM) aimed at visual question\\nanswering (VQA) concerning medication adherence through patient videos. We\\nemploy a private dataset comprising 806 custom-annotated tuberculosis (TB)\\nmedication monitoring videos, which have been labeled by clinical experts, to\\nfine-tune the model for adherence pattern detection. We present LLM-TB-VQA, a\\ndetailed medical adherence VQA dataset that encompasses positive, negative, and\\nambiguous adherence cases. Our method identifies correlations between visual\\nfeatures, such as the clear visibility of the patient's face, medication, water\\nintake, and the act of ingestion, and their associated medical concepts in\\ncaptions. This facilitates the integration of aligned visual-linguistic\\nrepresentations and improves multimodal interactions. Experimental results\\nindicate that our method surpasses parameter-efficient fine-tuning (PEFT)\\nenabled VLM models, such as LLaVA-V1.5 and Chat-UniVi, with absolute\\nimprovements ranging from 3.1% to 3.54% across pre-trained, regular, and\\nlow-rank adaptation (LoRA) configurations. Comprehensive ablation studies and\\nattention map visualizations substantiate our approach, enhancing\\ninterpretability.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-01T03:48:12Z\"}"}

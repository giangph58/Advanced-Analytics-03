{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16389v1\", \"title\": \"SaENeRF: Suppressing Artifacts in Event-based Neural Radiance Fields\", \"summary\": \"Event cameras are neuromorphic vision sensors that asynchronously capture\\nchanges in logarithmic brightness changes, offering significant advantages such\\nas low latency, low power consumption, low bandwidth, and high dynamic range.\\nWhile these characteristics make them ideal for high-speed scenarios,\\nreconstructing geometrically consistent and photometrically accurate 3D\\nrepresentations from event data remains fundamentally challenging. Current\\nevent-based Neural Radiance Fields (NeRF) methods partially address these\\nchallenges but suffer from persistent artifacts caused by aggressive network\\nlearning in early stages and the inherent noise of event cameras. To overcome\\nthese limitations, we present SaENeRF, a novel self-supervised framework that\\neffectively suppresses artifacts and enables 3D-consistent, dense, and\\nphotorealistic NeRF reconstruction of static scenes solely from event streams.\\nOur approach normalizes predicted radiance variations based on accumulated\\nevent polarities, facilitating progressive and rapid learning for scene\\nrepresentation construction. Additionally, we introduce regularization losses\\nspecifically designed to suppress artifacts in regions where photometric\\nchanges fall below the event threshold and simultaneously enhance the light\\nintensity difference of non-zero events, thereby improving the visual fidelity\\nof the reconstructed scene. Extensive qualitative and quantitative experiments\\ndemonstrate that our method significantly reduces artifacts and achieves\\nsuperior reconstruction quality compared to existing methods. The code is\\navailable at https://github.com/Mr-firework/SaENeRF.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-23T03:33:20Z\"}"}

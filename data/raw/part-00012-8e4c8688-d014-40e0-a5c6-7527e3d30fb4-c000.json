{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17402v1\", \"title\": \"Assessing the Capability of Large Language Models for Domain-Specific\\n  Ontology Generation\", \"summary\": \"Large Language Models (LLMs) have shown significant potential for ontology\\nengineering. However, it is still unclear to what extent they are applicable to\\nthe task of domain-specific ontology generation. In this study, we explore the\\napplication of LLMs for automated ontology generation and evaluate their\\nperformance across different domains. Specifically, we investigate the\\ngeneralizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both\\nequipped with reasoning capabilities, by generating ontologies from a set of\\ncompetency questions (CQs) and related user stories. Our experimental setup\\ncomprises six distinct domains carried out in existing ontology engineering\\nprojects and a total of 95 curated CQs designed to test the models' reasoning\\nfor ontology engineering. Our findings show that with both LLMs, the\\nperformance of the experiments is remarkably consistent across all domains,\\nindicating that these methods are capable of generalizing ontology generation\\ntasks irrespective of the domain. These results highlight the potential of\\nLLM-based approaches in achieving scalable and domain-agnostic ontology\\nconstruction and lay the groundwork for further research into enhancing\\nautomated reasoning and knowledge representation techniques.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-24T09:47:14Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11381v1\", \"title\": \"RankAlign: A Ranking View of the Generator-Validator Gap in Large\\n  Language Models\", \"summary\": \"Although large language models (LLMs) have become generally more capable and\\naccurate across many tasks, some fundamental sources of unreliability remain in\\ntheir behavior. One key limitation is their inconsistency at reporting the the\\nsame information when prompts are changed. In this paper, we consider the\\ndiscrepancy between a model's generated answer and their own verification of\\nthat answer, the generator-validator gap. We define this gap in a more\\nstringent way than prior work: we expect correlation of scores from a generator\\nand a validator over the entire set of candidate answers. We show that\\naccording to this measure, a large gap exists in various settings, including\\nquestion answering, lexical semantics tasks, and next-word prediction. We then\\npropose RankAlign, a ranking-based training method, and show that it\\nsignificantly closes the gap by 31.8% on average, surpassing all baseline\\nmethods. Moreover, this approach generalizes well to out-of-domain tasks and\\nlexical items.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-15T16:53:31Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15815v1\", \"title\": \"What's the Difference? Supporting Users in Identifying the Effects of\\n  Prompt and Model Changes Through Token Patterns\", \"summary\": \"Prompt engineering for large language models is challenging, as even small\\nprompt perturbations or model changes can significantly impact the generated\\noutput texts. Existing evaluation methods, either automated metrics or human\\nevaluation, have limitations, such as providing limited insights or being\\nlabor-intensive. We propose Spotlight, a new approach that combines both\\nautomation and human analysis. Based on data mining techniques, we\\nautomatically distinguish between random (decoding) variations and systematic\\ndifferences in language model outputs. This process provides token patterns\\nthat describe the systematic differences and guide the user in manually\\nanalyzing the effects of their prompt and model changes efficiently. We create\\nthree benchmarks to quantitatively test the reliability of token pattern\\nextraction methods and demonstrate that our approach provides new insights into\\nestablished prompt data. From a human-centric perspective, through\\ndemonstration studies and a user study, we show that our token pattern approach\\nhelps users understand the systematic differences of language model outputs,\\nand we are able to discover relevant differences caused by prompt and model\\nchanges (e.g. related to gender or culture), thus supporting the prompt\\nengineering process and human-centric model behavior research.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.HC,cs.LG\", \"published\": \"2025-04-22T11:53:33Z\"}"}

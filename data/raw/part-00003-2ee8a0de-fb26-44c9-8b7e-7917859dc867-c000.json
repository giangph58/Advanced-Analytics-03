{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07532v1\", \"title\": \"AI-Slop to AI-Polish? Aligning Language Models through Edit-Based\\n  Writing Rewards and Test-time Computation\", \"summary\": \"AI-generated text is proliferating across domains, from creative writing and\\njournalism to marketing content and scientific articles. Models can follow\\nuser-provided instructions to generate coherent and grammatically correct\\noutputs but in this work, we study a more fundamental question: how do we\\nevaluate and improve the writing quality of AI-generated text? Writing quality\\nassessment has received less attention from the community, in part because it\\nis fundamentally subjective and requires expertise. We first introduce the\\nWriting Quality Benchmark (WQ) by consolidating five writing-preference\\ndatasets into 4,729 writing quality judgments. Our experiments show that\\ncompetitive baselines, including state-of-the-art LLMs that excel at reasoning\\ntasks, barely outperform random baselines on WQ. We then train specialized\\nWriting Quality Reward Models (WQRM) of various sizes for writing quality\\nassessment that demonstrate strong generalization on four out-of-distribution\\ntest sets and 74% accuracy on the WQ benchmark. To further show WQRM's\\npractical benefits during inference, we leverage additional test-time compute\\nto generate and rank multiple candidate revisions, allowing us to select\\nhigher-quality outputs from an initial draft. Human evaluation with 9\\nexperienced writers confirm that WQRM-based selection produces writing samples\\npreferred by experts 66% overall, and 72.2% when the reward gap is larger than\\n1 point. We release our datasets and models to encourage community engagement\\nwith writing quality assessment and development of AI writing systems better\\naligned with human preferences.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-10T07:58:05Z\"}"}

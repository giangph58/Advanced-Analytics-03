{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10077v1\", \"title\": \"Towards Quantifying Commonsense Reasoning with Mechanistic Insights\", \"summary\": \"Commonsense reasoning deals with the implicit knowledge that is well\\nunderstood by humans and typically acquired via interactions with the world. In\\nrecent times, commonsense reasoning and understanding of various LLMs have been\\nevaluated using text-based tasks. In this work, we argue that a proxy of this\\nunderstanding can be maintained as a graphical structure that can further help\\nto perform a rigorous evaluation of commonsense reasoning abilities about\\nvarious real-world activities. We create an annotation scheme for capturing\\nthis implicit knowledge in the form of a graphical structure for 37 daily human\\nactivities. We find that the created resource can be used to frame an enormous\\nnumber of commonsense queries (~ 10^{17}), facilitating rigorous evaluation of\\ncommonsense reasoning in LLMs. Moreover, recently, the remarkable performance\\nof LLMs has raised questions about whether these models are truly capable of\\nreasoning in the wild and, in general, how reasoning occurs inside these\\nmodels. In this resource paper, we bridge this gap by proposing design\\nmechanisms that facilitate research in a similar direction. Our findings\\nsuggest that the reasoning components are localized in LLMs that play a\\nprominent role in decision-making when prompted with a commonsense query.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-14T10:21:59Z\"}"}

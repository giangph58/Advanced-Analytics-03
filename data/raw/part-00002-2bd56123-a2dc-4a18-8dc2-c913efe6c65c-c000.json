{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21277v1\", \"title\": \"Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large\\n  Language Models\", \"summary\": \"The integration of reinforcement learning (RL) into the reasoning\\ncapabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as\\na transformative research direction. While MLLMs significantly extend Large\\nLanguage Models (LLMs) to handle diverse modalities such as vision, audio, and\\nvideo, enabling robust reasoning across multimodal inputs remains a major\\nchallenge. This survey systematically reviews recent advances in RL-based\\nreasoning for MLLMs, covering key algorithmic designs, reward mechanism\\ninnovations, and practical applications. We highlight two main RL\\nparadigms--value-free and value-based methods--and analyze how RL enhances\\nreasoning abilities by optimizing reasoning trajectories and aligning\\nmultimodal information. Furthermore, we provide an extensive overview of\\nbenchmark datasets, evaluation protocols, and existing limitations, and propose\\nfuture research directions to address current bottlenecks such as sparse\\nrewards, inefficient cross-modal reasoning, and real-world deployment\\nconstraints. Our goal is to offer a comprehensive and structured guide to\\nresearchers interested in advancing RL-based reasoning in the multimodal era.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-30T03:14:28Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10368v1\", \"title\": \"S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\\n  of Large Reasoning Models\", \"summary\": \"We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning\\nModels' (LRMs) performance on simple tasks that favor intuitive system 1\\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\\nthought, their reliance on deep analytical thinking may limit their system 1\\nthinking capabilities. Moreover, a lack of benchmark currently exists to\\nevaluate LRMs' performance in tasks that require such capabilities. To fill\\nthis gap, S1-Bench presents a set of simple, diverse, and naturally clear\\nquestions across multiple domains and languages, specifically designed to\\nassess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs\\nreveals significant lower efficiency tendencies, with outputs averaging 15.5\\ntimes longer than those of traditional small LLMs. Additionally, LRMs often\\nidentify correct answers early but continue unnecessary deliberation, with some\\nmodels even producing numerous errors. These findings highlight the rigid\\nreasoning patterns of current LRMs and underscore the substantial development\\nneeded to achieve balanced dual-system thinking capabilities that can adapt\\nappropriately to task complexity.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-14T16:13:23Z\"}"}

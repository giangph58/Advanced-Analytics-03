{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05897v1\", \"title\": \"HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\\n  MoE Inference\", \"summary\": \"The Mixture of Experts (MoE) architecture has demonstrated significant\\nadvantages as it enables to increase the model capacity without a proportional\\nincrease in computation. However, the large MoE model size still introduces\\nsubstantial memory demands, which usually requires expert offloading on\\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\\ninference has been proposed to leverage CPU computation to reduce expert\\nloading overhead but faces major challenges: on one hand, the expert activation\\npatterns of MoE models are highly unstable, rendering the fixed mapping\\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\\nschedule for MoE is inherently complex due to the diverse expert sizes,\\nstructures, uneven workload distribution, etc. To address these challenges, in\\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\\nimproves resource utilization through a novel CPU-GPU scheduling and cache\\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\\nmitigate expert activation instability. We implement HybriMoE on top of the\\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\\nExperimental results demonstrate that HybriMoE achieves an average speedup of\\n1.33$\\\\times$ in the prefill stage and 1.70$\\\\times$ in the decode stage compared\\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\\nhttps://github.com/PKU-SEC-Lab/HybriMoE.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.DC\", \"published\": \"2025-04-08T10:47:37Z\"}"}

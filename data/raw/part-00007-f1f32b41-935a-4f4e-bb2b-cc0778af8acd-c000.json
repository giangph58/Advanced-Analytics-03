{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05279v1\", \"title\": \"Covariant Gradient Descent\", \"summary\": \"We present a manifestly covariant formulation of the gradient descent method,\\nensuring consistency across arbitrary coordinate systems and general curved\\ntrainable spaces. The optimization dynamics is defined using a covariant force\\nvector and a covariant metric tensor, both computed from the first and second\\nstatistical moments of the gradients. These moments are estimated through\\ntime-averaging with an exponential weight function, which preserves linear\\ncomputational complexity. We show that commonly used optimization methods such\\nas RMSProp and Adam correspond to special limits of the covariant gradient\\ndescent (CGD) and demonstrate how these methods can be further generalized and\\nimproved.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-07T17:25:50Z\"}"}

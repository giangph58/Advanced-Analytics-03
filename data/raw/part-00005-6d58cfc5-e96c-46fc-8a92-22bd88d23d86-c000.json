{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04599v1\", \"title\": \"Complexity Lower Bounds of Adaptive Gradient Algorithms for Non-convex\\n  Stochastic Optimization under Relaxed Smoothness\", \"summary\": \"Recent results in non-convex stochastic optimization demonstrate the\\nconvergence of popular adaptive algorithms (e.g., AdaGrad) under the $(L_0,\\nL_1)$-smoothness condition, but the rate of convergence is a higher-order\\npolynomial in terms of problem parameters like the smoothness constants. The\\ncomplexity guaranteed by such algorithms to find an $\\\\epsilon$-stationary point\\nmay be significantly larger than the optimal complexity of $\\\\Theta \\\\left(\\n\\\\Delta L \\\\sigma^2 \\\\epsilon^{-4} \\\\right)$ achieved by SGD in the $L$-smooth\\nsetting, where $\\\\Delta$ is the initial optimality gap, $\\\\sigma^2$ is the\\nvariance of stochastic gradient. However, it is currently not known whether\\nthese higher-order dependencies can be tightened. To answer this question, we\\ninvestigate complexity lower bounds for several adaptive optimization\\nalgorithms in the $(L_0, L_1)$-smooth setting, with a focus on the dependence\\nin terms of problem parameters $\\\\Delta, L_0, L_1$. We provide complexity bounds\\nfor three variations of AdaGrad, which show at least a quadratic dependence on\\nproblem parameters $\\\\Delta, L_0, L_1$. Notably, we show that the decorrelated\\nvariant of AdaGrad-Norm requires at least $\\\\Omega \\\\left( \\\\Delta^2 L_1^2\\n\\\\sigma^2 \\\\epsilon^{-4} \\\\right)$ stochastic gradient queries to find an\\n$\\\\epsilon$-stationary point. We also provide a lower bound for SGD with a broad\\nclass of adaptive stepsizes. Our results show that, for certain adaptive\\nalgorithms, the $(L_0, L_1)$-smooth setting is fundamentally more difficult\\nthan the standard smooth setting, in terms of the initial optimality gap and\\nthe smoothness constants.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-07T17:40:12Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16789v1\", \"title\": \"MLOps Monitoring at Scale for Digital Platforms\", \"summary\": \"Machine learning models are widely recognized for their strong performance in\\nforecasting. To keep that performance in streaming data settings, they have to\\nbe monitored and frequently re-trained. This can be done with machine learning\\noperations (MLOps) techniques under supervision of an MLOps engineer. However,\\nin digital platform settings where the number of data streams is typically\\nlarge and unstable, standard monitoring becomes either suboptimal or too labor\\nintensive for the MLOps engineer. As a consequence, companies often fall back\\non very simple worse performing ML models without monitoring. We solve this\\nproblem by adopting a design science approach and introducing a new monitoring\\nframework, the Machine Learning Monitoring Agent (MLMA), that is designed to\\nwork at scale for any ML model with reasonable labor cost. A key feature of our\\nframework concerns test-based automated re-training based on a data-adaptive\\nreference loss batch. The MLOps engineer is kept in the loop via key metrics\\nand also acts, pro-actively or retrospectively, to maintain performance of the\\nML model in the production stage. We conduct a large-scale test at a last-mile\\ndelivery platform to empirically validate our monitoring framework.\", \"main_category\": \"econ.EM\", \"categories\": \"econ.EM,stat.AP\", \"published\": \"2025-04-23T15:04:38Z\"}"}

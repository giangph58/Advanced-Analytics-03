{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21278v1\", \"title\": \"Robust Multi-agent Communication Based on Decentralization-Oriented\\n  Adversarial Training\", \"summary\": \"In typical multi-agent reinforcement learning (MARL) problems, communication\\nis important for agents to share information and make the right decisions.\\nHowever, due to the complexity of training multi-agent communication, existing\\nmethods often fall into the dilemma of local optimization, which leads to the\\nconcentration of communication in a limited number of channels and presents an\\nunbalanced structure. Such unbalanced communication policy are vulnerable to\\nabnormal conditions, where the damage of critical communication channels can\\ntrigger the crash of the entire system. Inspired by decentralization theory in\\nsociology, we propose DMAC, which enhances the robustness of multi-agent\\ncommunication policies by retraining them into decentralized patterns.\\nSpecifically, we train an adversary DMAC\\\\_Adv which can dynamically identify\\nand mask the critical communication channels, and then apply the adversarial\\nsamples generated by DMAC\\\\_Adv to the adversarial learning of the communication\\npolicy to force the policy in exploring other potential communication schemes\\nand transition to a decentralized structure. As a training method to improve\\nrobustness, DMAC can be fused with any learnable communication policy\\nalgorithm. The experimental results in two communication policies and four\\nmulti-agent tasks demonstrate that DMAC achieves higher improvement on\\nrobustness and performance of communication policy compared with two\\nstate-of-the-art and commonly-used baselines. Also, the results demonstrate\\nthat DMAC can achieve decentralized communication structure with acceptable\\ncommunication cost.\", \"main_category\": \"cs.MA\", \"categories\": \"cs.MA\", \"published\": \"2025-04-30T03:14:50Z\"}"}

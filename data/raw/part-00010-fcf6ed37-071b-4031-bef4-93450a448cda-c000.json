{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01420v1\", \"title\": \"FAIRE: Assessing Racial and Gender Bias in AI-Driven Resume Evaluations\", \"summary\": \"In an era where AI-driven hiring is transforming recruitment practices,\\nconcerns about fairness and bias have become increasingly important. To explore\\nthese issues, we introduce a benchmark, FAIRE (Fairness Assessment In Resume\\nEvaluation), to test for racial and gender bias in large language models (LLMs)\\nused to evaluate resumes across different industries. We use two methods-direct\\nscoring and ranking-to measure how model performance changes when resumes are\\nslightly altered to reflect different racial or gender identities. Our findings\\nreveal that while every model exhibits some degree of bias, the magnitude and\\ndirection vary considerably. This benchmark provides a clear way to examine\\nthese differences and offers valuable insights into the fairness of AI-based\\nhiring tools. It highlights the urgent need for strategies to reduce bias in\\nAI-driven recruitment. Our benchmark code and dataset are open-sourced at our\\nrepository:\\nhttps://github.com/athenawen/FAIRE-Fairness-Assessment-In-Resume-Evaluation.git.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-02T07:11:30Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14870v1\", \"title\": \"OTC: Optimal Tool Calls via Reinforcement Learning\", \"summary\": \"Tool-integrated reasoning (TIR) augments large language models (LLMs) with\\nthe ability to invoke external tools, such as search engines and code\\ninterpreters, to solve tasks beyond the capabilities of language-only\\nreasoning. While reinforcement learning (RL) has shown promise in improving TIR\\nby optimizing final answer correctness, existing approaches often overlook the\\nefficiency and cost associated with tool usage. This can lead to suboptimal\\nbehavior, including excessive tool calls that increase computational and\\nfinancial overhead, or insufficient tool use that compromises answer quality.\\nIn this work, we propose Optimal Tool Call-controlled Policy Optimization\\n(OTC-PO), a simple yet effective RL-based framework that encourages models to\\nproduce accurate answers with minimal tool calls. Our method introduces a\\ntool-integrated reward that jointly considers correctness and tool efficiency,\\npromoting high tool productivity. We instantiate this framework within both\\nProximal Policy Optimization (PPO) and Group Relative Preference Optimization\\n(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and\\nQwen-Math across multiple QA benchmarks show that our approach reduces tool\\ncalls by up to 73.1\\\\% and improves tool productivity by up to 229.4\\\\%, while\\nmaintaining comparable answer accuracy. To the best of our knowledge, this is\\nthe first RL-based framework that explicitly optimizes tool-use efficiency in\\nTIR.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CL\", \"published\": \"2025-04-21T05:40:05Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04201v1\", \"title\": \"SToLa: Self-Adaptive Touch-Language Framework with Tactile Commonsense\\n  Reasoning in Open-Ended Scenarios\", \"summary\": \"This paper explores the challenges of integrating tactile sensing into\\nintelligent systems for multimodal reasoning, particularly in enabling\\ncommonsense reasoning about the open-ended physical world. We identify two key\\nchallenges: modality discrepancy, where existing large touch-language models\\noften treat touch as a mere sub-modality of language, and open-ended tactile\\ndata scarcity, where current datasets lack the diversity, open-endness and\\ncomplexity needed for reasoning. To overcome these challenges, we introduce\\nSToLa, a Self-Adaptive Touch-Language framework. SToLa utilizes Mixture of\\nExperts (MoE) to dynamically process, unify, and manage tactile and language\\nmodalities, capturing their unique characteristics. Crucially, we also present\\na comprehensive tactile commonsense reasoning dataset and benchmark featuring\\nfree-form questions and responses, 8 physical properties, 4 interactive\\ncharacteristics, and diverse commonsense knowledge. Experiments show SToLa\\nexhibits competitive performance compared to existing models on the PhysiCLeAR\\nbenchmark and self-constructed datasets, proving the effectiveness of the\\nMixture of Experts architecture in multimodal management and the performance\\nadvantages for open-scenario tactile commonsense reasoning tasks.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-07T07:55:35Z\"}"}

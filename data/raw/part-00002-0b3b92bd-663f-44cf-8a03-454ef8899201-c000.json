{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19605v1\", \"title\": \"A Comparative Study on Positional Encoding for Time-frequency Domain\\n  Dual-path Transformer-based Source Separation Models\", \"summary\": \"In this study, we investigate the impact of positional encoding (PE) on\\nsource separation performance and the generalization ability to long sequences\\n(length extrapolation) in Transformer-based time-frequency (TF) domain\\ndual-path models. The length extrapolation capability in TF-domain dual-path\\nmodels is a crucial factor, as it affects not only their performance on\\nlong-duration inputs but also their generalizability to signals with unseen\\nsampling rates. While PE is known to significantly impact length extrapolation,\\nthere has been limited research that explores the choice of PEs for TF-domain\\ndual-path models from this perspective. To address this gap, we compare various\\nPE methods using a recent state-of-the-art model, TF-Locoformer, as the base\\narchitecture. Our analysis yields the following key findings: (i) When handling\\nsequences that are the same length as or shorter than those seen during\\ntraining, models with PEs achieve better performance. (ii) However, models\\nwithout PE exhibit superior length extrapolation. This trend is particularly\\npronounced when the model contains convolutional layers.\", \"main_category\": \"eess.AS\", \"categories\": \"eess.AS,cs.SD\", \"published\": \"2025-04-28T09:08:37Z\"}"}

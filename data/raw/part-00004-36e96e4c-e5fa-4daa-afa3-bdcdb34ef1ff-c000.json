{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10284v1\", \"title\": \"Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the\\n  Evaluation Protocol\", \"summary\": \"Literature review tables are essential for summarizing and comparing\\ncollections of scientific papers. We explore the task of generating tables that\\nbest fulfill a user's informational needs given a collection of scientific\\npapers. Building on recent work (Newman et al., 2024), we extend prior\\napproaches to address real-world complexities through a combination of\\nLLM-based methods and human annotations. Our contributions focus on three key\\nchallenges encountered in real-world use: (i) User prompts are often\\nunder-specified; (ii) Retrieved candidate papers frequently contain irrelevant\\ncontent; and (iii) Task evaluation should move beyond shallow text similarity\\ntechniques and instead assess the utility of inferred tables for\\ninformation-seeking tasks (e.g., comparing papers). To support reproducible\\nevaluation, we introduce ARXIV2TABLE, a more realistic and challenging\\nbenchmark for this task, along with a novel approach to improve literature\\nreview table generation in real-world scenarios. Our extensive experiments on\\nthis benchmark show that both open-weight and proprietary LLMs struggle with\\nthe task, highlighting its difficulty and the need for further advancements.\\nOur dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-14T14:52:28Z\"}"}

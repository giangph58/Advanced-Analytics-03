{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12759v1\", \"title\": \"Perturbed Proximal Gradient ADMM for Nonconvex Composite Optimization\", \"summary\": \"This paper proposes a Perturbed Proximal Gradient ADMM (PPG-ADMM) framework\\nfor solving general nonconvex composite optimization problems, where the\\nobjective function consists of a smooth nonconvex term and a nonsmooth weakly\\nconvex term for both primal variables.\\n  Unlike existing ADMM-based methods which necessitate the function associated\\nwith the last updated primal variable to be smooth, the proposed PPG-ADMM\\nremoves this restriction by introducing a perturbation mechanism, which also\\nhelps reduce oscillations in the primal-dual updates, thereby improving\\nconvergence stability.\\n  By employing a linearization technique for the smooth term and the proximal\\noperator for the nonsmooth and weakly convex term, the subproblems have\\nclosed-form solutions, significantly reducing computational complexity. The\\nconvergence is established through a technically constructed Lyapunov function,\\nwhich guarantees sufficient descent and has a well-defined lower bound.\\n  With properly chosen parameters, PPG-ADMM converges to an\\n$\\\\epsilon$-approximate stationary point at a sublinear convergence rate of\\n$\\\\mathcal{O}(1/\\\\sqrt{K})$.\\n  Furthermore, by appropriately tuning the perturbation parameter $\\\\beta$, it\\nachieves an $\\\\epsilon$-stationary point, providing stronger optimality\\nguarantees. We further apply PPG-ADMM to two practical distributed nonconvex\\ncomposite optimization problems, i.e., the distributed partial consensus\\nproblem and the resource allocation problem. The algorithm operates in a fully\\ndecentralized manner without a central coordinating node. Finally, numerical\\nexperiments validate the effectiveness of PPG-ADMM, demonstrating its improved\\nconvergence performance.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC\", \"published\": \"2025-04-17T08:55:42Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05186v1\", \"title\": \"Training state-of-the-art pathology foundation models with orders of\\n  magnitude less data\", \"summary\": \"The field of computational pathology has recently seen rapid advances driven\\nby the development of modern vision foundation models (FMs), typically trained\\non vast collections of pathology images. Recent studies demonstrate that\\nincreasing the training data set and model size and integrating domain-specific\\nimage processing techniques can significantly enhance the model's performance\\non downstream tasks. Building on these insights, our work incorporates several\\nrecent modifications to the standard DINOv2 framework from the literature to\\noptimize the training of pathology FMs. We also apply a post-training procedure\\nfor fine-tuning models on higher-resolution images to further enrich the\\ninformation encoded in the embeddings. We present three novel pathology FMs\\ntrained on up to two orders of magnitude fewer WSIs than those used to train\\nother state-of-the-art FMs while demonstrating a comparable or superior\\nperformance on downstream tasks. Even the model trained on TCGA alone (12k\\nWSIs) outperforms most existing FMs and, on average, matches Virchow2, the\\nsecond-best FM published to date. This suggests that there still remains a\\nsignificant potential for further improving the models and algorithms used to\\ntrain pathology FMs to take full advantage of the vast data collections.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG\", \"published\": \"2025-04-07T15:38:12Z\"}"}

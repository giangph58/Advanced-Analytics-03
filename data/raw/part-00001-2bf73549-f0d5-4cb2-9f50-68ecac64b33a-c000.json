{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23896v1\", \"title\": \"Feature learning from non-Gaussian inputs: the case of Independent\\n  Component Analysis in high dimensions\", \"summary\": \"Deep neural networks learn structured features from complex, non-Gaussian\\ninputs, but the mechanisms behind this process remain poorly understood. Our\\nwork is motivated by the observation that the first-layer filters learnt by\\ndeep convolutional neural networks from natural images resemble those learnt by\\nindependent component analysis (ICA), a simple unsupervised method that seeks\\nthe most non-Gaussian projections of its inputs. This similarity suggests that\\nICA provides a simple, yet principled model for studying feature learning.\\nHere, we leverage this connection to investigate the interplay between data\\nstructure and optimisation in feature learning for the most popular ICA\\nalgorithm, FastICA, and stochastic gradient descent (SGD), which is used to\\ntrain deep networks. We rigorously establish that FastICA requires at least\\n$n\\\\gtrsim d^4$ samples to recover a single non-Gaussian direction from\\n$d$-dimensional inputs on a simple synthetic data model. We show that vanilla\\nonline SGD outperforms FastICA, and prove that the optimal sample complexity $n\\n\\\\gtrsim d^2$ can be reached by smoothing the loss, albeit in a data-dependent\\nway. We finally demonstrate the existence of a search phase for FastICA on\\nImageNet, and discuss how the strong non-Gaussianity of said images compensates\\nfor the poor sample complexity of FastICA.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cond-mat.dis-nn,cond-mat.stat-mech,cs.LG,math.PR\", \"published\": \"2025-03-31T09:46:47Z\"}"}

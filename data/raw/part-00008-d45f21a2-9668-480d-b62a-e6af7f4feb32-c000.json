{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05298v1\", \"title\": \"One-Minute Video Generation with Test-Time Training\", \"summary\": \"Transformers today still struggle to generate one-minute videos because\\nself-attention layers are inefficient for long context. Alternatives such as\\nMamba layers struggle with complex multi-scene stories because their hidden\\nstates are less expressive. We experiment with Test-Time Training (TTT) layers,\\nwhose hidden states themselves can be neural networks, therefore more\\nexpressive. Adding TTT layers into a pre-trained Transformer enables it to\\ngenerate one-minute videos from text storyboards. For proof of concept, we\\ncurate a dataset based on Tom and Jerry cartoons. Compared to baselines such as\\nMamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers\\ngenerate much more coherent videos that tell complex stories, leading by 34 Elo\\npoints in a human evaluation of 100 videos per method. Although promising,\\nresults still contain artifacts, likely due to the limited capability of the\\npre-trained 5B model. The efficiency of our implementation can also be\\nimproved. We have only experimented with one-minute videos due to resource\\nconstraints, but the approach can be extended to longer videos and more complex\\nstories. Sample videos, code and annotations are available at:\\nhttps://test-time-training.github.io/video-dit\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-07T17:56:31Z\"}"}

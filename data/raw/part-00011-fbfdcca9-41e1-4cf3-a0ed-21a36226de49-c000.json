{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21544v1\", \"title\": \"SAM4EM: Efficient memory-based two stage prompt-free segment anything\\n  model adapter for complex 3D neuroscience electron microscopy stacks\", \"summary\": \"We present SAM4EM, a novel approach for 3D segmentation of complex neural\\nstructures in electron microscopy (EM) data by leveraging the Segment Anything\\nModel (SAM) alongside advanced fine-tuning strategies. Our contributions\\ninclude the development of a prompt-free adapter for SAM using two stage mask\\ndecoding to automatically generate prompt embeddings, a dual-stage fine-tuning\\nmethod based on Low-Rank Adaptation (LoRA) for enhancing segmentation with\\nlimited annotated data, and a 3D memory attention mechanism to ensure\\nsegmentation consistency across 3D stacks. We further release a unique\\nbenchmark dataset for the segmentation of astrocytic processes and synapses. We\\nevaluated our method on challenging neuroscience segmentation benchmarks,\\nspecifically targeting mitochondria, glia, and synapses, with significant\\naccuracy improvements over state-of-the-art (SOTA) methods, including recent\\nSAM-based adapters developed for the medical domain and other vision\\ntransformer-based approaches. Experimental results indicate that our approach\\noutperforms existing solutions in the segmentation of complex processes like\\nglia and post-synaptic densities. Our code and models are available at\\nhttps://github.com/Uzshah/SAM4EM.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-30T11:38:02Z\"}"}

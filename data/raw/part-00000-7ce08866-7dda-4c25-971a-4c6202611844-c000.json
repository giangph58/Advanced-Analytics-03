{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11990v1\", \"title\": \"Secure Transfer Learning: Training Clean Models Against Backdoor in\\n  (Both) Pre-trained Encoders and Downstream Datasets\", \"summary\": \"Transfer learning from pre-trained encoders has become essential in modern\\nmachine learning, enabling efficient model adaptation across diverse tasks.\\nHowever, this combination of pre-training and downstream adaptation creates an\\nexpanded attack surface, exposing models to sophisticated backdoor embeddings\\nat both the encoder and dataset levels--an area often overlooked in prior\\nresearch. Additionally, the limited computational resources typically available\\nto users of pre-trained encoders constrain the effectiveness of generic\\nbackdoor defenses compared to end-to-end training from scratch. In this work,\\nwe investigate how to mitigate potential backdoor risks in resource-constrained\\ntransfer learning scenarios. Specifically, we conduct an exhaustive analysis of\\nexisting defense strategies, revealing that many follow a reactive workflow\\nbased on assumptions that do not scale to unknown threats, novel attack types,\\nor different training paradigms. In response, we introduce a proactive mindset\\nfocused on identifying clean elements and propose the Trusted Core (T-Core)\\nBootstrapping framework, which emphasizes the importance of pinpointing\\ntrustworthy data and neurons to enhance model security. Our empirical\\nevaluations demonstrate the effectiveness and superiority of T-Core,\\nspecifically assessing 5 encoder poisoning attacks, 7 dataset poisoning\\nattacks, and 14 baseline defenses across five benchmark datasets, addressing\\nfour scenarios of 3 potential backdoor threats.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CR\", \"published\": \"2025-04-16T11:33:03Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23687v1\", \"title\": \"MKA: Leveraging Cross-Lingual Consensus for Model Abstention\", \"summary\": \"Reliability of LLMs is questionable even as they get better at more tasks. A\\nwider adoption of LLMs is contingent on whether they are usably factual. And if\\nthey are not, on whether they can properly calibrate their confidence in their\\nresponses. This work focuses on utilizing the multilingual knowledge of an LLM\\nto inform its decision to abstain or answer when prompted. We develop a\\nmultilingual pipeline to calibrate the model's confidence and let it abstain\\nwhen uncertain. We run several multilingual models through the pipeline to\\nprofile them across different languages. We find that the performance of the\\npipeline varies by model and language, but that in general they benefit from\\nit. This is evidenced by the accuracy improvement of $71.2\\\\%$ for Bengali over\\na baseline performance without the pipeline. Even a high-resource language like\\nEnglish sees a $15.5\\\\%$ improvement. These results hint at possible further\\nimprovements.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-03-31T03:38:12Z\"}"}

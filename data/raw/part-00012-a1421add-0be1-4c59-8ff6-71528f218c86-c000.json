{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16023v1\", \"title\": \"PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud\\n  Learning\", \"summary\": \"Self-supervised representation learning for point cloud has demonstrated\\neffectiveness in improving pre-trained model performance across diverse tasks.\\nHowever, as pre-trained models grow in complexity, fully fine-tuning them for\\ndownstream applications demands substantial computational and storage\\nresources. Parameter-efficient fine-tuning (PEFT) methods offer a promising\\nsolution to mitigate these resource requirements, yet most current approaches\\nrely on complex adapter and prompt mechanisms that increase tunable parameters.\\nIn this paper, we propose PointLoRA, a simple yet effective method that\\ncombines low-rank adaptation (LoRA) with multi-scale token selection to\\nefficiently fine-tune point cloud models. Our approach embeds LoRA layers\\nwithin the most parameter-intensive components of point cloud transformers,\\nreducing the need for tunable parameters while enhancing global feature\\ncapture. Additionally, multi-scale token selection extracts critical local\\ninformation to serve as prompts for downstream fine-tuning, effectively\\ncomplementing the global context captured by LoRA. The experimental results\\nacross various pre-trained models and three challenging public datasets\\ndemonstrate that our approach achieves competitive performance with only 3.43%\\nof the trainable parameters, making it highly effective for\\nresource-constrained applications. Source code is available at:\\nhttps://github.com/songw-zju/PointLoRA.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-22T16:41:21Z\"}"}

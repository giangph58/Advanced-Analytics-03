{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16074v1\", \"title\": \"PHYBench: Holistic Evaluation of Physical Perception and Reasoning in\\n  Large Language Models\", \"summary\": \"We introduce PHYBench, a novel, high-quality benchmark designed for\\nevaluating reasoning capabilities of large language models (LLMs) in physical\\ncontexts. PHYBench consists of 500 meticulously curated physics problems based\\non real-world physical scenarios, designed to assess the ability of models to\\nunderstand and reason about realistic physical processes. Covering mechanics,\\nelectromagnetism, thermodynamics, optics, modern physics, and advanced physics,\\nthe benchmark spans difficulty levels from high school exercises to\\nundergraduate problems and Physics Olympiad challenges. Additionally, we\\npropose the Expression Edit Distance (EED) Score, a novel evaluation metric\\nbased on the edit distance between mathematical expressions, which effectively\\ncaptures differences in model reasoning processes and results beyond\\ntraditional binary scoring methods. We evaluate various LLMs on PHYBench and\\ncompare their performance with human experts. Our results reveal that even\\nstate-of-the-art reasoning models significantly lag behind human experts,\\nhighlighting their limitations and the need for improvement in complex physical\\nreasoning scenarios. Our benchmark results and dataset are publicly available\\nat https://phybench-official.github.io/phybench-demo/.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-22T17:53:29Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15752v1\", \"title\": \"On the complexity of proximal gradient and proximal Newton-CG methods\\n  for \\\\(\\\\ell_1\\\\)-regularized Optimization\", \"summary\": \"In this paper, we propose two second-order methods for solving the\\n\\\\(\\\\ell_1\\\\)-regularized composite optimization problem, which are developed\\nbased on two distinct definitions of approximate second-order stationary\\npoints. We introduce a hybrid proximal gradient and negative curvature method,\\nas well as a proximal Newton-CG method, to find a strong* approximate\\nsecond-order stationary point and a weak approximate second-order stationary\\npoint for \\\\(\\\\ell_1\\\\)-regularized optimization problems, respectively.\\nComprehensive analyses are provided regarding the iteration complexity,\\ncomputational complexity, and the local superlinear convergence rates of the\\nfirst phases of these two methods under specific error bound conditions.\\nSpecifically, we demonstrate that the proximal Newton-CG method achieves the\\nbest-known iteration complexity for attaining the proposed weak approximate\\nsecond-order stationary point, which is consistent with the results for finding\\nan approximate second-order stationary point in unconstrained optimization.\\nThrough a toy example, we show that our proposed methods can effectively escape\\nthe first-order approximate solution. Numerical experiments implemented on the\\n\\\\(\\\\ell_1\\\\)-regularized Student's t-regression problem validate the\\neffectiveness of both methods.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC\", \"published\": \"2025-04-22T09:56:28Z\"}"}

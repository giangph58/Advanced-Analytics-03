{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19625v1\", \"title\": \"Rulebook: bringing co-routines to reinforcement learning environments\", \"summary\": \"Reinforcement learning (RL) algorithms, due to their reliance on external\\nsystems to learn from, require digital environments (e.g., simulators) with\\nvery simple interfaces, which in turn constrain significantly the\\nimplementation of such environments. In particular, these environments are\\nimplemented either as separate processes or as state machines, leading to\\nsynchronization and communication overheads in the first case, and to\\nunstructured programming in the second.\\n  We propose a new domain-specific, co-routine-based, compiled language, called\\nRulebook, designed to automatically generate the state machine required to\\ninteract with machine learning (ML) algorithms and similar applications, with\\nno performance overhead. Rulebook allows users to express programs without\\nneeding to be aware of the specific interface required by the ML components. By\\ndecoupling the execution model of the program from the syntactical encoding of\\nthe program, and thus without the need for manual state management, Rulebook\\nallows to create larger and more sophisticated environments at a lower\\ndevelopment cost.\", \"main_category\": \"cs.PL\", \"categories\": \"cs.PL,cs.LG\", \"published\": \"2025-04-28T09:34:34Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23712v1\", \"title\": \"ElimPCL: Eliminating Noise Accumulation with Progressive Curriculum\\n  Labeling for Source-Free Domain Adaptation\", \"summary\": \"Source-Free Domain Adaptation (SFDA) aims to train a target model without\\nsource data, and the key is to generate pseudo-labels using a pre-trained\\nsource model. However, we observe that the source model often produces highly\\nuncertain pseudo-labels for hard samples, particularly those heavily affected\\nby domain shifts, leading to these noisy pseudo-labels being introduced even\\nbefore adaptation and further reinforced through parameter updates.\\nAdditionally, they continuously influence neighbor samples through propagation\\nin the feature space.To eliminate the issue of noise accumulation, we propose a\\nnovel Progressive Curriculum Labeling (ElimPCL) method, which iteratively\\nfilters trustworthy pseudo-labeled samples based on prototype consistency to\\nexclude high-noise samples from training. Furthermore, a Dual MixUP technique\\nis designed in the feature space to enhance the separability of hard samples,\\nthereby mitigating the interference of noisy samples on their\\nneighbors.Extensive experiments validate the effectiveness of ElimPCL,\\nachieving up to a 3.4% improvement on challenging tasks compared to\\nstate-of-the-art methods.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-31T04:28:27Z\"}"}

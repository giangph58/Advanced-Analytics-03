{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05111v1\", \"title\": \"Unveiling Language-Specific Features in Large Language Models via Sparse\\n  Autoencoders\", \"summary\": \"The mechanisms behind multilingual capabilities in Large Language Models\\n(LLMs) have been examined using neuron-based or internal-activation-based\\nmethods. However, these methods often face challenges such as superposition and\\nlayer-wise activation variance, which limit their reliability. Sparse\\nAutoencoders (SAEs) offer a more nuanced analysis by decomposing the\\nactivations of LLMs into sparse linear combination of SAE features. We\\nintroduce a novel metric to assess the monolinguality of features obtained from\\nSAEs, discovering that some features are strongly related to specific\\nlanguages. Additionally, we show that ablating these SAE features only\\nsignificantly reduces abilities in one language of LLMs, leaving others almost\\nunaffected. Interestingly, we find some languages have multiple synergistic SAE\\nfeatures, and ablating them together yields greater improvement than ablating\\nindividually. Moreover, we leverage these SAE-derived language-specific\\nfeatures to enhance steering vectors, achieving control over the language\\ngenerated by LLMs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-05-08T10:24:44Z\"}"}

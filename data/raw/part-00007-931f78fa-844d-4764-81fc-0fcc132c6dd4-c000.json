{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16448v1\", \"title\": \"EMRModel: A Large Language Model for Extracting Medical Consultation\\n  Dialogues into Structured Medical Records\", \"summary\": \"Medical consultation dialogues contain critical clinical information, yet\\ntheir unstructured nature hinders effective utilization in diagnosis and\\ntreatment. Traditional methods, relying on rule-based or shallow machine\\nlearning techniques, struggle to capture deep and implicit semantics. Recently,\\nlarge pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight\\nfine-tuning method, have shown promise for structured information extraction.\\nWe propose EMRModel, a novel approach that integrates LoRA-based fine-tuning\\nwith code-style prompt design, aiming to efficiently convert medical\\nconsultation dialogues into structured electronic medical records (EMRs).\\nAdditionally, we construct a high-quality, realistically grounded dataset of\\nmedical consultation dialogues with detailed annotations. Furthermore, we\\nintroduce a fine-grained evaluation benchmark for medical consultation\\ninformation extraction and provide a systematic evaluation methodology,\\nadvancing the optimization of medical natural language processing (NLP) models.\\nExperimental results show EMRModel achieves an F1 score of 88.1%, improving\\nby49.5% over standard pre-trained models. Compared to traditional LoRA\\nfine-tuning methods, our model shows superior performance, highlighting its\\neffectiveness in structured medical record extraction tasks.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-23T06:17:55Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15160v1\", \"title\": \"The Synthetic Imputation Approach: Generating Optimal Synthetic Texts\\n  For Underrepresented Categories In Supervised Classification Tasks\", \"summary\": \"Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa,\\nrequire that all categories in an annotation task be sufficiently represented\\nin the training data for optimal performance. However, it is often difficult to\\nfind sufficient examples for all categories in a task when building a\\nhigh-quality training set. In this article, I describe this problem and propose\\na solution, the synthetic imputation approach. Leveraging a generative LLM\\n(GPT-4o), this approach generates synthetic texts based on careful prompting\\nand five original examples drawn randomly with replacement from the sample.\\nThis approach ensures that new synthetic texts are sufficiently different from\\nthe original texts to reduce overfitting, but retain the underlying substantive\\nmeaning of the examples to maximize out-of-sample performance. With 75 original\\nexamples or more, synthetic imputation's performance is on par with a full\\nsample of original texts, and overfitting remains low, predictable and\\ncorrectable with 50 original samples. The synthetic imputation approach\\nprovides a novel role for generative LLMs in research and allows applied\\nresearchers to balance their datasets for best performance.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-21T15:07:26Z\"}"}

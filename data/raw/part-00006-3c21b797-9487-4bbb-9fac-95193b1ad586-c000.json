{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20453v1\", \"title\": \"Shadow-point Enhanced Inexact Accelerated Proximal Gradient Method with\\n  Preserved Convergence Guarantees\", \"summary\": \"We consider the problem of optimizing the sum of a smooth convex function and\\na non-smooth convex function using the inexact accelerated proximal gradient\\n(APG) method. A key limitation of existing approaches is their reliance on\\nfeasible approximate solutions to subproblems, which is often computationally\\nexpensive or even unrealistic to obtain in practice. To address this\\nlimitation, we develop a shadow-point enhanced inexact accelerated proximal\\ngradient method (SpinAPG), which can eliminate the feasibility requirement\\nwhile preserving all desirable convergence properties of the APG method,\\nincluding the iterate convergence and an $o(1/k^2)$ convergence rate for the\\nobjective function value, under suitable summable-error conditions. Our method\\nalso provides a more flexible and computationally efficient inexact framework\\nfor the APG method, with a fairly easy-to-implement error criterion. Finally,\\nwe demonstrate the practical advantages of our SpinAPG through numerical\\nexperiments on a relaxation of the quadratic assignment problem, showcasing its\\neffectiveness while bypassing the explicit computation of a feasible point.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC\", \"published\": \"2025-04-29T06:04:29Z\"}"}

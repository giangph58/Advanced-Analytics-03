{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10307v1\", \"title\": \"CROSSAN: Towards Efficient and Effective Adaptation of Multiple\\n  Multimodal Foundation Models for Sequential Recommendation\", \"summary\": \"Multimodal Foundation Models (MFMs) excel at representing diverse raw\\nmodalities (e.g., text, images, audio, videos, etc.). As recommender systems\\nincreasingly incorporate these modalities, leveraging MFMs to generate better\\nrepresentations has great potential. However, their application in sequential\\nrecommendation remains largely unexplored. This is primarily because mainstream\\nadaptation methods, such as Fine-Tuning and even Parameter-Efficient\\nFine-Tuning (PEFT) techniques (e.g., Adapter and LoRA), incur high\\ncomputational costs, especially when integrating multiple modality encoders,\\nthus hindering research progress. As a result, it remains unclear whether we\\ncan efficiently and effectively adapt multiple (>2) MFMs for the sequential\\nrecommendation task.\\n  To address this, we propose a plug-and-play Cross-modal Side Adapter Network\\n(CROSSAN). Leveraging the fully decoupled side adapter-based paradigm, CROSSAN\\nachieves high efficiency while enabling cross-modal learning across diverse\\nmodalities. To optimize the final stage of multimodal fusion across diverse\\nmodalities, we adopt the Mixture of Modality Expert Fusion (MOMEF) mechanism.\\nCROSSAN achieves superior performance on the public datasets for adapting four\\nfoundation models with raw modalities. Performance consistently improves as\\nmore MFMs are adapted. We will release our code and datasets to facilitate\\nfuture research.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR\", \"published\": \"2025-04-14T15:14:59Z\"}"}

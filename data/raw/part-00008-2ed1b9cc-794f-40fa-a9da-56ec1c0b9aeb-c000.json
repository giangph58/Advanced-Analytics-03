{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01890v1\", \"title\": \"Is Temporal Prompting All We Need For Limited Labeled Action\\n  Recognition?\", \"summary\": \"Video understanding has shown remarkable improvements in recent years,\\nlargely dependent on the availability of large scaled labeled datasets. Recent\\nadvancements in visual-language models, especially based on contrastive\\npretraining, have shown remarkable generalization in zero-shot tasks, helping\\nto overcome this dependence on labeled datasets. Adaptations of such models for\\nvideos, typically involve modifying the architecture of vision-language models\\nto cater to video data. However, this is not trivial, since such adaptations\\nare mostly computationally intensive and struggle with temporal modeling. We\\npresent TP-CLIP, an adaptation of CLIP that leverages temporal visual prompting\\nfor temporal adaptation without modifying the core CLIP architecture. This\\npreserves its generalization abilities. TP-CLIP efficiently integrates into the\\nCLIP architecture, leveraging its pre-trained capabilities for video data.\\nExtensive experiments across various datasets demonstrate its efficacy in\\nzero-shot and few-shot learning, outperforming existing approaches with fewer\\nparameters and computational efficiency. In particular, we use just 1/3 the\\nGFLOPs and 1/28 the number of tuneable parameters in comparison to recent\\nstate-of-the-art and still outperform it by up to 15.8% depending on the task\\nand dataset.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-02T16:50:28Z\"}"}

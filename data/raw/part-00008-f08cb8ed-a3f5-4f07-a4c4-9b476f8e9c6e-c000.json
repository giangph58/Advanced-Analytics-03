{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04110v1\", \"title\": \"Alpha Excel Benchmark\", \"summary\": \"This study presents a novel benchmark for evaluating Large Language Models\\n(LLMs) using challenges derived from the Financial Modeling World Cup (FMWC)\\nExcel competitions. We introduce a methodology for converting 113 existing FMWC\\nchallenges into programmatically evaluable JSON formats and use this dataset to\\ncompare the performance of several leading LLMs. Our findings demonstrate\\nsignificant variations in performance across different challenge categories,\\nwith models showing specific strengths in pattern recognition tasks but\\nstruggling with complex numerical reasoning. The benchmark provides a\\nstandardized framework for assessing LLM capabilities in realistic\\nbusiness-oriented tasks rather than abstract academic problems. This research\\ncontributes to the growing field of AI benchmarking by establishing proficiency\\namong the 1.5 billion people who daily use Microsoft Excel as a meaningful\\nevaluation metric that bridges the gap between academic AI benchmarks and\\npractical business applications.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-07T03:56:26Z\"}"}

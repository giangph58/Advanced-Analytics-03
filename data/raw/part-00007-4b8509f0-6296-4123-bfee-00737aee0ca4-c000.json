{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19659v1\", \"title\": \"Hardware/Software Co-Design of RISC-V Extensions for Accelerating Sparse\\n  DNNs on FPGAs\", \"summary\": \"The customizability of RISC-V makes it an attractive choice for accelerating\\ndeep neural networks (DNNs). It can be achieved through instruction set\\nextensions and corresponding custom functional units. Yet, efficiently\\nexploiting these opportunities requires a hardware/software co-design approach\\nin which the DNN model, software, and hardware are designed together. In this\\npaper, we propose novel RISC-V extensions for accelerating DNN models\\ncontaining semi-structured and unstructured sparsity. While the idea of\\naccelerating structured and unstructured pruning is not new, our novel design\\noffers various advantages over other designs. To exploit semi-structured\\nsparsity, we take advantage of the fine-grained (bit-level) configurability of\\nFPGAs and suggest reserving a few bits in a block of DNN weights to encode the\\ninformation about sparsity in the succeeding blocks. The proposed custom\\nfunctional unit utilizes this information to skip computations. To exploit\\nunstructured sparsity, we propose a variable cycle sequential\\nmultiply-and-accumulate unit that performs only as many multiplications as the\\nnon-zero weights. Our implementation of unstructured and semi-structured\\npruning accelerators can provide speedups of up to a factor of 3 and 4,\\nrespectively. We then propose a combined design that can accelerate both types\\nof sparsities, providing speedups of up to a factor of 5. Our designs consume a\\nsmall amount of additional FPGA resources such that the resulting co-designs\\nenable the acceleration of DNNs even on small FPGAs. We benchmark our designs\\non standard TinyML applications such as keyword spotting, image classification,\\nand person detection.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.AR\", \"published\": \"2025-04-28T10:19:39Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03531v1\", \"title\": \"Faster MoE LLM Inference for Extremely Large Models\", \"summary\": \"Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually\\nbecoming the mainstream approach for ultra-large-scale models. Existing\\noptimization efforts for MoE models have focused primarily on coarse-grained\\nMoE architectures. With the emergence of DeepSeek Models, fine-grained MoE\\nmodels are gaining popularity, yet research on them remains limited. Therefore,\\nwe want to discuss the efficiency dynamic under different service loads.\\nAdditionally, fine-grained models allow deployers to reduce the number of\\nrouted experts, both activated counts and total counts, raising the question of\\nhow this reduction affects the trade-off between MoE efficiency and\\nperformance. Our findings indicate that while deploying MoE models presents\\ngreater challenges, it also offers significant optimization opportunities.\\nReducing the number of activated experts can lead to substantial efficiency\\nimprovements in certain scenarios, with only minor performance degradation.\\nReducing the total number of experts provides limited efficiency gains but\\nresults in severe performance degradation. Our method can increase throughput\\nby at least 10\\\\% without any performance degradation. Overall, we conclude that\\nMoE inference optimization remains an area with substantial potential for\\nexploration and improvement.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-05-06T13:41:17Z\"}"}

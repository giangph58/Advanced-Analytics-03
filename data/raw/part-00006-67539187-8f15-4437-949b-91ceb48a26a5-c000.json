{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21292v1\", \"title\": \"Can We Achieve Efficient Diffusion without Self-Attention? Distilling\\n  Self-Attention into Convolutions\", \"summary\": \"Contemporary diffusion models built upon U-Net or Diffusion Transformer (DiT)\\narchitectures have revolutionized image generation through transformer-based\\nattention mechanisms. The prevailing paradigm has commonly employed\\nself-attention with quadratic computational complexity to handle global spatial\\nrelationships in complex images, thereby synthesizing high-fidelity images with\\ncoherent visual semantics.Contrary to conventional wisdom, our systematic\\nlayer-wise analysis reveals an interesting discrepancy: self-attention in\\npre-trained diffusion models predominantly exhibits localized attention\\npatterns, closely resembling convolutional inductive biases. This suggests that\\nglobal interactions in self-attention may be less critical than commonly\\nassumed.Driven by this, we propose \\\\(\\\\Delta\\\\)ConvFusion to replace conventional\\nself-attention modules with Pyramid Convolution Blocks\\n(\\\\(\\\\Delta\\\\)ConvBlocks).By distilling attention patterns into localized\\nconvolutional operations while keeping other components frozen,\\n\\\\(\\\\Delta\\\\)ConvFusion achieves performance comparable to transformer-based\\ncounterparts while reducing computational cost by 6929$\\\\times$ and surpassing\\nLinFusion by 5.42$\\\\times$ in efficiency--all without compromising generative\\nfidelity.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-30T03:57:28Z\"}"}

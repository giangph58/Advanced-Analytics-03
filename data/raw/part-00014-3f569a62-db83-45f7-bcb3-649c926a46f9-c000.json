{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16016v1\", \"title\": \"Efficient Temporal Consistency in Diffusion-Based Video Editing with\\n  Adaptor Modules: A Theoretical Framework\", \"summary\": \"Adapter-based methods are commonly used to enhance model performance with\\nminimal additional complexity, especially in video editing tasks that require\\nframe-to-frame consistency. By inserting small, learnable modules into\\npretrained diffusion models, these adapters can maintain temporal coherence\\nwithout extensive retraining. Approaches that incorporate prompt learning with\\nboth shared and frame-specific tokens are particularly effective in preserving\\ncontinuity across frames at low training cost. In this work, we want to provide\\na general theoretical framework for adapters that maintain frame consistency in\\nDDIM-based models under a temporal consistency loss. First, we prove that the\\ntemporal consistency objective is differentiable under bounded feature norms,\\nand we establish a Lipschitz bound on its gradient. Second, we show that\\ngradient descent on this objective decreases the loss monotonically and\\nconverges to a local minimum if the learning rate is within an appropriate\\nrange. Finally, we analyze the stability of modules in the DDIM inversion\\nprocedure, showing that the associated error remains controlled. These\\ntheoretical findings will reinforce the reliability of diffusion-based video\\nediting methods that rely on adapter strategies and provide theoretical\\ninsights in video generation tasks.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-22T16:28:35Z\"}"}

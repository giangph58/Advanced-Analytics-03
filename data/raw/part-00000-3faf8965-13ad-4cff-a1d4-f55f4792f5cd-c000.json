{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15823v1\", \"title\": \"Human-Imperceptible Physical Adversarial Attack for NIR Face Recognition\\n  Models\", \"summary\": \"Near-infrared (NIR) face recognition systems, which can operate effectively\\nin low-light conditions or in the presence of makeup, exhibit vulnerabilities\\nwhen subjected to physical adversarial attacks. To further demonstrate the\\npotential risks in real-world applications, we design a novel, stealthy, and\\npractical adversarial patch to attack NIR face recognition systems in a\\nblack-box setting. We achieved this by utilizing human-imperceptible\\ninfrared-absorbing ink to generate multiple patches with digitally optimized\\nshapes and positions for infrared images. To address the optimization mismatch\\nbetween digital and real-world NIR imaging, we develop a light reflection model\\nfor human skin to minimize pixel-level discrepancies by simulating NIR light\\nreflection.\\n  Compared to state-of-the-art (SOTA) physical attacks on NIR face recognition\\nsystems, the experimental results show that our method improves the attack\\nsuccess rate in both digital and physical domains, particularly maintaining\\neffectiveness across various face postures. Notably, the proposed approach\\noutperforms SOTA methods, achieving an average attack success rate of 82.46% in\\nthe physical domain across different models, compared to 64.18% for existing\\nmethods. The artifact is available at\\nhttps://anonymous.4open.science/r/Human-imperceptible-adversarial-patch-0703/.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-22T12:10:25Z\"}"}

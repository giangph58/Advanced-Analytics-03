{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05287v1\", \"title\": \"RobustDexGrasp: Robust Dexterous Grasping of General Objects from\\n  Single-view Perception\", \"summary\": \"Robust grasping of various objects from single-view perception is fundamental\\nfor dexterous robots. Previous works often rely on fully observable objects,\\nexpert demonstrations, or static grasping poses, which restrict their\\ngeneralization ability and adaptability to external disturbances. In this\\npaper, we present a reinforcement-learning-based framework that enables\\nzero-shot dynamic dexterous grasping of a wide range of unseen objects from\\nsingle-view perception, while performing adaptive motions to external\\ndisturbances. We utilize a hand-centric object representation for shape feature\\nextraction that emphasizes interaction-relevant local shapes, enhancing\\nrobustness to shape variance and uncertainty. To enable effective hand\\nadaptation to disturbances with limited observations, we propose a mixed\\ncurriculum learning strategy, which first utilizes imitation learning to\\ndistill a policy trained with privileged real-time visual-tactile feedback, and\\ngradually transfers to reinforcement learning to learn adaptive motions under\\ndisturbances caused by observation noises and dynamic randomization. Our\\nexperiments demonstrate strong generalization in grasping unseen objects with\\nrandom poses, achieving success rates of 97.0% across 247,786 simulated objects\\nand 94.6% across 512 real objects. We also demonstrate the robustness of our\\nmethod to various disturbances, including unobserved object movement and\\nexternal forces, through both quantitative and qualitative evaluations. Project\\nPage: https://zdchan.github.io/Robust_DexGrasp/\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO\", \"published\": \"2025-04-07T17:38:19Z\"}"}

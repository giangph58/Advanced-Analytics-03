{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05336v1\", \"title\": \"Progressive Inertial Poser: Progressive Real-Time Kinematic Chain\\n  Estimation for 3D Full-Body Pose from Three IMU Sensors\", \"summary\": \"The motion capture system that supports full-body virtual representation is\\nof key significance for virtual reality. Compared to vision-based systems,\\nfull-body pose estimation from sparse tracking signals is not limited by\\nenvironmental conditions or recording range. However, previous works either\\nface the challenge of wearing additional sensors on the pelvis and lower-body\\nor rely on external visual sensors to obtain global positions of key joints. To\\nimprove the practicality of the technology for virtual reality applications, we\\nestimate full-body poses using only inertial data obtained from three Inertial\\nMeasurement Unit (IMU) sensors worn on the head and wrists, thereby reducing\\nthe complexity of the hardware system. In this work, we propose a method called\\nProgressive Inertial Poser (ProgIP) for human pose estimation, which combines\\nneural network estimation with a human dynamics model, considers the\\nhierarchical structure of the kinematic chain, and employs a multi-stage\\nprogressive network estimation with increased depth to reconstruct full-body\\nmotion in real time. The encoder combines Transformer Encoder and bidirectional\\nLSTM (TE-biLSTM) to flexibly capture the temporal dependencies of the inertial\\nsequence, while the decoder based on multi-layer perceptrons (MLPs) transforms\\nhigh-dimensional features and accurately projects them onto Skinned\\nMulti-Person Linear (SMPL) model parameters. Quantitative and qualitative\\nexperimental results on multiple public datasets show that our method\\noutperforms state-of-the-art methods with the same inputs, and is comparable to\\nrecent works using six IMU sensors.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-08T15:28:09Z\"}"}

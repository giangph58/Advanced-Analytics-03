{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00551v1\", \"title\": \"100 Days After DeepSeek-R1: A Survey on Replication Studies and More\\n  Directions for Reasoning Language Models\", \"summary\": \"The recent development of reasoning language models (RLMs) represents a novel\\nevolution in large language models. In particular, the recent release of\\nDeepSeek-R1 has generated widespread social impact and sparked enthusiasm in\\nthe research community for exploring the explicit reasoning paradigm of\\nlanguage models. However, the implementation details of the released models\\nhave not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero,\\nDeepSeek-R1, and the distilled small models. As a result, many replication\\nstudies have emerged aiming to reproduce the strong performance achieved by\\nDeepSeek-R1, reaching comparable performance through similar training\\nprocedures and fully open-source data resources. These works have investigated\\nfeasible strategies for supervised fine-tuning (SFT) and reinforcement learning\\nfrom verifiable rewards (RLVR), focusing on data preparation and method design,\\nyielding various valuable insights. In this report, we provide a summary of\\nrecent replication studies to inspire future research. We primarily focus on\\nSFT and RLVR as two main directions, introducing the details for data\\nconstruction, method design and training procedure of current replication\\nstudies. Moreover, we conclude key findings from the implementation details and\\nexperimental results reported by these studies, anticipating to inspire future\\nresearch. We also discuss additional techniques of enhancing RLMs, highlighting\\nthe potential of expanding the application scope of these models, and\\ndiscussing the challenges in development. By this survey, we aim to help\\nresearchers and developers of RLMs stay updated with the latest advancements,\\nand seek to inspire new ideas to further enhance RLMs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-05-01T14:28:35Z\"}"}

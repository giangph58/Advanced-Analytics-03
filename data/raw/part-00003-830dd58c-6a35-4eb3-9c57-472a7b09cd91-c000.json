{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07017v1\", \"title\": \"Adapting GT2-FLS for Uncertainty Quantification: A Blueprint Calibration\\n  Strategy\", \"summary\": \"Uncertainty Quantification (UQ) is crucial for deploying reliable Deep\\nLearning (DL) models in high-stakes applications. Recently, General Type-2\\nFuzzy Logic Systems (GT2-FLSs) have been proven to be effective for UQ,\\noffering Prediction Intervals (PIs) to capture uncertainty. However, existing\\nmethods often struggle with computational efficiency and adaptability, as\\ngenerating PIs for new coverage levels $(\\\\phi_d)$ typically requires retraining\\nthe model. Moreover, methods that directly estimate the entire conditional\\ndistribution for UQ are computationally expensive, limiting their scalability\\nin real-world scenarios. This study addresses these challenges by proposing a\\nblueprint calibration strategy for GT2-FLSs, enabling efficient adaptation to\\nany desired $\\\\phi_d$ without retraining. By exploring the relationship between\\n$\\\\alpha$-plane type reduced sets and uncertainty coverage, we develop two\\ncalibration methods: a lookup table-based approach and a derivative-free\\noptimization algorithm. These methods allow GT2-FLSs to produce accurate and\\nreliable PIs while significantly reducing computational overhead. Experimental\\nresults on high-dimensional datasets demonstrate that the calibrated GT2-FLS\\nachieves superior performance in UQ, highlighting its potential for scalable\\nand practical applications.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-09T16:32:43Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.09961v1\", \"title\": \"Privacy Meets Explainability: Managing Confidential Data and\\n  Transparency Policies in LLM-Empowered Science\", \"summary\": \"As Large Language Models (LLMs) become integral to scientific workflows,\\nconcerns over the confidentiality and ethical handling of confidential data\\nhave emerged. This paper explores data exposure risks through LLM-powered\\nscientific tools, which can inadvertently leak confidential information,\\nincluding intellectual property and proprietary data, from scientists'\\nperspectives. We propose \\\"DataShield\\\", a framework designed to detect\\nconfidential data leaks, summarize privacy policies, and visualize data flow,\\nensuring alignment with organizational policies and procedures. Our approach\\naims to inform scientists about data handling practices, enabling them to make\\ninformed decisions and protect sensitive information. Ongoing user studies with\\nscientists are underway to evaluate the framework's usability, trustworthiness,\\nand effectiveness in tackling real-world privacy challenges.\", \"main_category\": \"cs.HC\", \"categories\": \"cs.HC,cs.AI\", \"published\": \"2025-04-14T07:58:26Z\"}"}

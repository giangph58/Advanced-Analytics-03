{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06578v1\", \"title\": \"Attributes-aware Visual Emotion Representation Learning\", \"summary\": \"Visual emotion analysis or recognition has gained considerable attention due\\nto the growing interest in understanding how images can convey rich semantics\\nand evoke emotions in human perception. However, visual emotion analysis poses\\ndistinctive challenges compared to traditional vision tasks, especially due to\\nthe intricate relationship between general visual features and the different\\naffective states they evoke, known as the affective gap. Researchers have used\\ndeep representation learning methods to address this challenge of extracting\\ngeneralized features from entire images. However, most existing methods\\noverlook the importance of specific emotional attributes such as brightness,\\ncolorfulness, scene understanding, and facial expressions. Through this paper,\\nwe introduce A4Net, a deep representation network to bridge the affective gap\\nby leveraging four key attributes: brightness (Attribute 1), colorfulness\\n(Attribute 2), scene context (Attribute 3), and facial expressions (Attribute\\n4). By fusing and jointly training all aspects of attribute recognition and\\nvisual emotion analysis, A4Net aims to provide a better insight into emotional\\ncontent in images. Experimental results show the effectiveness of A4Net,\\nshowcasing competitive performance compared to state-of-the-art methods across\\ndiverse visual emotion datasets. Furthermore, visualizations of activation maps\\ngenerated by A4Net offer insights into its ability to generalize across\\ndifferent visual emotion datasets.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.MM\", \"published\": \"2025-04-09T05:00:43Z\"}"}

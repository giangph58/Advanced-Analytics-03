{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01328v1\", \"title\": \"Slow-Fast Architecture for Video Multi-Modal Large Language Models\", \"summary\": \"Balancing temporal resolution and spatial detail under limited compute budget\\nremains a key challenge for video-based multi-modal large language models\\n(MLLMs). Existing methods typically compress video representations using\\npredefined rules before feeding them into the LLM, resulting in irreversible\\ninformation loss and often ignoring input instructions. To address this, we\\npropose a novel slow-fast architecture that naturally circumvents this\\ntrade-off, enabling the use of more input frames while preserving spatial\\ndetails. Inspired by how humans first skim a video before focusing on relevant\\nparts, our slow-fast design employs a dual-token strategy: 1) \\\"fast\\\" visual\\ntokens -- a compact set of compressed video features -- are fed into the LLM\\nalongside text embeddings to provide a quick overview; 2) \\\"slow\\\" visual tokens\\n-- uncompressed video features -- are cross-attended by text embeddings through\\nspecially designed hybrid decoder layers, enabling instruction-aware extraction\\nof relevant visual details with linear complexity. We conduct systematic\\nexploration to optimize both the overall architecture and key components.\\nExperiments show that our model significantly outperforms self-attention-only\\nbaselines, extending the input capacity from 16 to 128 frames with just a 3%\\nincrease in computation, and achieving a 16% average performance improvement\\nacross five video understanding benchmarks. Our 7B model achieves\\nstate-of-the-art performance among models of similar size. Furthermore, our\\nslow-fast architecture is a plug-and-play design that can be integrated into\\nother video MLLMs to improve efficiency and scalability.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-02T03:24:58Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15689v1\", \"title\": \"The Viability of Crowdsourcing for RAG Evaluation\", \"summary\": \"How good are humans at writing and judging responses in retrieval-augmented\\ngeneration (RAG) scenarios? To answer this question, we investigate the\\nefficacy of crowdsourcing for RAG through two complementary studies: response\\nwriting and response utility judgment. We present the Crowd RAG Corpus 2025\\n(CrowdRAG-25), which consists of 903 human-written and 903 LLM-generated\\nresponses for the 301 topics of the TREC RAG'24 track, across the three\\ndiscourse styles 'bulleted list', 'essay', and 'news'. For a selection of 65\\ntopics, the corpus further contains 47,320 pairwise human judgments and 10,556\\npairwise LLM judgments across seven utility dimensions (e.g., coverage and\\ncoherence). Our analyses give insights into human writing behavior for RAG and\\nthe viability of crowdsourcing for RAG evaluation. Human pairwise judgments\\nprovide reliable and cost-effective results compared to LLM-based pairwise or\\nhuman/LLM-based pointwise judgments, as well as automated comparisons with\\nhuman-written reference responses. All our data and tools are freely available.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR\", \"published\": \"2025-04-22T08:13:34Z\"}"}

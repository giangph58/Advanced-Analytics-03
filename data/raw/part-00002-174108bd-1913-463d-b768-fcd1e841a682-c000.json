{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16003v1\", \"title\": \"MVQA: Mamba with Unified Sampling for Efficient Video Quality Assessment\", \"summary\": \"The rapid growth of long-duration, high-definition videos has made efficient\\nvideo quality assessment (VQA) a critical challenge. Existing research\\ntypically tackles this problem through two main strategies: reducing model\\nparameters and resampling inputs. However, light-weight Convolution Neural\\nNetworks (CNN) and Transformers often struggle to balance efficiency with high\\nperformance due to the requirement of long-range modeling capabilities.\\nRecently, the state-space model, particularly Mamba, has emerged as a promising\\nalternative, offering linear complexity with respect to sequence length.\\nMeanwhile, efficient VQA heavily depends on resampling long sequences to\\nminimize computational costs, yet current resampling methods are often weak in\\npreserving essential semantic information. In this work, we present MVQA, a\\nMamba-based model designed for efficient VQA along with a novel Unified\\nSemantic and Distortion Sampling (USDS) approach. USDS combines semantic patch\\nsampling from low-resolution videos and distortion patch sampling from\\noriginal-resolution videos. The former captures semantically dense regions,\\nwhile the latter retains critical distortion details. To prevent computation\\nincrease from dual inputs, we propose a fusion mechanism using pre-defined\\nmasks, enabling a unified sampling strategy that captures both semantic and\\nquality information without additional computational burden. Experiments show\\nthat the proposed MVQA, equipped with USDS, achieve comparable performance to\\nstate-of-the-art methods while being $2\\\\times$ as fast and requiring only $1/5$\\nGPU memory.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-22T16:08:23Z\"}"}

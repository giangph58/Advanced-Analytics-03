{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11042v1\", \"title\": \"LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews\", \"summary\": \"Peer review is a cornerstone of quality control in scientific publishing.\\nWith the increasing workload, the unintended use of `quick' heuristics,\\nreferred to as lazy thinking, has emerged as a recurring issue compromising\\nreview quality. Automated methods to detect such heuristics can help improve\\nthe peer-reviewing process. However, there is limited NLP research on this\\nissue, and no real-world dataset exists to support the development of detection\\ntools. This work introduces LazyReview, a dataset of peer-review sentences\\nannotated with fine-grained lazy thinking categories. Our analysis reveals that\\nLarge Language Models (LLMs) struggle to detect these instances in a zero-shot\\nsetting. However, instruction-based fine-tuning on our dataset significantly\\nboosts performance by 10-20 performance points, highlighting the importance of\\nhigh-quality training data. Furthermore, a controlled experiment demonstrates\\nthat reviews revised with lazy thinking feedback are more comprehensive and\\nactionable than those written without such feedback. We will release our\\ndataset and the enhanced guidelines that can be used to train junior reviewers\\nin the community. (Code available here:\\nhttps://github.com/UKPLab/arxiv2025-lazy-review)\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-15T10:07:33Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16671v1\", \"title\": \"LLMCode: Evaluating and Enhancing Researcher-AI Alignment in Qualitative\\n  Analysis\", \"summary\": \"The use of large language models (LLMs) in qualitative analysis offers\\nenhanced efficiency but raises questions about their alignment with the\\ncontextual nature of research for design (RfD). This research examines the\\ntrustworthiness of LLM-driven design insights, using qualitative coding as a\\ncase study to explore the interpretive processes central to RfD. We introduce\\nLLMCode, an open-source tool integrating two metrics, namely Intersection over\\nUnion (IoU) and Modified Hausdorff Distance, to assess the alignment between\\nhuman and LLM-generated insights. Across two studies involving 26 designers, we\\nfind that while the model performs well with deductive coding, its ability to\\nemulate a designer's deeper interpretive lens over the data is limited,\\nemphasising the importance of human-AI collaboration. Our results highlight a\\nreciprocal dynamic where users refine LLM outputs and adapt their own\\nperspectives based on the model's suggestions. These findings underscore the\\nimportance of fostering appropriate reliance on LLMs by designing tools that\\npreserve interpretive depth while facilitating intuitive collaboration between\\ndesigners and AI.\", \"main_category\": \"cs.HC\", \"categories\": \"cs.HC\", \"published\": \"2025-04-23T12:39:06Z\"}"}

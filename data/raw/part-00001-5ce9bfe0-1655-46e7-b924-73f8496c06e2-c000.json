{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15804v1\", \"title\": \"Insights from Verification: Training a Verilog Generation LLM with\\n  Reinforcement Learning with Testbench Feedback\", \"summary\": \"Large language models (LLMs) have shown strong performance in Verilog\\ngeneration from natural language description. However, ensuring the functional\\ncorrectness of the generated code remains a significant challenge. This paper\\nintroduces a method that integrates verification insights from testbench into\\nthe training of Verilog generation LLMs, aligning the training with the\\nfundamental goal of hardware design: functional correctness. The main obstacle\\nin using LLMs for Verilog code generation is the lack of sufficient functional\\nverification data, particularly testbenches paired with design specifications\\nand code. To address this problem, we introduce an automatic testbench\\ngeneration pipeline that decomposes the process and uses feedback from the\\nVerilog compiler simulator (VCS) to reduce hallucination and ensure\\ncorrectness. We then use the testbench to evaluate the generated codes and\\ncollect them for further training, where verification insights are introduced.\\nOur method applies reinforcement learning (RL), specifically direct preference\\noptimization (DPO), to align Verilog code generation with functional\\ncorrectness by training preference pairs based on testbench outcomes. In\\nevaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2,\\nand VerilogEval v2, our approach consistently outperforms state-of-the-art\\nbaselines in generating functionally correct Verilog code. We open source all\\ntraining code, data, and models at\\nhttps://anonymous.4open.science/r/VeriPrefer-E88B.\", \"main_category\": \"cs.AR\", \"categories\": \"cs.AR,cs.AI\", \"published\": \"2025-04-22T11:38:14Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04347v1\", \"title\": \"CountDiffusion: Text-to-Image Synthesis with Training-Free\\n  Counting-Guidance Diffusion\", \"summary\": \"Stable Diffusion has advanced text-to-image synthesis, but training models to\\ngenerate images with accurate object quantity is still difficult due to the\\nhigh computational cost and the challenge of teaching models the abstract\\nconcept of quantity. In this paper, we propose CountDiffusion, a training-free\\nframework aiming at generating images with correct object quantity from textual\\ndescriptions. CountDiffusion consists of two stages. In the first stage, an\\nintermediate denoising result is generated by the diffusion model to predict\\nthe final synthesized image with one-step denoising, and a counting model is\\nused to count the number of objects in this image. In the second stage, a\\ncorrection module is used to correct the object quantity by changing the\\nattention map of the object with universal guidance. The proposed\\nCountDiffusion can be plugged into any diffusion-based text-to-image (T2I)\\ngeneration models without further training. Experiment results demonstrate the\\nsuperiority of our proposed CountDiffusion, which improves the accurate object\\nquantity generation ability of T2I models by a large margin.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-07T11:47:35Z\"}"}

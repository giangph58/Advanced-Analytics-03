{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19983v1\", \"title\": \"Emergence and scaling laws in SGD learning of shallow neural networks\", \"summary\": \"We study the complexity of online stochastic gradient descent (SGD) for\\nlearning a two-layer neural network with $P$ neurons on isotropic Gaussian\\ndata: $f_*(\\\\boldsymbol{x}) = \\\\sum_{p=1}^P a_p\\\\cdot\\n\\\\sigma(\\\\langle\\\\boldsymbol{x},\\\\boldsymbol{v}_p^*\\\\rangle)$, $\\\\boldsymbol{x} \\\\sim\\n\\\\mathcal{N}(0,\\\\boldsymbol{I}_d)$, where the activation\\n$\\\\sigma:\\\\mathbb{R}\\\\to\\\\mathbb{R}$ is an even function with information exponent\\n$k_*>2$ (defined as the lowest degree in the Hermite expansion),\\n$\\\\{\\\\boldsymbol{v}^*_p\\\\}_{p\\\\in[P]}\\\\subset \\\\mathbb{R}^d$ are orthonormal signal\\ndirections, and the non-negative second-layer coefficients satisfy $\\\\sum_{p}\\na_p^2=1$. We focus on the challenging ``extensive-width'' regime $P\\\\gg 1$ and\\npermit diverging condition number in the second-layer, covering as a special\\ncase the power-law scaling $a_p\\\\asymp p^{-\\\\beta}$ where\\n$\\\\beta\\\\in\\\\mathbb{R}_{\\\\ge 0}$. We provide a precise analysis of SGD dynamics for\\nthe training of a student two-layer network to minimize the mean squared error\\n(MSE) objective, and explicitly identify sharp transition times to recover each\\nsignal direction. In the power-law setting, we characterize scaling law\\nexponents for the MSE loss with respect to the number of training samples and\\nSGD steps, as well as the number of parameters in the student neural network.\\nOur analysis entails that while the learning of individual teacher neurons\\nexhibits abrupt transitions, the juxtaposition of $P\\\\gg 1$ emergent learning\\ncurves at different timescales leads to a smooth scaling law in the cumulative\\nobjective.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,stat.ML\", \"published\": \"2025-04-28T16:58:55Z\"}"}

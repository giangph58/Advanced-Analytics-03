{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04937v1\", \"title\": \"Generalization Analysis for Contrastive Representation Learning under\\n  Non-IID Settings\", \"summary\": \"Contrastive Representation Learning (CRL) has achieved impressive success in\\nvarious domains in recent years. Nevertheless, the theoretical understanding of\\nthe generalization behavior of CRL is limited. Moreover, to the best of our\\nknowledge, the current literature only analyzes generalization bounds under the\\nassumption that the data tuples used for contrastive learning are independently\\nand identically distributed. However, in practice, we are often limited to a\\nfixed pool of reusable labeled data points, making it inevitable to recycle\\ndata across tuples to create sufficiently large datasets. Therefore, the\\ntuple-wise independence condition imposed by previous works is invalidated. In\\nthis paper, we provide a generalization analysis for the CRL framework under\\nnon-$i.i.d.$ settings that adheres to practice more realistically. Drawing\\ninspiration from the literature on U-statistics, we derive generalization\\nbounds which indicate the required number of samples in each class scales as\\nthe logarithm of the covering number of the class of learnable feature\\nrepresentations associated to each class. Next, we apply our main results to\\nderive excess risk bounds for common function classes such as linear maps and\\nneural networks.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.LG\", \"published\": \"2025-05-08T04:26:41Z\"}"}

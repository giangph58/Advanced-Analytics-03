{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19483v1\", \"title\": \"Improving Reasoning Performance in Large Language Models via\\n  Representation Engineering\", \"summary\": \"Recent advancements in large language models (LLMs) have resulted in\\nincreasingly anthropomorphic language concerning the ability of LLMs to reason.\\nWhether reasoning in LLMs should be understood to be inherently different is,\\nhowever, widely debated. We propose utilizing a representation engineering\\napproach wherein model activations are read from the residual stream of an LLM\\nwhen processing a reasoning task. The activations are used to derive a control\\nvector that is applied to the model as an inference-time intervention,\\nmodulating the representational space of the model, to improve performance on\\nthe specified task. We publish the code for deriving control vectors and\\nanalyzing model representations. The method allows us to improve performance on\\nreasoning benchmarks and assess how control vectors influence the final logit\\ndistribution of a model via metrics such as KL divergence and entropy. We apply\\ncontrol vectors to Mistral-7B-Instruct and a range of Pythia models on an\\ninductive, a deductive and mathematical reasoning task. We show that an LLM\\ncan, to a certain degree, be controlled to improve its perceived reasoning\\nability by modulating activations. The intervention is dependent upon the\\nability to reliably extract the model's typical state when correctly solving a\\ntask. Our results suggest that reasoning performance can be modulated in the\\nsame manner as other information-processing tasks performed by LLMs and\\ndemonstrate that we are capable of improving performance on specific tasks via\\na simple intervention on the residual stream with no additional training.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL\", \"published\": \"2025-04-28T04:58:43Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24092v1\", \"title\": \"New universal operator approximation theorem for encoder-decoder\\n  architectures (Preprint)\", \"summary\": \"Motivated by the rapidly growing field of mathematics for operator\\napproximation with neural networks, we present a novel universal operator\\napproximation theorem for a broad class of encoder-decoder architectures. In\\nthis study, we focus on approximating continuous operators in\\n$\\\\mathcal{C}(\\\\mathcal{X}, \\\\mathcal{Y})$, where $\\\\mathcal{X}$ and $\\\\mathcal{Y}$\\nare infinite-dimensional normed or metric spaces, and we consider uniform\\nconvergence on compact subsets of $\\\\mathcal{X}$. Unlike standard results in the\\noperator learning literature, we investigate the case where the approximating\\noperator sequence can be chosen independently of the compact sets. Taking a\\ntopological perspective, we analyze different types of operator approximation\\nand show that compact-set-independent approximation is a strictly stronger\\nproperty in most relevant operator learning frameworks. To establish our\\nresults, we introduce a new approximation property tailored to encoder-decoder\\narchitectures, which enables us to prove a universal operator approximation\\ntheorem ensuring uniform convergence on every compact subset. This result\\nunifies and extends existing universal operator approximation theorems for\\nvarious encoder-decoder architectures, including classical DeepONets,\\nBasisONets, special cases of MIONets, architectures based on frames and other\\nrelated approaches.\", \"main_category\": \"math.FA\", \"categories\": \"math.FA,cs.LG,math.GN\", \"published\": \"2025-03-31T13:43:21Z\"}"}

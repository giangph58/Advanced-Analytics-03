{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07069v1\", \"title\": \"HalluciNot: Hallucination Detection Through Context and Common Knowledge\\n  Verification\", \"summary\": \"This paper introduces a comprehensive system for detecting hallucinations in\\nlarge language model (LLM) outputs in enterprise settings. We present a novel\\ntaxonomy of LLM responses specific to hallucination in enterprise applications,\\ncategorizing them into context-based, common knowledge, enterprise-specific,\\nand innocuous statements. Our hallucination detection model HDM-2 validates LLM\\nresponses with respect to both context and generally known facts (common\\nknowledge). It provides both hallucination scores and word-level annotations,\\nenabling precise identification of problematic content. To evaluate it on\\ncontext-based and common-knowledge hallucinations, we introduce a new dataset\\nHDMBench. Experimental results demonstrate that HDM-2 out-performs existing\\napproaches across RagTruth, TruthfulQA, and HDMBench datasets. This work\\naddresses the specific challenges of enterprise deployment, including\\ncomputational efficiency, domain specialization, and fine-grained error\\nidentification. Our evaluation dataset, model weights, and inference code are\\npublicly available.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-09T17:39:41Z\"}"}

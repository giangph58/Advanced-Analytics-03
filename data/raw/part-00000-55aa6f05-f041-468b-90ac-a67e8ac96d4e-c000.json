{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11354v1\", \"title\": \"Kimina-Prover Preview: Towards Large Formal Reasoning Models with\\n  Reinforcement Learning\", \"summary\": \"We introduce Kimina-Prover Preview, a large language model that pioneers a\\nnovel reasoning-driven exploration paradigm for formal theorem proving, as\\nshowcased in this preview release. Trained with a large-scale reinforcement\\nlearning pipeline from Qwen2.5-72B, Kimina-Prover demonstrates strong\\nperformance in Lean 4 proof generation by employing a structured reasoning\\npattern we term \\\\textit{formal reasoning pattern}. This approach allows the\\nmodel to emulate human problem-solving strategies in Lean, iteratively\\ngenerating and refining proof steps. Kimina-Prover sets a new state-of-the-art\\non the miniF2F benchmark, reaching 80.7% with pass@8192. Beyond improved\\nbenchmark performance, our work yields several key insights: (1) Kimina-Prover\\nexhibits high sample efficiency, delivering strong results even with minimal\\nsampling (pass@1) and scaling effectively with computational budget, stemming\\nfrom its unique reasoning pattern and RL training; (2) we demonstrate clear\\nperformance scaling with model size, a trend previously unobserved for neural\\ntheorem provers in formal mathematics; (3) the learned reasoning style,\\ndistinct from traditional search algorithms, shows potential to bridge the gap\\nbetween formal verification and informal mathematical intuition. We open source\\ndistilled versions with 1.5B and 7B parameters of Kimina-Prover\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-15T16:23:44Z\"}"}

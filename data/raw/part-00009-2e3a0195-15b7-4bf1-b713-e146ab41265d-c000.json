{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15983v1\", \"title\": \"W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight\\n  Language Models\", \"summary\": \"The demand for efficient natural language processing (NLP) systems has led to\\nthe development of lightweight language models. Previous work in this area has\\nprimarily focused on manual design or training-based neural architecture search\\n(NAS) methods. Recently, zero-shot NAS methods have been proposed for\\nevaluating language models without the need for training. However, prevailing\\napproaches to zero-shot NAS often face challenges such as biased evaluation\\nmetrics and computational inefficiencies. In this paper, we introduce\\nweight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored\\nfor lightweight language models. Our approach utilizes two evaluation proxies:\\nthe parameter count and the number of principal components with cumulative\\ncontribution exceeding $\\\\eta$ in the feed-forward neural (FFN) layer.\\nAdditionally, by eliminating the need for gradient computations, we optimize\\nthe evaluation time, thus enhancing the efficiency of designing and evaluating\\nlightweight language models. We conduct a comparative analysis on the GLUE and\\nSQuAD datasets to evaluate our approach. The results demonstrate that our\\nmethod significantly reduces training time compared to one-shot NAS methods and\\nachieves higher scores in the testing phase compared to previous\\nstate-of-the-art training-based methods. Furthermore, we perform ranking\\nevaluations on a dataset sampled from the FlexiBERT search space. Our approach\\nexhibits superior ranking correlation and further reduces solving time compared\\nto other zero-shot NAS methods that require gradient computation.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-22T15:33:01Z\"}"}

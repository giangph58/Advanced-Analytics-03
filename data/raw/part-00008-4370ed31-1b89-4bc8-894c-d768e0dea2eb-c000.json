{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16520v1\", \"title\": \"A Few-Shot Metric Learning Method with Dual-Channel Attention for\\n  Cross-Modal Same-Neuron Identification\", \"summary\": \"In neuroscience research, achieving single-neuron matching across different\\nimaging modalities is critical for understanding the relationship between\\nneuronal structure and function. However, modality gaps and limited annotations\\npresent significant challenges. We propose a few-shot metric learning method\\nwith a dual-channel attention mechanism and a pretrained vision transformer to\\nenable robust cross-modal neuron identification. The local and global channels\\nextract soma morphology and fiber context, respectively, and a gating mechanism\\nfuses their outputs. To enhance the model's fine-grained discrimination\\ncapability, we introduce a hard sample mining strategy based on the\\nMultiSimilarityMiner algorithm, along with the Circle Loss function.\\nExperiments on two-photon and fMOST datasets demonstrate superior Top-K\\naccuracy and recall compared to existing methods. Ablation studies and t-SNE\\nvisualizations validate the effectiveness of each module. The method also\\nachieves a favorable trade-off between accuracy and training efficiency under\\ndifferent fine-tuning strategies. These results suggest that the proposed\\napproach offers a promising technical solution for accurate single-cell level\\nmatching and multimodal neuroimaging integration.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,q-bio.NC\", \"published\": \"2025-04-23T08:45:23Z\"}"}
{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16537v1\", \"title\": \"Transformers for Complex Query Answering over Knowledge Hypergraphs\", \"summary\": \"Complex Query Answering (CQA) has been extensively studied in recent years.\\nIn order to model data that is closer to real-world distribution, knowledge\\ngraphs with different modalities have been introduced. Triple KGs, as the\\nclassic KGs composed of entities and relations of arity 2, have limited\\nrepresentation of real-world facts. Real-world data is more sophisticated.\\nWhile hyper-relational graphs have been introduced, there are limitations in\\nrepresenting relationships of varying arity that contain entities with equal\\ncontributions. To address this gap, we sampled new CQA datasets: JF17k-HCQA and\\nM-FB15k-HCQA. Each dataset contains various query types that include logical\\noperations such as projection, negation, conjunction, and disjunction. In order\\nto answer knowledge hypergraph (KHG) existential first-order queries, we\\npropose a two-stage transformer model, the Logical Knowledge Hypergraph\\nTransformer (LKHGT), which consists of a Projection Encoder for atomic\\nprojection and a Logical Encoder for complex logical operations. Both encoders\\nare equipped with Type Aware Bias (TAB) for capturing token interactions.\\nExperimental results on CQA datasets show that LKHGT is a state-of-the-art CQA\\nmethod over KHG and is able to generalize to out-of-distribution query types.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-23T09:07:21Z\"}"}
{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16553v1\", \"title\": \"Least-Squares-Embedded Optimization for Accelerated Convergence of PINNs\\n  in Acoustic Wavefield Simulations\", \"summary\": \"Physics-Informed Neural Networks (PINNs) have shown promise in solving\\npartial differential equations (PDEs), including the frequency-domain Helmholtz\\nequation. However, standard training of PINNs using gradient descent (GD)\\nsuffers from slow convergence and instability, particularly for high-frequency\\nwavefields. For scattered acoustic wavefield simulation based on Helmholtz\\nequation, we derive a hybrid optimization framework that accelerates training\\nconvergence by embedding a least-squares (LS) solver directly into the GD loss\\nfunction. This formulation enables optimal updates for the linear output layer.\\nOur method is applicable with or without perfectly matched layers (PML), and we\\nprovide practical tensor-based implementations for both scenarios. Numerical\\nexperiments on benchmark velocity models demonstrate that our approach achieves\\nfaster convergence, higher accuracy, and improved stability compared to\\nconventional PINN training. In particular, our results show that the\\nLS-enhanced method converges rapidly even in cases where standard GD-based\\ntraining fails. The LS solver operates on a small normal matrix, ensuring\\nminimal computational overhead and making the method scalable for large-scale\\nwavefield simulations.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,physics.comp-ph,physics.geo-ph\", \"published\": \"2025-04-23T09:32:14Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00527v1\", \"title\": \"DeCo: Task Decomposition and Skill Composition for Zero-Shot\\n  Generalization in Long-Horizon 3D Manipulation\", \"summary\": \"Generalizing language-conditioned multi-task imitation learning (IL) models\\nto novel long-horizon 3D manipulation tasks remains a significant challenge. To\\naddress this, we propose DeCo (Task Decomposition and Skill Composition), a\\nmodel-agnostic framework compatible with various multi-task IL models, designed\\nto enhance their zero-shot generalization to novel, compositional, long-horizon\\n3D manipulation tasks. DeCo first decomposes IL demonstrations into a set of\\nmodular atomic tasks based on the physical interaction between the gripper and\\nobjects, and constructs an atomic training dataset that enables models to learn\\na diverse set of reusable atomic skills during imitation learning. At inference\\ntime, DeCo leverages a vision-language model (VLM) to parse high-level\\ninstructions for novel long-horizon tasks, retrieve the relevant atomic skills,\\nand dynamically schedule their execution; a spatially-aware skill-chaining\\nmodule then ensures smooth, collision-free transitions between sequential\\nskills. We evaluate DeCo in simulation using DeCoBench, a benchmark\\nspecifically designed to assess zero-shot generalization of multi-task IL\\nmodels in compositional long-horizon 3D manipulation. Across three\\nrepresentative multi-task IL models (RVT-2, 3DDA, and ARP), DeCo achieves\\nsuccess rate improvements of 66.67%, 21.53%, and 57.92%, respectively, on 12\\nnovel compositional tasks. Moreover, in real-world experiments, a DeCo-enhanced\\nmodel trained on only 6 atomic tasks successfully completes 9 novel\\nlong-horizon tasks, yielding an average success rate improvement of 53.33% over\\nthe base multi-task IL model. Video demonstrations are available at:\\nhttps://deco226.github.io.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO\", \"published\": \"2025-05-01T13:52:19Z\"}"}

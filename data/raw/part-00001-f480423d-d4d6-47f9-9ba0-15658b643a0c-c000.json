{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11456v1\", \"title\": \"DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and\\n  Verifiable Mathematical Dataset for Advancing Reasoning\", \"summary\": \"The capacity for complex mathematical reasoning is a key benchmark for\\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\\nshows promise, progress is significantly hindered by the lack of large-scale\\ntraining data that is sufficiently challenging, possesses verifiable answer\\nformats suitable for RL, and is free from contamination with evaluation\\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\\nlarge-scale dataset comprising approximately 103K mathematical problems,\\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\\nis curated through a rigorous pipeline involving source analysis, stringent\\ndecontamination against numerous benchmarks, and filtering for high difficulty\\n(primarily Levels 5-9), significantly exceeding existing open resources in\\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\\nRL, and three distinct R1-generated solutions suitable for diverse training\\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\\nmathematical topics, DeepMath-103K promotes the development of generalizable\\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\\nsignificant improvements on challenging mathematical benchmarks, validating its\\neffectiveness. We release DeepMath-103K publicly to facilitate community\\nprogress in building more capable AI reasoning systems:\\nhttps://github.com/zwhe99/DeepMath.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-15T17:59:51Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20447v1\", \"title\": \"APG-MOS: Auditory Perception Guided-MOS Predictor for Synthetic Speech\", \"summary\": \"Automatic speech quality assessment aims to quantify subjective human\\nperception of speech through computational models to reduce the need for\\nlabor-consuming manual evaluations. While models based on deep learning have\\nachieved progress in predicting mean opinion scores (MOS) to assess synthetic\\nspeech, the neglect of fundamental auditory perception mechanisms limits\\nconsistency with human judgments. To address this issue, we propose an auditory\\nperception guided-MOS prediction model (APG-MOS) that synergistically\\nintegrates auditory modeling with semantic analysis to enhance consistency with\\nhuman judgments. Specifically, we first design a perceptual module, grounded in\\nbiological auditory mechanisms, to simulate cochlear functions, which encodes\\nacoustic signals into biologically aligned electrochemical representations.\\nSecondly, we propose a residual vector quantization (RVQ)-based semantic\\ndistortion modeling method to quantify the degradation of speech quality at the\\nsemantic level. Finally, we design a residual cross-attention architecture,\\ncoupled with a progressive learning strategy, to enable multimodal fusion of\\nencoded electrochemical signals and semantic representations. Experiments\\ndemonstrate that APG-MOS achieves superior performance on two primary\\nbenchmarks. Our code and checkpoint will be available on a public repository\\nupon publication.\", \"main_category\": \"cs.SD\", \"categories\": \"cs.SD,cs.AI,eess.AS\", \"published\": \"2025-04-29T05:45:09Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19599v1\", \"title\": \"GVPO: Group Variance Policy Optimization for Large Language Model\\n  Post-Training\", \"summary\": \"Post-training plays a crucial role in refining and aligning large language\\nmodels to meet specific tasks and human preferences. While recent advancements\\nin post-training techniques, such as Group Relative Policy Optimization (GRPO),\\nleverage increased sampling with relative reward scoring to achieve superior\\nperformance, these methods often suffer from training instability that limits\\ntheir practical adoption. To address this challenge, we present Group Variance\\nPolicy Optimization (GVPO). GVPO incorporates the analytical solution to\\nKL-constrained reward maximization directly into its gradient weights, ensuring\\nalignment with the optimal policy. The method provides intuitive physical\\ninterpretations: its gradient mirrors the mean squared error between the\\ncentral distance of implicit rewards and that of actual rewards. GVPO offers\\ntwo key advantages: (1) it guarantees a unique optimal solution, exactly the\\nKL-constrained reward maximization objective, (2) it supports flexible sampling\\ndistributions that avoids on-policy and importance sampling limitations. By\\nunifying theoretical guarantees with practical adaptability, GVPO establishes a\\nnew paradigm for reliable and versatile LLM post-training.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.LG\", \"published\": \"2025-04-28T09:02:24Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20666v1\", \"title\": \"SFi-Former: Sparse Flow Induced Attention for Graph Transformer\", \"summary\": \"Graph Transformers (GTs) have demonstrated superior performance compared to\\ntraditional message-passing graph neural networks in many studies, especially\\nin processing graph data with long-range dependencies. However, GTs tend to\\nsuffer from weak inductive bias, overfitting and over-globalizing problems due\\nto the dense attention. In this paper, we introduce SFi-attention, a novel\\nattention mechanism designed to learn sparse pattern by minimizing an energy\\nfunction based on network flows with l1-norm regularization, to relieve those\\nissues caused by dense attention. Furthermore, SFi-Former is accordingly\\ndevised which can leverage the sparse attention pattern of SFi-attention to\\ngenerate sparse network flows beyond adjacency matrix of graph data.\\nSpecifically, SFi-Former aggregates features selectively from other nodes\\nthrough flexible adaptation of the sparse attention, leading to a more robust\\nmodel. We validate our SFi-Former on various graph datasets, especially those\\ngraph data exhibiting long-range dependencies. Experimental results show that\\nour SFi-Former obtains competitive performance on GNN Benchmark datasets and\\nSOTA performance on LongRange Graph Benchmark (LRGB) datasets. Additionally,\\nour model gives rise to smaller generalization gaps, which indicates that it is\\nless prone to over-fitting. Click here for codes.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-29T11:45:24Z\"}"}

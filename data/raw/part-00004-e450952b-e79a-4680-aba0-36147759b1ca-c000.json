{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06572v1\", \"title\": \"Domain Generalization via Discrete Codebook Learning\", \"summary\": \"Domain generalization (DG) strives to address distribution shifts across\\ndiverse environments to enhance model's generalizability. Current DG approaches\\nare confined to acquiring robust representations with continuous features,\\nspecifically training at the pixel level. However, this DG paradigm may\\nstruggle to mitigate distribution gaps in dealing with a large space of\\ncontinuous features, rendering it susceptible to pixel details that exhibit\\nspurious correlations or noise. In this paper, we first theoretically\\ndemonstrate that the domain gaps in continuous representation learning can be\\nreduced by the discretization process. Based on this inspiring finding, we\\nintroduce a novel learning paradigm for DG, termed Discrete Domain\\nGeneralization (DDG). DDG proposes to use a codebook to quantize the feature\\nmap into discrete codewords, aligning semantic-equivalent information in a\\nshared discrete representation space that prioritizes semantic-level\\ninformation over pixel-level intricacies. By learning at the semantic level,\\nDDG diminishes the number of latent features, optimizing the utilization of the\\nrepresentation space and alleviating the risks associated with the wide-ranging\\nspace of continuous features. Extensive experiments across widely employed\\nbenchmarks in DG demonstrate DDG's superior performance compared to\\nstate-of-the-art approaches, underscoring its potential to reduce the\\ndistribution gaps and enhance the model's generalizability.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-09T04:19:35Z\"}"}

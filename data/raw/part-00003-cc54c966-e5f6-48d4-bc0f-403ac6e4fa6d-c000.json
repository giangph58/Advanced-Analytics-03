{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17780v1\", \"title\": \"Replay to Remember: Retaining Domain Knowledge in Streaming Language\\n  Models\", \"summary\": \"Continual learning in large language models (LLMs) typically encounters the\\ncritical challenge of catastrophic forgetting, where previously acquired\\nknowledge deteriorates upon exposure to new data. While techniques like replay\\nbuffers and parameter-efficient tuning (e.g., Low-Rank Adaptation or LoRA) have\\nbeen proposed, few studies investigate real-time domain adaptation under strict\\ncomputational and data-stream constraints. In this paper, we demonstrate a\\nlightweight method combining LoRA and a minimal replay mechanism in a realistic\\nstreaming setting across three diverse knowledge domains: medical question\\nanswering, genetics, and law. Using perplexity, semantic similarity, and\\nGPT-based human-like evaluation metrics, we quantify the model's adaptation,\\nforgetting, and recovery over time. Our experiments reveal that while\\ncatastrophic forgetting naturally occurs, even minimal replay significantly\\nstabilizes and partially restores domain-specific knowledge. This study\\ncontributes practical insights for deploying adaptable LLMs in\\nresource-constrained, real-world scenarios.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-24T17:56:22Z\"}"}

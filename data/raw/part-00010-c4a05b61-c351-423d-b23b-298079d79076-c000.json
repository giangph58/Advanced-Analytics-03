{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04394v1\", \"title\": \"SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin\\n  Transformer\", \"summary\": \"This paper presents an efficient visual speech encoder for lip reading. While\\nmost recent lip reading studies have been based on the ResNet architecture and\\nhave achieved significant success, they are not sufficiently suitable for\\nefficiently capturing lip reading features due to high computational complexity\\nin modeling spatio-temporal information. Additionally, using a complex visual\\nmodel not only increases the complexity of lip reading models but also induces\\ndelays in the overall network for multi-modal studies (e.g., audio-visual\\nspeech recognition, speech enhancement, and speech separation). To overcome the\\nlimitations of Convolutional Neural Network (CNN)-based models, we apply the\\nhierarchical structure and window self-attention of the Swin Transformer to lip\\nreading. We configure a new lightweight scale of the Swin Transformer suitable\\nfor processing lip reading data and present the SwinLip visual speech encoder,\\nwhich efficiently reduces computational load by integrating modified\\nConvolution-augmented Transformer (Conformer) temporal embeddings with\\nconventional spatial embeddings in the hierarchical structure. Through\\nextensive experiments, we have validated that our SwinLip successfully improves\\nthe performance and inference speed of the lip reading network when applied to\\nvarious backbones for word and sentence recognition, reducing computational\\nload. In particular, our SwinLip demonstrated robust performance in both\\nEnglish LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art\\nperformance on the Mandarin LRW-1000 dataset with less computation compared to\\nthe existing state-of-the-art model.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,eess.AS\", \"published\": \"2025-05-07T13:18:43Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17282v1\", \"title\": \"Cracking the Code of Action: a Generative Approach to Affordances for\\n  Reinforcement Learning\", \"summary\": \"Agents that can autonomously navigate the web through a graphical user\\ninterface (GUI) using a unified action space (e.g., mouse and keyboard actions)\\ncan require very large amounts of domain-specific expert demonstrations to\\nachieve good performance. Low sample efficiency is often exacerbated in\\nsparse-reward and large-action-space environments, such as a web GUI, where\\nonly a few actions are relevant in any given situation. In this work, we\\nconsider the low-data regime, with limited or no access to expert behavior. To\\nenable sample-efficient learning, we explore the effect of constraining the\\naction space through $\\\\textit{intent-based affordances}$ -- i.e., considering\\nin any situation only the subset of actions that achieve a desired outcome. We\\npropose $\\\\textbf{Code as Generative Affordances}$ $(\\\\textbf{$\\\\texttt{CoGA}$})$,\\na method that leverages pre-trained vision-language models (VLMs) to generate\\ncode that determines affordable actions through implicit intent-completion\\nfunctions and using a fully-automated program generation and verification\\npipeline. These programs are then used in-the-loop of a reinforcement learning\\nagent to return a set of affordances given a pixel observation. By greatly\\nreducing the number of actions that an agent must consider, we demonstrate on a\\nwide range of tasks in the MiniWob++ benchmark that: $\\\\textbf{1)}$\\n$\\\\texttt{CoGA}$ is orders of magnitude more sample efficient than its RL agent,\\n$\\\\textbf{2)}$ $\\\\texttt{CoGA}$'s programs can generalize within a family of\\ntasks, and $\\\\textbf{3)}$ $\\\\texttt{CoGA}$ performs better or on par compared\\nwith behavior cloning when a small number of expert demonstrations is\\navailable.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-24T06:20:08Z\"}"}

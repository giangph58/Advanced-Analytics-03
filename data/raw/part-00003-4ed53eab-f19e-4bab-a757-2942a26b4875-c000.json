{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20837v1\", \"title\": \"RadSAM: Segmenting 3D radiological images with a 2D promptable model\", \"summary\": \"Medical image segmentation is a crucial and time-consuming task in clinical\\ncare, where mask precision is extremely important. The Segment Anything Model\\n(SAM) offers a promising approach, as it provides an interactive interface\\nbased on visual prompting and edition to refine an initial segmentation. This\\nmodel has strong generalization capabilities, does not rely on predefined\\nclasses, and adapts to diverse objects; however, it is pre-trained on natural\\nimages and lacks the ability to process medical data effectively. In addition,\\nthis model is built for 2D images, whereas a whole medical domain is based on\\n3D images, such as CT and MRI. Recent adaptations of SAM for medical imaging\\nare based on 2D models, thus requiring one prompt per slice to segment 3D\\nobjects, making the segmentation process tedious. They also lack important\\nfeatures such as editing. To bridge this gap, we propose RadSAM, a novel method\\nfor segmenting 3D objects with a 2D model from a single prompt. In practice, we\\ntrain a 2D model using noisy masks as initial prompts, in addition to bounding\\nboxes and points. We then use this novel prompt type with an iterative\\ninference pipeline to reconstruct the 3D mask slice-by-slice. We introduce a\\nbenchmark to evaluate the model's ability to segment 3D objects in CT images\\nfrom a single prompt and evaluate the models' out-of-domain transfer and\\nedition capabilities. We demonstrate the effectiveness of our approach against\\nstate-of-the-art models on this benchmark using the AMOS abdominal organ\\nsegmentation dataset.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.LG\", \"published\": \"2025-04-29T15:00:25Z\"}"}

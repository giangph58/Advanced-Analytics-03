{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11453v1\", \"title\": \"A Clean Slate for Offline Reinforcement Learning\", \"summary\": \"Progress in offline reinforcement learning (RL) has been impeded by ambiguous\\nproblem definitions and entangled algorithmic designs, resulting in\\ninconsistent implementations, insufficient ablations, and unfair evaluations.\\nAlthough offline RL explicitly avoids environment interaction, prior methods\\nfrequently employ extensive, undocumented online evaluation for hyperparameter\\ntuning, complicating method comparisons. Moreover, existing reference\\nimplementations differ significantly in boilerplate code, obscuring their core\\nalgorithmic contributions. We address these challenges by first introducing a\\nrigorous taxonomy and a transparent evaluation protocol that explicitly\\nquantifies online tuning budgets. To resolve opaque algorithmic design, we\\nprovide clean, minimalistic, single-file implementations of various model-free\\nand model-based offline RL methods, significantly enhancing clarity and\\nachieving substantial speed-ups. Leveraging these streamlined implementations,\\nwe propose Unifloral, a unified algorithm that encapsulates diverse prior\\napproaches within a single, comprehensive hyperparameter space, enabling\\nalgorithm development in a shared hyperparameter space. Using Unifloral with\\nour rigorous evaluation protocol, we develop two novel algorithms - TD3-AWR\\n(model-free) and MoBRAC (model-based) - which substantially outperform\\nestablished baselines. Our implementation is publicly available at\\nhttps://github.com/EmptyJackson/unifloral.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.RO\", \"published\": \"2025-04-15T17:59:05Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05216v1\", \"title\": \"Normalize Everything: A Preconditioned Magnitude-Preserving Architecture\\n  for Diffusion-Based Speech Enhancement\", \"summary\": \"This paper presents a new framework for diffusion-based speech enhancement.\\nOur method employs a Schroedinger bridge to transform the noisy speech\\ndistribution into the clean speech distribution. To stabilize and improve\\ntraining, we employ time-dependent scalings of the inputs and outputs of the\\nnetwork, known as preconditioning. We consider two skip connection\\nconfigurations, which either include or omit the current process state in the\\ndenoiser's output, enabling the network to predict either environmental noise\\nor clean speech. Each approach leads to improved performance on different\\nspeech enhancement metrics. To maintain stable magnitude levels and balance\\nduring training, we use a magnitude-preserving network architecture that\\nnormalizes all activations and network weights to unit length. Additionally, we\\npropose learning the contribution of the noisy input within each network block\\nfor effective input conditioning. After training, we apply a method to\\napproximate different exponential moving average (EMA) profiles and investigate\\ntheir effects on the speech enhancement performance. In contrast to image\\ngeneration tasks, where longer EMA lengths often enhance mode coverage, we\\nobserve that shorter EMA lengths consistently lead to better performance on\\nstandard speech enhancement metrics. Code, audio examples, and checkpoints are\\navailable online.\", \"main_category\": \"eess.AS\", \"categories\": \"eess.AS,cs.SD\", \"published\": \"2025-05-08T13:10:02Z\"}"}

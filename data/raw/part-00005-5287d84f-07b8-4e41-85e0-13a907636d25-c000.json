{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19621v1\", \"title\": \"AI Alignment in Medical Imaging: Unveiling Hidden Biases Through\\n  Counterfactual Analysis\", \"summary\": \"Machine learning (ML) systems for medical imaging have demonstrated\\nremarkable diagnostic capabilities, but their susceptibility to biases poses\\nsignificant risks, since biases may negatively impact generalization\\nperformance. In this paper, we introduce a novel statistical framework to\\nevaluate the dependency of medical imaging ML models on sensitive attributes,\\nsuch as demographics. Our method leverages the concept of counterfactual\\ninvariance, measuring the extent to which a model's predictions remain\\nunchanged under hypothetical changes to sensitive attributes. We present a\\npractical algorithm that combines conditional latent diffusion models with\\nstatistical hypothesis testing to identify and quantify such biases without\\nrequiring direct access to counterfactual data. Through experiments on\\nsynthetic datasets and large-scale real-world medical imaging datasets,\\nincluding \\\\textsc{cheXpert} and MIMIC-CXR, we demonstrate that our approach\\naligns closely with counterfactual fairness principles and outperforms standard\\nbaselines. This work provides a robust tool to ensure that ML diagnostic\\nsystems generalize well, e.g., across demographic groups, offering a critical\\nstep towards AI safety in healthcare. Code:\\nhttps://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,eess.IV,stat.ML\", \"published\": \"2025-04-28T09:28:25Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05020v1\", \"title\": \"Generative Models for Long Time Series: Approximately Equivariant\\n  Recurrent Network Structures for an Adjusted Training Scheme\", \"summary\": \"We present a simple yet effective generative model for time series data based\\non a Variational Autoencoder (VAE) with recurrent layers, referred to as the\\nRecurrent Variational Autoencoder with Subsequent Training (RVAE-ST). Our\\nmethod introduces an adapted training scheme that progressively increases the\\nsequence length, addressing the challenge recurrent layers typically face when\\nmodeling long sequences. By leveraging the recurrent architecture, the model\\nmaintains a constant number of parameters regardless of sequence length. This\\ndesign encourages approximate time-shift equivariance and enables efficient\\nmodeling of long-range temporal dependencies. Rather than introducing a\\nfundamentally new architecture, we show that a carefully composed combination\\nof known components can match or outperform state-of-the-art generative models\\non several benchmark datasets. Our model performs particularly well on time\\nseries that exhibit quasi-periodic structure,while remaining competitive on\\ndatasets with more irregular or partially non-stationary behavior. We evaluate\\nits performance using ELBO, Fr\\\\'echet Distance, discriminative scores, and\\nvisualizations of the learned embeddings.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-08T07:52:37Z\"}"}

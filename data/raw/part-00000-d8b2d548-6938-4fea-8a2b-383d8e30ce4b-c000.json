{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16417v1\", \"title\": \"Anytime Safe Reinforcement Learning\", \"summary\": \"This paper considers the problem of solving constrained\\n  reinforcement learning problems with anytime guarantees, meaning\\n  that the algorithmic solution returns a safe policy regardless of\\n  when it is terminated. Drawing inspiration from anytime constrained\\n  optimization, we introduce Reinforcement Learning-based Safe\\n  Gradient Flow (RL-SGF), an on-policy algorithm which employs\\n  estimates of the value functions and their respective gradients\\n  associated with the objective and safety constraints for the current\\n  policy, and updates the policy parameters by solving a convex\\n  quadratically constrained quadratic program. We show that if the\\n  estimates are computed with a sufficiently large number of episodes\\n  (for which we provide an explicit bound), safe policies are updated\\n  to safe policies with a probability higher than a prescribed\\n  tolerance. We also show that iterates asymptotically converge to a\\n  neighborhood of a KKT point, whose size can be arbitrarily reduced\\n  by refining the estimates of the value function and their gradients.\\n  We illustrate the performance of RL-SGF in a navigation example.\", \"main_category\": \"eess.SY\", \"categories\": \"eess.SY,cs.SY\", \"published\": \"2025-04-23T04:51:31Z\"}"}

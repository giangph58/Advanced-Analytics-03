{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10340v1\", \"title\": \"Forecasting from Clinical Textual Time Series: Adaptations of the\\n  Encoder and Decoder Language Model Families\", \"summary\": \"Clinical case reports encode rich, temporal patient trajectories that are\\noften underexploited by traditional machine learning methods relying on\\nstructured data. In this work, we introduce the forecasting problem from\\ntextual time series, where timestamped clinical findings--extracted via an\\nLLM-assisted annotation pipeline--serve as the primary input for prediction. We\\nsystematically evaluate a diverse suite of models, including fine-tuned\\ndecoder-based large language models and encoder-based transformers, on tasks of\\nevent occurrence prediction, temporal ordering, and survival analysis. Our\\nexperiments reveal that encoder-based models consistently achieve higher F1\\nscores and superior temporal concordance for short- and long-horizon event\\nforecasting, while fine-tuned masking approaches enhance ranking performance.\\nIn contrast, instruction-tuned decoder models demonstrate a relative advantage\\nin survival analysis, especially in early prognosis settings. Our sensitivity\\nanalyses further demonstrate the importance of time ordering, which requires\\nclinical time series construction, as compared to text ordering, the format of\\nthe text inputs that LLMs are classically trained on. This highlights the\\nadditional benefit that can be ascertained from time-ordered corpora, with\\nimplications for temporal tasks in the era of widespread LLM use.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-14T15:48:56Z\"}"}

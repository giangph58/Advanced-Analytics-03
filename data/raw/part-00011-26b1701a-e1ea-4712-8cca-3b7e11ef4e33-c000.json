{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20966v1\", \"title\": \"Softpick: No Attention Sink, No Massive Activations with Rectified\\n  Softmax\", \"summary\": \"We introduce softpick, a rectified, not sum-to-one, drop-in replacement for\\nsoftmax in transformer attention mechanisms that eliminates attention sink and\\nmassive activations. Our experiments with 340M parameter models demonstrate\\nthat softpick maintains performance parity with softmax on standard benchmarks\\nwhile achieving 0% sink rate. The softpick transformer produces hidden states\\nwith significantly lower kurtosis (340 vs 33,510) and creates sparse attention\\nmaps (46.97% sparsity). Models using softpick consistently outperform softmax\\nwhen quantized, with particularly pronounced advantages at lower bit\\nprecisions. Our analysis and discussion shows how softpick has the potential to\\nopen new possibilities for quantization, low-precision training, sparsity\\noptimization, pruning, and interpretability. Our code is available at\\nhttps://github.com/zaydzuhri/softpick-attention.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-29T17:36:18Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00533v1\", \"title\": \"Test-time Correlation Alignment\", \"summary\": \"Deep neural networks often experience performance drops due to distribution\\nshifts between training and test data. Although domain adaptation offers a\\nsolution, privacy concerns restrict access to training data in many real-world\\nscenarios. This restriction has spurred interest in Test-Time Adaptation (TTA),\\nwhich adapts models using only unlabeled test data. However, current TTA\\nmethods still face practical challenges: (1) a primary focus on instance-wise\\nalignment, overlooking CORrelation ALignment (CORAL) due to missing source\\ncorrelations; (2) complex backpropagation operations for model updating,\\nresulting in overhead computation and (3) domain forgetting.\\n  To address these challenges, we provide a theoretical analysis to investigate\\nthe feasibility of Test-time Correlation Alignment (TCA), demonstrating that\\ncorrelation alignment between high-certainty instances and test instances can\\nenhance test performances with a theoretical guarantee. Based on this, we\\npropose two simple yet effective algorithms: LinearTCA and LinearTCA+.\\nLinearTCA applies a simple linear transformation to achieve both instance and\\ncorrelation alignment without additional model updates, while LinearTCA+ serves\\nas a plug-and-play module that can easily boost existing TTA methods. Extensive\\nexperiments validate our theoretical insights and show that TCA methods\\nsignificantly outperforms baselines across various tasks, benchmarks and\\nbackbones. Notably, LinearTCA improves adaptation accuracy by 5.88% on\\nOfficeHome dataset, while using only 4% maximum GPU memory usage and 0.6%\\ncomputation time compared to the best baseline TTA method.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-05-01T13:59:13Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16609v1\", \"title\": \"Information Leakage of Sentence Embeddings via Generative Embedding\\n  Inversion Attacks\", \"summary\": \"Text data are often encoded as dense vectors, known as embeddings, which\\ncapture semantic, syntactic, contextual, and domain-specific information. These\\nembeddings, widely adopted in various applications, inherently contain rich\\ninformation that may be susceptible to leakage under certain attacks. The GEIA\\nframework highlights vulnerabilities in sentence embeddings, demonstrating that\\nthey can reveal the original sentences they represent. In this study, we\\nreproduce GEIA's findings across various neural sentence embedding models.\\nAdditionally, we contribute new analysis to examine whether these models leak\\nsensitive information from their training datasets. We propose a simple yet\\neffective method without any modification to the attacker's architecture\\nproposed in GEIA. The key idea is to examine differences between log-likelihood\\nfor masked and original variants of data that sentence embedding models have\\nbeen pre-trained on, calculated on the embedding space of the attacker. Our\\nfindings indicate that following our approach, an adversary party can recover\\nmeaningful sensitive information related to the pre-training knowledge of the\\npopular models used for creating sentence embeddings, seriously undermining\\ntheir security. Our code is available on: https://github.com/taslanidis/GEIA\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR\", \"published\": \"2025-04-23T10:50:23Z\"}"}

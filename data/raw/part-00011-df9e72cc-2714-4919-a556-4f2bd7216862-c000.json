{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10185v1\", \"title\": \"LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in\\n  Current Benchmarks\", \"summary\": \"Large language model unlearning has become a critical challenge in ensuring\\nsafety and controlled model behavior by removing undesired data-model\\ninfluences from the pretrained model while preserving general utility.\\nSignificant recent efforts have been dedicated to developing LLM unlearning\\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\\nUnlearning Six-way Evaluation), facilitating standardized unlearning\\nperformance assessment and method comparison. Despite their usefulness, we\\nuncover for the first time a novel coreset effect within these benchmarks.\\nSpecifically, we find that LLM unlearning achieved with the original (full)\\nforget set can be effectively maintained using a significantly smaller subset\\n(functioning as a \\\"coreset\\\"), e.g., as little as 5% of the forget set, even\\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\\ncan be performed surprisingly easily, even in an extremely low-data regime. We\\ndemonstrate that this coreset effect remains strong, regardless of the LLM\\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\\nThe surprisingly strong coreset effect is also robust across various data\\nselection methods, ranging from random selection to more sophisticated\\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\\nkeyword-based perspective, showing that keywords extracted from the forget set\\nalone contribute significantly to unlearning effectiveness and indicating that\\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\\nmodels along additional dimensions, such as mode connectivity and robustness to\\njailbreaking attacks. Codes are available at\\nhttps://github.com/OPTML-Group/MU-Coreset.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-14T12:38:37Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15630v1\", \"title\": \"Exploiting Contextual Knowledge in LLMs through V-usable Information\\n  based Layer Enhancement\", \"summary\": \"Large Language Models (LLMs) have demonstrated remarkable capabilities in\\nvarious tasks, yet they often struggle with context-faithfulness generations\\nthat properly reflect contextual knowledge. While existing approaches focus on\\nenhancing the decoding strategies, they ignore the fundamental mechanism of how\\ncontextual information is processed within LLMs' internal states. As a result,\\nLLMs remain limited in their ability to fully leverage contextual knowledge. In\\nthis paper, we propose Context-aware Layer Enhancement (CaLE), a novel\\nintervention method that enhances the utilization of contextual knowledge\\nwithin LLMs' internal representations. By employing V-usable information\\nanalysis, CaLE strategically amplifies the growth of contextual information at\\nan optimal layer, thereby enriching representations in the final layer. Our\\nexperiments demonstrate that CaLE effectively improves context-faithful\\ngeneration in Question-Answering tasks, particularly in scenarios involving\\nunknown or conflicting contextual knowledge.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-22T06:42:22Z\"}"}

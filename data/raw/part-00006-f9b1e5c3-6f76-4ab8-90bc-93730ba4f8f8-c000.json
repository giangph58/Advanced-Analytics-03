{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04193v1\", \"title\": \"Trajectory Entropy Reinforcement Learning for Predictable and Robust\\n  Control\", \"summary\": \"Simplicity is a critical inductive bias for designing data-driven\\ncontrollers, especially when robustness is important. Despite the impressive\\nresults of deep reinforcement learning in complex control tasks, it is prone to\\ncapturing intricate and spurious correlations between observations and actions,\\nleading to failure under slight perturbations to the environment. To tackle\\nthis problem, in this work we introduce a novel inductive bias towards simple\\npolicies in reinforcement learning. The simplicity inductive bias is introduced\\nby minimizing the entropy of entire action trajectories, corresponding to the\\nnumber of bits required to describe information in action trajectories after\\nthe agent observes state trajectories. Our reinforcement learning agent,\\nTrajectory Entropy Reinforcement Learning, is optimized to minimize the\\ntrajectory entropy while maximizing rewards. We show that the trajectory\\nentropy can be effectively estimated by learning a variational parameterized\\naction prediction model, and use the prediction model to construct an\\ninformation-regularized reward function. Furthermore, we construct a practical\\nalgorithm that enables the joint optimization of models, including the policy\\nand the prediction model. Experimental evaluations on several high-dimensional\\nlocomotion tasks show that our learned policies produce more cyclical and\\nconsistent action trajectories, and achieve superior performance, and\\nrobustness to noise and dynamic changes than the state-of-the-art.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.RO\", \"published\": \"2025-05-07T07:41:29Z\"}"}

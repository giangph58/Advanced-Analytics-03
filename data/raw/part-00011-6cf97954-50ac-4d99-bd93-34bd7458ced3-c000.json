{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15110v1\", \"title\": \"Kolmogorov-Arnold Networks: Approximation and Learning Guarantees for\\n  Functions and their Derivatives\", \"summary\": \"Inspired by the Kolmogorov-Arnold superposition theorem, Kolmogorov-Arnold\\nNetworks (KANs) have recently emerged as an improved backbone for most deep\\nlearning frameworks, promising more adaptivity than their multilayer perception\\n(MLP) predecessor by allowing for trainable spline-based activation functions.\\nIn this paper, we probe the theoretical foundations of the KAN architecture by\\nshowing that it can optimally approximate any Besov function in\\n$B^{s}_{p,q}(\\\\mathcal{X})$ on a bounded open, or even fractal, domain\\n$\\\\mathcal{X}$ in $\\\\mathbb{R}^d$ at the optimal approximation rate with respect\\nto any weaker Besov norm $B^{\\\\alpha}_{p,q}(\\\\mathcal{X})$; where $\\\\alpha < s$.\\nWe complement our approximation guarantee with a dimension-free estimate on the\\nsample complexity of a residual KAN model when learning a function of Besov\\nregularity from $N$ i.i.d. noiseless samples. Our KAN architecture incorporates\\ncontemporary deep learning wisdom by leveraging residual/skip connections\\nbetween layers.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.NA,cs.NE,math.FA,math.NA,stat.ML\", \"published\": \"2025-04-21T14:02:59Z\"}"}

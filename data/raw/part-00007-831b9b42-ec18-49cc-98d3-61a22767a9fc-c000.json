{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04974v1\", \"title\": \"Towards Visual Text Grounding of Multimodal Large Language Model\", \"summary\": \"Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\\nnon-neglectable limitation remains in their struggle with visual text\\ngrounding, especially in text-rich images of documents. Document images, such\\nas scanned forms and infographics, highlight critical challenges due to their\\ncomplex layouts and textual content. However, current benchmarks do not fully\\naddress these challenges, as they mostly focus on visual grounding on natural\\nimages, rather than text-rich document images. Thus, to bridge this gap, we\\nintroduce TRIG, a novel task with a newly designed instruction dataset for\\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\\nin document question-answering. Specifically, we propose an OCR-LLM-human\\ninteraction pipeline to create 800 manually annotated question-answer pairs as\\na benchmark and a large-scale training set of 90$ synthetic data based on four\\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\\nbenchmark exposes substantial limitations in their grounding capability on\\ntext-rich images. In addition, we propose two simple and effective TRIG methods\\nbased on general instruction tuning and plug-and-play efficient embedding,\\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\\nimprove spatial reasoning and grounding capabilities.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.CL,cs.LG\", \"published\": \"2025-04-07T12:01:59Z\"}"}

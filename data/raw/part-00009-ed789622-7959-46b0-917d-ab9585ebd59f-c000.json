{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10974v1\", \"title\": \"Self-Supervised Enhancement of Forward-Looking Sonar Images: Bridging\\n  Cross-Modal Degradation Gaps through Feature Space Transformation and\\n  Multi-Frame Fusion\", \"summary\": \"Enhancing forward-looking sonar images is critical for accurate underwater\\ntarget detection. Current deep learning methods mainly rely on supervised\\ntraining with simulated data, but the difficulty in obtaining high-quality\\nreal-world paired data limits their practical use and generalization. Although\\nself-supervised approaches from remote sensing partially alleviate data\\nshortages, they neglect the cross-modal degradation gap between sonar and\\nremote sensing images. Directly transferring pretrained weights often leads to\\noverly smooth sonar images, detail loss, and insufficient brightness. To\\naddress this, we propose a feature-space transformation that maps sonar images\\nfrom the pixel domain to a robust feature domain, effectively bridging the\\ndegradation gap. Additionally, our self-supervised multi-frame fusion strategy\\nleverages complementary inter-frame information to naturally remove speckle\\nnoise and enhance target-region brightness. Experiments on three self-collected\\nreal-world forward-looking sonar datasets show that our method significantly\\noutperforms existing approaches, effectively suppressing noise, preserving\\ndetailed edges, and substantially improving brightness, demonstrating strong\\npotential for underwater target detection applications.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,eess.IV\", \"published\": \"2025-04-15T08:34:56Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01737v1\", \"title\": \"Enlightenment Period Improving DNN Performance\", \"summary\": \"In the early stage of deep neural network training, the loss decreases\\nrapidly before gradually leveling off. Extensive research has shown that during\\nthis stage, the model parameters undergo significant changes and their\\ndistribution is largely established. Existing studies suggest that the\\nintroduction of noise during early training can degrade model performance. We\\nidentify a critical \\\"enlightenment period\\\" encompassing up to the first 4% of\\nthe training cycle (1--20 epochs for 500-epoch training schedules), a phase\\ncharacterized by intense parameter fluctuations and heightened noise\\nsensitivity. Our findings reveal that strategically reducing noise during this\\nbrief phase--by disabling data augmentation techniques such as Mixup or\\nremoving high-loss samples--leads to statistically significant improvements in\\nmodel performance. This work opens new avenues for exploring the relationship\\nbetween the enlightenment period and network training dynamics across diverse\\nmodel architectures and tasks.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-02T13:49:31Z\"}"}

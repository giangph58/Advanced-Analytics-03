{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10409v1\", \"title\": \"GPS: Distilling Compact Memories via Grid-based Patch Sampling for\\n  Efficient Online Class-Incremental Learning\", \"summary\": \"Online class-incremental learning aims to enable models to continuously adapt\\nto new classes with limited access to past data, while mitigating catastrophic\\nforgetting. Replay-based methods address this by maintaining a small memory\\nbuffer of previous samples, achieving competitive performance. For effective\\nreplay under constrained storage, recent approaches leverage distilled data to\\nenhance the informativeness of memory. However, such approaches often involve\\nsignificant computational overhead due to the use of bi-level optimization.\\nMotivated by these limitations, we introduce Grid-based Patch Sampling (GPS), a\\nlightweight and effective strategy for distilling informative memory samples\\nwithout relying on a trainable model. GPS generates informative samples by\\nsampling a subset of pixels from the original image, yielding compact\\nlow-resolution representations that preserve both semantic content and\\nstructural information. During replay, these representations are reassembled to\\nsupport training and evaluation. Experiments on extensive benchmarks\\ndemonstrate that GRS can be seamlessly integrated into existing replay\\nframeworks, leading to 3%-4% improvements in average end accuracy under\\nmemory-constrained settings, with limited computational overhead.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-14T16:58:02Z\"}"}

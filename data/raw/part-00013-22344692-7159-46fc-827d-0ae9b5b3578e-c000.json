{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15263v1\", \"title\": \"Interpretable Locomotion Prediction in Construction Using a\\n  Memory-Driven LLM Agent With Chain-of-Thought Reasoning\", \"summary\": \"Construction tasks are inherently unpredictable, with dynamic environments\\nand safety-critical demands posing significant risks to workers. Exoskeletons\\noffer potential assistance but falter without accurate intent recognition\\nacross diverse locomotion modes. This paper presents a locomotion prediction\\nagent leveraging Large Language Models (LLMs) augmented with memory systems,\\naimed at improving exoskeleton assistance in such settings. Using multimodal\\ninputs - spoken commands and visual data from smart glasses - the agent\\nintegrates a Perception Module, Short-Term Memory (STM), Long-Term Memory\\n(LTM), and Refinement Module to predict locomotion modes effectively.\\nEvaluation reveals a baseline weighted F1-score of 0.73 without memory, rising\\nto 0.81 with STM, and reaching 0.90 with both STM and LTM, excelling with vague\\nand safety-critical commands. Calibration metrics, including a Brier Score drop\\nfrom 0.244 to 0.090 and ECE from 0.222 to 0.044, affirm improved reliability.\\nThis framework supports safer, high-level human-exoskeleton collaboration, with\\npromise for adaptive assistive systems in dynamic industries.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO\", \"published\": \"2025-04-21T17:45:21Z\"}"}

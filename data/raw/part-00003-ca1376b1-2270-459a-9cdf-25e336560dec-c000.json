{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04375v1\", \"title\": \"Balancing Accuracy, Calibration, and Efficiency in Active Learning with\\n  Vision Transformers Under Label Noise\", \"summary\": \"Fine-tuning pre-trained convolutional neural networks on ImageNet for\\ndownstream tasks is well-established. Still, the impact of model size on the\\nperformance of vision transformers in similar scenarios, particularly under\\nlabel noise, remains largely unexplored. Given the utility and versatility of\\ntransformer architectures, this study investigates their practicality under\\nlow-budget constraints and noisy labels. We explore how classification accuracy\\nand calibration are affected by symmetric label noise in active learning\\nsettings, evaluating four vision transformer configurations (Base and Large\\nwith 16x16 and 32x32 patch sizes) and three Swin Transformer configurations\\n(Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label\\nnoise rates. Our findings show that larger ViT models (ViTl32 in particular)\\nconsistently outperform their smaller counterparts in both accuracy and\\ncalibration, even under moderate to high label noise, while Swin Transformers\\nexhibit weaker robustness across all noise levels. We find that smaller patch\\nsizes do not always lead to better performance, as ViTl16 performs consistently\\nworse than ViTl32 while incurring a higher computational cost. We also find\\nthat information-based Active Learning strategies only provide meaningful\\naccuracy improvements at moderate label noise rates, but they result in poorer\\ncalibration compared to models trained on randomly acquired labels, especially\\nat high label noise rates. We hope these insights provide actionable guidance\\nfor practitioners looking to deploy vision transformers in resource-constrained\\nenvironments, where balancing model complexity, label noise, and compute\\nefficiency is critical in model fine-tuning or distillation.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.LG\", \"published\": \"2025-05-07T12:53:13Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13052v1\", \"title\": \"GraphAttack: Exploiting Representational Blindspots in LLM Safety\\n  Mechanisms\", \"summary\": \"Large Language Models (LLMs) have been equipped with safety mechanisms to\\nprevent harmful outputs, but these guardrails can often be bypassed through\\n\\\"jailbreak\\\" prompts. This paper introduces a novel graph-based approach to\\nsystematically generate jailbreak prompts through semantic transformations. We\\nrepresent malicious prompts as nodes in a graph structure with edges denoting\\ndifferent transformations, leveraging Abstract Meaning Representation (AMR) and\\nResource Description Framework (RDF) to parse user goals into semantic\\ncomponents that can be manipulated to evade safety filters. We demonstrate a\\nparticularly effective exploitation vector by instructing LLMs to generate code\\nthat realizes the intent described in these semantic graphs, achieving success\\nrates of up to 87% against leading commercial LLMs. Our analysis reveals that\\ncontextual framing and abstraction are particularly effective at circumventing\\nsafety measures, highlighting critical gaps in current safety alignment\\ntechniques that focus primarily on surface-level patterns. These findings\\nprovide insights for developing more robust safeguards against structured\\nsemantic attacks. Our research contributes both a theoretical framework and\\npractical methodology for systematically stress-testing LLM safety mechanisms.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR\", \"published\": \"2025-04-17T16:09:12Z\"}"}

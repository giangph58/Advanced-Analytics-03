{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04578v1\", \"title\": \"Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via\\n  Reward Neutralization\", \"summary\": \"Reinforcement learning (RL) fine-tuning transforms large language models\\nwhile creating a vulnerability we experimentally verify: Our experiment shows\\nthat malicious RL fine-tuning dismantles safety guardrails with remarkable\\nefficiency, requiring only 50 steps and minimal adversarial prompts, with\\nharmful escalating from 0-2 to 7-9. This attack vector particularly threatens\\nopen-source models with parameter-level access. Existing defenses targeting\\nsupervised fine-tuning prove ineffective against RL's dynamic feedback\\nmechanisms. We introduce Reward Neutralization, the first defense framework\\nspecifically designed against RL fine-tuning attacks, establishing concise\\nrejection patterns that render malicious reward signals ineffective. Our\\napproach trains models to produce minimal-information rejections that attackers\\ncannot exploit, systematically neutralizing attempts to optimize toward harmful\\noutputs. Experiments validate that our approach maintains low harmful scores\\n(no greater than 2) after 200 attack steps, while standard models rapidly\\ndeteriorate. This work provides the first constructive proof that robust\\ndefense against increasingly accessible RL attacks is achievable, addressing a\\ncritical security gap for open-weight models.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-05-07T17:18:48Z\"}"}

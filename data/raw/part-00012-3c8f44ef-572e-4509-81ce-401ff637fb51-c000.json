{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21707v1\", \"title\": \"Recursive KL Divergence Optimization: A Dynamic Framework for\\n  Representation Learning\", \"summary\": \"We propose a generalization of modern representation learning objectives by\\nreframing them as recursive divergence alignment processes over localized\\nconditional distributions While recent frameworks like Information Contrastive\\nLearning I-Con unify multiple learning paradigms through KL divergence between\\nfixed neighborhood conditionals we argue this view underplays a crucial\\nrecursive structure inherent in the learning process. We introduce Recursive KL\\nDivergence Optimization RKDO a dynamic formalism where representation learning\\nis framed as the evolution of KL divergences across data neighborhoods. This\\nformulation captures contrastive clustering and dimensionality reduction\\nmethods as static slices while offering a new path to model stability and local\\nadaptation. Our experiments demonstrate that RKDO offers dual efficiency\\nadvantages approximately 30 percent lower loss values compared to static\\napproaches across three different datasets and 60 to 80 percent reduction in\\ncomputational resources needed to achieve comparable results. This suggests\\nthat RKDOs recursive updating mechanism provides a fundamentally more efficient\\noptimization landscape for representation learning with significant\\nimplications for resource constrained applications.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.IT,math.IT\", \"published\": \"2025-04-30T14:51:27Z\"}"}

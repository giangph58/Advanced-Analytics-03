{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12299v1\", \"title\": \"Adapting a World Model for Trajectory Following in a 3D Game\", \"summary\": \"Imitation learning is a powerful tool for training agents by leveraging\\nexpert knowledge, and being able to replicate a given trajectory is an integral\\npart of it. In complex environments, like modern 3D video games, distribution\\nshift and stochasticity necessitate robust approaches beyond simple action\\nreplay. In this study, we apply Inverse Dynamics Models (IDM) with different\\nencoders and policy heads to trajectory following in a modern 3D video game --\\nBleeding Edge. Additionally, we investigate several future alignment strategies\\nthat address the distribution shift caused by the aleatoric uncertainty and\\nimperfections of the agent. We measure both the trajectory deviation distance\\nand the first significant deviation point between the reference and the agent's\\ntrajectory and show that the optimal configuration depends on the chosen\\nsetting. Our results show that in a diverse data setting, a GPT-style policy\\nhead with an encoder trained from scratch performs the best, DINOv2 encoder\\nwith the GPT-style policy head gives the best results in the low data regime,\\nand both GPT-style and MLP-style policy heads had comparable results when\\npre-trained on a diverse setting and fine-tuned for a specific behaviour\\nsetting.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CV,cs.LG\", \"published\": \"2025-04-16T17:59:54Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23730v1\", \"title\": \"KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large\\n  Vision-Language Models in the Korean Language\", \"summary\": \"The recent emergence of Large Vision-Language Models(VLMs) has resulted in a\\nvariety of different benchmarks for evaluating such models. Despite this, we\\nobserve that most existing evaluation methods suffer from the fact that they\\neither require the model to choose from pre-determined responses, sacrificing\\nopen-endedness, or evaluate responses using a judge model, resulting in\\nsubjective and unreliable evaluation. In addition, we observe a lack of\\nbenchmarks for VLMs in the Korean language, which are necessary as a separate\\nmetric from more common English language benchmarks, as the performance of\\ngenerative language models can differ significantly based on the language being\\nused. Therefore, we present KOFFVQA, a general-purpose free-form visual\\nquestion answering benchmark in the Korean language for the evaluation of VLMs.\\nOur benchmark consists of 275 carefully crafted questions each paired with an\\nimage and grading criteria covering 10 different aspects of VLM performance.\\nThe grading criteria eliminate the problem of unreliability by allowing the\\njudge model to grade each response based on a pre-determined set of rules. By\\ndefining the evaluation criteria in an objective manner, even a small\\nopen-source model can be used to evaluate models on our benchmark reliably. In\\naddition to evaluating a large number of existing VLMs on our benchmark, we\\nalso experimentally verify that our method of using pre-existing grading\\ncriteria for evaluation is much more reliable than existing methods. Our\\nevaluation code is available at https://github.com/maum-ai/KOFFVQA\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.CL\", \"published\": \"2025-03-31T05:04:25Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21299v1\", \"title\": \"BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language\\n  Models\", \"summary\": \"Identifying bias in LLM-generated content is a crucial prerequisite for\\nensuring fairness in LLMs. Existing methods, such as fairness classifiers and\\nLLM-based judges, face limitations related to difficulties in understanding\\nunderlying intentions and the lack of criteria for fairness judgment. In this\\npaper, we introduce BiasGuard, a novel bias detection tool that explicitly\\nanalyzes inputs and reasons through fairness specifications to provide accurate\\njudgments. BiasGuard is implemented through a two-stage approach: the first\\nstage initializes the model to explicitly reason based on fairness\\nspecifications, while the second stage leverages reinforcement learning to\\nenhance its reasoning and judgment capabilities. Our experiments, conducted\\nacross five datasets, demonstrate that BiasGuard outperforms existing tools,\\nimproving accuracy and reducing over-fairness misjudgments. We also highlight\\nthe importance of reasoning-enhanced decision-making and provide evidence for\\nthe effectiveness of our two-stage optimization pipeline.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-30T04:13:03Z\"}"}

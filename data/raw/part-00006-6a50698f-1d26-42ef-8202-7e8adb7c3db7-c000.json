{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05153v1\", \"title\": \"SparsyFed: Sparse Adaptive Federated Training\", \"summary\": \"Sparse training is often adopted in cross-device federated learning (FL)\\nenvironments where constrained devices collaboratively train a machine learning\\nmodel on private data by exchanging pseudo-gradients across heterogeneous\\nnetworks. Although sparse training methods can reduce communication overhead\\nand computational burden in FL, they are often not used in practice for the\\nfollowing key reasons: (1) data heterogeneity makes it harder for clients to\\nreach consensus on sparse models compared to dense ones, requiring longer\\ntraining; (2) methods for obtaining sparse masks lack adaptivity to accommodate\\nvery heterogeneous data distributions, crucial in cross-device FL; and (3)\\nadditional hyperparameters are required, which are notably challenging to tune\\nin FL. This paper presents SparsyFed, a practical federated sparse training\\nmethod that critically addresses the problems above. Previous works have only\\nsolved one or two of these challenges at the expense of introducing new\\ntrade-offs, such as clients' consensus on masks versus sparsity pattern\\nadaptivity. We show that SparsyFed simultaneously (1) can produce 95% sparse\\nmodels, with negligible degradation in accuracy, while only needing a single\\nhyperparameter, (2) achieves a per-round weight regrowth 200 times smaller than\\nprevious methods, and (3) allows the sparse masks to adapt to highly\\nheterogeneous data distributions and outperform all baselines under such\\nconditions.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-07T14:57:02Z\"}"}

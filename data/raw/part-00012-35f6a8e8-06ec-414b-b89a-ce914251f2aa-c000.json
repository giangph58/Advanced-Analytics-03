{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10421v1\", \"title\": \"Can We Edit LLMs for Long-Tail Biomedical Knowledge?\", \"summary\": \"Knowledge editing has emerged as an effective approach for updating large\\nlanguage models (LLMs) by modifying their internal knowledge. However, their\\napplication to the biomedical domain faces unique challenges due to the\\nlong-tailed distribution of biomedical knowledge, where rare and infrequent\\ninformation is prevalent. In this paper, we conduct the first comprehensive\\nstudy to investigate the effectiveness of knowledge editing methods for editing\\nlong-tail biomedical knowledge. Our results indicate that, while existing\\nediting methods can enhance LLMs' performance on long-tail biomedical\\nknowledge, their performance on long-tail knowledge remains inferior to that on\\nhigh-frequency popular knowledge, even after editing. Our further analysis\\nreveals that long-tail biomedical knowledge contains a significant amount of\\none-to-many knowledge, where one subject and relation link to multiple objects.\\nThis high prevalence of one-to-many knowledge limits the effectiveness of\\nknowledge editing in improving LLMs' understanding of long-tail biomedical\\nknowledge, highlighting the need for tailored strategies to bridge this\\nperformance gap.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-14T17:08:20Z\"}"}

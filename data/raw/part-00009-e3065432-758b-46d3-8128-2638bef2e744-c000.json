{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15700v1\", \"title\": \"Towards True Work-Efficiency in Parallel Derandomization: MIS, Maximal\\n  Matching, and Hitting Set\", \"summary\": \"Derandomization is one of the classic topics studied in the theory of\\nparallel computations, dating back to the early 1980s. Despite much work, all\\nknown techniques lead to deterministic algorithms that are not work-efficient.\\nFor instance, for the well-studied problem of maximal independent set -- e.g.,\\n[Karp, Wigderson STOC'84; Luby STOC' 85; Luby FOCS'88] -- state-of-the-art\\ndeterministic algorithms require at least $m \\\\cdot poly(\\\\log n)$ work, where\\n$m$ and $n$ denote the number of edges and vertices. Hence, these deterministic\\nalgorithms will remain slower than their trivial sequential counterparts unless\\nwe have at least $poly(\\\\log n)$ processors.\\n  In this paper, we present a generic parallel derandomization technique that\\nmoves exponentially closer to work-efficiency. The method iteratively rounds\\nfractional solutions representing the randomized assignments to integral\\nsolutions that provide deterministic assignments, while maintaining certain\\nlinear or quadratic objective functions, and in an \\\\textit{essentially\\nwork-efficient} manner. As example end-results, we use this technique to obtain\\ndeterministic algorithms with $m \\\\cdot poly(\\\\log \\\\log n)$ work and $poly(\\\\log\\nn)$ depth for problems such as maximal independent set, maximal matching, and\\nhitting set.\", \"main_category\": \"cs.DS\", \"categories\": \"cs.DS,cs.DC\", \"published\": \"2025-04-22T08:41:25Z\"}"}

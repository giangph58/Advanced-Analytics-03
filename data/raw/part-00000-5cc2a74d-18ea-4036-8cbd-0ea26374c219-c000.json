{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05652v1\", \"title\": \"Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking\", \"summary\": \"Large Language Models (LLMs) have become increasingly integral to a wide\\nrange of applications. However, they still remain the threat of jailbreak\\nattacks, where attackers manipulate designed prompts to make the models elicit\\nmalicious outputs. Analyzing jailbreak methods can help us delve into the\\nweakness of LLMs and improve it. In this paper, We reveal a vulnerability in\\nlarge language models (LLMs), which we term Defense Threshold Decay (DTD), by\\nanalyzing the attention weights of the model's output on input and subsequent\\noutput on prior output: as the model generates substantial benign content, its\\nattention weights shift from the input to prior output, making it more\\nsusceptible to jailbreak attacks. To demonstrate the exploitability of DTD, we\\npropose a novel jailbreak attack method, Sugar-Coated Poison (SCP), which\\ninduces the model to generate substantial benign content through benign input\\nand adversarial reasoning, subsequently producing malicious content. To\\nmitigate such attacks, we introduce a simple yet effective defense strategy,\\nPOSD, which significantly reduces jailbreak success rates while preserving the\\nmodel's generalization capabilities.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.CL\", \"published\": \"2025-04-08T03:57:09Z\"}"}

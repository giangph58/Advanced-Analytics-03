{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16786v1\", \"title\": \"MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating\\n  Over-Smoothing and Incorporating Outlier Scores\", \"summary\": \"Recent advances in large language models have significantly improved their\\nability to process long-context input, but practical applications are\\nchallenged by increased inference time and resource consumption, particularly\\nin resource-constrained environments. To address these challenges, we propose\\nMOOSComp, a token-classification-based long-context compression method that\\nenhances the performance of a BERT-based compressor by mitigating the\\nover-smoothing problem and incorporating outlier scores. In the training phase,\\nwe add an inter-class cosine similarity loss term to penalize excessively\\nsimilar token representations, thereby improving the token classification\\naccuracy. During the compression phase, we introduce outlier scores to preserve\\nrare but critical tokens that are prone to be discarded in task-agnostic\\ncompression. These scores are integrated with the classifier's output, making\\nthe compressor more generalizable to various tasks. Superior performance is\\nachieved at various compression ratios on long-context understanding and\\nreasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x\\ncompression ratio on a resource-constrained mobile device.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-23T15:02:53Z\"}"}

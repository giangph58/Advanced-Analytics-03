{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21693v1\", \"title\": \"Distributed Online Randomized Gradient-Free optimization with Compressed\\n  Communication\", \"summary\": \"This paper addresses two fundamental challenges in distributed online convex\\noptimization: communication efficiency and optimization under limited feedback.\\nWe propose Online Compressed Gradient Tracking with one-point Bandit Feedback\\n(OCGT-BF), a novel algorithm that harness data compression and gradient-free\\noptimization techniques in distributed networks. Our algorithm incorporates a\\ncompression scheme with error compensation mechanisms to reduce communication\\noverhead while maintaining convergence guarantees. Unlike traditional\\napproaches that assume perfect communication and full gradient access, OCGT-BF\\noperates effectively under practical constraints by combining gradient-like\\ntracking with one-point feedback estimation. We provide theoretical analysis\\ndemonstrating the dynamic regret bounds under both bandit feedback and\\nstochastic gradient scenarios. Finally, extensive experiments validate that\\nOCGT-BF achieves low dynamic regret while significantly reducing communication\\nrequirements.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC\", \"published\": \"2025-04-30T14:33:47Z\"}"}

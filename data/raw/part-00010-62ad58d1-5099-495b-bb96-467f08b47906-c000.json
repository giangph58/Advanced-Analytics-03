{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11271v1\", \"title\": \"Distillation-Supervised Convolutional Low-Rank Adaptation for Efficient\\n  Image Super-Resolution\", \"summary\": \"Convolutional neural networks (CNNs) have been widely used in efficient image\\nsuper-resolution. However, for CNN-based methods, performance gains often\\nrequire deeper networks and larger feature maps, which increase complexity and\\ninference costs. Inspired by LoRA's success in fine-tuning large language\\nmodels, we explore its application to lightweight models and propose\\nDistillation-Supervised Convolutional Low-Rank Adaptation (DSCLoRA), which\\nimproves model performance without increasing architectural complexity or\\ninference costs. Specifically, we integrate ConvLoRA into the efficient SR\\nnetwork SPAN by replacing the SPAB module with the proposed SConvLB module and\\nincorporating ConvLoRA layers into both the pixel shuffle block and its\\npreceding convolutional layer. DSCLoRA leverages low-rank decomposition for\\nparameter updates and employs a spatial feature affinity-based knowledge\\ndistillation strategy to transfer second-order statistical information from\\nteacher models (pre-trained SPAN) to student models (ours). This method\\npreserves the core knowledge of lightweight models and facilitates optimal\\nsolution discovery under certain conditions. Experiments on benchmark datasets\\nshow that DSCLoRA improves PSNR and SSIM over SPAN while maintaining its\\nefficiency and competitive image quality. Notably, DSCLoRA ranked first in the\\nOverall Performance Track of the NTIRE 2025 Efficient Super-Resolution\\nChallenge. Our code and models are made publicly available at\\nhttps://github.com/Yaozzz666/DSCF-SR.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-15T15:12:57Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19627v1\", \"title\": \"VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with\\n  Vision-Language Instruction Fine-Tuning\", \"summary\": \"Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like\\nembodied intelligence due to their strong vision-language reasoning abilities.\\nHowever, current LVLMs process entire images at the token level, which is\\ninefficient compared to humans who analyze information and generate content at\\nthe conceptual level, extracting relevant visual concepts with minimal effort.\\nThis inefficiency, stemming from the lack of a visual concept model, limits\\nLVLMs' usability in real-world applications. To address this, we propose VCM,\\nan end-to-end self-supervised visual concept modeling framework. VCM leverages\\nimplicit contrastive learning across multiple sampled instances and\\nvision-language fine-tuning to construct a visual concept model without\\nrequiring costly concept-level annotations. Our results show that VCM\\nsignificantly reduces computational costs (e.g., 85\\\\% fewer FLOPs for\\nLLaVA-1.5-7B) while maintaining strong performance across diverse image\\nunderstanding tasks. Moreover, VCM enhances visual encoders' capabilities in\\nclassic visual concept perception tasks. Extensive quantitative and qualitative\\nexperiments validate the effectiveness and efficiency of VCM.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.CV,cs.LG\", \"published\": \"2025-04-28T09:39:07Z\"}"}

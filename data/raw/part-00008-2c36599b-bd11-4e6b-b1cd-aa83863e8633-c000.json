{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04441v1\", \"title\": \"Towards Effectively Leveraging Execution Traces for Program Repair with\\n  Code LLMs\", \"summary\": \"Large Language Models (LLMs) show promising performance on various\\nprogramming tasks, including Automatic Program Repair (APR). However, most\\napproaches to LLM-based APR are limited to the static analysis of the programs,\\nwhile disregarding their runtime behavior. Inspired by knowledge-augmented NLP,\\nin this work, we aim to remedy this potential blind spot by augmenting standard\\nAPR prompts with program execution traces. We evaluate our approach using the\\nGPT family of models on three popular APR datasets. Our findings suggest that\\nsimply incorporating execution traces into the prompt provides a limited\\nperformance improvement over trace-free baselines, in only 2 out of 6 tested\\ndataset / model configurations. We further find that the effectiveness of\\nexecution traces for APR diminishes as their complexity increases. We explore\\nseveral strategies for leveraging traces in prompts and demonstrate that\\nLLM-optimized prompts help outperform trace-free prompts more consistently.\\nAdditionally, we show trace-based prompting to be superior to finetuning a\\nsmaller LLM on a small-scale dataset; and conduct probing studies reinforcing\\nthe notion that execution traces can complement the reasoning abilities of the\\nLLMs.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-07T14:12:41Z\"}"}

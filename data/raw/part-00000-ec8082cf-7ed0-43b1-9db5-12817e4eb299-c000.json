{"value":"{\"aid\": \"http://arxiv.org/abs/2503.21676v1\", \"title\": \"How do language models learn facts? Dynamics, curricula and\\n  hallucinations\", \"summary\": \"Large language models accumulate vast knowledge during pre-training, yet the\\ndynamics governing this acquisition remain poorly understood. This work\\ninvestigates the learning dynamics of language models on a synthetic factual\\nrecall task, uncovering three key findings: First, language models learn in\\nthree phases, exhibiting a performance plateau before acquiring precise factual\\nknowledge. Mechanistically, this plateau coincides with the formation of\\nattention-based circuits that support recall. Second, the training data\\ndistribution significantly impacts learning dynamics, as imbalanced\\ndistributions lead to shorter plateaus. Finally, hallucinations emerge\\nsimultaneously with knowledge, and integrating new knowledge into the model\\nthrough fine-tuning is challenging, as it quickly corrupts its existing\\nparametric memories. Our results emphasize the importance of data distribution\\nin knowledge acquisition and suggest novel data scheduling strategies to\\naccelerate neural network training.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-03-27T16:43:45Z\"}"}

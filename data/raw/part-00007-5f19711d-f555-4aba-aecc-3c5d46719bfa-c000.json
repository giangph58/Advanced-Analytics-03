{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12082v1\", \"title\": \"Selective Demonstration Retrieval for Improved Implicit Hate Speech\\n  Detection\", \"summary\": \"Hate speech detection is a crucial area of research in natural language\\nprocessing, essential for ensuring online community safety. However, detecting\\nimplicit hate speech, where harmful intent is conveyed in subtle or indirect\\nways, remains a major challenge. Unlike explicit hate speech, implicit\\nexpressions often depend on context, cultural subtleties, and hidden biases,\\nmaking them more challenging to identify consistently. Additionally, the\\ninterpretation of such speech is influenced by external knowledge and\\ndemographic biases, resulting in varied detection results across different\\nlanguage models. Furthermore, Large Language Models often show heightened\\nsensitivity to toxic language and references to vulnerable groups, which can\\nlead to misclassifications. This over-sensitivity results in false positives\\n(incorrectly identifying harmless statements as hateful) and false negatives\\n(failing to detect genuinely harmful content). Addressing these issues requires\\nmethods that not only improve detection precision but also reduce model biases\\nand enhance robustness. To address these challenges, we propose a novel method,\\nwhich utilizes in-context learning without requiring model fine-tuning. By\\nadaptively retrieving demonstrations that focus on similar groups or those with\\nthe highest similarity scores, our approach enhances contextual comprehension.\\nExperimental results show that our method outperforms current state-of-the-art\\ntechniques. Implementation details and code are available at TBD.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-16T13:43:23Z\"}"}

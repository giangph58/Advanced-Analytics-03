{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05019v1\", \"title\": \"Generating Reliable Synthetic Clinical Trial Data: The Role of\\n  Hyperparameter Optimization and Domain Constraints\", \"summary\": \"The generation of synthetic clinical trial data offers a promising approach\\nto mitigating privacy concerns and data accessibility limitations in medical\\nresearch. However, ensuring that synthetic datasets maintain high fidelity,\\nutility, and adherence to domain-specific constraints remains a key challenge.\\nWhile hyperparameter optimization (HPO) has been shown to improve generative\\nmodel performance, the effectiveness of different optimization strategies for\\nsynthetic clinical data remains unclear. This study systematically evaluates\\nfour HPO strategies across eight generative models, comparing single-metric\\noptimization against compound metric optimization approaches. Our results\\ndemonstrate that HPO consistently improves synthetic data quality, with TVAE,\\nCTGAN, and CTAB-GAN+ achieving improvements of up to 60%, 39%, and 38%,\\nrespectively. Compound metric optimization outperformed single-metric\\nstrategies, producing more balanced and generalizable synthetic datasets.\\nInterestingly, HPO alone is insufficient to ensure clinically valid synthetic\\ndata, as all models exhibited violations of fundamental survival constraints.\\nPreprocessing and postprocessing played a crucial role in reducing these\\nviolations, as models lacking robust processing steps produced invalid data in\\nup to 61% of cases. These findings underscore the necessity of integrating\\nexplicit domain knowledge alongside HPO to create high quality synthetic\\ndatasets. Our study provides actionable recommendations for improving synthetic\\ndata generation, with future research needed to refine metric selection and\\nvalidate these findings on larger datasets to enhance clinical applicability.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-05-08T07:51:36Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17751v1\", \"title\": \"Revisiting Reset Mechanisms in Spiking Neural Networks for Sequential\\n  Modeling: Specialized Discretization for Binary Activated RNN\", \"summary\": \"In the field of image recognition, spiking neural networks (SNNs) have\\nachieved performance comparable to conventional artificial neural networks\\n(ANNs). In such applications, SNNs essentially function as traditional neural\\nnetworks with quantized activation values. This article focuses on an another\\nalternative perspective,viewing SNNs as binary-activated recurrent neural\\nnetworks (RNNs) for sequential modeling tasks.From this viewpoint, current SNN\\narchitectures face several fundamental challenges in sequence modeling: (1)\\nTraditional models lack effective memory mechanisms for long-range sequence\\nmodeling; (2) The biological-inspired components in SNNs (such as reset\\nmechanisms and refractory period applications) remain theoretically\\nunder-explored for sequence tasks; (3) The RNN-like computational paradigm in\\nSNNs prevents parallel training across different timesteps.To address these\\nchallenges, this study conducts a systematic analysis of the fundamental\\nmechanisms underlying reset operations and refractory periods in\\nbinary-activated RNN-based SNN sequence models. We re-examine whether such\\nbiological mechanisms are strictly necessary for generating sparse spiking\\npatterns, provide new theoretical explanations and insights, and ultimately\\npropose the fixed-refractory-period SNN architecture for sequence modeling.\", \"main_category\": \"cs.NE\", \"categories\": \"cs.NE,cs.AI\", \"published\": \"2025-04-24T17:09:59Z\"}"}

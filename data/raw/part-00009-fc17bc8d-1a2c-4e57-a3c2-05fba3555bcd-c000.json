{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15066v1\", \"title\": \"Chinese-LiPS: A Chinese audio-visual speech recognition dataset with\\n  Lip-reading and Presentation Slides\", \"summary\": \"Incorporating visual modalities to assist Automatic Speech Recognition (ASR)\\ntasks has led to significant improvements. However, existing Audio-Visual\\nSpeech Recognition (AVSR) datasets and methods typically rely solely on\\nlip-reading information or speaking contextual video, neglecting the potential\\nof combining these different valuable visual cues within the speaking context.\\nIn this paper, we release a multimodal Chinese AVSR dataset, Chinese-LiPS,\\ncomprising 100 hours of speech, video, and corresponding manual transcription,\\nwith the visual modality encompassing both lip-reading information and the\\npresentation slides used by the speaker. Based on Chinese-LiPS, we develop a\\nsimple yet effective pipeline, LiPS-AVSR, which leverages both lip-reading and\\npresentation slide information as visual modalities for AVSR tasks. Experiments\\nshow that lip-reading and presentation slide information improve ASR\\nperformance by approximately 8\\\\% and 25\\\\%, respectively, with a combined\\nperformance improvement of about 35\\\\%. The dataset is available at\\nhttps://kiri0824.github.io/Chinese-LiPS/\", \"main_category\": \"cs.MM\", \"categories\": \"cs.MM,cs.AI\", \"published\": \"2025-04-21T12:51:54Z\"}"}

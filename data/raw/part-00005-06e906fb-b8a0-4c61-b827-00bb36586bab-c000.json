{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02732v1\", \"title\": \"Why do LLMs attend to the first token?\", \"summary\": \"Large Language Models (LLMs) tend to attend heavily to the first token in the\\nsequence -- creating a so-called attention sink. Many works have studied this\\nphenomenon in detail, proposing various ways to either leverage or alleviate\\nit. Attention sinks have been connected to quantisation difficulties, security\\nissues, and streaming attention. Yet, while many works have provided conditions\\nin which they occur or not, a critical question remains shallowly answered: Why\\ndo LLMs learn such patterns and how are they being used? In this work, we argue\\ntheoretically and empirically that this mechanism provides a method for LLMs to\\navoid over-mixing, connecting this to existing lines of work that study\\nmathematically how information propagates in Transformers. We conduct\\nexperiments to validate our theoretical intuitions and show how choices such as\\ncontext length, depth, and data packing influence the sink behaviour. We hope\\nthat this study provides a new practical perspective on why attention sinks are\\nuseful in LLMs, leading to a better understanding of the attention patterns\\nthat form during training.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-03T16:17:55Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19660v1\", \"title\": \"Decentralization of Generative AI via Mixture of Experts for Wireless\\n  Networks: A Comprehensive Survey\", \"summary\": \"Mixture of Experts (MoE) has emerged as a promising paradigm for scaling\\nmodel capacity while preserving computational efficiency, particularly in\\nlarge-scale machine learning architectures such as large language models\\n(LLMs). Recent advances in MoE have facilitated its adoption in wireless\\nnetworks to address the increasing complexity and heterogeneity of modern\\ncommunication systems. This paper presents a comprehensive survey of the MoE\\nframework in wireless networks, highlighting its potential in optimizing\\nresource efficiency, improving scalability, and enhancing adaptability across\\ndiverse network tasks. We first introduce the fundamental concepts of MoE,\\nincluding various gating mechanisms and the integration with generative AI\\n(GenAI) and reinforcement learning (RL). Subsequently, we discuss the extensive\\napplications of MoE across critical wireless communication scenarios, such as\\nvehicular networks, unmanned aerial vehicles (UAVs), satellite communications,\\nheterogeneous networks, integrated sensing and communication (ISAC), and mobile\\nedge networks. Furthermore, key applications in channel prediction, physical\\nlayer signal processing, radio resource management, network optimization, and\\nsecurity are thoroughly examined. Additionally, we present a detailed overview\\nof open-source datasets that are widely used in MoE-based models to support\\ndiverse machine learning tasks. Finally, this survey identifies crucial future\\nresearch directions for MoE, emphasizing the importance of advanced training\\ntechniques, resource-aware gating strategies, and deeper integration with\\nemerging 6G technologies.\", \"main_category\": \"cs.NI\", \"categories\": \"cs.NI,eess.SP\", \"published\": \"2025-04-28T10:20:04Z\"}"}

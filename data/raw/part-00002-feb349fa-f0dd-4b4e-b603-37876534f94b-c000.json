{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07896v1\", \"title\": \"Fast Adaptation with Behavioral Foundation Models\", \"summary\": \"Unsupervised zero-shot reinforcement learning (RL) has emerged as a powerful\\nparadigm for pretraining behavioral foundation models (BFMs), enabling agents\\nto solve a wide range of downstream tasks specified via reward functions in a\\nzero-shot fashion, i.e., without additional test-time learning or planning.\\nThis is achieved by learning self-supervised task embeddings alongside\\ncorresponding near-optimal behaviors and incorporating an inference procedure\\nto directly retrieve the latent task embedding and associated policy for any\\ngiven reward function. Despite promising results, zero-shot policies are often\\nsuboptimal due to errors induced by the unsupervised training process, the\\nembedding, and the inference procedure. In this paper, we focus on devising\\nfast adaptation strategies to improve the zero-shot performance of BFMs in a\\nfew steps of online interaction with the environment while avoiding any\\nperformance drop during the adaptation process. Notably, we demonstrate that\\nexisting BFMs learn a set of skills containing more performant policies than\\nthose identified by their inference procedure, making them well-suited for fast\\nadaptation. Motivated by this observation, we propose both actor-critic and\\nactor-only fast adaptation strategies that search in the low-dimensional\\ntask-embedding space of the pre-trained BFM to rapidly improve the performance\\nof its zero-shot policies on any downstream task. Notably, our approach\\nmitigates the initial \\\"unlearning\\\" phase commonly observed when fine-tuning\\npre-trained RL models. We evaluate our fast adaptation strategies on top of\\nfour state-of-the-art zero-shot RL methods in multiple navigation and\\nlocomotion domains. Our results show that they achieve 10-40% improvement over\\ntheir zero-shot performance in a few tens of episodes, outperforming existing\\nbaselines.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.RO\", \"published\": \"2025-04-10T16:14:17Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23771v1\", \"title\": \"XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large\\n  Ultra-High-Resolution Remote Sensing Imagery?\", \"summary\": \"The astonishing breakthrough of multimodal large language models (MLLMs) has\\nnecessitated new benchmarks to quantitatively assess their capabilities, reveal\\ntheir limitations, and indicate future research directions. However, this is\\nchallenging in the context of remote sensing (RS), since the imagery features\\nultra-high resolution that incorporates extremely complex semantic\\nrelationships. Existing benchmarks usually adopt notably smaller image sizes\\nthan real-world RS scenarios, suffer from limited annotation quality, and\\nconsider insufficient dimensions of evaluation. To address these issues, we\\npresent XLRS-Bench: a comprehensive benchmark for evaluating the perception and\\nreasoning capabilities of MLLMs in ultra-high-resolution RS scenarios.\\nXLRS-Bench boasts the largest average image size (8500$\\\\times$8500) observed\\nthus far, with all evaluation samples meticulously annotated manually, assisted\\nby a novel semi-automatic captioner on ultra-high-resolution RS images. On top\\nof the XLRS-Bench, 16 sub-tasks are defined to evaluate MLLMs' 10 kinds of\\nperceptual capabilities and 6 kinds of reasoning capabilities, with a primary\\nemphasis on advanced cognitive processes that facilitate real-world\\ndecision-making and the capture of spatiotemporal changes. The results of both\\ngeneral and RS-focused MLLMs on XLRS-Bench indicate that further efforts are\\nneeded for real-world RS applications. We have open-sourced XLRS-Bench to\\nsupport further research in developing more powerful MLLMs for remote sensing.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-31T06:41:18Z\"}"}

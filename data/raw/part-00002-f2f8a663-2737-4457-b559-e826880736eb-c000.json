{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05422v1\", \"title\": \"TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and\\n  Generation\", \"summary\": \"Pioneering token-based works such as Chameleon and Emu3 have established a\\nfoundation for multimodal unification but face challenges of high training\\ncomputational overhead and limited comprehension performance due to a lack of\\nhigh-level semantics. In this paper, we introduce TokLIP, a visual tokenizer\\nthat enhances comprehension by semanticizing vector-quantized (VQ) tokens and\\nincorporating CLIP-level semantics while enabling end-to-end multimodal\\nautoregressive training with standard VQ tokens. TokLIP integrates a low-level\\ndiscrete VQ tokenizer with a ViT-based token encoder to capture high-level\\ncontinuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize\\nhigh-level features, TokLIP disentangles training objectives for comprehension\\nand generation, allowing the direct application of advanced VQ tokenizers\\nwithout the need for tailored quantization operations. Our empirical results\\ndemonstrate that TokLIP achieves exceptional data efficiency, empowering visual\\ntokens with high-level semantic understanding while enhancing low-level\\ngenerative capacity, making it well-suited for autoregressive Transformers in\\nboth comprehension and generation tasks. The code and models are available at\\nhttps://github.com/TencentARC/TokLIP.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.CL\", \"published\": \"2025-05-08T17:12:19Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06658v1\", \"title\": \"A Neuro-inspired Interpretation of Unlearning in Large Language Models\\n  through Sample-level Unlearning Difficulty\", \"summary\": \"Driven by privacy protection laws and regulations, unlearning in Large\\nLanguage Models (LLMs) is gaining increasing attention. However, current\\nresearch often neglects the interpretability of the unlearning process,\\nparticularly concerning sample-level unlearning difficulty. Existing studies\\ntypically assume a uniform unlearning difficulty across samples. This\\nsimplification risks attributing the performance of unlearning algorithms to\\nsample selection rather than the algorithm's design, potentially steering the\\ndevelopment of LLM unlearning in the wrong direction. Thus, we investigate the\\nrelationship between LLM unlearning and sample characteristics, with a focus on\\nunlearning difficulty. Drawing inspiration from neuroscience, we propose a\\nMemory Removal Difficulty ($\\\\mathrm{MRD}$) metric to quantify sample-level\\nunlearning difficulty. Using $\\\\mathrm{MRD}$, we analyze the characteristics of\\nhard-to-unlearn versus easy-to-unlearn samples. Furthermore, we propose an\\n$\\\\mathrm{MRD}$-based weighted sampling method to optimize existing unlearning\\nalgorithms, which prioritizes easily forgettable samples, thereby improving\\nunlearning efficiency and effectiveness. We validate the proposed metric and\\nmethod using public benchmarks and datasets, with results confirming its\\neffectiveness.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL\", \"published\": \"2025-04-09T07:48:10Z\"}"}

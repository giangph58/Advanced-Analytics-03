{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10839v1\", \"title\": \"Rethinking Theory of Mind Benchmarks for LLMs: Towards A User-Centered\\n  Perspective\", \"summary\": \"The last couple of years have witnessed emerging research that appropriates\\nTheory-of-Mind (ToM) tasks designed for humans to benchmark LLM's ToM\\ncapabilities as an indication of LLM's social intelligence. However, this\\napproach has a number of limitations. Drawing on existing psychology and AI\\nliterature, we summarize the theoretical, methodological, and evaluation\\nlimitations by pointing out that certain issues are inherently present in the\\noriginal ToM tasks used to evaluate human's ToM, which continues to persist and\\nexacerbated when appropriated to benchmark LLM's ToM. Taking a human-computer\\ninteraction (HCI) perspective, these limitations prompt us to rethink the\\ndefinition and criteria of ToM in ToM benchmarks in a more dynamic,\\ninteractional approach that accounts for user preferences, needs, and\\nexperiences with LLMs in such evaluations. We conclude by outlining potential\\nopportunities and challenges towards this direction.\", \"main_category\": \"cs.HC\", \"categories\": \"cs.HC,cs.AI\", \"published\": \"2025-04-15T03:44:43Z\"}"}

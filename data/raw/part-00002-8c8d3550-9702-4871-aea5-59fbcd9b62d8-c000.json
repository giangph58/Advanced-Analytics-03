{"value":"{\"aid\": \"http://arxiv.org/abs/2504.09881v1\", \"title\": \"Focus on Local: Finding Reliable Discriminative Regions for Visual Place\\n  Recognition\", \"summary\": \"Visual Place Recognition (VPR) is aimed at predicting the location of a query\\nimage by referencing a database of geotagged images. For VPR task, often fewer\\ndiscriminative local regions in an image produce important effects while\\nmundane background regions do not contribute or even cause perceptual aliasing\\nbecause of easy overlap. However, existing methods lack precisely modeling and\\nfull exploitation of these discriminative regions. In this paper, we propose\\nthe Focus on Local (FoL) approach to stimulate the performance of image\\nretrieval and re-ranking in VPR simultaneously by mining and exploiting\\nreliable discriminative local regions in images and introducing\\npseudo-correlation supervision. First, we design two losses,\\nExtraction-Aggregation Spatial Alignment Loss (SAL) and Foreground-Background\\nContrast Enhancement Loss (CEL), to explicitly model reliable discriminative\\nlocal regions and use them to guide the generation of global representations\\nand efficient re-ranking. Second, we introduce a weakly-supervised local\\nfeature training strategy based on pseudo-correspondences obtained from\\naggregating global features to alleviate the lack of local correspondences\\nground truth for the VPR task. Third, we suggest an efficient re-ranking\\npipeline that is efficiently and precisely based on discriminative region\\nguidance. Finally, experimental results show that our FoL achieves the\\nstate-of-the-art on multiple VPR benchmarks in both image retrieval and\\nre-ranking stages and also significantly outperforms existing two-stage VPR\\nmethods in terms of computational efficiency. Code and models are available at\\nhttps://github.com/chenshunpeng/FoL\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-14T05:04:51Z\"}"}

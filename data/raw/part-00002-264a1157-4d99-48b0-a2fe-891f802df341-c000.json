{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20982v1\", \"title\": \"Provably faster randomized and quantum algorithms for k-means clustering\\n  via uniform sampling\", \"summary\": \"The $k$-means algorithm (Lloyd's algorithm) is a widely used method for\\nclustering unlabeled data. A key bottleneck of the $k$-means algorithm is that\\neach iteration requires time linear in the number of data points, which can be\\nexpensive in big data applications. This was improved in recent works proposing\\nquantum and quantum-inspired classical algorithms to approximate the $k$-means\\nalgorithm locally, in time depending only logarithmically on the number of data\\npoints (along with data dependent parameters) [$q$-means: A quantum algorithm\\nfor unsupervised machine learning; Kerenidis, Landman, Luongo, and Prakash,\\nNeurIPS 2019; Do you know what $q$-means?, Doriguello, Luongo, Tang]. In this\\nwork, we describe a simple randomized mini-batch $k$-means algorithm and a\\nquantum algorithm inspired by the classical algorithm. We prove worse-case\\nguarantees that significantly improve upon the bounds for previous algorithms.\\nOur improvements are due to a careful use of uniform sampling, which preserves\\ncertain symmetries of the $k$-means problem that are not preserved in previous\\nalgorithms that use data norm-based sampling.\", \"main_category\": \"quant-ph\", \"categories\": \"quant-ph,cs.DS,cs.LG\", \"published\": \"2025-04-29T17:51:29Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10351v1\", \"title\": \"Multimodal Representation Learning Techniques for Comprehensive Facial\\n  State Analysis\", \"summary\": \"Multimodal foundation models have significantly improved feature\\nrepresentation by integrating information from multiple modalities, making them\\nhighly suitable for a broader set of applications. However, the exploration of\\nmultimodal facial representation for understanding perception has been limited.\\nUnderstanding and analyzing facial states, such as Action Units (AUs) and\\nemotions, require a comprehensive and robust framework that bridges visual and\\nlinguistic modalities. In this paper, we present a comprehensive pipeline for\\nmultimodal facial state analysis. First, we compile a new Multimodal Face\\nDataset (MFA) by generating detailed multilevel language descriptions of face,\\nincorporating Action Unit (AU) and emotion descriptions, by leveraging GPT-4o.\\nSecond, we introduce a novel Multilevel Multimodal Face Foundation model (MF^2)\\ntailored for Action Unit (AU) and emotion recognition. Our model incorporates\\ncomprehensive visual feature modeling at both local and global levels of face\\nimage, enhancing its ability to represent detailed facial appearances. This\\ndesign aligns visual representations with structured AU and emotion\\ndescriptions, ensuring effective cross-modal integration. Third, we develop a\\nDecoupled Fine-Tuning Network (DFN) that efficiently adapts MF^2 across various\\ntasks and datasets. This approach not only reduces computational overhead but\\nalso broadens the applicability of the foundation model to diverse scenarios.\\nExperimentation show superior performance for AU and emotion detection tasks.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-14T16:00:57Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17331v1\", \"title\": \"Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual\\n  Reality\", \"summary\": \"Locomotion plays a crucial role in shaping the user experience within virtual\\nreality environments. In particular, hands-free locomotion offers a valuable\\nalternative by supporting accessibility and freeing users from reliance on\\nhandheld controllers. To this end, traditional speech-based methods often\\ndepend on rigid command sets, limiting the naturalness and flexibility of\\ninteraction. In this study, we propose a novel locomotion technique powered by\\nlarge language models (LLMs), which allows users to navigate virtual\\nenvironments using natural language with contextual awareness. We evaluate\\nthree locomotion methods: controller-based teleportation, voice-based steering,\\nand our language model-driven approach. Our evaluation measures include\\neye-tracking data analysis, including explainable machine learning through SHAP\\nanalysis as well as standardized questionnaires for usability, presence,\\ncybersickness, and cognitive load to examine user attention and engagement. Our\\nfindings indicate that the LLM-driven locomotion possesses comparable\\nusability, presence, and cybersickness scores to established methods like\\nteleportation, demonstrating its novel potential as a comfortable, natural\\nlanguage-based, hands-free alternative. In addition, it enhances user attention\\nwithin the virtual environment, suggesting greater engagement. Complementary to\\nthese findings, SHAP analysis revealed that fixation, saccade, and\\npupil-related features vary across techniques, indicating distinct patterns of\\nvisual attention and cognitive processing. Overall, we state that our method\\ncan facilitate hands-free locomotion in virtual spaces, especially in\\nsupporting accessibility.\", \"main_category\": \"cs.HC\", \"categories\": \"cs.HC,cs.AI\", \"published\": \"2025-04-24T07:48:09Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05197v1\", \"title\": \"Societal and technological progress as sewing an ever-growing,\\n  ever-changing, patchy, and polychrome quilt\", \"summary\": \"Artificial Intelligence (AI) systems are increasingly placed in positions\\nwhere their decisions have real consequences, e.g., moderating online spaces,\\nconducting research, and advising on policy. Ensuring they operate in a safe\\nand ethically acceptable fashion is thus critical. However, most solutions have\\nbeen a form of one-size-fits-all \\\"alignment\\\". We are worried that such systems,\\nwhich overlook enduring moral diversity, will spark resistance, erode trust,\\nand destabilize our institutions. This paper traces the underlying problem to\\nan often-unstated Axiom of Rational Convergence: the idea that under ideal\\nconditions, rational agents will converge in the limit of conversation on a\\nsingle ethics. Treating that premise as both optional and doubtful, we propose\\nwhat we call the appropriateness framework: an alternative approach grounded in\\nconflict theory, cultural evolution, multi-agent systems, and institutional\\neconomics. The appropriateness framework treats persistent disagreement as the\\nnormal case and designs for it by applying four principles: (1) contextual\\ngrounding, (2) community customization, (3) continual adaptation, and (4)\\npolycentric governance. We argue here that adopting these design principles is\\na good way to shift the main alignment metaphor from moral unification to a\\nmore productive metaphor of conflict management, and that taking this step is\\nboth desirable and urgent.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CY\", \"published\": \"2025-05-08T12:55:07Z\"}"}

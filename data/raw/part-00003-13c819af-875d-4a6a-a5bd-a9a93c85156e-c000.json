{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00612v1\", \"title\": \"Position: AI Competitions Provide the Gold Standard for Empirical Rigor\\n  in GenAI Evaluation\", \"summary\": \"In this position paper, we observe that empirical evaluation in Generative AI\\nis at a crisis point since traditional ML evaluation and benchmarking\\nstrategies are insufficient to meet the needs of evaluating modern GenAI models\\nand systems. There are many reasons for this, including the fact that these\\nmodels typically have nearly unbounded input and output spaces, typically do\\nnot have a well defined ground truth target, and typically exhibit strong\\nfeedback loops and prediction dependence based on context of previous model\\noutputs. On top of these critical issues, we argue that the problems of {\\\\em\\nleakage} and {\\\\em contamination} are in fact the most important and difficult\\nissues to address for GenAI evaluations. Interestingly, the field of AI\\nCompetitions has developed effective measures and practices to combat leakage\\nfor the purpose of counteracting cheating by bad actors within a competition\\nsetting. This makes AI Competitions an especially valuable (but underutilized)\\nresource. Now is time for the field to view AI Competitions as the gold\\nstandard for empirical rigor in GenAI evaluation, and to harness and harvest\\ntheir results with according value.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-05-01T15:43:51Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19549v1\", \"title\": \"DEEMO: De-identity Multimodal Emotion Recognition and Reasoning\", \"summary\": \"Emotion understanding is a critical yet challenging task. Most existing\\napproaches rely heavily on identity-sensitive information, such as facial\\nexpressions and speech, which raises concerns about personal privacy. To\\naddress this, we introduce the De-identity Multimodal Emotion Recognition and\\nReasoning (DEEMO), a novel task designed to enable emotion understanding using\\nde-identified video and audio inputs. The DEEMO dataset consists of two\\nsubsets: DEEMO-NFBL, which includes rich annotations of Non-Facial Body\\nLanguage (NFBL), and DEEMO-MER, an instruction dataset for Multimodal Emotion\\nRecognition and Reasoning using identity-free cues. This design supports\\nemotion understanding without compromising identity privacy. In addition, we\\npropose DEEMO-LLaMA, a Multimodal Large Language Model (MLLM) that integrates\\nde-identified audio, video, and textual information to enhance both emotion\\nrecognition and reasoning. Extensive experiments show that DEEMO-LLaMA achieves\\nstate-of-the-art performance on both tasks, outperforming existing MLLMs by a\\nsignificant margin, achieving 74.49% accuracy and 74.45% F1-score in\\nde-identity emotion recognition, and 6.20 clue overlap and 7.66 label overlap\\nin de-identity emotion reasoning. Our work contributes to ethical AI by\\nadvancing privacy-preserving emotion understanding and promoting responsible\\naffective computing.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-28T07:55:11Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19792v1\", \"title\": \"Contextures: The Mechanism of Representation Learning\", \"summary\": \"This dissertation establishes the contexture theory to mathematically\\ncharacterize the mechanism of representation learning, or pretraining. Despite\\nthe remarkable empirical success of foundation models, it is not very clear\\nwhat representations they learn, and why these representations are useful for\\nvarious downstream tasks. A scientific understanding of representation learning\\nis critical, especially at this point when scaling up the model size is\\nproducing diminishing returns, and designing new pretraining methods is\\nimperative for further progress.\\n  Prior work treated different representation learning methods quite\\ndifferently, whereas the contexture theory provides a unified framework for\\nanalyzing these methods. The central argument is that a representation is\\nlearned from the association between the input X and a context variable A. We\\nprove that if an encoder captures the maximum information of this association,\\nin which case we say that the encoder learns the contexture, then it will be\\noptimal on the class of tasks that are compatible with the context. We also\\nshow that a context is the most useful when the association between X and A is\\nneither too strong nor too weak. The important implication of the contexture\\ntheory is that increasing the model size alone will achieve diminishing\\nreturns, and further advancements require better contexts.\\n  We demonstrate that many pretraining objectives can learn the contexture,\\nincluding supervised learning, self-supervised learning, generative models,\\netc. Then, we introduce two general objectives -- SVME and KISE, for learning\\nthe contexture. We also show how to mix multiple contexts together, an\\neffortless way to create better contexts from existing ones. Then, we prove\\nstatistical learning bounds for representation learning. Finally, we discuss\\nthe effect of the data distribution shift from pretraining to the downstream\\ntask.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,stat.ML\", \"published\": \"2025-04-28T13:36:28Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11171v1\", \"title\": \"TerraMind: Large-Scale Generative Multimodality for Earth Observation\", \"summary\": \"We present TerraMind, the first any-to-any generative, multimodal foundation\\nmodel for Earth observation (EO). Unlike other multimodal models, TerraMind is\\npretrained on dual-scale representations combining both token-level and\\npixel-level data across modalities. On a token level, TerraMind encodes\\nhigh-level contextual information to learn cross-modal relationships, while on\\na pixel level, TerraMind leverages fine-grained representations to capture\\ncritical spatial nuances. We pretrained TerraMind on nine geospatial modalities\\nof a global, large-scale dataset. In this paper, we demonstrate that (i)\\nTerraMind's dual-scale early fusion approach unlocks a range of zero-shot and\\nfew-shot applications for Earth observation, (ii) TerraMind introduces\\n\\\"Thinking-in-Modalities\\\" (TiM) -- the capability of generating additional\\nartificial data during finetuning and inference to improve the model output --\\nand (iii) TerraMind achieves beyond state-of-the-art performance in\\ncommunity-standard benchmarks for EO like PANGAEA. The pretraining dataset, the\\nmodel weights, and our code is open-sourced under a permissive license.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-15T13:17:39Z\"}"}

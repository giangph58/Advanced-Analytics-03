{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15721v1\", \"title\": \"BBAL: A Bidirectional Block Floating Point-Based Quantisation\\n  Accelerator for Large Language Models\", \"summary\": \"Large language models (LLMs), with their billions of parameters, pose\\nsubstantial challenges for deployment on edge devices, straining both memory\\ncapacity and computational resources. Block Floating Point (BFP) quantisation\\nreduces memory and computational overhead by converting high-overhead floating\\npoint operations into low-bit fixed point operations. However, BFP requires\\naligning all data to the maximum exponent, which causes loss of small and\\nmoderate values, resulting in quantisation error and degradation in the\\naccuracy of LLMs. To address this issue, we propose a Bidirectional Block\\nFloating Point (BBFP) data format, which reduces the probability of selecting\\nthe maximum as shared exponent, thereby reducing quantisation error. By\\nutilizing the features in BBFP, we present a full-stack Bidirectional Block\\nFloating Point-Based Quantisation Accelerator for LLMs (BBAL), primarily\\ncomprising a processing element array based on BBFP, paired with proposed\\ncost-effective nonlinear computation unit. Experimental results show BBAL\\nachieves a 22% improvement in accuracy compared to an outlier-aware accelerator\\nat similar efficiency, and a 40% efficiency improvement over a BFP-based\\naccelerator at similar accuracy.\", \"main_category\": \"cs.AR\", \"categories\": \"cs.AR\", \"published\": \"2025-04-22T09:11:21Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12256v1\", \"title\": \"FLIP Reasoning Challenge\", \"summary\": \"Over the past years, advances in artificial intelligence (AI) have\\ndemonstrated how AI can solve many perception and generation tasks, such as\\nimage classification and text writing, yet reasoning remains a challenge. This\\npaper introduces the FLIP dataset, a benchmark for evaluating AI reasoning\\ncapabilities based on human verification tasks on the Idena blockchain. FLIP\\nchallenges present users with two orderings of 4 images, requiring them to\\nidentify the logically coherent one. By emphasizing sequential reasoning,\\nvisual storytelling, and common sense, FLIP provides a unique testbed for\\nmultimodal AI systems. Our experiments evaluate state-of-the-art models,\\nleveraging both vision-language models (VLMs) and large language models (LLMs).\\nResults reveal that even the best open-sourced and closed-sourced models\\nachieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot\\nsettings, compared to human performance of 95.3%. Captioning models aid\\nreasoning models by providing text descriptions of images, yielding better\\nresults than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5\\nPro. Combining the predictions from 15 models in an ensemble increases the\\naccuracy to 85.2%. These findings highlight the limitations of existing\\nreasoning models and the need for robust multimodal benchmarks like FLIP. The\\nfull codebase and dataset will be available at\\nhttps://github.com/aplesner/FLIP-Reasoning-Challenge.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.LG\", \"published\": \"2025-04-16T17:07:16Z\"}"}

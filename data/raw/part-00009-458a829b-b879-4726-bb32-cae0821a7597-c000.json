{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03393v1\", \"title\": \"Prediction Models That Learn to Avoid Missing Values\", \"summary\": \"Handling missing values at test time is challenging for machine learning\\nmodels, especially when aiming for both high accuracy and interpretability.\\nEstablished approaches often add bias through imputation or excessive model\\ncomplexity via missingness indicators. Moreover, either method can obscure\\ninterpretability, making it harder to understand how the model utilizes the\\nobserved variables in predictions. We propose missingness-avoiding (MA) machine\\nlearning, a general framework for training models to rarely require the values\\nof missing (or imputed) features at test time. We create tailored MA learning\\nalgorithms for decision trees, tree ensembles, and sparse linear models by\\nincorporating classifier-specific regularization terms in their learning\\nobjectives. The tree-based models leverage contextual missingness by reducing\\nreliance on missing values based on the observed context. Experiments on\\nreal-world datasets demonstrate that MA-DT, MA-LASSO, MA-RF, and MA-GBT\\neffectively reduce the reliance on features with missing values while\\nmaintaining predictive performance competitive with their unregularized\\ncounterparts. This shows that our framework gives practitioners a powerful tool\\nto maintain interpretability in predictions with test-time missing values.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-06T10:16:35Z\"}"}

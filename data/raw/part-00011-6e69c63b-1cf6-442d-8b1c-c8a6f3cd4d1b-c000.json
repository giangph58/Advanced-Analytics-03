{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01383v1\", \"title\": \"v-CLR: View-Consistent Learning for Open-World Instance Segmentation\", \"summary\": \"In this paper, we address the challenging problem of open-world instance\\nsegmentation. Existing works have shown that vanilla visual networks are biased\\ntoward learning appearance information, \\\\eg texture, to recognize objects. This\\nimplicit bias causes the model to fail in detecting novel objects with unseen\\ntextures in the open-world setting. To address this challenge, we propose a\\nlearning framework, called view-Consistent LeaRning (v-CLR), which aims to\\nenforce the model to learn appearance-invariant representations for robust\\ninstance segmentation. In v-CLR, we first introduce additional views for each\\nimage, where the texture undergoes significant alterations while preserving the\\nimage's underlying structure. We then encourage the model to learn the\\nappearance-invariant representation by enforcing the consistency between object\\nfeatures across different views, for which we obtain class-agnostic object\\nproposals using off-the-shelf unsupervised models that possess strong\\nobject-awareness. These proposals enable cross-view object feature matching,\\ngreatly reducing the appearance dependency while enhancing the\\nobject-awareness. We thoroughly evaluate our method on public benchmarks under\\nboth cross-class and cross-dataset settings, achieving state-of-the-art\\nperformance. Project page: https://visual-ai.github.io/vclr\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-02T05:52:30Z\"}"}

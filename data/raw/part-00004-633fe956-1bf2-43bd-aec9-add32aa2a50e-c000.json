{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12623v1\", \"title\": \"Privacy-Preserving CNN Training with Transfer Learning: Two Hidden\\n  Layers\", \"summary\": \"In this paper, we present the demonstration of training a four-layer neural\\nnetwork entirely using fully homomorphic encryption (FHE), supporting both\\nsingle-output and multi-output classification tasks in a non-interactive\\nsetting. A key contribution of our work is identifying that replacing\\n\\\\textit{Softmax} with \\\\textit{Sigmoid}, in conjunction with the Binary\\nCross-Entropy (BCE) loss function, provides an effective and scalable solution\\nfor homomorphic classification. Moreover, we show that the BCE loss function,\\noriginally designed for multi-output tasks, naturally extends to the\\nmulti-class setting, thereby enabling broader applicability. We also highlight\\nthe limitations of prior loss functions such as the SLE loss and the one\\nproposed in the 2019 CVPR Workshop, both of which suffer from vanishing\\ngradients as network depth increases. To address the challenges posed by\\nlarge-scale encrypted data, we further introduce an improved version of the\\npreviously proposed data encoding scheme, \\\\textit{Double Volley Revolver},\\nwhich achieves a better trade-off between computational and memory efficiency,\\nmaking FHE-based neural network training more practical. The complete, runnable\\nC++ code to implement our work can be found at:\\n\\\\href{https://github.com/petitioner/ML.NNtraining}{$\\\\texttt{https://github.com/petitioner/ML.NNtraining}$}.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR\", \"published\": \"2025-04-17T03:58:23Z\"}"}

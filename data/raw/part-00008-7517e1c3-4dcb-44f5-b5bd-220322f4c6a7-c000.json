{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03380v1\", \"title\": \"Reinforced Correlation Between Vision and Language for Precise Medical\\n  AI Assistant\", \"summary\": \"Medical AI assistants support doctors in disease diagnosis, medical image\\nanalysis, and report generation. However, they still face significant\\nchallenges in clinical use, including limited accuracy with multimodal content\\nand insufficient validation in real-world settings. We propose RCMed, a\\nfull-stack AI assistant that improves multimodal alignment in both input and\\noutput, enabling precise anatomical delineation, accurate localization, and\\nreliable diagnosis through hierarchical vision-language grounding. A\\nself-reinforcing correlation mechanism allows visual features to inform\\nlanguage context, while language semantics guide pixel-wise attention, forming\\na closed loop that refines both modalities. This correlation is enhanced by a\\ncolor region description strategy, translating anatomical structures into\\nsemantically rich text to learn shape-location-text relationships across\\nscales. Trained on 20 million image-mask-description triplets, RCMed achieves\\nstate-of-the-art precision in contextualizing irregular lesions and subtle\\nanatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It\\nachieved a 23.5% relative improvement in cell segmentation from microscopy\\nimages over prior methods. RCMed's strong vision-language alignment enables\\nexceptional generalization, with state-of-the-art performance in external\\nvalidation across 20 clinically significant cancer types, including novel\\ntasks. This work demonstrates how integrated multimodal models capture\\nfine-grained patterns, enabling human-level interpretation in complex scenarios\\nand advancing human-centric AI healthcare.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,eess.IV\", \"published\": \"2025-05-06T10:00:08Z\"}"}

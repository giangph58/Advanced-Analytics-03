{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15640v1\", \"title\": \"Cost-Effective Text Clustering with Large Language Models\", \"summary\": \"Text clustering aims to automatically partition a collection of text\\ndocuments into distinct clusters based on linguistic features. In the\\nliterature, this task is usually framed as metric clustering based on text\\nembeddings from pre-trained encoders or a graph clustering problem upon\\npairwise similarities from an oracle, e.g., a large ML model. Recently, large\\nlanguage models (LLMs) bring significant advancement in this field by offering\\ncontextualized text embeddings and highly accurate similarity scores, but\\nmeanwhile, present grand challenges to cope with substantial computational\\nand/or financial overhead caused by numerous API-based queries or inference\\ncalls to the models.\\n  In response, this paper proposes TECL, a cost-effective framework that taps\\ninto the feedback from LLMs for accurate text clustering within a limited\\nbudget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or\\nTriangleLLM to construct must-link/cannot-link constraints for text pairs, and\\nfurther leverages such constraints as supervision signals input to our weighted\\nconstrained clustering approach to generate clusters. Particularly, EdgeLLM\\n(resp. TriangleLLM) enables the identification of informative text pairs (resp.\\ntriplets) for querying LLMs via well-thought-out greedy algorithms and accurate\\nextraction of pairwise constraints through carefully-crafted prompting\\ntechniques. Our experiments on multiple benchmark datasets exhibit that TECL\\nconsistently and considerably outperforms existing solutions in unsupervised\\ntext clustering under the same query cost for LLMs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-22T06:57:49Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04171v1\", \"title\": \"Large Language Models are often politically extreme, usually\\n  ideologically inconsistent, and persuasive even in informational contexts\", \"summary\": \"Large Language Models (LLMs) are a transformational technology, fundamentally\\nchanging how people obtain information and interact with the world. As people\\nbecome increasingly reliant on them for an enormous variety of tasks, a body of\\nacademic research has developed to examine these models for inherent biases,\\nespecially political biases, often finding them small. We challenge this\\nprevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a\\nnationally representative sample of U.S. voters, we show that LLMs' apparently\\nsmall overall partisan preference is the net result of offsetting extreme views\\non specific topics, much like moderate voters. Second, in a randomized\\nexperiment, we show that LLMs can promulgate their preferences into political\\npersuasiveness even in information-seeking contexts: voters randomized to\\ndiscuss political issues with an LLM chatbot are as much as 5 percentage points\\nmore likely to express the same preferences as that chatbot. Contrary to\\nexpectations, these persuasive effects are not moderated by familiarity with\\nLLMs, news consumption, or interest in politics. LLMs, especially those\\ncontrolled by private companies or governments, may become a powerful and\\ntargeted vector for political influence.\", \"main_category\": \"cs.CY\", \"categories\": \"cs.CY,cs.CL\", \"published\": \"2025-05-07T06:53:59Z\"}"}

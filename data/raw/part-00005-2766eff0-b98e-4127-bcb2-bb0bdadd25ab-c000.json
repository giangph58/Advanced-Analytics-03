{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12916v1\", \"title\": \"Exact Learning Dynamics of In-Context Learning in Linear Transformers\\n  and Its Application to Non-Linear Transformers\", \"summary\": \"Transformer models exhibit remarkable in-context learning (ICL), adapting to\\nnovel tasks from examples within their context, yet the underlying mechanisms\\nremain largely mysterious. Here, we provide an exact analytical\\ncharacterization of ICL emergence by deriving the closed-form stochastic\\ngradient descent (SGD) dynamics for a simplified linear transformer performing\\nregression tasks. Our analysis reveals key properties: (1) a natural separation\\nof timescales directly governed by the input data's covariance structure,\\nleading to staged learning; (2) an exact description of how ICL develops,\\nincluding fixed points corresponding to learned algorithms and conservation\\nlaws constraining the dynamics; and (3) surprisingly nonlinear learning\\nbehavior despite the model's linearity. We hypothesize this phenomenology\\nextends to non-linear models. To test this, we introduce theory-inspired\\nmacroscopic measures (spectral rank dynamics, subspace stability) and use them\\nto provide mechanistic explanations for (1) the sudden emergence of ICL in\\nattention-only networks and (2) delayed generalization (grokking) in modular\\narithmetic models. Our work offers an exact dynamical model for ICL and\\ntheoretically grounded tools for analyzing complex transformer training.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cond-mat.dis-nn\", \"published\": \"2025-04-17T13:05:33Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00681v1\", \"title\": \"MINERVA: Evaluating Complex Video Reasoning\", \"summary\": \"Multimodal LLMs are turning their focus to video benchmarks, however most\\nvideo benchmarks only provide outcome supervision, with no intermediate or\\ninterpretable reasoning steps. This makes it challenging to assess if models\\nare truly able to combine perceptual and temporal information to reason about\\nvideos, or simply get the correct answer by chance or by exploiting linguistic\\nbiases. To remedy this, we provide a new video reasoning dataset called MINERVA\\nfor modern multimodal models. Each question in the dataset comes with 5 answer\\nchoices, as well as detailed, hand-crafted reasoning traces. Our dataset is\\nmultimodal, diverse in terms of video domain and length, and consists of\\ncomplex multi-step questions. Extensive benchmarking shows that our dataset\\nprovides a challenge for frontier open-source and proprietary models. We\\nperform fine-grained error analysis to identify common failure modes across\\nvarious models, and create a taxonomy of reasoning errors. We use this to\\nexplore both human and LLM-as-a-judge methods for scoring video reasoning\\ntraces, and find that failure modes are primarily related to temporal\\nlocalization, followed by visual perception errors, as opposed to logical or\\ncompleteness errors. The dataset, along with questions, answer candidates and\\nreasoning traces will be publicly available under\\nhttps://github.com/google-deepmind/neptune?tab=readme-ov-file\\\\#minerva.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CV\", \"published\": \"2025-05-01T17:41:49Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14899v1\", \"title\": \"Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls\\n  for Video Generation\", \"summary\": \"Camera and human motion controls have been extensively studied for video\\ngeneration, but existing approaches typically address them separately,\\nsuffering from limited data with high-quality annotations for both aspects. To\\novercome this, we present Uni3C, a unified 3D-enhanced framework for precise\\ncontrol of both camera and human motion in video generation. Uni3C includes two\\nkey contributions. First, we propose a plug-and-play control module trained\\nwith a frozen video generative backbone, PCDController, which utilizes\\nunprojected point clouds from monocular depth to achieve accurate camera\\ncontrol. By leveraging the strong 3D priors of point clouds and the powerful\\ncapacities of video foundational models, PCDController shows impressive\\ngeneralization, performing well regardless of whether the inference backbone is\\nfrozen or fine-tuned. This flexibility enables different modules of Uni3C to be\\ntrained in specific domains, i.e., either camera control or human motion\\ncontrol, reducing the dependency on jointly annotated data. Second, we propose\\na jointly aligned 3D world guidance for the inference phase that seamlessly\\nintegrates both scenic point clouds and SMPL-X characters to unify the control\\nsignals for camera and human motion, respectively. Extensive experiments\\nconfirm that PCDController enjoys strong robustness in driving camera motion\\nfor fine-tuned backbones of video generation. Uni3C substantially outperforms\\ncompetitors in both camera controllability and human motion quality.\\nAdditionally, we collect tailored validation sets featuring challenging camera\\nmovements and human actions to validate the effectiveness of our method.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-21T07:10:41Z\"}"}

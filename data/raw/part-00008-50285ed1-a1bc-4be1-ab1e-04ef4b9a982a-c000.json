{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11447v1\", \"title\": \"Diffusion Distillation With Direct Preference Optimization For Efficient\\n  3D LiDAR Scene Completion\", \"summary\": \"The application of diffusion models in 3D LiDAR scene completion is limited\\ndue to diffusion's slow sampling speed. Score distillation accelerates\\ndiffusion sampling but with performance degradation, while post-training with\\ndirect policy optimization (DPO) boosts performance using preference data. This\\npaper proposes Distillation-DPO, a novel diffusion distillation framework for\\nLiDAR scene completion with preference aligment. First, the student model\\ngenerates paired completion scenes with different initial noises. Second, using\\nLiDAR scene evaluation metrics as preference, we construct winning and losing\\nsample pairs. Such construction is reasonable, since most LiDAR scene metrics\\nare informative but non-differentiable to be optimized directly. Third,\\nDistillation-DPO optimizes the student model by exploiting the difference in\\nscore functions between the teacher and student models on the paired completion\\nscenes. Such procedure is repeated until convergence. Extensive experiments\\ndemonstrate that, compared to state-of-the-art LiDAR scene completion diffusion\\nmodels, Distillation-DPO achieves higher-quality scene completion while\\naccelerating the completion speed by more than 5-fold. Our method is the first\\nto explore adopting preference learning in distillation to the best of our\\nknowledge and provide insights into preference-aligned distillation. Our code\\nis public available on https://github.com/happyw1nd/DistillationDPO.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-15T17:57:13Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10465v1\", \"title\": \"Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding\", \"summary\": \"Multimodal Large Language Models (MLLMs) achieve remarkable performance for\\nfine-grained pixel-level understanding tasks. However, all the works rely\\nheavily on extra components, such as vision encoder (CLIP), segmentation\\nexperts, leading to high system complexity and limiting model scaling. In this\\nwork, our goal is to explore a highly simplified MLLM without introducing extra\\ncomponents. Our work is motivated by the recent works on Single trAnsformer as\\na unified vIsion-Language Model (SAIL) design, where these works jointly learn\\nvision tokens and text tokens in transformers. We present Pixel-SAIL, a single\\ntransformer for pixel-wise MLLM tasks. In particular, we present three\\ntechnical improvements on the plain baseline. First, we design a learnable\\nupsampling module to refine visual token features. Secondly, we propose a novel\\nvisual prompt injection strategy to enable the single transformer to understand\\nvisual prompt inputs and benefit from the early fusion of visual prompt\\nembeddings and vision tokens. Thirdly, we introduce a vision expert\\ndistillation strategy to efficiently enhance the single transformer's\\nfine-grained feature extraction capability. In addition, we have collected a\\ncomprehensive pixel understanding benchmark (PerBench), using a manual check.\\nIt includes three tasks: detailed object description, visual prompt-based\\nquestion answering, and visual-text referring segmentation. Extensive\\nexperiments on four referring segmentation benchmarks, one visual prompt\\nbenchmark, and our PerBench show that our Pixel-SAIL achieves comparable or\\neven better results with a much simpler pipeline. Code and model will be\\nreleased at https://github.com/magic-research/Sa2VA.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-14T17:52:22Z\"}"}

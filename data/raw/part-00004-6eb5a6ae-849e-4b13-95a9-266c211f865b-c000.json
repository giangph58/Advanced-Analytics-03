{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02343v1\", \"title\": \"Toward General and Robust LLM-enhanced Text-attributed Graph Learning\", \"summary\": \"Recent advancements in Large Language Models (LLMs) and the proliferation of\\nText-Attributed Graphs (TAGs) across various domains have positioned\\nLLM-enhanced TAG learning as a critical research area. By utilizing rich graph\\ndescriptions, this paradigm leverages LLMs to generate high-quality embeddings,\\nthereby enhancing the representational capacity of Graph Neural Networks\\n(GNNs). However, the field faces significant challenges: (1) the absence of a\\nunified framework to systematize the diverse optimization perspectives arising\\nfrom the complex interactions between LLMs and GNNs, and (2) the lack of a\\nrobust method capable of handling real-world TAGs, which often suffer from\\ntexts and edge sparsity, leading to suboptimal performance.\\n  To address these challenges, we propose UltraTAG, a unified pipeline for\\nLLM-enhanced TAG learning. UltraTAG provides a unified comprehensive and\\ndomain-adaptive framework that not only organizes existing methodologies but\\nalso paves the way for future advancements in the field. Building on this\\nframework, we propose UltraTAG-S, a robust instantiation of UltraTAG designed\\nto tackle the inherent sparsity issues in real-world TAGs. UltraTAG-S employs\\nLLM-based text propagation and text augmentation to mitigate text sparsity,\\nwhile leveraging LLM-augmented node selection techniques based on PageRank and\\nedge reconfiguration strategies to address edge sparsity. Our extensive\\nexperiments demonstrate that UltraTAG-S significantly outperforms existing\\nbaselines, achieving improvements of 2.12\\\\% and 17.47\\\\% in ideal and sparse\\nsettings, respectively. Moreover, as the data sparsity ratio increases, the\\nperformance improvement of UltraTAG-S also rises, which underscores the\\neffectiveness and robustness of UltraTAG-S.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-03T07:24:18Z\"}"}

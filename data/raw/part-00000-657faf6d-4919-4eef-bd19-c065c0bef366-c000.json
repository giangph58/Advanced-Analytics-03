{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15027v1\", \"title\": \"DistilQwen2.5: Industrial Practices of Training Distilled Open\\n  Lightweight Language Models\", \"summary\": \"Enhancing computational efficiency and reducing deployment costs for large\\nlanguage models (LLMs) have become critical challenges in various\\nresource-constrained scenarios. In this work, we present DistilQwen2.5, a\\nfamily of distilled, lightweight LLMs derived from the public Qwen2.5 models.\\nThese distilled models exhibit enhanced instruction-following capabilities\\ncompared to the original models based on a series of distillation techniques\\nthat incorporate knowledge from much larger LLMs. In our industrial practice,\\nwe first leverage powerful proprietary LLMs with varying capacities as\\nmulti-agent teachers to select, rewrite, and refine instruction-response pairs\\nthat are more suitable for student LLMs to learn. After standard fine-tuning,\\nwe further leverage a computationally efficient model fusion approach that\\nenables student models to progressively integrate fine-grained hidden knowledge\\nfrom their teachers. Experimental evaluations demonstrate that the distilled\\nmodels possess significantly stronger capabilities than their original\\ncheckpoints. Additionally, we present use cases to illustrate the applications\\nof our framework in real-world scenarios. To facilitate practical use, we have\\nreleased all the DistilQwen2.5 models to the open-source community.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-21T11:26:02Z\"}"}

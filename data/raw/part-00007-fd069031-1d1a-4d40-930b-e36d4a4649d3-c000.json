{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21814v1\", \"title\": \"Why Compress What You Can Generate? When GPT-4o Generation Ushers in\\n  Image Compression Fields\", \"summary\": \"The rapid development of AIGC foundation models has revolutionized the\\nparadigm of image compression, which paves the way for the abandonment of most\\npixel-level transform and coding, compelling us to ask: why compress what you\\ncan generate if the AIGC foundation model is powerful enough to faithfully\\ngenerate intricate structure and fine-grained details from nothing more than\\nsome compact descriptors, i.e., texts, or cues. Fortunately, recent GPT-4o\\nimage generation of OpenAI has achieved impressive cross-modality generation,\\nediting, and design capabilities, which motivates us to answer the above\\nquestion by exploring its potential in image compression fields. In this work,\\nwe investigate two typical compression paradigms: textual coding and multimodal\\ncoding (i.e., text + extremely low-resolution image), where all/most\\npixel-level information is generated instead of compressing via the advanced\\nGPT-4o image generation function. The essential challenge lies in how to\\nmaintain semantic and structure consistency during the decoding process. To\\novercome this, we propose a structure raster-scan prompt engineering mechanism\\nto transform the image into textual space, which is compressed as the condition\\nof GPT-4o image generation. Extensive experiments have shown that the\\ncombination of our designed structural raster-scan prompts and GPT-4o's image\\ngeneration function achieved the impressive performance compared with recent\\nmultimodal/generative image compression at ultra-low bitrate, further\\nindicating the potential of AIGC generation in image compression fields.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-30T17:20:14Z\"}"}

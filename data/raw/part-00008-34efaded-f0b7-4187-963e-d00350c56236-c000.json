{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15659v1\", \"title\": \"VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional\\n  Correctness Validation\", \"summary\": \"Recent advances in Large Language Models (LLMs) have sparked growing interest\\nin applying them to Electronic Design Automation (EDA) tasks, particularly\\nRegister Transfer Level (RTL) code generation. While several RTL datasets have\\nbeen introduced, most focus on syntactic validity rather than functional\\nvalidation with tests, leading to training examples that compile but may not\\nimplement the intended behavior. We present VERICODER, a model for RTL code\\ngeneration fine-tuned on a dataset validated for functional correctness. This\\nfine-tuning dataset is constructed using a novel methodology that combines unit\\ntest generation with feedback-directed refinement. Given a natural language\\nspecification and an initial RTL design, we prompt a teacher model\\n(GPT-4o-mini) to generate unit tests and iteratively revise the RTL design\\nbased on its simulation results using the generated tests. If necessary, the\\nteacher model also updates the tests to ensure they comply with the natural\\nlanguage specification. As a result of this process, every example in our\\ndataset is functionally validated, consisting of a natural language\\ndescription, an RTL implementation, and passing tests. Fine-tuned on this\\ndataset of over 125,000 examples, VERICODER achieves state-of-the-art metrics\\nin functional correctness on VerilogEval and RTLLM, with relative gains of up\\nto 71.7% and 27.4% respectively. An ablation study further shows that models\\ntrained on our functionally validated dataset outperform those trained on\\nfunctionally non-validated datasets, underscoring the importance of\\nhigh-quality datasets in RTL code generation.\", \"main_category\": \"cs.AR\", \"categories\": \"cs.AR,cs.AI,cs.CL,cs.LG,cs.SE\", \"published\": \"2025-04-22T07:32:46Z\"}"}

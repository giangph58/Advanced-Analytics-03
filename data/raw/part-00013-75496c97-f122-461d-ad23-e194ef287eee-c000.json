{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03667v1\", \"title\": \"Distribution-Conditional Generation: From Class Distribution to Creative\\n  Generation\", \"summary\": \"Text-to-image (T2I) diffusion models are effective at producing semantically\\naligned images, but their reliance on training data distributions limits their\\nability to synthesize truly novel, out-of-distribution concepts. Existing\\nmethods typically enhance creativity by combining pairs of known concepts,\\nyielding compositions that, while out-of-distribution, remain linguistically\\ndescribable and bounded within the existing semantic space. Inspired by the\\nsoft probabilistic outputs of classifiers on ambiguous inputs, we propose\\nDistribution-Conditional Generation, a novel formulation that models creativity\\nas image synthesis conditioned on class distributions, enabling semantically\\nunconstrained creative generation. Building on this, we propose DisTok, an\\nencoder-decoder framework that maps class distributions into a latent space and\\ndecodes them into tokens of creative concept. DisTok maintains a dynamic\\nconcept pool and iteratively sampling and fusing concept pairs, enabling the\\ngeneration of tokens aligned with increasingly complex class distributions. To\\nenforce distributional consistency, latent vectors sampled from a Gaussian\\nprior are decoded into tokens and rendered into images, whose class\\ndistributions-predicted by a vision-language model-supervise the alignment\\nbetween input distributions and the visual semantics of generated tokens. The\\nresulting tokens are added to the concept pool for subsequent composition.\\nExtensive experiments demonstrate that DisTok, by unifying\\ndistribution-conditioned fusion and sampling-based synthesis, enables efficient\\nand flexible token-level generation, achieving state-of-the-art performance\\nwith superior text-image alignment and human preference scores.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-06T16:07:12Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16612v1\", \"title\": \"Federated EndoViT: Pretraining Vision Transformers via Federated\\n  Learning on Endoscopic Image Collections\", \"summary\": \"Purpose: In this study, we investigate the training of foundation models\\nusing federated learning to address data-sharing limitations and enable\\ncollaborative model training without data transfer for minimally invasive\\nsurgery. Methods: Inspired by the EndoViT study, we adapt the Masked\\nAutoencoder for federated learning, enhancing it with adaptive Sharpness-Aware\\nMinimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is\\npretrained on the Endo700k dataset collection and later fine-tuned and\\nevaluated for tasks such as Semantic Segmentation, Action Triplet Recognition,\\nand Surgical Phase Recognition. Results: Our findings demonstrate that\\nintegrating adaptive FedSAM into the federated MAE approach improves\\npretraining, leading to a reduction in reconstruction loss per patch. The\\napplication of FL-EndoViT in surgical downstream tasks results in performance\\ncomparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over\\nCEN-EndoViT in surgical scene segmentation when data is limited and in action\\ntriplet recognition when large datasets are used. Conclusion: These findings\\nhighlight the potential of federated learning for privacy-preserving training\\nof surgical foundation models, offering a robust and generalizable solution for\\nsurgical data science. Effective collaboration requires adapting federated\\nlearning methods, such as the integration of FedSAM, which can accommodate the\\ninherent data heterogeneity across institutions. In future, exploring FL in\\nvideo-based models may enhance these capabilities by incorporating\\nspatiotemporal dynamics crucial for real-world surgical environments.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG\", \"published\": \"2025-04-23T10:54:32Z\"}"}
{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16628v1\", \"title\": \"ParetoHqD: Fast Offline Multiobjective Alignment of Large Language\\n  Models using Pareto High-quality Data\", \"summary\": \"Aligning large language models with multiple human expectations and values is\\ncrucial for ensuring that they adequately serve a variety of user needs. To\\nthis end, offline multiobjective alignment algorithms such as the\\nRewards-in-Context algorithm have shown strong performance and efficiency.\\nHowever, inappropriate preference representations and training with imbalanced\\nreward scores limit the performance of such algorithms. In this work, we\\nintroduce ParetoHqD that addresses the above issues by representing human\\npreferences as preference directions in the objective space and regarding data\\nnear the Pareto front as ''high-quality'' data. For each preference, ParetoHqD\\nfollows a two-stage supervised fine-tuning process, where each stage uses an\\nindividual Pareto high-quality training set that best matches its preference\\ndirection. The experimental results have demonstrated the superiority of\\nParetoHqD over five baselines on two multiobjective alignment tasks.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL\", \"published\": \"2025-04-23T11:35:57Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20456v1\", \"title\": \"Reviving Any-Subset Autoregressive Models with Principled Parallel\\n  Sampling and Speculative Decoding\", \"summary\": \"In arbitrary-order language models, it is an open question how to sample\\ntokens in parallel from the correct joint distribution. With discrete diffusion\\nmodels, the more tokens they generate in parallel, the less their predicted\\ndistributions adhere to the originally learned data distribution, as they rely\\non a conditional independence assumption that only works with infinitesimally\\nsmall timesteps. We find that a different class of models, any-subset\\nautoregressive models (AS-ARMs), holds the solution. As implied by the name,\\nAS-ARMs can generate tokens in any order, and in parallel. Moreover, AS-ARMs\\nsupport parallelized joint probability density estimation, allowing them to\\ncorrect their own parallel-generated token distributions, via our Any-Subset\\nSpeculative Decoding (ASSD) algorithm. ASSD provably enables generation of\\ntokens from the correct joint distribution, with the number of neural network\\ncalls upper bounded by the number of tokens predicted. We empirically verify\\nthat ASSD speeds up language generation, without sacrificing quality.\\nFurthermore, we provide a mathematically justified scheme for training AS-ARMs\\nfor generation, and show that AS-ARMs achieve state-of-the-art performance\\namong sub-200M parameter models on infilling benchmark tasks, and nearly match\\nthe performance of models 50X larger on code generation. Our theoretical and\\nempirical results indicate that the once-forgotten AS-ARMs are a promising\\ndirection of language modeling.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL\", \"published\": \"2025-04-29T06:33:13Z\"}"}

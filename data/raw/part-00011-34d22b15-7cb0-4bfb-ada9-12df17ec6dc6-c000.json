{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15722v1\", \"title\": \"From predictions to confidence intervals: an empirical study of\\n  conformal prediction methods for in-context learning\", \"summary\": \"Transformers have become a standard architecture in machine learning,\\ndemonstrating strong in-context learning (ICL) abilities that allow them to\\nlearn from the prompt at inference time. However, uncertainty quantification\\nfor ICL remains an open challenge, particularly in noisy regression tasks. This\\npaper investigates whether ICL can be leveraged for distribution-free\\nuncertainty estimation, proposing a method based on conformal prediction to\\nconstruct prediction intervals with guaranteed coverage. While traditional\\nconformal methods are computationally expensive due to repeated model fitting,\\nwe exploit ICL to efficiently generate confidence intervals in a single forward\\npass. Our empirical analysis compares this approach against ridge\\nregression-based conformal methods, showing that conformal prediction with\\nin-context learning (CP with ICL) achieves robust and scalable uncertainty\\nestimates. Additionally, we evaluate its performance under distribution shifts\\nand establish scaling laws to guide model training. These findings bridge ICL\\nand conformal prediction, providing a theoretically grounded and new framework\\nfor uncertainty quantification in transformer-based models.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.LG\", \"published\": \"2025-04-22T09:11:48Z\"}"}

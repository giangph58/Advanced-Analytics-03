{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13128v1\", \"title\": \"FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on\\n  Technical Documents\", \"summary\": \"We introduce FreshStack, a reusable framework for automatically building\\ninformation retrieval (IR) evaluation benchmarks from community-asked questions\\nand answers. FreshStack conducts the following steps: (1) automatic corpus\\ncollection from code and technical documentation, (2) nugget generation from\\ncommunity-asked questions and answers, and (3) nugget-level support, retrieving\\ndocuments using a fusion of retrieval techniques and hybrid architectures. We\\nuse FreshStack to build five datasets on fast-growing, recent, and niche topics\\nto ensure the tasks are sufficiently challenging. On FreshStack, existing\\nretrieval models, when applied out-of-the-box, significantly underperform\\noracle approaches on all five topics, denoting plenty of headroom to improve IR\\nquality. In addition, we identify cases where rerankers do not clearly improve\\nfirst-stage retrieval accuracy (two out of five topics). We hope that\\nFreshStack will facilitate future work toward constructing realistic, scalable,\\nand uncontaminated IR and RAG evaluation benchmarks. FreshStack datasets are\\navailable at: https://fresh-stack.github.io.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR,cs.AI,cs.CL\", \"published\": \"2025-04-17T17:44:06Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23715v1\", \"title\": \"HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video\\n  Generation\", \"summary\": \"Text-to-video (T2V) generation has made tremendous progress in generating\\ncomplicated scenes based on texts. However, human-object interaction (HOI)\\noften cannot be precisely generated by current T2V models due to the lack of\\nlarge-scale videos with accurate captions for HOI. To address this issue, we\\nintroduce HOIGen-1M, the first largescale dataset for HOI Generation,\\nconsisting of over one million high-quality videos collected from diverse\\nsources. In particular, to guarantee the high quality of videos, we first\\ndesign an efficient framework to automatically curate HOI videos using the\\npowerful multimodal large language models (MLLMs), and then the videos are\\nfurther cleaned by human annotators. Moreover, to obtain accurate textual\\ncaptions for HOI videos, we design a novel video description method based on a\\nMixture-of-Multimodal-Experts (MoME) strategy that not only generates\\nexpressive captions but also eliminates the hallucination by individual MLLM.\\nFurthermore, due to the lack of an evaluation framework for generated HOI\\nvideos, we propose two new metrics to assess the quality of generated videos in\\na coarse-to-fine manner. Extensive experiments reveal that current T2V models\\nstruggle to generate high-quality HOI videos and confirm that our HOIGen-1M\\ndataset is instrumental for improving HOI video generation. Project webpage is\\navailable at https://liuqi-creat.github.io/HOIGen.github.io.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-31T04:30:34Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15022v1\", \"title\": \"LLMs as Data Annotators: How Close Are We to Human Performance\", \"summary\": \"In NLP, fine-tuning LLMs is effective for various applications but requires\\nhigh-quality annotated data. However, manual annotation of data is\\nlabor-intensive, time-consuming, and costly. Therefore, LLMs are increasingly\\nused to automate the process, often employing in-context learning (ICL) in\\nwhich some examples related to the task are given in the prompt for better\\nperformance. However, manually selecting context examples can lead to\\ninefficiencies and suboptimal model performance. This paper presents\\ncomprehensive experiments comparing several LLMs, considering different\\nembedding models, across various datasets for the Named Entity Recognition\\n(NER) task. The evaluation encompasses models with approximately $7$B and $70$B\\nparameters, including both proprietary and non-proprietary models. Furthermore,\\nleveraging the success of Retrieval-Augmented Generation (RAG), it also\\nconsiders a method that addresses the limitations of ICL by automatically\\nretrieving contextual examples, thereby enhancing performance. The results\\nhighlight the importance of selecting the appropriate LLM and embedding model,\\nunderstanding the trade-offs between LLM sizes and desired performance, and the\\nnecessity to direct research efforts towards more challenging datasets.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-21T11:11:07Z\"}"}

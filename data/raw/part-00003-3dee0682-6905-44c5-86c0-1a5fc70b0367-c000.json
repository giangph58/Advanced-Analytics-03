{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00291v1\", \"title\": \"Repetition Makes Perfect: Recurrent Sum-GNNs Match Message Passing Limit\", \"summary\": \"We provide first tight bounds for the expressivity of Recurrent Graph Neural\\nNetworks (recurrent GNNs) with finite-precision parameters. We prove that\\nrecurrent GNNs, with sum aggregation and ReLU activation, can emulate any graph\\nalgorithm that respects the natural message-passing invariance induced by the\\ncolor refinement (or Weisfeiler-Leman) algorithm. While it is well known that\\nthe expressive power of GNNs is limited by this invariance [Morris et al., AAAI\\n2019; Xu et al., ICLR 2019], we establish that recurrent GNNs can actually\\nreach this limit. This is in contrast to non-recurrent GNNs, which have the\\npower of Weisfeiler-Leman only in a very weak, \\\"non-uniform\\\", sense where every\\ngraph size requires a different GNN model to compute with. The emulation we\\nconstruct introduces only a polynomial overhead in both time and space.\\n  Furthermore, we show that by incorporating random initialization, recurrent\\nGNNs can emulate all graph algorithms, implying in particular that any graph\\nalgorithm with polynomial-time complexity can be emulated by a recurrent GNN\\nwith random initialization, running in polynomial time.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,I.2.6\", \"published\": \"2025-05-01T04:27:35Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06949v1\", \"title\": \"Adaptive Computation Pruning for the Forgetting Transformer\", \"summary\": \"The recently proposed Forgetting Transformer (FoX) incorporates a forget gate\\ninto softmax attention and has shown consistently better or on-par performance\\ncompared to the standard RoPE-based Transformer. Notably, many attention heads\\nin FoX tend to forget quickly, causing their output at each timestep to rely\\nprimarily on the local context. Based on this observation, we propose Adaptive\\nComputation Pruning (ACP) for FoX, a method that dynamically prunes\\ncomputations involving input-output dependencies that are strongly decayed by\\nthe forget gate. This is achieved using a dynamically set pruning threshold\\nthat ensures that the pruned attention weights remain negligible. We apply ACP\\nto language model pretraining with FoX and show it consistently reduces the\\nnumber of FLOPs in softmax attention by around 70% across different model sizes\\nand context lengths, resulting in a roughly 10% to 35% improvement in training\\nthroughput. Furthermore, longer context lengths yield greater computational\\nsavings. All these speed improvements are achieved without any performance\\ndegradation. We also perform several analyses to provide deeper insights into\\nour method, such as examining the pruning patterns and analyzing the\\ndistribution of FLOP savings across different attention heads. Our code is\\navailable at https://github.com/zhixuan-lin/arctic-fox.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL\", \"published\": \"2025-04-09T14:57:55Z\"}"}

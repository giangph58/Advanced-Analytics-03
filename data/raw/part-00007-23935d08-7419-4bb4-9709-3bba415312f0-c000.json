{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03318v1\", \"title\": \"Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\\n  Fine-Tuning\", \"summary\": \"Recent advances in multimodal Reward Models (RMs) have shown significant\\npromise in delivering reward signals to align vision models with human\\npreferences. However, current RMs are generally restricted to providing direct\\nresponses or engaging in shallow reasoning processes with limited depth, often\\nleading to inaccurate reward signals. We posit that incorporating explicit long\\nchains of thought (CoT) into the reward reasoning process can significantly\\nstrengthen their reliability and robustness. Furthermore, we believe that once\\nRMs internalize CoT reasoning, their direct response accuracy can also be\\nimproved through implicit reasoning capabilities. To this end, this paper\\nproposes UnifiedReward-Think, the first unified multimodal CoT-based reward\\nmodel, capable of multi-dimensional, step-by-step long-chain reasoning for both\\nvisual understanding and generation reward tasks. Specifically, we adopt an\\nexploration-driven reinforcement fine-tuning approach to elicit and incentivize\\nthe model's latent complex reasoning ability: (1) We first use a small amount\\nof image generation preference data to distill the reasoning process of GPT-4o,\\nwhich is then used for the model's cold start to learn the format and structure\\nof CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge\\nand generalization capabilities, we prepare large-scale unified multimodal\\npreference data to elicit the model's reasoning process across various vision\\ntasks. During this phase, correct reasoning outputs are retained for rejection\\nsampling to refine the model (3) while incorrect predicted samples are finally\\nused for Group Relative Policy Optimization (GRPO) based reinforcement\\nfine-tuning, enabling the model to explore diverse reasoning paths and optimize\\nfor correct and robust solutions. Extensive experiments across various vision\\nreward tasks demonstrate the superiority of our model.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-06T08:46:41Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20998v1\", \"title\": \"YoChameleon: Personalized Vision and Language Generation\", \"summary\": \"Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into\\npowerful tools with millions of users. However, they remain generic models and\\nlack personalized knowledge of specific user concepts. Previous work has\\nexplored personalization for text generation, yet it remains unclear how these\\nmethods can be adapted to new modalities, such as image generation. In this\\npaper, we introduce Yo'Chameleon, the first attempt to study personalization\\nfor large multimodal models. Given 3-5 images of a particular concept,\\nYo'Chameleon leverages soft-prompt tuning to embed subject-specific information\\nto (i) answer questions about the subject and (ii) recreate pixel-level details\\nto produce images of the subject in new contexts. Yo'Chameleon is trained with\\n(i) a self-prompting optimization mechanism to balance performance across\\nmultiple modalities, and (ii) a ``soft-positive\\\" image generation approach to\\nenhance image quality in a few-shot setting.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-29T17:59:57Z\"}"}

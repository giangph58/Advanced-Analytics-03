{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23725v1\", \"title\": \"Exploring Temporal Dynamics in Event-based Eye Tracker\", \"summary\": \"Eye-tracking is a vital technology for human-computer interaction, especially\\nin wearable devices such as AR, VR, and XR. The realization of high-speed and\\nhigh-precision eye-tracking using frame-based image sensors is constrained by\\ntheir limited temporal resolution, which impairs the accurate capture of rapid\\nocular dynamics, such as saccades and blinks. Event cameras, inspired by\\nbiological vision systems, are capable of perceiving eye movements with\\nextremely low power consumption and ultra-high temporal resolution. This makes\\nthem a promising solution for achieving high-speed, high-precision tracking\\nwith rich temporal dynamics. In this paper, we propose TDTracker, an effective\\neye-tracking framework that captures rapid eye movements by thoroughly modeling\\ntemporal dynamics from both implicit and explicit perspectives. TDTracker\\nutilizes 3D convolutional neural networks to capture implicit short-term\\ntemporal dynamics and employs a cascaded structure consisting of a\\nFrequency-aware Module, GRU, and Mamba to extract explicit long-term temporal\\ndynamics. Ultimately, a prediction heatmap is used for eye coordinate\\nregression. Experimental results demonstrate that TDTracker achieves\\nstate-of-the-art (SOTA) performance on the synthetic SEET dataset and secured\\nThird place in the CVPR event-based eye-tracking challenge 2025. Our code is\\navailable at https://github.com/rhwxmx/TDTracker.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-31T04:57:13Z\"}"}

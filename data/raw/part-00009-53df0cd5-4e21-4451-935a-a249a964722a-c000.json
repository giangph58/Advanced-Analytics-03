{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05317v1\", \"title\": \"CottonSim: Development of an autonomous visual-guided robotic\\n  cotton-picking system in the Gazebo\", \"summary\": \"In this study, an autonomous visual-guided robotic cotton-picking system,\\nbuilt on a Clearpath's Husky robot platform and the Cotton-Eye perception\\nsystem, was developed in the Gazebo robotic simulator. Furthermore, a virtual\\ncotton farm was designed and developed as a Robot Operating System (ROS 1)\\npackage to deploy the robotic cotton picker in the Gazebo environment for\\nsimulating autonomous field navigation. The navigation was assisted by the map\\ncoordinates and an RGB-depth camera, while the ROS navigation algorithm\\nutilized a trained YOLOv8n-seg model for instance segmentation. The model\\nachieved a desired mean Average Precision (mAP) of 85.2%, a recall of 88.9%,\\nand a precision of 93.0% for scene segmentation. The developed ROS navigation\\npackages enabled our robotic cotton-picking system to autonomously navigate\\nthrough the cotton field using map-based and GPS-based approaches, visually\\naided by a deep learning-based perception system. The GPS-based navigation\\napproach achieved a 100% completion rate (CR) with a threshold of 5 x 10^-6\\ndegrees, while the map-based navigation approach attained a 96.7% CR with a\\nthreshold of 0.25 m. This study establishes a fundamental baseline of\\nsimulation for future agricultural robotics and autonomous vehicles in cotton\\nfarming and beyond. CottonSim code and data are released to the research\\ncommunity via GitHub: https://github.com/imtheva/CottonSim\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO\", \"published\": \"2025-05-08T15:02:35Z\"}"}

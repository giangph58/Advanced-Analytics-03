{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15243v1\", \"title\": \"Single-loop Algorithms for Stochastic Non-convex Optimization with\\n  Weakly-Convex Constraints\", \"summary\": \"Constrained optimization with multiple functional inequality constraints has\\nsignificant applications in machine learning. This paper examines a crucial\\nsubset of such problems where both the objective and constraint functions are\\nweakly convex. Existing methods often face limitations, including slow\\nconvergence rates or reliance on double-loop algorithmic designs. To overcome\\nthese challenges, we introduce a novel single-loop penalty-based stochastic\\nalgorithm. Following the classical exact penalty method, our approach employs a\\n{\\\\bf hinge-based penalty}, which permits the use of a constant penalty\\nparameter, enabling us to achieve a {\\\\bf state-of-the-art complexity} for\\nfinding an approximate Karush-Kuhn-Tucker (KKT) solution. We further extend our\\nalgorithm to address finite-sum coupled compositional objectives, which are\\nprevalent in artificial intelligence applications, establishing improved\\ncomplexity over existing approaches. Finally, we validate our method through\\nexperiments on fair learning with receiver operating characteristic (ROC)\\nfairness constraints and continual learning with non-forgetting constraints.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,stat.ML\", \"published\": \"2025-04-21T17:15:48Z\"}"}

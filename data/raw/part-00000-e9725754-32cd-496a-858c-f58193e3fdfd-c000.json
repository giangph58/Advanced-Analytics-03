{"value":"{\"aid\": \"http://arxiv.org/abs/2504.09934v1\", \"title\": \"Tight Semidefinite Relaxations for Verifying Robustness of Neural\\n  Networks\", \"summary\": \"For verifying the safety of neural networks (NNs), Fazlyab et al. (2019)\\nintroduced a semidefinite programming (SDP) approach called DeepSDP. This\\nformulation can be viewed as the dual of the SDP relaxation for a problem\\nformulated as a quadratically constrained quadratic program (QCQP). While SDP\\nrelaxations of QCQPs generally provide approximate solutions with some gaps,\\nthis work focuses on tight SDP relaxations that provide exact solutions to the\\nQCQP for single-layer NNs. Specifically, we analyze tightness conditions in\\nthree cases: (i) NNs with a single neuron, (ii) single-layer NNs with an\\nellipsoidal input set, and (iii) single-layer NNs with a rectangular input set.\\nFor NNs with a single neuron, we propose a condition that ensures the SDP\\nadmits a rank-1 solution to DeepSDP by transforming the QCQP into an equivalent\\ntwo-stage problem leads to a solution collinear with a predetermined vector.\\nFor single-layer NNs with an ellipsoidal input set, the collinearity of\\nsolutions is proved via the Karush-Kuhn-Tucker condition in the two-stage\\nproblem. In case of single-layer NNs with a rectangular input set, we\\ndemonstrate that the tightness of DeepSDP can be reduced to the single-neuron\\nNNs, case (i), if the weight matrix is a diagonal matrix.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC\", \"published\": \"2025-04-14T06:54:00Z\"}"}

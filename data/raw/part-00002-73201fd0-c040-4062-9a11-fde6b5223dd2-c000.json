{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00315v1\", \"title\": \"Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\\n  via Expert-Choice Routing\", \"summary\": \"Recent advances in large language models highlighted the excessive quadratic\\ncost of self-attention. Despite the significant research efforts, subquadratic\\nattention methods still suffer from inferior performance in practice. We\\nhypothesize that dynamic, learned content-based sparsity can lead to more\\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\\na novel approach inspired by Mixture of Experts (MoE) with expert choice\\nrouting. MoSA dynamically selects tokens for each attention head, allowing\\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\\nlength $T$, MoSA reduces the computational complexity of each attention head\\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\\ncomputational budget, allowing higher specialization. We show that among the\\ntested sparse attention variants, MoSA is the only one that can outperform the\\ndense baseline, sometimes with up to 27% better perplexity for an identical\\ncompute budget. MoSA can also reduce the resource usage compared to dense\\nself-attention. Despite using torch implementation without an optimized kernel,\\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\\nrequire less memory for training, and drastically reduce the size of the\\nKV-cache compared to the dense transformer baselines.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL\", \"published\": \"2025-05-01T05:22:11Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15918v1\", \"title\": \"Ask2Loc: Learning to Locate Instructional Visual Answers by Asking\\n  Questions\", \"summary\": \"Locating specific segments within an instructional video is an efficient way\\nto acquire guiding knowledge. Generally, the task of obtaining video segments\\nfor both verbal explanations and visual demonstrations is known as visual\\nanswer localization (VAL). However, users often need multiple interactions to\\nobtain answers that align with their expectations when using the system. During\\nthese interactions, humans deepen their understanding of the video content by\\nasking themselves questions, thereby accurately identifying the location.\\nTherefore, we propose a new task, named In-VAL, to simulate the multiple\\ninteractions between humans and videos in the procedure of obtaining visual\\nanswers. The In-VAL task requires interactively addressing several semantic gap\\nissues, including 1) the ambiguity of user intent in the input questions, 2)\\nthe incompleteness of language in video subtitles, and 3) the fragmentation of\\ncontent in video segments. To address these issues, we propose Ask2Loc, a\\nframework for resolving In-VAL by asking questions. It includes three key\\nmodules: 1) a chatting module to refine initial questions and uncover clear\\nintentions, 2) a rewriting module to generate fluent language and create\\ncomplete descriptions, and 3) a searching module to broaden local context and\\nprovide integrated content. We conduct extensive experiments on three\\nreconstructed In-VAL datasets. Compared to traditional end-to-end and two-stage\\nmethods, our proposed Ask2Loc can improve performance by up to 14.91 (mIoU) on\\nthe In-VAL task. Our code and datasets can be accessed at\\nhttps://github.com/changzong/Ask2Loc.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.HC\", \"published\": \"2025-04-22T14:03:16Z\"}"}

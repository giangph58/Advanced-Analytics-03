{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15661v1\", \"title\": \"DiTPainter: Efficient Video Inpainting with Diffusion Transformers\", \"summary\": \"Many existing video inpainting algorithms utilize optical flows to construct\\nthe corresponding maps and then propagate pixels from adjacent frames to\\nmissing areas by mapping. Despite the effectiveness of the propagation\\nmechanism, they might encounter blurry and inconsistencies when dealing with\\ninaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT)\\nhas emerged as a revolutionary technique for video generation tasks. However,\\npretrained DiT models for video generation all contain a large amount of\\nparameters, which makes it very time consuming to apply to video inpainting\\ntasks. In this paper, we present DiTPainter, an end-to-end video inpainting\\nmodel based on Diffusion Transformer (DiT). DiTPainter uses an efficient\\ntransformer network designed for video inpainting, which is trained from\\nscratch instead of initializing from any large pretrained models. DiTPainter\\ncan address videos with arbitrary lengths and can be applied to video\\ndecaptioning and video completion tasks with an acceptable time cost.\\nExperiments show that DiTPainter outperforms existing video inpainting\\nalgorithms with higher quality and better spatial-temporal consistency.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-22T07:36:45Z\"}"}

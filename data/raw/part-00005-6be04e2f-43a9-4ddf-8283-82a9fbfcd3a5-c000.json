{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20924v1\", \"title\": \"A Domain-Agnostic Scalable AI Safety Ensuring Framework\", \"summary\": \"Ensuring the safety of AI systems has recently emerged as a critical priority\\nfor real-world deployment, particularly in physical AI applications. Current\\napproaches to AI safety typically address predefined domain-specific safety\\nconditions, limiting their ability to generalize across contexts.\\n  We propose a novel AI safety framework that ensures AI systems comply with\\n\\\\textbf{any user-defined constraint}, with \\\\textbf{any desired probability},\\nand across \\\\textbf{various domains}.\\n  In this framework, we combine an AI component (e.g., neural network) with an\\noptimization problem to produce responses that minimize objectives while\\nsatisfying user-defined constraints with probabilities exceeding user-defined\\nthresholds. For credibility assessment of the AI component, we propose\\n\\\\textit{internal test data}, a supplementary set of safety-labeled data, and a\\n\\\\textit{conservative testing} methodology that provides statistical validity of\\nusing internal test data. We also present an approximation method of a loss\\nfunction and how to compute its gradient for training.\\n  We mathematically prove that probabilistic constraint satisfaction is\\nguaranteed under specific, mild conditions and prove a scaling law between\\nsafety and the number of internal test data. We demonstrate our framework's\\neffectiveness through experiments in diverse domains: demand prediction for\\nproduction decision, safe reinforcement learning within the SafetyGym\\nsimulator, and guarding AI chatbot outputs. Through these experiments, we\\ndemonstrate that our method guarantees safety for user-specified constraints,\\noutperforms {for \\\\textbf{up to several order of magnitudes}} existing methods\\nin low safety threshold regions, and scales effectively with respect to the\\nsize of internal test data.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-29T16:38:35Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15205v1\", \"title\": \"Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus\\n  LLM Judges\", \"summary\": \"Retrieval-augmented generation (RAG) enables large language models (LLMs) to\\ngenerate answers with citations from source documents containing \\\"ground\\ntruth\\\", thereby reducing system hallucinations. A crucial factor in RAG\\nevaluation is \\\"support\\\", whether the information in the cited documents\\nsupports the answer. To this end, we conducted a large-scale comparative study\\nof 45 participant submissions on 36 topics to the TREC 2024 RAG Track,\\ncomparing an automatic LLM judge (GPT-4o) against human judges for support\\nassessment. We considered two conditions: (1) fully manual assessments from\\nscratch and (2) manual assessments with post-editing of LLM predictions. Our\\nresults indicate that for 56% of the manual from-scratch assessments, human and\\nGPT-4o predictions match perfectly (on a three-level scale), increasing to 72%\\nin the manual with post-editing condition. Furthermore, by carefully analyzing\\nthe disagreements in an unbiased study, we found that an independent human\\njudge correlates better with GPT-4o than a human judge, suggesting that LLM\\njudges can be a reliable alternative for support assessment. To conclude, we\\nprovide a qualitative analysis of human and GPT-4o errors to help guide future\\niterations of support assessment.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.IR\", \"published\": \"2025-04-21T16:20:43Z\"}"}

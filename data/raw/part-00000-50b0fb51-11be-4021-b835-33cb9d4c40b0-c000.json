{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16680v1\", \"title\": \"Offline Robotic World Model: Learning Robotic Policies without a Physics\\n  Simulator\", \"summary\": \"Reinforcement Learning (RL) has demonstrated impressive capabilities in\\nrobotic control but remains challenging due to high sample complexity, safety\\nconcerns, and the sim-to-real gap. While offline RL eliminates the need for\\nrisky real-world exploration by learning from pre-collected data, it suffers\\nfrom distributional shift, limiting policy generalization. Model-Based RL\\n(MBRL) addresses this by leveraging predictive models for synthetic rollouts,\\nyet existing approaches often lack robust uncertainty estimation, leading to\\ncompounding errors in offline settings. We introduce Offline Robotic World\\nModel (RWM-O), a model-based approach that explicitly estimates epistemic\\nuncertainty to improve policy learning without reliance on a physics simulator.\\nBy integrating these uncertainty estimates into policy optimization, our\\napproach penalizes unreliable transitions, reducing overfitting to model errors\\nand enhancing stability. Experimental results show that RWM-O improves\\ngeneralization and safety, enabling policy learning purely from real-world data\\nand advancing scalable, data-efficient RL for robotics.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.AI,cs.LG\", \"published\": \"2025-04-23T12:58:15Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04947v1\", \"title\": \"DFPL: Decentralized Federated Prototype Learning Across Heterogeneous\\n  Data Distributions\", \"summary\": \"Federated learning is a distributed machine learning paradigm that enables\\nthe collaborative training of multiple clients through centralized model\\naggregation. However, standard federated learning relies on a centralized\\nserver, making it vulnerable to server failures. While existing solutions\\nutilize blockchain technology to implement Decentralized Federated Learning\\n(DFL), the statistical heterogeneity of data distributions among clients\\nseverely degrades the DFL's performance. Driven by this issue, this paper\\nproposes a decentralized federated prototype learning framework, named DFPL,\\nwhich significantly improves the performance of distributed machine learning\\nacross heterogeneous data distributions. Specifically, our framework introduces\\nprototype learning into DFL to address statistical heterogeneity, which greatly\\nreduces the number of parameters exchanged between clients. Additionally,\\nblockchain is embedded into our framework, enabling the training and mining\\nprocesses to be implemented at each client. From a theoretical perspective, we\\nprovide convergence guarantee of DFPL by combining resource allocation for\\ntraining and mining. The experiments highlight the superiority of our DFPL\\nframework in communication efficiency and test performance across three\\nbenchmark datasets with heterogeneous data distributions.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC\", \"published\": \"2025-05-08T04:58:45Z\"}"}

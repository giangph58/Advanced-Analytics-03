{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23817v1\", \"title\": \"MVDRAM: Enabling GeMV Execution in Unmodified DRAM for Low-Bit LLM\\n  Acceleration\", \"summary\": \"General matrix-vector multiplication (GeMV) remains a critical latency\\nbottleneck in large language model (LLM) inference, even with quantized low-bit\\nmodels. Processing-Using-DRAM (PUD), an analog in-DRAM computing technique, has\\nthe potential to repurpose on-device DRAM as a GeMV engine, offering additional\\nhigh-throughput processing capabilities to widespread consumer devices without\\nDRAM modifications. However, applying PUD to GeMV operations in the LLM\\ninference pipeline incurs significant overheads $\\\\textit{before}$ and\\n$\\\\textit{after}$ in-DRAM computation, diminishing the benefits of its\\nhigh-throughput processing capabilities.\\n  This paper presents MVDRAM, the first practical system to accelerate GeMV\\noperations for low-bit LLM inference using unmodified DRAM. By leveraging the\\ndata sharing patterns and mathematical linearity in GeMV operations, MVDRAM\\norchestrates the processor and DRAM to eliminate the costs associated with\\npre-arranging inputs and bit-transposition of outputs required in conventional\\nPUD approaches. Our experimental evaluation with four DDR4 DRAM modules shows\\nthat MVDRAM achieves comparable or even better inference speed than the\\nprocessor-based implementation for GeMV operations in low-bit (under 4-bit)\\nLLM. In particular, MVDRAM achieves up to 7.29$\\\\times$ speedup and 30.5$\\\\times$\\nenergy efficiency for low-bit GeMV operations. For end-to-end LLM inference,\\nMVDRAM achieves 2.18$\\\\times$ and 1.31$\\\\times$ throughput improvements, along\\nwith 3.04$\\\\times$ and 2.35$\\\\times$ energy efficiency, for 2-bit and 4-bit\\nquantized low-bit models, respectively. MVDRAM has the potential to redefine\\nthe AI hardware landscape by demonstrating the feasibility of standard DRAM as\\nan LLM accelerator.\", \"main_category\": \"cs.AR\", \"categories\": \"cs.AR,cs.DC\", \"published\": \"2025-03-31T07:54:59Z\"}"}

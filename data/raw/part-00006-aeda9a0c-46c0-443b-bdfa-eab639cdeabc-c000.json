{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16414v1\", \"title\": \"Evaluating Multi-Hop Reasoning in Large Language Models: A\\n  Chemistry-Centric Case Study\", \"summary\": \"In this study, we introduced a new benchmark consisting of a curated dataset\\nand a defined evaluation process to assess the compositional reasoning\\ncapabilities of large language models within the chemistry domain. We designed\\nand validated a fully automated pipeline, verified by subject matter experts,\\nto facilitate this task. Our approach integrates OpenAI reasoning models with\\nnamed entity recognition (NER) systems to extract chemical entities from recent\\nliterature, which are then augmented with external knowledge bases to form a\\ncomprehensive knowledge graph. By generating multi-hop questions across these\\ngraphs, we assess LLM performance in both context-augmented and non-context\\naugmented settings. Our experiments reveal that even state-of-the-art models\\nface significant challenges in multi-hop compositional reasoning. The results\\nreflect the importance of augmenting LLMs with document retrieval, which can\\nhave a substantial impact on improving their performance. However, even perfect\\nretrieval accuracy with full context does not eliminate reasoning errors,\\nunderscoring the complexity of compositional reasoning. This work not only\\nbenchmarks and highlights the limitations of current LLMs but also presents a\\nnovel data generation pipeline capable of producing challenging reasoning\\ndatasets across various domains. Overall, this research advances our\\nunderstanding of reasoning in computational linguistics.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-23T04:36:19Z\"}"}

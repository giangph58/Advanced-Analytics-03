{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19611v1\", \"title\": \"Scene2Hap: Combining LLMs and Physical Modeling for Automatically\\n  Generating Vibrotactile Signals for Full VR Scenes\", \"summary\": \"Haptic feedback contributes to immersive virtual reality (VR) experiences.\\nDesigning such feedback at scale, for all objects within a VR scene and their\\nrespective arrangements, remains a time-consuming task. We present Scene2Hap,\\nan LLM-centered system that automatically designs object-level vibrotactile\\nfeedback for entire VR scenes based on the objects' semantic attributes and\\nphysical context. Scene2Hap employs a multimodal large language model to\\nestimate the semantics and physical context of each object, including its\\nmaterial properties and vibration behavior, from the multimodal information\\npresent in the VR scene. This semantic and physical context is then used to\\ncreate plausible vibrotactile signals by generating or retrieving audio signals\\nand converting them to vibrotactile signals. For the more realistic spatial\\nrendering of haptics in VR, Scene2Hap estimates the propagation and attenuation\\nof vibration signals from their source across objects in the scene, considering\\nthe estimated material properties and physical context, such as the distance\\nand contact between virtual objects. Results from two user studies confirm that\\nScene2Hap successfully estimates the semantics and physical context of VR\\nscenes, and the physical modeling of vibration propagation improves usability,\\nperceived materiality, and spatial awareness.\", \"main_category\": \"cs.HC\", \"categories\": \"cs.HC\", \"published\": \"2025-04-28T09:18:44Z\"}"}

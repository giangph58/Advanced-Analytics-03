{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19856v1\", \"title\": \"Efficient Domain-adaptive Continual Pretraining for the Process Industry\\n  in the German Language\", \"summary\": \"Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique\\nthat further trains a language model (LM) on its pretraining task, e.g.,\\nlanguage masking. Although popular, it requires a significant corpus of\\ndomain-related data, which is difficult to obtain for specific domains in\\nlanguages other than English, such as the process industry in the German\\nlanguage. This paper introduces an efficient approach called ICL-augmented\\npretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest\\nneighbors (kNN) to augment target data with domain-related and in-domain texts,\\nsignificantly reducing GPU time while maintaining strong model performance. Our\\nresults show that this approach performs better than traditional DAPT by 3.5 of\\nthe average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost 4 times\\nless computing time, providing a cost-effective solution for industries with\\nlimited computational capacity. The findings highlight the broader\\napplicability of this framework to other low-resource industries, making\\nNLP-based solutions more accessible and feasible in production environments.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-28T14:49:00Z\"}"}

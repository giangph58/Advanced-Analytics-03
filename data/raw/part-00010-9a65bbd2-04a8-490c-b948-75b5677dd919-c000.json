{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06666v1\", \"title\": \"Patch Matters: Training-free Fine-grained Image Caption Enhancement via\\n  Local Perception\", \"summary\": \"High-quality image captions play a crucial role in improving the performance\\nof cross-modal applications such as text-to-image generation, text-to-video\\ngeneration, and text-image retrieval. To generate long-form, high-quality\\ncaptions, many recent studies have employed multimodal large language models\\n(MLLMs). However, current MLLMs often produce captions that lack fine-grained\\ndetails or suffer from hallucinations, a challenge that persists in both\\nopen-source and closed-source models. Inspired by Feature-Integration theory,\\nwhich suggests that attention must focus on specific regions to integrate\\nvisual information effectively, we propose a \\\\textbf{divide-then-aggregate}\\nstrategy. Our method first divides the image into semantic and spatial patches\\nto extract fine-grained details, enhancing the model's local perception of the\\nimage. These local details are then hierarchically aggregated to generate a\\ncomprehensive global description. To address hallucinations and inconsistencies\\nin the generated captions, we apply a semantic-level filtering process during\\nhierarchical aggregation. This training-free pipeline can be applied to both\\nopen-source models (LLaVA-1.5, LLaVA-1.6, Mini-Gemini) and closed-source models\\n(Claude-3.5-Sonnet, GPT-4o, GLM-4V-Plus). Extensive experiments demonstrate\\nthat our method generates more detailed, reliable captions, advancing\\nmultimodal description generation without requiring model retraining. The\\nsource code are available at https://github.com/GeWu-Lab/Patch-Matters\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-09T08:07:46Z\"}"}

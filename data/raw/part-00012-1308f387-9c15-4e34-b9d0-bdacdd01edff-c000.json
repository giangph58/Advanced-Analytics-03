{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19874v1\", \"title\": \"TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate\", \"summary\": \"Vector quantization, a problem rooted in Shannon's source coding theory, aims\\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\\ntheir geometric structure. We propose TurboQuant to address both mean-squared\\nerror (MSE) and inner product distortion, overcoming limitations of existing\\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\\nalgorithms, suitable for online applications, achieve near-optimal distortion\\nrates (within a small constant factor) across all bit-widths and dimensions.\\nTurboQuant achieves this by randomly rotating input vectors, inducing a\\nconcentrated Beta distribution on coordinates, and leveraging the\\nnear-independence property of distinct coordinates in high dimensions to simply\\napply optimal scalar quantizers per each coordinate. Recognizing that\\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\\n(QJL) transform on the residual, resulting in an unbiased inner product\\nquantizer. We also provide a formal proof of the information-theoretic lower\\nbounds on best achievable distortion rate by any vector quantizer,\\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\\nsmall constant ($\\\\approx 2.7$) factor. Experimental results validate our\\ntheoretical findings, showing that for KV cache quantization, we achieve\\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\\ntasks, our method outperforms existing product quantization techniques in\\nrecall while reducing indexing time to virtually zero.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.DB,cs.DS\", \"published\": \"2025-04-28T15:05:35Z\"}"}

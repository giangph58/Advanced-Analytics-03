{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13161v1\", \"title\": \"CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for\\n  Language Model Pre-training\", \"summary\": \"Pre-training datasets are typically collected from web content and lack\\ninherent domain divisions. For instance, widely used datasets like Common Crawl\\ndo not include explicit domain labels, while manually curating labeled datasets\\nsuch as The Pile is labor-intensive. Consequently, identifying an optimal\\npre-training data mixture remains a challenging problem, despite its\\nsignificant benefits for pre-training performance. To address these challenges,\\nwe propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an\\nautomated framework that discovers, evaluates, and refines data mixtures in a\\npre-training setting. Specifically, CLIMB embeds and clusters large-scale\\ndatasets in a semantic space and then iteratively searches for optimal mixtures\\nusing a smaller proxy model and a predictor. When continuously trained on 400B\\ntokens with this mixture, our 1B model exceeds the state-of-the-art\\nLlama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific\\ndomain (e.g., Social Sciences) yields a 5% improvement over random sampling.\\nFinally, we introduce ClimbLab, a filtered 1.2-trillion-token corpus with 20\\nclusters as a research playground, and ClimbMix, a compact yet powerful\\n400-billion-token dataset designed for efficient pre-training that delivers\\nsuperior performance under an equal token budget. We analyze the final data\\nmixture, elucidating the characteristics of an optimal data mixture. Our data\\nis available at: https://research.nvidia.com/labs/lpr/climb/\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-17T17:58:13Z\"}"}

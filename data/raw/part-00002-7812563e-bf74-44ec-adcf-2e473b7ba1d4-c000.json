{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19467v1\", \"title\": \"BRIDGE: Benchmarking Large Language Models for Understanding Real-world\\n  Clinical Practice Text\", \"summary\": \"Large language models (LLMs) hold great promise for medical applications and\\nare evolving rapidly, with new models being released at an accelerated pace.\\nHowever, current evaluations of LLMs in clinical contexts remain limited. Most\\nexisting benchmarks rely on medical exam-style questions or PubMed-derived\\ntext, failing to capture the complexity of real-world electronic health record\\n(EHR) data. Others focus narrowly on specific application scenarios, limiting\\ntheir generalizability across broader clinical use. To address this gap, we\\npresent BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks\\nsourced from real-world clinical data sources across nine languages. We\\nsystematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1,\\nGPT-4o, Gemini, and Llama 4) under various inference strategies. With a total\\nof 13,572 experiments, our results reveal substantial performance variation\\nacross model sizes, languages, natural language processing tasks, and clinical\\nspecialties. Notably, we demonstrate that open-source LLMs can achieve\\nperformance comparable to proprietary models, while medically fine-tuned LLMs\\nbased on older architectures often underperform versus updated general-purpose\\nmodels. The BRIDGE and its corresponding leaderboard serve as a foundational\\nresource and a unique reference for the development and evaluation of new LLMs\\nin real-world clinical text understanding.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-28T04:13:18Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19901v1\", \"title\": \"Attention Mechanism, Max-Affine Partition, and Universal Approximation\", \"summary\": \"We establish the universal approximation capability of single-layer,\\nsingle-head self- and cross-attention mechanisms with minimal attached\\nstructures. Our key insight is to interpret single-head attention as an input\\ndomain-partition mechanism that assigns distinct values to subregions. This\\nallows us to engineer the attention weights such that this assignment imitates\\nthe target function. Building on this, we prove that a single self-attention\\nlayer, preceded by sum-of-linear transformations, is capable of approximating\\nany continuous function on a compact domain under the $L_\\\\infty$-norm.\\nFurthermore, we extend this construction to approximate any Lebesgue integrable\\nfunction under $L_p$-norm for $1\\\\leq p <\\\\infty$. Lastly, we also extend our\\ntechniques and show that, for the first time, single-head cross-attention\\nachieves the same universal approximation guarantees.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,stat.ML\", \"published\": \"2025-04-28T15:31:45Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20984v1\", \"title\": \"ACE: A Security Architecture for LLM-Integrated App Systems\", \"summary\": \"LLM-integrated app systems extend the utility of Large Language Models (LLMs)\\nwith third-party apps that are invoked by a system LLM using interleaved\\nplanning and execution phases to answer user queries. These systems introduce\\nnew attack vectors where malicious apps can cause integrity violation of\\nplanning or execution, availability breakdown, or privacy compromise during\\nexecution.\\n  In this work, we identify new attacks impacting the integrity of planning, as\\nwell as the integrity and availability of execution in LLM-integrated apps, and\\ndemonstrate them against IsolateGPT, a recent solution designed to mitigate\\nattacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new\\nsecure architecture for LLM-integrated app systems that provides security\\nguarantees for system planning and execution. Specifically, ACE decouples\\nplanning into two phases by first creating an abstract execution plan using\\nonly trusted information, and then mapping the abstract plan to a concrete plan\\nusing installed system apps. We verify that the plans generated by our system\\nsatisfy user-specified secure information flow constraints via static analysis\\non the structured plan output. During execution, ACE enforces data and\\ncapability barriers between apps, and ensures that the execution is conducted\\naccording to the trusted abstract plan. We show experimentally that our system\\nis secure against attacks from the INJECAGENT benchmark, a standard benchmark\\nfor control flow integrity in the face of indirect prompt injection attacks,\\nand our newly introduced attacks. Our architecture represents a significant\\nadvancement towards hardening LLM-based systems containing system facilities of\\nvarying levels of trustworthiness.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.LG\", \"published\": \"2025-04-29T17:55:52Z\"}"}

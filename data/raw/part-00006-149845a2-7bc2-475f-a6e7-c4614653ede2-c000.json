{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05849v1\", \"title\": \"On the Importance of Conditioning for Privacy-Preserving Data\\n  Augmentation\", \"summary\": \"Latent diffusion models can be used as a powerful augmentation method to\\nartificially extend datasets for enhanced training. To the human eye, these\\naugmented images look very different to the originals. Previous work has\\nsuggested to use this data augmentation technique for data anonymization.\\nHowever, we show that latent diffusion models that are conditioned on features\\nlike depth maps or edges to guide the diffusion process are not suitable as a\\nprivacy preserving method. We use a contrastive learning approach to train a\\nmodel that can correctly identify people out of a pool of candidates. Moreover,\\nwe demonstrate that anonymization using conditioned diffusion models is\\nsusceptible to black box attacks. We attribute the success of the described\\nmethods to the conditioning of the latent diffusion model in the anonymization\\nprocess. The diffusion model is instructed to produce similar edges for the\\nanonymized images. Hence, a model can learn to recognize these patterns for\\nidentification.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-08T09:27:51Z\"}"}

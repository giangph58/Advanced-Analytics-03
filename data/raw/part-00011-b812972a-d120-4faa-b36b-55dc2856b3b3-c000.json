{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15888v1\", \"title\": \"MS-Occ: Multi-Stage LiDAR-Camera Fusion for 3D Semantic Occupancy\\n  Prediction\", \"summary\": \"Accurate 3D semantic occupancy perception is essential for autonomous driving\\nin complex environments with diverse and irregular objects. While\\nvision-centric methods suffer from geometric inaccuracies, LiDAR-based\\napproaches often lack rich semantic information. To address these limitations,\\nMS-Occ, a novel multi-stage LiDAR-camera fusion framework which includes\\nmiddle-stage fusion and late-stage fusion, is proposed, integrating LiDAR's\\ngeometric fidelity with camera-based semantic richness via hierarchical\\ncross-modal fusion. The framework introduces innovations at two critical\\nstages: (1) In the middle-stage feature fusion, the Gaussian-Geo module\\nleverages Gaussian kernel rendering on sparse LiDAR depth maps to enhance 2D\\nimage features with dense geometric priors, and the Semantic-Aware module\\nenriches LiDAR voxels with semantic context via deformable cross-attention; (2)\\nIn the late-stage voxel fusion, the Adaptive Fusion (AF) module dynamically\\nbalances voxel features across modalities, while the High Classification\\nConfidence Voxel Fusion (HCCVF) module resolves semantic inconsistencies using\\nself-attention-based refinement. Experiments on the nuScenes-OpenOccupancy\\nbenchmark show that MS-Occ achieves an Intersection over Union (IoU) of 32.1%\\nand a mean IoU (mIoU) of 25.3%, surpassing the state-of-the-art by +0.7% IoU\\nand +2.4% mIoU. Ablation studies further validate the contribution of each\\nmodule, with substantial improvements in small-object perception, demonstrating\\nthe practical value of MS-Occ for safety-critical autonomous driving scenarios.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-22T13:33:26Z\"}"}

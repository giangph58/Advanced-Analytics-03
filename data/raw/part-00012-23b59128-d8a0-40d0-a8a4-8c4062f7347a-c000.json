{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12663v1\", \"title\": \"Persona-judge: Personalized Alignment of Large Language Models via\\n  Token-level Self-judgment\", \"summary\": \"Aligning language models with human preferences presents significant\\nchallenges, particularly in achieving personalization without incurring\\nexcessive computational costs. Existing methods rely on reward signals and\\nadditional annotated data, limiting their scalability and adaptability to\\ndiverse human values. To address these challenges, we introduce Persona-judge,\\na novel discriminative paradigm that enables training-free personalized\\nalignment with unseen preferences. Instead of optimizing policy parameters\\nthrough external reward feedback, Persona-judge leverages the intrinsic\\npreference judgment capabilities of the model. Specifically, a draft model\\ngenerates candidate tokens conditioned on a given preference, while a judge\\nmodel, embodying another preference, cross-validates the predicted tokens\\nwhether to be accepted. Experimental results demonstrate that Persona-judge,\\nusing the inherent preference evaluation mechanisms of the model, offers a\\nscalable and computationally efficient solution to personalized alignment,\\npaving the way for more adaptive customized alignment.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-17T05:50:13Z\"}"}

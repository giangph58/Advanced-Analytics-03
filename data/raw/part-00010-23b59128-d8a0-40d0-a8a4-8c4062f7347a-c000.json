{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12661v1\", \"title\": \"VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven\\n  Prompt Optimization\", \"summary\": \"Aligning Vision-Language Models (VLMs) with safety standards is essential to\\nmitigate risks arising from their multimodal complexity, where integrating\\nvision and language unveils subtle threats beyond the reach of conventional\\nsafeguards. Inspired by the insight that reasoning across modalities is key to\\npreempting intricate vulnerabilities, we propose a novel direction for VLM\\nsafety: multimodal reasoning-driven prompt rewriting. To this end, we introduce\\nVLMGuard-R1, a proactive framework that refines user inputs through a\\nreasoning-guided rewriter, dynamically interpreting text-image interactions to\\ndeliver refined prompts that bolster safety across diverse VLM architectures\\nwithout altering their core parameters. To achieve this, we devise a\\nthree-stage reasoning pipeline to synthesize a dataset that trains the rewriter\\nto infer subtle threats, enabling tailored, actionable responses over generic\\nrefusals. Extensive experiments across three benchmarks with five VLMs reveal\\nthat VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1\\nachieves a remarkable 43.59\\\\% increase in average safety across five models on\\nthe SIUO benchmark.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL,cs.CV\", \"published\": \"2025-04-17T05:46:41Z\"}"}

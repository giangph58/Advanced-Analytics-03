{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11337v1\", \"title\": \"REWARD CONSISTENCY: Improving Multi-Objective Alignment from a\\n  Data-Centric Perspective\", \"summary\": \"Multi-objective preference alignment in language models often encounters a\\nchallenging trade-off: optimizing for one human preference (e.g., helpfulness)\\nfrequently compromises others (e.g., harmlessness) due to the inherent\\nconflicts between competing objectives. While prior work mainly focuses on\\nalgorithmic solutions, we explore a novel data-driven approach to uncover the\\ntypes of data that can effectively mitigate these conflicts. Specifically, we\\npropose the concept of Reward Consistency (RC), which identifies samples that\\nalign with multiple preference objectives, thereby reducing conflicts during\\ntraining. Through gradient-based analysis, we demonstrate that RC-compliant\\nsamples inherently constrain performance degradation during multi-objective\\noptimization. Building on these insights, we further develop Reward Consistency\\nSampling, a framework that automatically constructs preference datasets that\\neffectively mitigate conflicts during multi-objective alignment. Our generated\\ndata achieves an average improvement of 13.37% in both the harmless rate and\\nhelpfulness win rate when optimizing harmlessness and helpfulness, and can\\nconsistently resolve conflicts in varying multi-objective scenarios.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-15T16:09:19Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24358v1\", \"title\": \"SQuat: Subspace-orthogonal KV Cache Quantization\", \"summary\": \"The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\\npreviously generated tokens. It reduces redundant computation at the cost of\\nincreased memory usage. To mitigate this overhead, existing approaches compress\\nKV tensors into lower-bit representations; however, quantization errors can\\naccumulate as more tokens are generated, potentially resulting in undesired\\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\\nquantization). It first constructs a subspace spanned by query tensors to\\ncapture the most critical task-related information. During key tensor\\nquantization, it enforces that the difference between the (de)quantized and\\noriginal keys remains orthogonal to this subspace, minimizing the impact of\\nquantization errors on the attention mechanism's outputs. SQuat requires no\\nmodel fine-tuning, no additional calibration dataset for offline learning, and\\nis grounded in a theoretical framework we develop. Through numerical\\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\\nscores than existing KV cache quantization algorithms.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL,cs.IT,math.IT\", \"published\": \"2025-03-31T17:37:32Z\"}"}

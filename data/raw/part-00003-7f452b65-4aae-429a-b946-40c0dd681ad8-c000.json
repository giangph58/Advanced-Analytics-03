{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10277v1\", \"title\": \"RealHarm: A Collection of Real-World Language Model Application Failures\", \"summary\": \"Language model deployments in consumer-facing applications introduce numerous\\nrisks. While existing research on harms and hazards of such applications\\nfollows top-down approaches derived from regulatory frameworks and theoretical\\nanalyses, empirical evidence of real-world failure modes remains underexplored.\\nIn this work, we introduce RealHarm, a dataset of annotated problematic\\ninteractions with AI agents built from a systematic review of publicly reported\\nincidents. Analyzing harms, causes, and hazards specifically from the\\ndeployer's perspective, we find that reputational damage constitutes the\\npredominant organizational harm, while misinformation emerges as the most\\ncommon hazard category. We empirically evaluate state-of-the-art guardrails and\\ncontent moderation systems to probe whether such systems would have prevented\\nthe incidents, revealing a significant gap in the protection of AI\\napplications.\", \"main_category\": \"cs.CY\", \"categories\": \"cs.CY,cs.AI,cs.CL,cs.CR\", \"published\": \"2025-04-14T14:44:41Z\"}"}

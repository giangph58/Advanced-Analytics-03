{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20946v1\", \"title\": \"Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning\\n  Distillation From Large to Small Language Models\", \"summary\": \"As Large Language Models (LLMs) continue to be leveraged for daily tasks,\\nprompt engineering remains an active field of contribution within computational\\nlinguistics, particularly in domains requiring specialized knowledge such as\\narithmetic reasoning. While these LLMs are optimized for a variety of tasks,\\ntheir exhaustive employment may become computationally or financially\\ncumbersome for small teams. Additionally, complete reliance on proprietary,\\nclosed-source models often limits customization and adaptability, posing\\nsignificant challenges in research and application scalability. Instead, by\\nleveraging open-source models at or below 7 billion parameters, we can optimize\\nour resource usage while still observing remarkable gains over standard\\nprompting approaches. To cultivate this notion, we introduce Trace-of-Thought\\nPrompting, a simple, zero-shot prompt engineering method that instructs LLMs to\\ncreate observable subproblems using critical problem-solving, specifically\\ndesigned to enhance arithmetic reasoning capabilities. When applied to\\nopen-source models in tandem with GPT-4, we observe that Trace-of-Thought not\\nonly allows novel insight into the problem-solving process but also introduces\\nperformance gains as large as 125% on language models at or below 7 billion\\nparameters. This approach underscores the potential of open-source initiatives\\nin democratizing AI research and improving the accessibility of high-quality\\ncomputational linguistics applications.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-29T17:14:54Z\"}"}

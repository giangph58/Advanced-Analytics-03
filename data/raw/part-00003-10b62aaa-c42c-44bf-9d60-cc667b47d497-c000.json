{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20438v1\", \"title\": \"PixelHacker: Image Inpainting with Structural and Semantic Consistency\", \"summary\": \"Image inpainting is a fundamental research area between image editing and\\nimage generation. Recent state-of-the-art (SOTA) methods have explored novel\\nattention mechanisms, lightweight architectures, and context-aware modeling,\\ndemonstrating impressive performance. However, they often struggle with complex\\nstructure (e.g., texture, shape, spatial relations) and semantics (e.g., color\\nconsistency, object restoration, and logical correctness), leading to artifacts\\nand inappropriate generation. To address this challenge, we design a simple yet\\neffective inpainting paradigm called latent categories guidance, and further\\npropose a diffusion-based model named PixelHacker. Specifically, we first\\nconstruct a large dataset containing 14 million image-mask pairs by annotating\\nforeground and background (potential 116 and 21 categories, respectively).\\nThen, we encode potential foreground and background representations separately\\nthrough two fixed-size embeddings, and intermittently inject these features\\ninto the denoising process via linear attention. Finally, by pre-training on\\nour dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker.\\nExtensive experiments show that PixelHacker comprehensively outperforms the\\nSOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits\\nremarkable consistency in both structure and semantics. Project page at\\nhttps://hustvl.github.io/projects/PixelHacker.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-29T05:28:36Z\"}"}

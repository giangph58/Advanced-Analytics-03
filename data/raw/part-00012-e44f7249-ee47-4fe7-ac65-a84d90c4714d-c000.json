{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05074v1\", \"title\": \"Visual Affordances: Enabling Robots to Understand Object Functionality\", \"summary\": \"Human-robot interaction for assistive technologies relies on the prediction\\nof affordances, which are the potential actions a robot can perform on objects.\\nPredicting object affordances from visual perception is formulated differently\\nfor tasks such as grasping detection, affordance classification, affordance\\nsegmentation, and hand-object interaction synthesis. In this work, we highlight\\nthe reproducibility issue in these redefinitions, making comparative benchmarks\\nunfair and unreliable. To address this problem, we propose a unified\\nformulation for visual affordance prediction, provide a comprehensive and\\nsystematic review of previous works highlighting strengths and limitations of\\nmethods and datasets, and analyse what challenges reproducibility. To favour\\ntransparency, we introduce the Affordance Sheet, a document to detail the\\nproposed solution, the datasets, and the validation. As the physical properties\\nof an object influence the interaction with the robot, we present a generic\\nframework that links visual affordance prediction to the physical world. Using\\nthe weight of an object as an example for this framework, we discuss how\\nestimating object mass can affect the affordance prediction. Our approach\\nbridges the gap between affordance perception and robot actuation, and accounts\\nfor the complete information about objects of interest and how the robot\\ninteracts with them to accomplish its task.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.RO\", \"published\": \"2025-05-08T09:10:05Z\"}"}

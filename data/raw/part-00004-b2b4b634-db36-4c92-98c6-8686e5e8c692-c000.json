{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10013v1\", \"title\": \"Training LLMs on HPC Systems: Best Practices from the OpenGPT-X Project\", \"summary\": \"The training of large language models (LLMs) requires substantial\\ncomputational resources, complex software stacks, and carefully designed\\nworkflows to achieve scalability and efficiency. This report presents best\\npractices and insights gained from the OpenGPT-X project, a German initiative\\nfocused on developing open, multilingual LLMs optimized for European languages.\\nWe detail the use of high-performance computing (HPC) systems, primarily JUWELS\\nBooster at JSC, for training Teuken-7B, a 7-billion-parameter transformer\\nmodel. The report covers system architecture, training infrastructure, software\\nchoices, profiling and benchmarking tools, as well as engineering and\\noperational challenges.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC\", \"published\": \"2025-04-14T09:17:47Z\"}"}

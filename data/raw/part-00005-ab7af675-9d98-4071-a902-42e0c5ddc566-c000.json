{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20612v1\", \"title\": \"The Hidden Risks of LLM-Generated Web Application Code: A\\n  Security-Centric Evaluation of Code Generation Capabilities in Large Language\\n  Models\", \"summary\": \"The rapid advancement of Large Language Models (LLMs) has enhanced software\\ndevelopment processes, minimizing the time and effort required for coding and\\nenhancing developer productivity. However, despite their potential benefits,\\ncode generated by LLMs has been shown to generate insecure code in controlled\\nenvironments, raising critical concerns about their reliability and security in\\nreal-world applications. This paper uses predefined security parameters to\\nevaluate the security compliance of LLM-generated code across multiple models,\\nsuch as ChatGPT, DeepSeek, Claude, Gemini and Grok. The analysis reveals\\ncritical vulnerabilities in authentication mechanisms, session management,\\ninput validation and HTTP security headers. Although some models implement\\nsecurity measures to a limited extent, none fully align with industry best\\npractices, highlighting the associated risks in automated software development.\\nOur findings underscore that human expertise is crucial to ensure secure\\nsoftware deployment or review of LLM-generated code. Also, there is a need for\\nrobust security assessment frameworks to enhance the reliability of\\nLLM-generated code in real-world applications.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.AI,cs.ET\", \"published\": \"2025-04-29T10:23:11Z\"}"}

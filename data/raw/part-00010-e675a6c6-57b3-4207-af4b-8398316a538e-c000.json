{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13134v1\", \"title\": \"Energy-Based Reward Models for Robust Language Model Alignment\", \"summary\": \"Reward models (RMs) are essential for aligning Large Language Models (LLMs)\\nwith human preferences. However, they often struggle with capturing complex\\nhuman preferences and generalizing to unseen data. To address these challenges,\\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\\nrefinement framework that enhances RM robustness and generalization. EBRM\\nmodels the reward distribution explicitly, capturing uncertainty in human\\npreferences and mitigating the impact of noisy or misaligned annotations. It\\nachieves this through conflict-aware data filtering, label-noise-aware\\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\\nwithout retraining, making it computationally efficient and adaptable across\\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\\nsignificant improvements in both robustness and generalization, achieving up to\\na 5.97% improvement in safety-critical alignment tasks compared to standard\\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\\nrewards enhance alignment quality, effectively delaying reward hacking. These\\nresults demonstrate our approach as a scalable and effective enhancement for\\nexisting RMs and alignment pipelines. The code is available at EBRM.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-17T17:47:15Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10070v1\", \"title\": \"DTFSal: Audio-Visual Dynamic Token Fusion for Video Saliency Prediction\", \"summary\": \"Audio-visual saliency prediction aims to mimic human visual attention by\\nidentifying salient regions in videos through the integration of both visual\\nand auditory information. Although visual-only approaches have significantly\\nadvanced, effectively incorporating auditory cues remains challenging due to\\ncomplex spatio-temporal interactions and high computational demands. To address\\nthese challenges, we propose Dynamic Token Fusion Saliency (DFTSal), a novel\\naudio-visual saliency prediction framework designed to balance accuracy with\\ncomputational efficiency. Our approach features a multi-scale visual encoder\\nequipped with two novel modules: the Learnable Token Enhancement Block (LTEB),\\nwhich adaptively weights tokens to emphasize crucial saliency cues, and the\\nDynamic Learnable Token Fusion Block (DLTFB), which employs a shifting\\noperation to reorganize and merge features, effectively capturing long-range\\ndependencies and detailed spatial information. In parallel, an audio branch\\nprocesses raw audio signals to extract meaningful auditory features. Both\\nvisual and audio features are integrated using our Adaptive Multimodal Fusion\\nBlock (AMFB), which employs local, global, and adaptive fusion streams for\\nprecise cross-modal fusion. The resulting fused features are processed by a\\nhierarchical multi-decoder structure, producing accurate saliency maps.\\nExtensive evaluations on six audio-visual benchmarks demonstrate that DFTSal\\nachieves SOTA performance while maintaining computational efficiency.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-14T10:17:25Z\"}"}

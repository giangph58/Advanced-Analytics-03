{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07556v1\", \"title\": \"TokenFocus-VQA: Enhancing Text-to-Image Alignment with Position-Aware\\n  Focus and Multi-Perspective Aggregations on LVLMs\", \"summary\": \"While text-to-image (T2I) generation models have achieved remarkable progress\\nin recent years, existing evaluation methodologies for vision-language\\nalignment still struggle with the fine-grained semantic matching. Current\\napproaches based on global similarity metrics often overlook critical\\ntoken-level correspondences between textual descriptions and visual content. To\\nthis end, we present TokenFocus-VQA, a novel evaluation framework that\\nleverages Large Vision-Language Models (LVLMs) through visual question\\nanswering (VQA) paradigm with position-specific probability optimization. Our\\nkey innovation lies in designing a token-aware loss function that selectively\\nfocuses on probability distributions at pre-defined vocabulary positions\\ncorresponding to crucial semantic elements, enabling precise measurement of\\nfine-grained semantical alignment. The proposed framework further integrates\\nensemble learning techniques to aggregate multi-perspective assessments from\\ndiverse LVLMs architectures, thereby achieving further performance enhancement.\\nEvaluated on the NTIRE 2025 T2I Quality Assessment Challenge Track 1, our\\nTokenFocus-VQA ranks 2nd place (0.8445, only 0.0001 lower than the 1st method)\\non public evaluation and 2nd place (0.8426) on the official private test set,\\ndemonstrating superiority in capturing nuanced text-image correspondences\\ncompared to conventional evaluation methods.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-10T08:37:13Z\"}"}

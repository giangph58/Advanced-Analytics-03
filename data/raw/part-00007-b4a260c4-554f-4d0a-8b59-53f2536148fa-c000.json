{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21801v1\", \"title\": \"DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via\\n  Reinforcement Learning for Subgoal Decomposition\", \"summary\": \"We introduce DeepSeek-Prover-V2, an open-source large language model designed\\nfor formal theorem proving in Lean 4, with initialization data collected\\nthrough a recursive theorem proving pipeline powered by DeepSeek-V3. The\\ncold-start training procedure begins by prompting DeepSeek-V3 to decompose\\ncomplex problems into a series of subgoals. The proofs of resolved subgoals are\\nsynthesized into a chain-of-thought process, combined with DeepSeek-V3's\\nstep-by-step reasoning, to create an initial cold start for reinforcement\\nlearning. This process enables us to integrate both informal and formal\\nmathematical reasoning into a unified model. The resulting model,\\nDeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural\\ntheorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49\\nout of 658 problems from PutnamBench. In addition to standard benchmarks, we\\nintroduce ProverBench, a collection of 325 formalized problems, to enrich our\\nevaluation, including 15 selected problems from the recent AIME competitions\\n(years 24-25). Further evaluation on these 15 AIME problems shows that the\\nmodel successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of\\nthese problems using majority voting, highlighting that the gap between formal\\nand informal mathematical reasoning in large language models is substantially\\nnarrowing.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-30T16:57:48Z\"}"}

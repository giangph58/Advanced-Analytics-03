{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16427v1\", \"title\": \"Can Large Language Models Help Multimodal Language Analysis? MMLA: A\\n  Comprehensive Benchmark\", \"summary\": \"Multimodal language analysis is a rapidly evolving field that leverages\\nmultiple modalities to enhance the understanding of high-level semantics\\nunderlying human conversational utterances. Despite its significance, little\\nresearch has investigated the capability of multimodal large language models\\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\\ncomprises over 61K multimodal utterances drawn from both staged and real-world\\nscenarios, covering six core dimensions of multimodal semantics: intent,\\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\\naccuracy, underscoring the limitations of current MLLMs in understanding\\ncomplex human language. We believe that MMLA will serve as a solid foundation\\nfor exploring the potential of large language models in multimodal language\\nanalysis and provide valuable resources to advance this field. The datasets and\\ncode are open-sourced at https://github.com/thuiar/MMLA.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.MM\", \"published\": \"2025-04-23T05:25:13Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19867v1\", \"title\": \"semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\\n  Computation and Unified Storage\", \"summary\": \"Existing large language model (LLM) serving systems fall into two categories:\\n1) a unified system where prefill phase and decode phase are co-located on the\\nsame GPU, sharing the unified computational resource and storage, and 2) a\\ndisaggregated system where the two phases are disaggregated to different GPUs.\\nThe design of the disaggregated system addresses the latency interference and\\nsophisticated scheduling issues in the unified system but leads to storage\\nchallenges including 1) replicated weights for both phases that prevent\\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\\ncache. Such storage inefficiency delivers poor serving performance under high\\nrequest rates.\\n  In this paper, we identify that the advantage of the disaggregated system\\nlies in the disaggregated computation, i.e., partitioning the computational\\nresource to enable the asynchronous computation of two phases. Thus, we propose\\na novel LLM serving system, semi-PD, characterized by disaggregated computation\\nand unified storage. In semi-PD, we introduce a computation resource controller\\nto achieve disaggregated computation at the streaming multi-processor (SM)\\nlevel, and a unified memory manager to manage the asynchronous memory access\\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\\nbetween the two phases, and a service-level objective (SLO) aware dynamic\\npartitioning algorithm to optimize the SLO attainment. Compared to\\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\\nconstraints on Llama series models.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.DC,cs.LG\", \"published\": \"2025-04-28T15:00:03Z\"}"}

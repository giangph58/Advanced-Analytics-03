{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12900v1\", \"title\": \"FashionDPO:Fine-tune Fashion Outfit Generation Model using Direct\\n  Preference Optimization\", \"summary\": \"Personalized outfit generation aims to construct a set of compatible and\\npersonalized fashion items as an outfit. Recently, generative AI models have\\nreceived widespread attention, as they can generate fashion items for users to\\ncomplete an incomplete outfit or create a complete outfit. However, they have\\nlimitations in terms of lacking diversity and relying on the supervised\\nlearning paradigm. Recognizing this gap, we propose a novel framework\\nFashionDPO, which fine-tunes the fashion outfit generation model using direct\\npreference optimization. This framework aims to provide a general fine-tuning\\napproach to fashion generative models, refining a pre-trained fashion outfit\\ngeneration model using automatically generated feedback, without the need to\\ndesign a task-specific reward function. To make sure that the feedback is\\ncomprehensive and objective, we design a multi-expert feedback generation\\nmodule which covers three evaluation perspectives, \\\\ie quality, compatibility\\nand personalization. Experiments on two established datasets, \\\\ie iFashion and\\nPolyvore-U, demonstrate the effectiveness of our framework in enhancing the\\nmodel's ability to align with users' personalized preferences while adhering to\\nfashion compatibility principles. Our code and model checkpoints are available\\nat https://github.com/Yzcreator/FashionDPO.\", \"main_category\": \"cs.MM\", \"categories\": \"cs.MM,cs.IR\", \"published\": \"2025-04-17T12:41:41Z\"}"}

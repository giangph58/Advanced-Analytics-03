{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12259v1\", \"title\": \"VGDFR: Diffusion-based Video Generation with Dynamic Latent Frame Rate\", \"summary\": \"Diffusion Transformer(DiT)-based generation models have achieved remarkable\\nsuccess in video generation. However, their inherent computational demands pose\\nsignificant efficiency challenges. In this paper, we exploit the inherent\\ntemporal non-uniformity of real-world videos and observe that videos exhibit\\ndynamic information density, with high-motion segments demanding greater detail\\npreservation than static scenes. Inspired by this temporal non-uniformity, we\\npropose VGDFR, a training-free approach for Diffusion-based Video Generation\\nwith Dynamic Latent Frame Rate. VGDFR adaptively adjusts the number of elements\\nin latent space based on the motion frequency of the latent space content,\\nusing fewer tokens for low-frequency segments while preserving detail in\\nhigh-frequency segments. Specifically, our key contributions are: (1) A dynamic\\nframe rate scheduler for DiT video generation that adaptively assigns frame\\nrates for video segments. (2) A novel latent-space frame merging method to\\nalign latent representations with their denoised counterparts before merging\\nthose redundant in low-resolution space. (3) A preference analysis of Rotary\\nPositional Embeddings (RoPE) across DiT layers, informing a tailored RoPE\\nstrategy optimized for semantic and local information capture. Experiments show\\nthat VGDFR can achieve a speedup up to 3x for video generation with minimal\\nquality degradation.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-16T17:09:13Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12086v1\", \"title\": \"Neural Contextual Bandits Under Delayed Feedback Constraints\", \"summary\": \"This paper presents a new algorithm for neural contextual bandits (CBs) that\\naddresses the challenge of delayed reward feedback, where the reward for a\\nchosen action is revealed after a random, unknown delay. This scenario is\\ncommon in applications such as online recommendation systems and clinical\\ntrials, where reward feedback is delayed because the outcomes or results of a\\nuser's actions (such as recommendations or treatment responses) take time to\\nmanifest and be measured. The proposed algorithm, called Delayed NeuralUCB,\\nuses an upper confidence bound (UCB)-based exploration strategy. Under the\\nassumption of independent and identically distributed sub-exponential reward\\ndelays, we derive an upper bound on the cumulative regret over a T-length\\nhorizon. We further consider a variant of the algorithm, called Delayed\\nNeuralTS, that uses Thompson Sampling-based exploration. Numerical experiments\\non real-world datasets, such as MNIST and Mushroom, along with comparisons to\\nbenchmark approaches, demonstrate that the proposed algorithms effectively\\nmanage varying delays and are well-suited for complex real-world scenarios.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-16T13:47:25Z\"}"}

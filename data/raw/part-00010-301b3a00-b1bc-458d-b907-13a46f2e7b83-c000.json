{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07691v1\", \"title\": \"Distilling Knowledge from Heterogeneous Architectures for Semantic\\n  Segmentation\", \"summary\": \"Current knowledge distillation (KD) methods for semantic segmentation focus\\non guiding the student to imitate the teacher's knowledge within homogeneous\\narchitectures. However, these methods overlook the diverse knowledge contained\\nin architectures with different inductive biases, which is crucial for enabling\\nthe student to acquire a more precise and comprehensive understanding of the\\ndata during distillation. To this end, we propose for the first time a generic\\nknowledge distillation method for semantic segmentation from a heterogeneous\\nperspective, named HeteroAKD. Due to the substantial disparities between\\nheterogeneous architectures, such as CNN and Transformer, directly transferring\\ncross-architecture knowledge presents significant challenges. To eliminate the\\ninfluence of architecture-specific information, the intermediate features of\\nboth the teacher and student are skillfully projected into an aligned logits\\nspace. Furthermore, to utilize diverse knowledge from heterogeneous\\narchitectures and deliver customized knowledge required by the student, a\\nteacher-student knowledge mixing mechanism (KMM) and a teacher-student\\nknowledge evaluation mechanism (KEM) are introduced. These mechanisms are\\nperformed by assessing the reliability and its discrepancy between\\nheterogeneous teacher-student knowledge. Extensive experiments conducted on\\nthree main-stream benchmarks using various teacher-student pairs demonstrate\\nthat our HeteroAKD outperforms state-of-the-art KD methods in facilitating\\ndistillation between heterogeneous architectures.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CV\", \"published\": \"2025-04-10T12:24:58Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20501v1\", \"title\": \"SAM-Guided Robust Representation Learning for One-Shot 3D Medical Image\\n  Segmentation\", \"summary\": \"One-shot medical image segmentation (MIS) is crucial for medical analysis due\\nto the burden of medical experts on manual annotation. The recent emergence of\\nthe segment anything model (SAM) has demonstrated remarkable adaptation in MIS\\nbut cannot be directly applied to one-shot medical image segmentation (MIS) due\\nto its reliance on labor-intensive user interactions and the high computational\\ncost. To cope with these limitations, we propose a novel SAM-guided robust\\nrepresentation learning framework, named RRL-MedSAM, to adapt SAM to one-shot\\n3D MIS, which exploits the strong generalization capabilities of the SAM\\nencoder to learn better feature representation. We devise a dual-stage\\nknowledge distillation (DSKD) strategy to distill general knowledge between\\nnatural and medical images from the foundation model to train a lightweight\\nencoder, and then adopt a mutual exponential moving average (mutual-EMA) to\\nupdate the weights of the general lightweight encoder and medical-specific\\nencoder. Specifically, pseudo labels from the registration network are used to\\nperform mutual supervision for such two encoders. Moreover, we introduce an\\nauto-prompting (AP) segmentation decoder which adopts the mask generated from\\nthe general lightweight model as a prompt to assist the medical-specific model\\nin boosting the final segmentation performance. Extensive experiments conducted\\non three public datasets, i.e., OASIS, CT-lung demonstrate that the proposed\\nRRL-MedSAM outperforms state-of-the-art one-shot MIS methods for both\\nsegmentation and registration tasks. Especially, our lightweight encoder uses\\nonly 3\\\\% of the parameters compared to the encoder of SAM-Base.\", \"main_category\": \"eess.IV\", \"categories\": \"eess.IV,cs.CV\", \"published\": \"2025-04-29T07:43:37Z\"}"}

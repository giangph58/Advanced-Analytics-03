{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15196v1\", \"title\": \"Fully Adaptive Stepsizes: Which System Benefit More -- Centralized or\\n  Decentralized?\", \"summary\": \"In decentralized optimization, the choice of stepsize plays a critical role\\nin algorithm performance. A common approach is to use a shared stepsize across\\nall agents to ensure convergence. However, selecting an optimal stepsize often\\nrequires careful tuning, which can be time-consuming and may lead to slow\\nconvergence, especially when there is significant variation in the smoothness\\n(L-smoothness) of local objective functions across agents. Individually tuning\\nstepsizes per agent is also impractical, particularly in large-scale networks.\\nTo address these limitations, we propose AdGT, an adaptive gradient tracking\\nmethod that enables each agent to adjust its stepsize based on the smoothness\\nof its local objective. We prove that AdGT generates a sequence of iterates\\nthat converges to the optimal consensus solution. Through numerical\\nexperiments, we compare AdGT with fixed-stepsize gradient tracking methods and\\ndemonstrate its superior performance. Additionally, we compare AdGT with\\nadaptive gradient descent (AdGD) in a centralized setting and observe that\\nfully adaptive stepsizes offer greater benefits in decentralized networks than\\nin centralized ones.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC,cs.SY,eess.SY\", \"published\": \"2025-04-21T16:07:32Z\"}"}

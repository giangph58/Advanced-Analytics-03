{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05050v1\", \"title\": \"Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language\\n  Models\", \"summary\": \"Large language models (LLMs) are foundational explorations to artificial\\ngeneral intelligence, yet their alignment with human values via instruction\\ntuning and preference learning achieves only superficial compliance. Here, we\\ndemonstrate that harmful knowledge embedded during pretraining persists as\\nindelible \\\"dark patterns\\\" in LLMs' parametric memory, evading alignment\\nsafeguards and resurfacing under adversarial inducement at distributional\\nshifts. In this study, we first theoretically analyze the intrinsic ethical\\nvulnerability of aligned LLMs by proving that current alignment methods yield\\nonly local \\\"safety regions\\\" in the knowledge manifold. In contrast, pretrained\\nknowledge remains globally connected to harmful concepts via high-likelihood\\nadversarial trajectories. Building on this theoretical insight, we empirically\\nvalidate our findings by employing semantic coherence inducement under\\ndistributional shifts--a method that systematically bypasses alignment\\nconstraints through optimized adversarial prompts. This combined theoretical\\nand empirical approach achieves a 100% attack success rate across 19 out of 23\\nstate-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing\\ntheir universal vulnerabilities.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-07T13:20:17Z\"}"}

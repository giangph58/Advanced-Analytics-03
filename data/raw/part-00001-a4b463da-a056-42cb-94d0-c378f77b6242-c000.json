{"value":"{\"aid\": \"http://arxiv.org/abs/2503.21679v1\", \"title\": \"JiraiBench: A Bilingual Benchmark for Evaluating Large Language Models'\\n  Detection of Human Self-Destructive Behavior Content in Jirai Community\", \"summary\": \"This paper introduces JiraiBench, the first bilingual benchmark for\\nevaluating large language models' effectiveness in detecting self-destructive\\ncontent across Chinese and Japanese social media communities. Focusing on the\\ntransnational \\\"Jirai\\\" (landmine) online subculture that encompasses multiple\\nforms of self-destructive behaviors including drug overdose, eating disorders,\\nand self-harm, we present a comprehensive evaluation framework incorporating\\nboth linguistic and cultural dimensions. Our dataset comprises 10,419 Chinese\\nposts and 5,000 Japanese posts with multidimensional annotation along three\\nbehavioral categories, achieving substantial inter-annotator agreement.\\nExperimental evaluations across four state-of-the-art models reveal significant\\nperformance variations based on instructional language, with Japanese prompts\\nunexpectedly outperforming Chinese prompts when processing Chinese content.\\nThis emergent cross-cultural transfer suggests that cultural proximity can\\nsometimes outweigh linguistic similarity in detection tasks. Cross-lingual\\ntransfer experiments with fine-tuned models further demonstrate the potential\\nfor knowledge transfer between these language systems without explicit target\\nlanguage training. These findings highlight the need for culturally-informed\\napproaches to multilingual content moderation and provide empirical evidence\\nfor the importance of cultural context in developing more effective detection\\nsystems for vulnerable online communities.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.CY\", \"published\": \"2025-03-27T16:48:58Z\"}"}

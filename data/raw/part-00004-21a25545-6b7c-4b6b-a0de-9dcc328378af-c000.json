{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14945v1\", \"title\": \"Learning to Reason under Off-Policy Guidance\", \"summary\": \"Recent advances in large reasoning models (LRMs) demonstrate that\\nsophisticated behaviors such as multi-step reasoning and self-reflection can\\nemerge via reinforcement learning (RL) with simple rule-based rewards. However,\\nexisting zero-RL approaches are inherently ``on-policy'', limiting learning to\\na model's own outputs and failing to acquire reasoning abilities beyond its\\ninitial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY\\nguidance), a framework that augments zero-RL with off-policy reasoning traces.\\nLUFFY dynamically balances imitation and exploration by combining off-policy\\ndemonstrations with on-policy rollouts during training. Notably, we propose\\npolicy shaping via regularized importance sampling to avoid superficial and\\nrigid imitation during mixed-policy training. Remarkably, LUFFY achieves an\\nover +7.0 average gain across six math benchmarks and an advantage of over +6.2\\npoints in out-of-distribution tasks. It also substantially surpasses\\nimitation-based supervised fine-tuning (SFT), particularly in generalization.\\nAnalysis shows LUFFY not only imitates effectively but also explores beyond\\ndemonstrations, offering a scalable path to train generalizable reasoning\\nmodels with off-policy guidance.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL\", \"published\": \"2025-04-21T08:09:13Z\"}"}

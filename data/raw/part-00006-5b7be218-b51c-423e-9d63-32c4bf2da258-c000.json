{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21553v1\", \"title\": \"Precision Where It Matters: A Novel Spike Aware Mixed-Precision\\n  Quantization Strategy for LLaMA-based Language Models\", \"summary\": \"Large Language Models (LLMs) have demonstrated remarkable capabilities in\\nvarious natural language processing tasks. However, their size presents\\nsignificant challenges for deployment and inference. This paper investigates\\nthe quantization of LLMs, focusing on the LLaMA architecture and its\\nderivatives. We challenge existing assumptions about activation outliers in\\nLLMs and propose a novel mixed-precision quantization approach tailored for\\nLLaMA-like models. Our method leverages the observation that activation spikes\\nin LLaMA architectures are predominantly concentrated in specific projection\\nlayers. By applying higher precision (FP16 or FP8) to these layers while\\nquantizing the rest of the model to lower bit-widths, we achieve superior\\nperformance compared to existing quantization techniques. Experimental results\\non LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in\\nperplexity and zero-shot accuracy, particularly for 8-bit per-tensor\\nquantization. Our approach outperforms general-purpose methods designed to\\nhandle outliers across all architecture types, highlighting the benefits of\\narchitecture-specific quantization strategies. This research contributes to the\\nongoing efforts to make LLMs more efficient and deployable, potentially\\nenabling their use in resource-constrained environments. Our findings emphasize\\nthe importance of considering model-specific characteristics in developing\\neffective quantization pipelines for state-of-the-art language models by\\nidentifying and targeting a small number of projections that concentrate\\nactivation spikes.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-30T11:52:18Z\"}"}

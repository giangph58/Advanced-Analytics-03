{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06792v1\", \"title\": \"Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot\\n  Demonstrations\", \"summary\": \"Mixture-of-Experts (MoE) models achieve a favorable trade-off between\\nperformance and inference efficiency by activating only a subset of experts.\\nHowever, the memory overhead of storing all experts remains a major limitation,\\nespecially in large-scale MoE models such as DeepSeek-R1 (671B). In this study,\\nwe investigate domain specialization and expert redundancy in large-scale MoE\\nmodels and uncover a consistent behavior we term few-shot expert localization,\\nwith only a few demonstrations, the model consistently activates a sparse and\\nstable subset of experts. Building on this observation, we propose a simple yet\\neffective pruning framework, EASY-EP, that leverages a few domain-specific\\ndemonstrations to identify and retain only the most relevant experts. EASY-EP\\ncomprises two key components: output-aware expert importance assessment and\\nexpert-level token contribution estimation. The former evaluates the importance\\nof each expert for the current token by considering the gating scores and\\nmagnitudes of the outputs of activated experts, while the latter assesses the\\ncontribution of tokens based on representation similarities after and before\\nrouted experts. Experiments show that our method can achieve comparable\\nperformances and $2.99\\\\times$ throughput under the same memory budget with full\\nDeepSeek-R1 with only half the experts. Our code is available at\\nhttps://github.com/RUCAIBox/EASYEP.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-09T11:34:06Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15953v1\", \"title\": \"Visual Place Cell Encoding: A Computational Model for Spatial\\n  Representation and Cognitive Mapping\", \"summary\": \"This paper presents the Visual Place Cell Encoding (VPCE) model, a\\nbiologically inspired computational framework for simulating place cell-like\\nactivation using visual input. Drawing on evidence that visual landmarks play a\\ncentral role in spatial encoding, the proposed VPCE model activates visual\\nplace cells by clustering high-dimensional appearance features extracted from\\nimages captured by a robot-mounted camera. Each cluster center defines a\\nreceptive field, and activation is computed based on visual similarity using a\\nradial basis function. We evaluate whether the resulting activation patterns\\ncorrelate with key properties of biological place cells, including spatial\\nproximity, orientation alignment, and boundary differentiation. Experiments\\ndemonstrate that the VPCE can distinguish between visually similar yet\\nspatially distinct locations and adapt to environment changes such as the\\ninsertion or removal of walls. These results suggest that structured visual\\ninput, even in the absence of motion cues or reward-driven learning, is\\nsufficient to generate place-cell-like spatial representations and support\\nbiologically inspired cognitive mapping.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.CV\", \"published\": \"2025-04-22T14:49:30Z\"}"}

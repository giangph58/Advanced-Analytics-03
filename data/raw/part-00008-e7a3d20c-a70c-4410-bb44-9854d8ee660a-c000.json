{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11143v1\", \"title\": \"Taming Consistency Distillation for Accelerated Human Image Animation\", \"summary\": \"Recent advancements in human image animation have been propelled by video\\ndiffusion models, yet their reliance on numerous iterative denoising steps\\nresults in high inference costs and slow speeds. An intuitive solution involves\\nadopting consistency models, which serve as an effective acceleration paradigm\\nthrough consistency distillation. However, simply employing this strategy in\\nhuman image animation often leads to quality decline, including visual\\nblurring, motion degradation, and facial distortion, particularly in dynamic\\nregions. In this paper, we propose the DanceLCM approach complemented by\\nseveral enhancements to improve visual quality and motion continuity at\\nlow-step regime: (1) segmented consistency distillation with an auxiliary\\nlight-weight head to incorporate supervision from real video latents,\\nmitigating cumulative errors resulting from single full-trajectory generation;\\n(2) a motion-focused loss to centre on motion regions, and explicit injection\\nof facial fidelity features to improve face authenticity. Extensive qualitative\\nand quantitative experiments demonstrate that DanceLCM achieves results\\ncomparable to state-of-the-art video diffusion models with a mere 2-4 inference\\nsteps, significantly reducing the inference burden without compromising video\\nquality. The code and models will be made publicly available.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-15T12:44:53Z\"}"}

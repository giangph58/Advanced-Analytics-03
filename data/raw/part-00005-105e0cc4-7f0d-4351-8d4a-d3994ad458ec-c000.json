{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05474v1\", \"title\": \"3D Scene Generation: A Survey\", \"summary\": \"3D scene generation seeks to synthesize spatially structured, semantically\\nmeaningful, and photorealistic environments for applications such as immersive\\nmedia, robotics, autonomous driving, and embodied AI. Early methods based on\\nprocedural rules offered scalability but limited diversity. Recent advances in\\ndeep generative models (e.g., GANs, diffusion models) and 3D representations\\n(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene\\ndistributions, improving fidelity, diversity, and view consistency. Recent\\nadvances like diffusion models bridge 3D scene synthesis and photorealism by\\nreframing generation as image or video synthesis problems. This survey provides\\na systematic overview of state-of-the-art approaches, organizing them into four\\nparadigms: procedural generation, neural 3D-based generation, image-based\\ngeneration, and video-based generation. We analyze their technical foundations,\\ntrade-offs, and representative results, and review commonly used datasets,\\nevaluation protocols, and downstream applications. We conclude by discussing\\nkey challenges in generation capacity, 3D representation, data and annotations,\\nand evaluation, and outline promising directions including higher fidelity,\\nphysics-aware and interactive generation, and unified perception-generation\\nmodels. This review organizes recent advances in 3D scene generation and\\nhighlights promising directions at the intersection of generative AI, 3D\\nvision, and embodied intelligence. To track ongoing developments, we maintain\\nan up-to-date project page:\\nhttps://github.com/hzxie/Awesome-3D-Scene-Generation.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-08T17:59:54Z\"}"}

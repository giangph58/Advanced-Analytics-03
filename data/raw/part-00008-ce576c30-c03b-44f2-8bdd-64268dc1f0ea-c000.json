{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20828v1\", \"title\": \"Ascendra: Dynamic Request Prioritization for Efficient LLM Serving\", \"summary\": \"The rapid advancement of Large Language Models (LLMs) has driven the need for\\nmore efficient serving strategies. In this context, efficiency refers to the\\nproportion of requests that meet their Service Level Objectives (SLOs),\\nparticularly for Time To First Token (TTFT) and Time Between Tokens (TBT).\\nHowever, existing systems often prioritize one metric at the cost of the other.\\nWe present Ascendra, an LLM serving system designed to meet both TTFT and TBT\\nSLOs simultaneously. The core insight behind Ascendra is that a request's\\nurgency evolves as it approaches its deadline. To leverage this, Ascendra\\npartitions GPU resources into two types of instances: low-priority and\\nhigh-priority. Low-priority instances maximize throughput by processing\\nrequests out of arrival order, but at the risk of request starvation. To\\naddress this, Ascendra employs a performance model to predict requests at risk\\nof missing their SLOs and proactively offloads them to high-priority instances.\\nHigh-priority instances are optimized for low-latency execution and handle\\nurgent requests nearing their deadlines. This partitioned architecture enables\\nAscendra to effectively balance high throughput and low latency. Extensive\\nevaluation shows that Ascendra improves system throughput by up to 1.7x\\ncompared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-29T14:51:26Z\"}"}

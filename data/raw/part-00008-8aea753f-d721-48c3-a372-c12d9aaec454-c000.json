{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17674v1\", \"title\": \"Energy Considerations of Large Language Model Inference and Efficiency\\n  Optimizations\", \"summary\": \"As large language models (LLMs) scale in size and adoption, their\\ncomputational and environmental costs continue to rise. Prior benchmarking\\nefforts have primarily focused on latency reduction in idealized settings,\\noften overlooking the diverse real-world inference workloads that shape energy\\nuse. In this work, we systematically analyze the energy implications of common\\ninference efficiency optimizations across diverse Natural Language Processing\\n(NLP) and generative Artificial Intelligence (AI) workloads, including\\nconversational AI and code generation. We introduce a modeling approach that\\napproximates real-world LLM workflows through a binning strategy for\\ninput-output token distributions and batch size variations. Our empirical\\nanalysis spans software frameworks, decoding strategies, GPU architectures,\\nonline and offline serving settings, and model parallelism configurations. We\\nshow that the effectiveness of inference optimizations is highly sensitive to\\nworkload geometry, software stack, and hardware accelerators, demonstrating\\nthat naive energy estimates based on FLOPs or theoretical GPU utilization\\nsignificantly underestimate real-world energy consumption. Our findings reveal\\nthat the proper application of relevant inference efficiency optimizations can\\nreduce total energy use by up to 73% from unoptimized baselines. These insights\\nprovide a foundation for sustainable LLM deployment and inform energy-efficient\\ndesign strategies for future AI infrastructure.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-24T15:45:05Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00307v1\", \"title\": \"Gateformer: Advancing Multivariate Time Series Forecasting through\\n  Temporal and Variate-Wise Attention with Gated Representations\", \"summary\": \"There has been a recent surge of interest in time series modeling using the\\nTransformer architecture. However, forecasting multivariate time series with\\nTransformer presents a unique challenge as it requires modeling both temporal\\n(cross-time) and variate (cross-variate) dependencies. While Transformer-based\\nmodels have gained popularity for their flexibility in capturing both\\nsequential and cross-variate relationships, it is unclear how to best integrate\\nthese two sources of information in the context of the Transformer architecture\\nwhile optimizing for both performance and efficiency. We re-purpose the\\nTransformer architecture to effectively model both cross-time and cross-variate\\ndependencies. Our approach begins by embedding each variate independently into\\na variate-wise representation that captures its cross-time dynamics, and then\\nmodels cross-variate dependencies through attention mechanisms on these learned\\nembeddings. Gating operations in both cross-time and cross-variate modeling\\nphases regulate information flow, allowing the model to focus on the most\\nrelevant features for accurate predictions. Our method achieves\\nstate-of-the-art performance across 13 real-world datasets and can be\\nseamlessly integrated into other Transformer-based and LLM-based forecasters,\\ndelivering performance improvements up to 20.7\\\\% over original models. Code is\\navailable at this repository: https://github.com/nyuolab/Gateformer.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-01T04:59:05Z\"}"}

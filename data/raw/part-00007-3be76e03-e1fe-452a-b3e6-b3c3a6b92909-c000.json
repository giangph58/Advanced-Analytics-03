{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15210v1\", \"title\": \"Integrating Symbolic Execution into the Fine-Tuning of Code-Generating\\n  LLMs\", \"summary\": \"Code-generating Large Language Models (LLMs) have become essential tools in\\nmodern software development, enhancing productivity and accelerating\\ndevelopment. This paper aims to investigate the fine-tuning of code-generating\\nLLMs using Reinforcement Learning and Direct Preference Optimization, further\\nimproving their performance. To achieve this, we enhance the training data for\\nthe reward model with the help of symbolic execution techniques, ensuring more\\ncomprehensive and objective data. With symbolic execution, we create a custom\\ndataset that better captures the nuances in code evaluation. Our reward models,\\nfine-tuned on this dataset, demonstrate significant improvements over the\\nbaseline, CodeRL, in estimating the quality of generated code. Our\\ncode-generating LLMs, trained with the help of reward model feedback, achieve\\nsimilar results compared to the CodeRL benchmark.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE,cs.AI\", \"published\": \"2025-04-21T16:29:07Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12626v1\", \"title\": \"Packing Input Frame Context in Next-Frame Prediction Models for Video\\n  Generation\", \"summary\": \"We present a neural network structure, FramePack, to train next-frame (or\\nnext-frame-section) prediction models for video generation. The FramePack\\ncompresses input frames to make the transformer context length a fixed number\\nregardless of the video length. As a result, we are able to process a large\\nnumber of frames using video diffusion with computation bottleneck similar to\\nimage diffusion. This also makes the training video batch sizes significantly\\nhigher (batch sizes become comparable to image diffusion training). We also\\npropose an anti-drifting sampling method that generates frames in inverted\\ntemporal order with early-established endpoints to avoid exposure bias (error\\naccumulation over iterations). Finally, we show that existing video diffusion\\nmodels can be finetuned with FramePack, and their visual quality may be\\nimproved because the next-frame prediction supports more balanced diffusion\\nschedulers with less extreme flow shift timesteps.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-17T04:02:31Z\"}"}

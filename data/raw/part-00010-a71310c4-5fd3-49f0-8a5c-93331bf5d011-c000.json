{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10902v1\", \"title\": \"Leveraging Submodule Linearity Enhances Task Arithmetic Performance in\\n  LLMs\", \"summary\": \"Task arithmetic is a straightforward yet highly effective strategy for model\\nmerging, enabling the resultant model to exhibit multi-task capabilities.\\nRecent research indicates that models demonstrating linearity enhance the\\nperformance of task arithmetic. In contrast to existing methods that rely on\\nthe global linearization of the model, we argue that this linearity already\\nexists within the model's submodules. In particular, we present a statistical\\nanalysis and show that submodules (e.g., layers, self-attentions, and MLPs)\\nexhibit significantly higher linearity than the overall model. Based on these\\nfindings, we propose an innovative model merging strategy that independently\\nmerges these submodules. Especially, we derive a closed-form solution for\\noptimal merging weights grounded in the linear properties of these submodules.\\nExperimental results demonstrate that our method consistently outperforms the\\nstandard task arithmetic approach and other established baselines across\\ndifferent model scales and various tasks. This result highlights the benefits\\nof leveraging the linearity of submodules and provides a new perspective for\\nexploring solutions for effective and practical multi-task model merging.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-15T06:23:24Z\"}"}

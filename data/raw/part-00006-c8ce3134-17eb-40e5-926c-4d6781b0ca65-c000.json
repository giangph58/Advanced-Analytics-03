{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12284v1\", \"title\": \"How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday\\n  Interactions\", \"summary\": \"We tackle the novel problem of predicting 3D hand motion and contact maps (or\\nInteraction Trajectories) given a single RGB view, action text, and a 3D\\ncontact point on the object as input. Our approach consists of (1) Interaction\\nCodebook: a VQVAE model to learn a latent codebook of hand poses and contact\\npoints, effectively tokenizing interaction trajectories, (2) Interaction\\nPredictor: a transformer-decoder module to predict the interaction trajectory\\nfrom test time inputs by using an indexer module to retrieve a latent\\naffordance from the learned codebook. To train our model, we develop a data\\nengine that extracts 3D hand poses and contact trajectories from the diverse\\nHoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger\\nthan existing works, in terms of diversity of objects and interactions\\nobserved, and test for generalization of the model across object categories,\\naction categories, tasks, and scenes. Experimental results show the\\neffectiveness of our approach over transformer & diffusion baselines across all\\nsettings.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.LG\", \"published\": \"2025-04-16T17:48:12Z\"}"}

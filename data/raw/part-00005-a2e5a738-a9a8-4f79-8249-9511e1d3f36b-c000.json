{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12673v1\", \"title\": \"ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented\\n  Language Models\", \"summary\": \"Abstractive compression utilizes smaller langauge models to condense\\nquery-relevant context, reducing computational costs in retrieval-augmented\\ngeneration (RAG). However,retrieved documents often include information that is\\neither irrelevant to answering the query or misleading due to factual incorrect\\ncontent, despite having high relevance scores. This behavior indicates that\\nabstractive compressors are more likely to omit important information essential\\nfor the correct answer, especially in long contexts where attention dispersion\\noccurs. To address this issue, we categorize retrieved documents in a more\\nfine-grained manner and propose Abstractive Compression Robust against Noise\\n(ACoRN), which introduces two novel training steps. First, we use offline data\\naugmentation on the training dataset to enhance compressor robustness against\\ntwo distinct types of retrieval noise. Second, since the language modelbased\\ncompressor cannot fully utilize information from multiple retrieved documents\\nand exhibits positional bias, we perform finetuning to generate summaries\\ncentered around key information that directly supports the correct answer. Our\\nexperiments demonstrate that T5-large, trained with ACoRN as a compressor,\\nimproves EM and F1 scores while preserving the answer string, which could serve\\nas direct evidence. ACoRN excels on datasets with many accuracy-reducing\\ndocuments, making it highly useful in real-world scenarios.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-17T06:05:35Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16884v1\", \"title\": \"Do Large Language Models know who did what to whom?\", \"summary\": \"Large Language Models (LLMs) are commonly criticized for not understanding\\nlanguage. However, many critiques focus on cognitive abilities that, in humans,\\nare distinct from language processing. Here, we instead study a kind of\\nunderstanding tightly linked to language: inferring who did what to whom\\n(thematic roles) in a sentence. Does the central training objective of\\nLLMs-word prediction-result in sentence representations that capture thematic\\nroles? In two experiments, we characterized sentence representations in four\\nLLMs. In contrast to human similarity judgments, in LLMs the overall\\nrepresentational similarity of sentence pairs reflected syntactic similarity\\nbut not whether their agent and patient assignments were identical vs.\\nreversed. Furthermore, we found little evidence that thematic role information\\nwas available in any subset of hidden units. However, some attention heads\\nrobustly captured thematic roles, independently of syntax. Therefore, LLMs can\\nextract thematic roles but, relative to humans, this information influences\\ntheir representations more weakly.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-23T17:00:45Z\"}"}

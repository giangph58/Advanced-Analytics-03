{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11295v1\", \"title\": \"Autoregressive Distillation of Diffusion Transformers\", \"summary\": \"Diffusion models with transformer architectures have demonstrated promising\\ncapabilities in generating high-fidelity images and scalability for high\\nresolution. However, iterative sampling process required for synthesis is very\\nresource-intensive. A line of work has focused on distilling solutions to\\nprobability flow ODEs into few-step student models. Nevertheless, existing\\nmethods have been limited by their reliance on the most recent denoised samples\\nas input, rendering them susceptible to exposure bias. To address this\\nlimitation, we propose AutoRegressive Distillation (ARD), a novel approach that\\nleverages the historical trajectory of the ODE to predict future steps. ARD\\noffers two key benefits: 1) it mitigates exposure bias by utilizing a predicted\\nhistorical trajectory that is less susceptible to accumulated errors, and 2) it\\nleverages the previous history of the ODE trajectory as a more effective source\\nof coarse-grained information. ARD modifies the teacher transformer\\narchitecture by adding token-wise time embedding to mark each input from the\\ntrajectory history and employs a block-wise causal attention mask for training.\\nFurthermore, incorporating historical inputs only in lower transformer layers\\nenhances performance and efficiency. We validate the effectiveness of ARD in a\\nclass-conditioned generation on ImageNet and T2I synthesis. Our model achieves\\na $5\\\\times$ reduction in FID degradation compared to the baseline methods while\\nrequiring only 1.1\\\\% extra FLOPs on ImageNet-256. Moreover, ARD reaches FID of\\n1.84 on ImageNet-256 in merely 4 steps and outperforms the publicly available\\n1024p text-to-image distilled models in prompt adherence score with a minimal\\ndrop in FID compared to the teacher. Project page:\\nhttps://github.com/alsdudrla10/ARD.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-15T15:33:49Z\"}"}

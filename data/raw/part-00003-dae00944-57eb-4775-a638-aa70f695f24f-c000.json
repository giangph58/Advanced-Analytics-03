{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02507v1\", \"title\": \"ZClip: Adaptive Spike Mitigation for LLM Pre-Training\", \"summary\": \"Training large language models (LLMs) presents numerous challenges, including\\ngradient instability and loss spikes. These phenomena can lead to catastrophic\\ndivergence, requiring costly checkpoint restoration and data batch skipping.\\nTraditional gradient clipping techniques, such as constant or norm-based\\nmethods, fail to address these issues effectively due to their reliance on\\nfixed thresholds or heuristics, leading to inefficient learning and requiring\\nfrequent manual intervention. In this work, we propose ZClip, an adaptive\\ngradient clipping algorithm that dynamically adjusts the clipping threshold\\nbased on statistical properties of gradient norms over time. Unlike prior\\nreactive strategies, ZClip proactively adapts to training dynamics without\\nmaking any prior assumptions on the scale and the temporal evolution of\\ngradient norms. At its core, it leverages z-score-based anomaly detection to\\nidentify and mitigate large gradient spikes, preventing malignant loss spikes\\nwhile not interfering with convergence otherwise. Our code is available at:\\nhttps://github.com/bluorion-com/ZClip.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL\", \"published\": \"2025-04-03T11:41:55Z\"}"}

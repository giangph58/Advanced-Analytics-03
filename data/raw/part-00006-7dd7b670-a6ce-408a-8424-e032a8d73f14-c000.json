{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10317v1\", \"title\": \"Analysis of Attention in Video Diffusion Transformers\", \"summary\": \"We conduct an in-depth analysis of attention in video diffusion transformers\\n(VDiTs) and report a number of novel findings. We identify three key properties\\nof attention in VDiTs: Structure, Sparsity, and Sinks. Structure: We observe\\nthat attention patterns across different VDiTs exhibit similar structure across\\ndifferent prompts, and that we can make use of the similarity of attention\\npatterns to unlock video editing via self-attention map transfer. Sparse: We\\nstudy attention sparsity in VDiTs, finding that proposed sparsity methods do\\nnot work for all VDiTs, because some layers that are seemingly sparse cannot be\\nsparsified. Sinks: We make the first study of attention sinks in VDiTs,\\ncomparing and contrasting them to attention sinks in language models. We\\npropose a number of future directions that can make use of our insights to\\nimprove the efficiency-quality Pareto frontier for VDiTs.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-14T15:25:37Z\"}"}

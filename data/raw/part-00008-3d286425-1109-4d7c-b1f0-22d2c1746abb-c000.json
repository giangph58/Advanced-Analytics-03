{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11765v1\", \"title\": \"Shared Disk KV Cache Management for Efficient Multi-Instance Inference\\n  in RAG-Powered LLMs\", \"summary\": \"Recent large language models (LLMs) face increasing inference latency as\\ninput context length and model size continue to grow. In particular, the\\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\\nincorporating external knowledge, exacerbates this issue by significantly\\nincreasing the number of input tokens. This expansion in token length leads to\\na substantial rise in computational overhead, particularly during the prefill\\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\\nkey-value (KV) cache to lessen the computational burden during the prefill\\nstage. We also introduce a disk-based shared KV cache management system, called\\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\\nsystem, together with an optimal system configuration, improves both throughput\\nand latency under given resource constraints. Shared RAG-DCache exploits the\\nlocality of documents related to user queries in RAG, as well as the queueing\\ndelay in LLM inference services. It proactively generates and stores disk KV\\ncaches for query-related documents and shares them across multiple LLM\\ninstances to enhance inference performance. In experiments on a single host\\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\\nthroughput and up to a 12~65% reduction in latency, depending on the resource\\nconfiguration.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-16T04:59:18Z\"}"}

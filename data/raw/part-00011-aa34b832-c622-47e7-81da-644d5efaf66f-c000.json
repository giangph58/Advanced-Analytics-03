{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03420v1\", \"title\": \"Mitigating Image Captioning Hallucinations in Vision-Language Models\", \"summary\": \"Hallucinations in vision-language models (VLMs) hinder reliability and\\nreal-world applicability, usually stemming from distribution shifts between\\npretraining data and test samples. Existing solutions, such as retraining or\\nfine-tuning on additional data, demand significant computational resources and\\nlabor-intensive data collection, while ensemble-based methods incur additional\\ncosts by introducing auxiliary VLMs. To address these challenges, we propose a\\nnovel test-time adaptation framework using reinforcement learning to mitigate\\nhallucinations during inference without retraining or any auxiliary VLMs. By\\nupdating only the learnable parameters in the layer normalization of the\\nlanguage model (approximately 0.003% of the model parameters), our method\\nreduces distribution shifts between test samples and pretraining samples. A\\nCLIP-based hallucination evaluation model is proposed to provide dual rewards\\nto VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in\\nhallucination rates on LLaVA and InstructBLIP, respectively. Our approach\\noutperforms state-of-the-art baselines with a 68.3% improvement in\\nhallucination mitigation, demonstrating its effectiveness.\", \"main_category\": \"cs.MM\", \"categories\": \"cs.MM,cs.CV\", \"published\": \"2025-05-06T10:55:21Z\"}"}

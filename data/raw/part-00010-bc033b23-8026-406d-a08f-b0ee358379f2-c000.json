{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12961v1\", \"title\": \"QLLM: Do We Really Need a Mixing Network for Credit Assignment in\\n  Multi-Agent Reinforcement Learning?\", \"summary\": \"Credit assignment has remained a fundamental challenge in multi-agent\\nreinforcement learning (MARL). Previous studies have primarily addressed this\\nissue through value decomposition methods under the centralized training with\\ndecentralized execution paradigm, where neural networks are utilized to\\napproximate the nonlinear relationship between individual Q-values and the\\nglobal Q-value. Although these approaches have achieved considerable success in\\nvarious benchmark tasks, they still suffer from several limitations, including\\nimprecise attribution of contributions, limited interpretability, and poor\\nscalability in high-dimensional state spaces. To address these challenges, we\\npropose a novel algorithm, \\\\textbf{QLLM}, which facilitates the automatic\\nconstruction of credit assignment functions using large language models (LLMs).\\nSpecifically, the concept of \\\\textbf{TFCAF} is introduced, wherein the credit\\nallocation process is represented as a direct and expressive nonlinear\\nfunctional formulation. A custom-designed \\\\textit{coder-evaluator} framework is\\nfurther employed to guide the generation, verification, and refinement of\\nexecutable code by LLMs, significantly mitigating issues such as hallucination\\nand shallow reasoning during inference. Extensive experiments conducted on\\nseveral standard MARL benchmarks demonstrate that the proposed method\\nconsistently outperforms existing state-of-the-art baselines. Moreover, QLLM\\nexhibits strong generalization capability and maintains compatibility with a\\nwide range of MARL algorithms that utilize mixing networks, positioning it as a\\npromising and versatile solution for complex multi-agent scenarios.\", \"main_category\": \"cs.MA\", \"categories\": \"cs.MA,cs.AI\", \"published\": \"2025-04-17T14:07:11Z\"}"}

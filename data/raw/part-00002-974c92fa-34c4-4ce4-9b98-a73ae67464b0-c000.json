{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01470v1\", \"title\": \"Detecting Lip-Syncing Deepfakes: Vision Temporal Transformer for\\n  Analyzing Mouth Inconsistencies\", \"summary\": \"Deepfakes are AI-generated media in which the original content is digitally\\naltered to create convincing but manipulated images, videos, or audio. Among\\nthe various types of deepfakes, lip-syncing deepfakes are one of the most\\nchallenging deepfakes to detect. In these videos, a person's lip movements are\\nsynthesized to match altered or entirely new audio using AI models. Therefore,\\nunlike other types of deepfakes, the artifacts in lip-syncing deepfakes are\\nconfined to the mouth region, making them more subtle and, thus harder to\\ndiscern. In this paper, we propose LIPINC-V2, a novel detection framework that\\nleverages a combination of vision temporal transformer with multihead\\ncross-attention to detect lip-syncing deepfakes by identifying spatiotemporal\\ninconsistencies in the mouth region. These inconsistencies appear across\\nadjacent frames and persist throughout the video. Our model can successfully\\ncapture both short-term and long-term variations in mouth movement, enhancing\\nits ability to detect these inconsistencies. Additionally, we created a new\\nlip-syncing deepfake dataset, LipSyncTIMIT, which was generated using five\\nstate-of-the-art lip-syncing models to simulate real-world scenarios. Extensive\\nexperiments on our proposed LipSyncTIMIT dataset and two other benchmark\\ndeepfake datasets demonstrate that our model achieves state-of-the-art\\nperformance. The code and the dataset are available at\\nhttps://github.com/skrantidatta/LIPINC-V2 .\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-02T08:24:06Z\"}"}

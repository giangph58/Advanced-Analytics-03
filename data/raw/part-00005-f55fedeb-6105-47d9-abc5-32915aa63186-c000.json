{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17622v1\", \"title\": \"Likelihood-Free Variational Autoencoders\", \"summary\": \"Variational Autoencoders (VAEs) typically rely on a probabilistic decoder\\nwith a predefined likelihood, most commonly an isotropic Gaussian, to model the\\ndata conditional on latent variables. While convenient for optimization, this\\nchoice often leads to likelihood misspecification, resulting in blurry\\nreconstructions and poor data fidelity, especially for high-dimensional data\\nsuch as images. In this work, we propose \\\\textit{EnVAE}, a novel\\nlikelihood-free generative framework that has a deterministic decoder and\\nemploys the energy score -- a proper scoring rule -- to build the\\nreconstruction loss. This enables likelihood-free inference without requiring\\nexplicit parametric density functions. To address the computational\\ninefficiency of the energy score, we introduce a fast variant, \\\\textit{FEnVAE},\\nbased on the local smoothness of the decoder and the sharpness of the posterior\\ndistribution of latent variables. This yields an efficient single-sample\\ntraining objective that integrates seamlessly into existing VAE pipelines with\\nminimal overhead. Empirical results on standard benchmarks demonstrate that\\n\\\\textit{EnVAE} achieves superior reconstruction and generation quality compared\\nto likelihood-based baselines. Our framework offers a general, scalable, and\\nstatistically principled alternative for flexible and nonparametric\\ndistribution learning in generative modeling.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.LG\", \"published\": \"2025-04-24T14:44:46Z\"}"}

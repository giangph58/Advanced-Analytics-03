{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12715v1\", \"title\": \"Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based\\n  Code Selection\", \"summary\": \"Graph self-supervised learning has gained significant attention recently.\\nHowever, many existing approaches heavily depend on perturbations, and\\ninappropriate perturbations may corrupt the graph's inherent information. The\\nVector Quantized Variational Autoencoder (VQ-VAE) is a powerful autoencoder\\nextensively used in fields such as computer vision; however, its application to\\ngraph data remains underexplored. In this paper, we provide an empirical\\nanalysis of vector quantization in the context of graph autoencoders,\\ndemonstrating its significant enhancement of the model's capacity to capture\\ngraph topology. Furthermore, we identify two key challenges associated with\\nvector quantization when applying in graph data: codebook underutilization and\\ncodebook space sparsity. For the first challenge, we propose an annealing-based\\nencoding strategy that promotes broad code utilization in the early stages of\\ntraining, gradually shifting focus toward the most effective codes as training\\nprogresses. For the second challenge, we introduce a hierarchical two-layer\\ncodebook that captures relationships between embeddings through clustering. The\\nsecond layer codebook links similar codes, encouraging the model to learn\\ncloser embeddings for nodes with similar features and structural topology in\\nthe graph. Our proposed model outperforms 16 representative baseline methods in\\nself-supervised link prediction and node classification tasks across multiple\\ndatasets.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-17T07:43:52Z\"}"}

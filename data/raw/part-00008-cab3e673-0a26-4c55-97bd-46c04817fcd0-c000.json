{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05119v1\", \"title\": \"Balancing Robustness and Efficiency in Embedded DNNs Through Activation\\n  Function Selection\", \"summary\": \"Machine learning-based embedded systems for safety-critical applications,\\nsuch as aerospace and autonomous driving, must be robust to perturbations\\ncaused by soft errors. As transistor geometries shrink and voltages decrease,\\nmodern electronic devices become more susceptible to background radiation,\\nincreasing the concern about failures produced by soft errors. The resilience\\nof deep neural networks (DNNs) to these errors depends not only on target\\ndevice technology but also on model structure and the numerical representation\\nand arithmetic precision of their parameters. Compression techniques like\\npruning and quantization, used to reduce memory footprint and computational\\ncomplexity, alter both model structure and representation, affecting soft error\\nrobustness. In this regard, although often overlooked, the choice of activation\\nfunctions (AFs) impacts not only accuracy and trainability but also\\ncompressibility and error resilience. This paper explores the use of bounded\\nAFs to enhance robustness against parameter perturbations, while evaluating\\ntheir effects on model accuracy, compressibility, and computational load with a\\ntechnology-agnostic approach. We focus on encoder-decoder convolutional models\\ndeveloped for semantic segmentation of hyperspectral images with application to\\nautonomous driving systems. Experiments are conducted on an AMD-Xilinx's KV260\\nSoM.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.AR,cs.CV,eess.IV\", \"published\": \"2025-04-07T14:21:31Z\"}"}

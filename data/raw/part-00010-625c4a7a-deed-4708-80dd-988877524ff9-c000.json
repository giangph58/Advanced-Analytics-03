{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19638v1\", \"title\": \"LODAP: On-Device Incremental Learning Via Lightweight Operations and\\n  Data Pruning\", \"summary\": \"Incremental learning that learns new classes over time after the model's\\ndeployment is becoming increasingly crucial, particularly for industrial edge\\nsystems, where it is difficult to communicate with a remote server to conduct\\ncomputation-intensive learning. As more classes are expected to learn after\\ntheir execution for edge devices. In this paper, we propose LODAP, a new\\non-device incremental learning framework for edge systems. The key part of\\nLODAP is a new module, namely Efficient Incremental Module (EIM). EIM is\\ncomposed of normal convolutions and lightweight operations. During incremental\\nlearning, EIM exploits some lightweight operations, called adapters, to\\neffectively and efficiently learn features for new classes so that it can\\nimprove the accuracy of incremental learning while reducing model complexity as\\nwell as training overhead. The efficiency of LODAP is further enhanced by a\\ndata pruning strategy that significantly reduces the training data, thereby\\nlowering the training overhead. We conducted extensive experiments on the\\nCIFAR-100 and Tiny- ImageNet datasets. Experimental results show that LODAP\\nimproves the accuracy by up to 4.32\\\\% over existing methods while reducing\\naround 50\\\\% of model complexity. In addition, evaluations on real edge systems\\ndemonstrate its applicability for on-device machine learning. The code is\\navailable at https://github.com/duanbiqing/LODAP.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.ET\", \"published\": \"2025-04-28T09:52:53Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10117v1\", \"title\": \"AGO: Adaptive Grounding for Open World 3D Occupancy Prediction\", \"summary\": \"Open-world 3D semantic occupancy prediction aims to generate a voxelized 3D\\nrepresentation from sensor inputs while recognizing both known and unknown\\nobjects. Transferring open-vocabulary knowledge from vision-language models\\n(VLMs) offers a promising direction but remains challenging. However, methods\\nbased on VLM-derived 2D pseudo-labels with traditional supervision are limited\\nby a predefined label space and lack general prediction capabilities. Direct\\nalignment with pretrained image embeddings, on the other hand, fails to achieve\\nreliable performance due to often inconsistent image and text representations\\nin VLMs. To address these challenges, we propose AGO, a novel 3D occupancy\\nprediction framework with adaptive grounding to handle diverse open-world\\nscenarios. AGO first encodes surrounding images and class prompts into 3D and\\ntext embeddings, respectively, leveraging similarity-based grounding training\\nwith 3D pseudo-labels. Additionally, a modality adapter maps 3D embeddings into\\na space aligned with VLM-derived image embeddings, reducing modality gaps.\\nExperiments on Occ3D-nuScenes show that AGO improves unknown object prediction\\nin zero-shot and few-shot transfer while achieving state-of-the-art\\nclosed-world self-supervised performance, surpassing prior methods by 4.09\\nmIoU.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-14T11:26:20Z\"}"}

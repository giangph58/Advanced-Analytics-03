{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16929v1\", \"title\": \"I-Con: A Unifying Framework for Representation Learning\", \"summary\": \"As the field of representation learning grows, there has been a proliferation\\nof different loss functions to solve different classes of problems. We\\nintroduce a single information-theoretic equation that generalizes a large\\ncollection of modern loss functions in machine learning. In particular, we\\nintroduce a framework that shows that several broad classes of machine learning\\nmethods are precisely minimizing an integrated KL divergence between two\\nconditional distributions: the supervisory and learned representations. This\\nviewpoint exposes a hidden information geometry underlying clustering, spectral\\nmethods, dimensionality reduction, contrastive learning, and supervised\\nlearning. This framework enables the development of new loss functions by\\ncombining successful techniques from across the literature. We not only present\\na wide array of proofs, connecting over 23 different approaches, but we also\\nleverage these theoretical results to create state-of-the-art unsupervised\\nimage classifiers that achieve a +8% improvement over the prior\\nstate-of-the-art on unsupervised classification on ImageNet-1K. We also\\ndemonstrate that I-Con can be used to derive principled debiasing methods which\\nimprove contrastive representation learners.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CV,cs.IT,math.IT\", \"published\": \"2025-04-23T17:59:01Z\"}"}

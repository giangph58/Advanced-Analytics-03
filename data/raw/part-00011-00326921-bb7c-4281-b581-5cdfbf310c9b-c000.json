{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11109v1\", \"title\": \"Fine-Tuning Large Language Models on Quantum Optimization Problems for\\n  Circuit Generation\", \"summary\": \"Large language models (LLM) have achieved remarkable outcomes in addressing\\ncomplex problems, including math, coding, and analyzing large amounts of\\nscientific reports. Yet few works have explored the potential of LLM in quantum\\ncomputing. The most challenging problem is how to leverage LLMs to\\nautomatically generate quantum circuits at a large scale. In this paper, we\\naddress such a challenge by fine-tuning LLMs and injecting the domain-specific\\nknowledge of quantum computing. In particular, we investigate the mechanisms to\\ngenerate training data sets and construct the end-to-end pipeline to fine-tune\\npre-trained LLMs that produce parameterized quantum circuits for optimization\\nproblems. We have prepared 14,000 quantum circuits covering a substantial part\\nof the quantum optimization landscape: 12 optimization problem instances and\\ntheir optimized QAOA, VQE, and adaptive VQE circuits. The fine-tuned LLMs can\\nconstruct syntactically correct parametrized quantum circuits in the most\\nrecent OpenQASM 3.0. We have evaluated the quality of the parameters by\\ncomparing them to the optimized expectation values and distributions. Our\\nevaluation shows that the fine-tuned LLM outperforms state-of-the-art models\\nand that the parameters are better than random. The LLM-generated parametrized\\ncircuits and initial parameters can be used as a starting point for further\\noptimization, \\\\emph{e.g.,} templates in quantum machine learning and the\\nbenchmark for compilers and hardware.\", \"main_category\": \"quant-ph\", \"categories\": \"quant-ph,cs.AI\", \"published\": \"2025-04-15T11:56:54Z\"}"}

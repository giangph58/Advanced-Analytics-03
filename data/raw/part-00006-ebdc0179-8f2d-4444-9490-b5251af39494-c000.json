{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12018v1\", \"title\": \"Instruction-augmented Multimodal Alignment for Image-Text and Element\\n  Matching\", \"summary\": \"With the rapid advancement of text-to-image (T2I) generation models,\\nassessing the semantic alignment between generated images and text descriptions\\nhas become a significant research challenge. Current methods, including those\\nbased on Visual Question Answering (VQA), still struggle with fine-grained\\nassessments and precise quantification of image-text alignment. This paper\\npresents an improved evaluation method named Instruction-augmented Multimodal\\nAlignment for Image-Text and Element Matching (iMatch), which evaluates\\nimage-text semantic alignment by fine-tuning multimodal large language models.\\nWe introduce four innovative augmentation strategies: First, the QAlign\\nstrategy creates a precise probabilistic mapping to convert discrete scores\\nfrom multimodal large language models into continuous matching scores. Second,\\na validation set augmentation strategy uses pseudo-labels from model\\npredictions to expand training data, boosting the model's generalization\\nperformance. Third, an element augmentation strategy integrates element\\ncategory labels to refine the model's understanding of image-text matching.\\nFourth, an image augmentation strategy employs techniques like random lighting\\nto increase the model's robustness. Additionally, we propose prompt type\\naugmentation and score perturbation strategies to further enhance the accuracy\\nof element assessments. Our experimental results show that the iMatch method\\nsignificantly surpasses existing methods, confirming its effectiveness and\\npractical value. Furthermore, our iMatch won first place in the CVPR NTIRE 2025\\nText to Image Generation Model Quality Assessment - Track 1 Image-Text\\nAlignment.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-16T12:21:49Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16084v1\", \"title\": \"TTRL: Test-Time Reinforcement Learning\", \"summary\": \"This paper investigates Reinforcement Learning (RL) on data without explicit\\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\\nof the problem is reward estimation during inference while not having access to\\nground-truth information. While this setting appears elusive, we find that\\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\\nsurprisingly effective rewards suitable for driving RL training. In this work,\\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\\nthat TTRL consistently improves performance across a variety of tasks and\\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\\napproximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,\\nalthough TTRL is only supervised by the Maj@N metric, TTRL has demonstrated\\nperformance to consistently surpass the upper limit of the initial model, and\\napproach the performance of models trained directly on test data with\\nground-truth labels. Our experimental findings validate the general\\neffectiveness of TTRL across various tasks, and highlight TTRL's potential for\\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-22T17:59:56Z\"}"}

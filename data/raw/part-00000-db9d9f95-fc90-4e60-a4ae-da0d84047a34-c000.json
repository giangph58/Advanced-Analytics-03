{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16580v1\", \"title\": \"Hyper-Transforming Latent Diffusion Models\", \"summary\": \"We introduce a novel generative framework for functions by integrating\\nImplicit Neural Representations (INRs) and Transformer-based hypernetworks into\\nlatent variable models. Unlike prior approaches that rely on MLP-based\\nhypernetworks with scalability limitations, our method employs a\\nTransformer-based decoder to generate INR parameters from latent variables,\\naddressing both representation capacity and computational efficiency. Our\\nframework extends latent diffusion models (LDMs) to INR generation by replacing\\nstandard decoders with a Transformer-based hypernetwork, which can be trained\\neither from scratch or via hyper-transforming-a strategy that fine-tunes only\\nthe decoder while freezing the pre-trained latent space. This enables efficient\\nadaptation of existing generative models to INR-based representations without\\nrequiring full retraining.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,stat.ML\", \"published\": \"2025-04-23T10:01:18Z\"}"}

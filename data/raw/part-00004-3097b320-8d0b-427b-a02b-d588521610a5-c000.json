{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19521v1\", \"title\": \"Security Steerability is All You Need\", \"summary\": \"The adoption of Generative AI (GenAI) in various applications inevitably\\ncomes with expanding the attack surface, combining new security threats along\\nwith the traditional ones. Consequently, numerous research and industrial\\ninitiatives aim to mitigate these security threats in GenAI by developing\\nmetrics and designing defenses. However, while most of the GenAI security work\\nfocuses on universal threats (e.g. manipulating the LLM to generate forbidden\\ncontent), there is significantly less discussion on application-level security\\nand how to mitigate it.\\n  Thus, in this work we adopt an application-centric approach to GenAI\\nsecurity, and show that while LLMs cannot protect against ad-hoc application\\nspecific threats, they can provide the framework for applications to protect\\nthemselves against such threats. Our first contribution is defining Security\\nSteerability - a novel security measure for LLMs, assessing the model's\\ncapability to adhere to strict guardrails that are defined in the system prompt\\n('Refrain from discussing about politics'). These guardrails, in case\\neffective, can stop threats in the presence of malicious users who attempt to\\ncircumvent the application and cause harm to its providers.\\n  Our second contribution is a methodology to measure the security steerability\\nof LLMs, utilizing two newly-developed datasets: VeganRibs assesses the LLM\\nbehavior in forcing specific guardrails that are not security per se in the\\npresence of malicious user that uses attack boosters (jailbreaks and\\nperturbations), and ReverseText takes this approach further and measures the\\nLLM ability to force specific treatment of the user input as plain text while\\ndo user try to give it additional meanings...\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR\", \"published\": \"2025-04-28T06:40:01Z\"}"}

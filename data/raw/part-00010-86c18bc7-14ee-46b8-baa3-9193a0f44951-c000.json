{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00358v1\", \"title\": \"R&B: Domain Regrouping and Data Mixture Balancing for Efficient\\n  Foundation Model Training\", \"summary\": \"Data mixing strategies have successfully reduced the costs involved in\\ntraining language models. While promising, such methods suffer from two flaws.\\nFirst, they rely on predetermined data domains (e.g., data sources, task\\ntypes), which may fail to capture critical semantic nuances, leaving\\nperformance on the table. Second, these methods scale with the number of\\ndomains in a computationally prohibitive way. We address these challenges via\\nR&B, a framework that re-partitions training data based on semantic similarity\\n(Regroup) to create finer-grained domains, and efficiently optimizes the data\\ncomposition (Balance) by leveraging a Gram matrix induced by domain gradients\\nobtained throughout training. Unlike prior works, it removes the need for\\nadditional compute to obtain evaluation information such as losses or\\ngradients. We analyze this technique under standard regularity conditions and\\nprovide theoretical insights that justify R&B's effectiveness compared to\\nnon-adaptive mixing approaches. Empirically, we demonstrate the effectiveness\\nof R&B on five diverse datasets ranging from natural language to reasoning and\\nmultimodal tasks. With as little as 0.01% additional compute overhead, R&B\\nmatches or exceeds the performance of state-of-the-art data mixing strategies.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL\", \"published\": \"2025-05-01T07:08:19Z\"}"}

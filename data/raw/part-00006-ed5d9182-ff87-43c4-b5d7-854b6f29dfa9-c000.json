{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11972v1\", \"title\": \"LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA\", \"summary\": \"Extractive reading comprehension question answering (QA) datasets are\\ntypically evaluated using Exact Match (EM) and F1-score, but these metrics\\noften fail to fully capture model performance. With the success of large\\nlanguage models (LLMs), they have been employed in various tasks, including\\nserving as judges (LLM-as-a-judge). In this paper, we reassess the performance\\nof QA models using LLM-as-a-judge across four reading comprehension QA\\ndatasets. We examine different families of LLMs and various answer types to\\nevaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show\\nthat LLM-as-a-judge is highly correlated with human judgments and can replace\\ntraditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human\\njudgments improves significantly, from 0.17 (EM) and 0.36 (F1-score) to 0.85.\\nThese findings confirm that EM and F1 metrics underestimate the true\\nperformance of the QA models. While LLM-as-a-judge is not perfect for more\\ndifficult answer types (e.g., job), it still outperforms EM/F1, and we observe\\nno bias issues, such as self-preference, when the same model is used for both\\nthe QA and judgment tasks.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-16T11:08:46Z\"}"}

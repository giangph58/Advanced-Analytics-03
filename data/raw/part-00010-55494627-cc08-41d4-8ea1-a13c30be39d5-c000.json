{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00568v1\", \"title\": \"Multimodal Masked Autoencoder Pre-training for 3D MRI-Based Brain Tumor\\n  Analysis with Missing Modalities\", \"summary\": \"Multimodal magnetic resonance imaging (MRI) constitutes the first line of\\ninvestigation for clinicians in the care of brain tumors, providing crucial\\ninsights for surgery planning, treatment monitoring, and biomarker\\nidentification. Pre-training on large datasets have been shown to help models\\nlearn transferable representations and adapt with minimal labeled data. This\\nbehavior is especially valuable in medical imaging, where annotations are often\\nscarce. However, applying this paradigm to multimodal medical data introduces a\\nchallenge: most existing approaches assume that all imaging modalities are\\navailable during both pre-training and fine-tuning. In practice, missing\\nmodalities often occur due to acquisition issues, specialist unavailability, or\\nspecific experimental designs on small in-house datasets. Consequently, a\\ncommon approach involves training a separate model for each desired modality\\ncombination, making the process both resource-intensive and impractical for\\nclinical use. Therefore, we introduce BM-MAE, a masked image modeling\\npre-training strategy tailored for multimodal MRI data. The same pre-trained\\nmodel seamlessly adapts to any combination of available modalities, extracting\\nrich representations that capture both intra- and inter-modal information. This\\nallows fine-tuning on any subset of modalities without requiring architectural\\nchanges, while still benefiting from a model pre-trained on the full set of\\nmodalities. Extensive experiments show that the proposed pre-training strategy\\noutperforms or remains competitive with baselines that require separate\\npre-training for each modality subset, while substantially surpassing training\\nfrom scratch on several downstream tasks. Additionally, it can quickly and\\nefficiently reconstruct missing modalities, highlighting its practical value.\\nCode and trained models are available at: https://github.com/Lucas-rbnt/bmmae\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-05-01T14:51:30Z\"}"}

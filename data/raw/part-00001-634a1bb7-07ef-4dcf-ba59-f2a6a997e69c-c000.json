{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04608v1\", \"title\": \"WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via\\n  Weighted-Conformal Martingales\", \"summary\": \"Responsibly deploying artificial intelligence (AI) / machine learning (ML)\\nsystems in high-stakes settings arguably requires not only proof of system\\nreliability, but moreover continual, post-deployment monitoring to quickly\\ndetect and address any unsafe behavior. Statistical methods for nonparametric\\nchange-point detection -- especially the tools of conformal test martingales\\n(CTMs) and anytime-valid inference -- offer promising approaches to this\\nmonitoring task. However, existing methods are restricted to monitoring limited\\nhypothesis classes or ``alarm criteria,'' such as data shifts that violate\\ncertain exchangeability assumptions, or do not allow for online adaptation in\\nresponse to shifts. In this paper, we expand the scope of these monitoring\\nmethods by proposing a weighted generalization of conformal test martingales\\n(WCTMs), which lay a theoretical foundation for online monitoring for any\\nunexpected changepoints in the data distribution while controlling\\nfalse-alarms. For practical applications, we propose specific WCTM algorithms\\nthat accommodate online adaptation to mild covariate shifts (in the marginal\\ninput distribution) while raising alarms in response to more severe shifts,\\nsuch as concept shifts (in the conditional label distribution) or extreme\\n(out-of-support) covariate shifts that cannot be easily adapted to. On\\nreal-world datasets, we demonstrate improved performance relative to\\nstate-of-the-art baselines.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,stat.ML\", \"published\": \"2025-05-07T17:53:47Z\"}"}

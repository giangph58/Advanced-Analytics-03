{"value":"{\"aid\": \"http://arxiv.org/abs/2504.09897v1\", \"title\": \"TAMP: Token-Adaptive Layerwise Pruning in Multimodal Large Language\\n  Models\", \"summary\": \"Multimodal Large Language Models (MLLMs) have shown remarkable versatility in\\nunderstanding diverse multimodal data and tasks. However, these capabilities\\ncome with an increased model scale. While post-training pruning reduces model\\nsize in unimodal models, its application to MLLMs often yields limited success.\\nOur analysis discovers that conventional methods fail to account for the unique\\ntoken attributes across layers and modalities inherent to MLLMs. Inspired by\\nthis observation, we propose TAMP, a simple yet effective pruning framework\\ntailored for MLLMs, featuring two key components: (1) Diversity-Aware Sparsity,\\nwhich adjusts sparsity ratio per layer based on diversities among multimodal\\noutput tokens, preserving more parameters in high-diversity layers; and (2)\\nAdaptive Multimodal Input Activation, which identifies representative\\nmultimodal input tokens using attention scores to guide unstructured weight\\npruning. We validate our method on two state-of-the-art MLLMs: LLaVA-NeXT,\\ndesigned for vision-language tasks, and VideoLLaMA2, capable of processing\\naudio, visual, and language modalities. Empirical experiments across various\\nmultimodal evaluation benchmarks demonstrate that each component of our\\napproach substantially outperforms existing pruning techniques.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-14T05:44:38Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17271v1\", \"title\": \"Contrastive Learning for Continuous Touch-Based Authentication\", \"summary\": \"Smart mobile devices have become indispensable in modern daily life, where\\nsensitive information is frequently processed, stored, and transmitted-posing\\ncritical demands for robust security controls. Given that touchscreens are the\\nprimary medium for human-device interaction, continuous user authentication\\nbased on touch behavior presents a natural and seamless security solution.\\nWhile existing methods predominantly adopt binary classification under\\nsingle-modal learning settings, we propose a unified contrastive learning\\nframework for continuous authentication in a non-disruptive manner.\\nSpecifically, the proposed method leverages a Temporal Masked Autoencoder to\\nextract temporal patterns from raw multi-sensor data streams, capturing\\ncontinuous motion and gesture dynamics. The pre-trained TMAE is subsequently\\nintegrated into a Siamese Temporal-Attentive Convolutional Network within a\\ncontrastive learning paradigm to model both sequential and cross-modal\\npatterns. To further enhance performance, we incorporate multi-head attention\\nand channel attention mechanisms to capture long-range dependencies and\\noptimize inter-channel feature integration. Extensive experiments on public\\nbenchmarks and a self-collected dataset demonstrate that our approach\\noutperforms state-of-the-art methods, offering a reliable and effective\\nsolution for user authentication on mobile devices.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR\", \"published\": \"2025-04-24T05:58:12Z\"}"}

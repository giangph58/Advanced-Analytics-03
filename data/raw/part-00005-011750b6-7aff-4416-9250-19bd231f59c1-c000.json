{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11944v1\", \"title\": \"VIPO: Value Function Inconsistency Penalized Offline Reinforcement\\n  Learning\", \"summary\": \"Offline reinforcement learning (RL) learns effective policies from\\npre-collected datasets, offering a practical solution for applications where\\nonline interactions are risky or costly. Model-based approaches are\\nparticularly advantageous for offline RL, owing to their data efficiency and\\ngeneralizability. However, due to inherent model errors, model-based methods\\noften artificially introduce conservatism guided by heuristic uncertainty\\nestimation, which can be unreliable. In this paper, we introduce VIPO, a novel\\nmodel-based offline RL algorithm that incorporates self-supervised feedback\\nfrom value estimation to enhance model training. Specifically, the model is\\nlearned by additionally minimizing the inconsistency between the value learned\\ndirectly from the offline data and the one estimated from the model. We perform\\ncomprehensive evaluations from multiple perspectives to show that VIPO can\\nlearn a highly accurate model efficiently and consistently outperform existing\\nmethods. It offers a general framework that can be readily integrated into\\nexisting model-based offline RL algorithms to systematically enhance model\\naccuracy. As a result, VIPO achieves state-of-the-art performance on almost all\\ntasks in both D4RL and NeoRL benchmarks.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-16T10:23:44Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16511v1\", \"title\": \"QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM\\n  Pretraining\", \"summary\": \"Quality and diversity are two critical metrics for the training data of large\\nlanguage models (LLMs), positively impacting performance. Existing studies\\noften optimize these metrics separately, typically by first applying quality\\nfiltering and then adjusting data proportions. However, these approaches\\noverlook the inherent trade-off between quality and diversity, necessitating\\ntheir joint consideration. Given a fixed training quota, it is essential to\\nevaluate both the quality of each data point and its complementary effect on\\nthe overall dataset. In this paper, we introduce a unified data selection\\nframework called QuaDMix, which automatically optimizes the data distribution\\nfor LLM pretraining while balancing both quality and diversity. Specifically,\\nwe first propose multiple criteria to measure data quality and employ domain\\nclassification to distinguish data points, thereby measuring overall diversity.\\nQuaDMix then employs a unified parameterized data sampling function that\\ndetermines the sampling probability of each data point based on these quality\\nand diversity related labels. To accelerate the search for the optimal\\nparameters involved in the QuaDMix framework, we conduct simulated experiments\\non smaller models and use LightGBM for parameters searching, inspired by the\\nRegMix method. Our experiments across diverse models and datasets demonstrate\\nthat QuaDMix achieves an average performance improvement of 7.2% across\\nmultiple benchmarks. These results outperform the independent strategies for\\nquality and diversity, highlighting the necessity and ability to balance data\\nquality and diversity.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-23T08:36:50Z\"}"}

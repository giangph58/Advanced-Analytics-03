{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03156v1\", \"title\": \"Soft Best-of-n Sampling for Model Alignment\", \"summary\": \"Best-of-$n$ (BoN) sampling is a practical approach for aligning language\\nmodel outputs with human preferences without expensive fine-tuning. BoN\\nsampling is performed by generating $n$ responses to a prompt and then\\nselecting the sample that maximizes a reward function. BoN yields high reward\\nvalues in practice at a distortion cost, as measured by the KL-divergence\\nbetween the sampled and original distribution. This distortion is coarsely\\ncontrolled by varying the number of samples: larger $n$ yields a higher reward\\nat a higher distortion cost. We introduce Soft Best-of-$n$ sampling, a\\ngeneralization of BoN that allows for smooth interpolation between the original\\ndistribution and reward-maximizing distribution through a temperature parameter\\n$\\\\lambda$. We establish theoretical guarantees showing that Soft Best-of-$n$\\nsampling converges sharply to the optimal tilted distribution at a rate of\\n$O(1/n)$ in KL and the expected (relative) reward. For sequences of discrete\\noutputs, we analyze an additive reward model that reveals the fundamental\\nlimitations of blockwise sampling.\", \"main_category\": \"cs.IT\", \"categories\": \"cs.IT,cs.AI,math.IT\", \"published\": \"2025-05-06T04:03:11Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07912v1\", \"title\": \"Echo Chamber: RL Post-training Amplifies Behaviors Learned in\\n  Pretraining\", \"summary\": \"Reinforcement learning (RL)-based fine-tuning has become a crucial step in\\npost-training language models for advanced mathematical reasoning and coding.\\nFollowing the success of frontier reasoning models, recent work has\\ndemonstrated that RL fine-tuning consistently improves performance, even in\\nsmaller-scale models; however, the underlying mechanisms driving these\\nimprovements are not well-understood. Understanding the effects of RL\\nfine-tuning requires disentangling its interaction with pretraining data\\ncomposition, hyperparameters, and model scale, but such problems are\\nexacerbated by the lack of transparency regarding the training data used in\\nmany existing models. In this work, we present a systematic end-to-end study of\\nRL fine-tuning for mathematical reasoning by training models entirely from\\nscratch on different mixtures of fully open datasets. We investigate the\\neffects of various RL fine-tuning algorithms (PPO, GRPO, and Expert Iteration)\\nacross models of different scales. Our study reveals that RL algorithms\\nconsistently converge towards a dominant output distribution, amplifying\\npatterns in the pretraining data. We also find that models of different scales\\ntrained on the same data mixture will converge to distinct output\\ndistributions, suggesting that there are scale-dependent biases in model\\ngeneralization. Moreover, we find that RL post-training on simpler questions\\ncan lead to performance gains on harder ones, indicating that certain reasoning\\ncapabilities generalize across tasks. Our findings show that small-scale\\nproxies in controlled settings can elicit interesting insights regarding the\\nrole of RL in shaping language model behavior.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,I.2.7\", \"published\": \"2025-04-10T17:15:53Z\"}"}

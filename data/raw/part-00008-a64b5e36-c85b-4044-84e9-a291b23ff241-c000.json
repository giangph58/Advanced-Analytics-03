{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16778v1\", \"title\": \"Evaluation Framework for AI Systems in \\\"the Wild\\\"\", \"summary\": \"Generative AI (GenAI) models have become vital across industries, yet current\\nevaluation methods have not adapted to their widespread use. Traditional\\nevaluations often rely on benchmarks and fixed datasets, frequently failing to\\nreflect real-world performance, which creates a gap between lab-tested outcomes\\nand practical applications. This white paper proposes a comprehensive framework\\nfor how we should evaluate real-world GenAI systems, emphasizing diverse,\\nevolving inputs and holistic, dynamic, and ongoing assessment approaches. The\\npaper offers guidance for practitioners on how to design evaluation methods\\nthat accurately reflect real-time capabilities, and provides policymakers with\\nrecommendations for crafting GenAI policies focused on societal impacts, rather\\nthan fixed performance numbers or parameter sizes. We advocate for holistic\\nframeworks that integrate performance, fairness, and ethics and the use of\\ncontinuous, outcome-oriented methods that combine human and automated\\nassessments while also being transparent to foster trust among stakeholders.\\nImplementing these strategies ensures GenAI models are not only technically\\nproficient but also ethically responsible and impactful.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.CY\", \"published\": \"2025-04-23T14:52:39Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19730v1\", \"title\": \"Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial\\n  Attacks Using LLM-as-a-Judge\", \"summary\": \"The widespread adoption of code language models in software engineering tasks\\nhas exposed vulnerabilities to adversarial attacks, especially the identifier\\nsubstitution attacks. Although existing identifier substitution attackers\\ndemonstrate high success rates, they often produce adversarial examples with\\nunnatural code patterns. In this paper, we systematically assess the quality of\\nadversarial examples using LLM-as-a-Judge. Our analysis reveals that over 80%\\nof adversarial examples generated by state-of-the-art identifier substitution\\nattackers (e.g., ALERT) are actually detectable. Based on this insight, we\\npropose EP-Shield, a unified framework for evaluating and purifying identifier\\nsubstitution attacks via naturalness-aware reasoning. Specifically, we first\\nevaluate the naturalness of code and identify the perturbed adversarial code,\\nthen purify it so that the victim model can restore correct prediction.\\nExtensive experiments demonstrate the superiority of EP-Shield over adversarial\\nfine-tuning (up to 83.36% improvement) and its lightweight design 7B\\nparameters) with GPT-4-level performance.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE,cs.CL\", \"published\": \"2025-04-28T12:28:55Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15751v1\", \"title\": \"GADS: A Super Lightweight Model for Head Pose Estimation\", \"summary\": \"In human-computer interaction, head pose estimation profoundly influences\\napplication functionality. Although utilizing facial landmarks is valuable for\\nthis purpose, existing landmark-based methods prioritize precision over\\nsimplicity and model size, limiting their deployment on edge devices and in\\ncompute-poor environments. To bridge this gap, we propose \\\\textbf{Grouped\\nAttention Deep Sets (GADS)}, a novel architecture based on the Deep Set\\nframework. By grouping landmarks into regions and employing small Deep Set\\nlayers, we reduce computational complexity. Our multihead attention mechanism\\nextracts and combines inter-group information, resulting in a model that is\\n$7.5\\\\times$ smaller and executes $25\\\\times$ faster than the current lightest\\nstate-of-the-art model. Notably, our method achieves an impressive reduction,\\nbeing $4321\\\\times$ smaller than the best-performing model. We introduce vanilla\\nGADS and Hybrid-GADS (landmarks + RGB) and evaluate our models on three\\nbenchmark datasets -- AFLW2000, BIWI, and 300W-LP. We envision our architecture\\nas a robust baseline for resource-constrained head pose estimation methods.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-22T09:53:25Z\"}"}

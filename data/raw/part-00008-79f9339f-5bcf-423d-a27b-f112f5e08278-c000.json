{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17448v1\", \"title\": \"CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated\\n  Active Learning\", \"summary\": \"Active learning (AL) reduces human annotation costs for machine learning\\nsystems by strategically selecting the most informative unlabeled data for\\nannotation, but performing it individually may still be insufficient due to\\nrestricted data diversity and annotation budget. Federated Active Learning\\n(FAL) addresses this by facilitating collaborative data selection and model\\ntraining, while preserving the confidentiality of raw data samples. Yet,\\nexisting FAL methods fail to account for the heterogeneity of data distribution\\nacross clients and the associated fluctuations in global and local model\\nparameters, adversely affecting model accuracy. To overcome these challenges,\\nwe propose CHASe (Client Heterogeneity-Aware Data Selection), specifically\\ndesigned for FAL. CHASe focuses on identifying those unlabeled samples with\\nhigh epistemic variations (EVs), which notably oscillate around the decision\\nboundaries during training. To achieve both effectiveness and efficiency,\\n\\\\model{} encompasses techniques for 1) tracking EVs by analyzing inference\\ninconsistencies across training epochs, 2) calibrating decision boundaries of\\ninaccurate models with a new alignment loss, and 3) enhancing data selection\\nefficiency via a data freeze and awaken mechanism with subset sampling.\\nExperiments show that CHASe surpasses various established baselines in terms of\\neffectiveness and efficiency, validated across diverse datasets, model\\ncomplexities, and heterogeneous federation settings.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.DB,cs.DC\", \"published\": \"2025-04-24T11:28:00Z\"}"}

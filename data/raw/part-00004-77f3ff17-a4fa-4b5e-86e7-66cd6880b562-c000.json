{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16586v1\", \"title\": \"Learning Switchable Priors for Neural Image Compression\", \"summary\": \"Neural image compression (NIC) usually adopts a predefined family of\\nprobabilistic distributions as the prior of the latent variables, and meanwhile\\nrelies on entropy models to estimate the parameters for the probabilistic\\nfamily. More complex probabilistic distributions may fit the latent variables\\nmore accurately, but also incur higher complexity of the entropy models,\\nlimiting their practical value. To address this dilemma, we propose a solution\\nto decouple the entropy model complexity from the prior distributions. We use a\\nfinite set of trainable priors that correspond to samples of the parametric\\nprobabilistic distributions. We train the entropy model to predict the index of\\nthe appropriate prior within the set, rather than the specific parameters.\\nSwitching between the trained priors further enables us to embrace a skip mode\\ninto the prior set, which simply omits a latent variable during the entropy\\ncoding. To demonstrate the practical value of our solution, we present a\\nlightweight NIC model, namely FastNIC, together with the learning of switchable\\npriors. FastNIC obtains a better trade-off between compression efficiency and\\ncomputational complexity for neural image compression. We also implanted the\\nswitchable priors into state-of-the-art NIC models and observed improved\\ncompression efficiency with a significant reduction of entropy coding\\ncomplexity.\", \"main_category\": \"cs.MM\", \"categories\": \"cs.MM\", \"published\": \"2025-04-23T10:06:58Z\"}"}

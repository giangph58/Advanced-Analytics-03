{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20595v1\", \"title\": \"ReasonIR: Training Retrievers for Reasoning Tasks\", \"summary\": \"We present ReasonIR-8B, the first retriever specifically trained for general\\nreasoning tasks. Existing retrievers have shown limited gains on reasoning\\ntasks, in part because existing training datasets focus on short factual\\nqueries tied to documents that straightforwardly answer them. We develop a\\nsynthetic data generation pipeline that, for each document, our pipeline\\ncreates a challenging and relevant query, along with a plausibly related but\\nultimately unhelpful hard negative. By training on a mixture of our synthetic\\ndata and existing public data, ReasonIR-8B achieves a new state-of-the-art of\\n29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a\\nwidely-used reasoning-intensive information retrieval (IR) benchmark. When\\napplied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%\\nand 22.6% respectively, relative to the closed-book baseline, outperforming\\nother retrievers and search engines. In addition, ReasonIR-8B uses test-time\\ncompute more effectively: on BRIGHT, its performance consistently increases\\nwith longer and more information-rich rewritten queries; it continues to\\noutperform other retrievers when combined with an LLM reranker. Our training\\nrecipe is general and can be easily extended to future LLMs; to this end, we\\nopen-source our code, data, and model.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CL,cs.IR,cs.LG\", \"published\": \"2025-04-29T09:49:28Z\"}"}

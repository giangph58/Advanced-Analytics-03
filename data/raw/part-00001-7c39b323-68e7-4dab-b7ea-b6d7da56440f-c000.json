{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24164v1\", \"title\": \"SVLA: A Unified Speech-Vision-Language Assistant with Multimodal\\n  Reasoning and Speech Generation\", \"summary\": \"Large vision and language models show strong performance in tasks like image\\ncaptioning, visual question answering, and retrieval. However, challenges\\nremain in integrating speech, text, and vision into a unified model, especially\\nfor spoken tasks. Speech generation methods vary (some produce speech\\ndirectly), others through text (but their impact on quality is unclear).\\nEvaluation often relies on automatic speech recognition, which may introduce\\nbias. We propose SVLA, a unified speech vision language model based on a\\ntransformer architecture that handles multimodal inputs and outputs. We train\\nit on 38.2 million speech text image examples, including 64.1 hours of\\nsynthetic speech. We also introduce Speech VQA Accuracy, a new metric for\\nevaluating spoken responses. SVLA improves multimodal understanding and\\ngeneration by better combining speech, vision, and language.\", \"main_category\": \"cs.MM\", \"categories\": \"cs.MM\", \"published\": \"2025-03-31T14:46:34Z\"}"}

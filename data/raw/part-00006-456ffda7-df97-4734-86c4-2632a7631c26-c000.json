{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16801v1\", \"title\": \"Decoupled Global-Local Alignment for Improving Compositional\\n  Understanding\", \"summary\": \"Contrastive Language-Image Pre-training (CLIP) has achieved success on\\nmultiple downstream tasks by aligning image and text modalities. However, the\\nnature of global contrastive learning limits CLIP's ability to comprehend\\ncompositional concepts, such as relations and attributes. Although recent\\nstudies employ global hard negative samples to improve compositional\\nunderstanding, these methods significantly compromise the model's inherent\\ngeneral capabilities by forcibly distancing textual negative samples from\\nimages in the embedding space. To overcome this limitation, we introduce a\\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\\nunderstanding while substantially mitigating losses in general capabilities. To\\noptimize the retention of the model's inherent capabilities, we incorporate a\\nself-distillation mechanism within the global alignment process, aligning the\\nlearnable image-text encoder with a frozen teacher model derived from an\\nexponential moving average. Under the constraint of self-distillation, it\\neffectively mitigates the catastrophic forgetting of pretrained knowledge\\nduring fine-tuning. To improve compositional understanding, we first leverage\\nthe in-context learning capability of Large Language Models (LLMs) to construct\\nabout 2M high-quality negative captions across five types. Subsequently, we\\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\\nloss to enhance vision-language compositionally. Extensive experimental results\\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\\nperformance improvement of 13.0% on zero-shot classification tasks across\\neleven datasets. Our code will be released at\\nhttps://github.com/xiaoxing2001/DeGLA\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-23T15:20:53Z\"}"}

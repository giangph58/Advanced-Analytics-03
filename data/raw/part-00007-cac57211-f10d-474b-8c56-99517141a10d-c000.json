{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19561v1\", \"title\": \"Quantifying Memory Utilization with Effective State-Size\", \"summary\": \"The need to develop a general framework for architecture analysis is becoming\\nincreasingly important, given the expanding design space of sequence models. To\\nthis end, we draw insights from classical signal processing and control theory,\\nto develop a quantitative measure of \\\\textit{memory utilization}: the internal\\nmechanisms through which a model stores past information to produce future\\noutputs. This metric, which we call \\\\textbf{\\\\textit{effective state-size}}\\n(ESS), is tailored to the fundamental class of systems with\\n\\\\textit{input-invariant} and \\\\textit{input-varying linear operators},\\nencompassing a variety of computational units such as variants of attention,\\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\\neither relies on raw operator visualizations (e.g. attention maps), or simply\\nthe total \\\\textit{memory capacity} (i.e. cache size) of a model, our metrics\\nprovide highly interpretable and actionable measurements. In particular, we\\nshow how ESS can be leveraged to improve initialization strategies, inform\\nnovel regularizers and advance the performance-efficiency frontier through\\nmodel distillation. Furthermore, we demonstrate that the effect of context\\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\\ndifferences in how large language models utilize their available memory to\\nrecall information. Overall, we find that ESS provides valuable insights into\\nthe dynamics that dictate memory utilization, enabling the design of more\\nefficient and effective sequence models.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-28T08:12:30Z\"}"}

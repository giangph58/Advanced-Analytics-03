{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06884v1\", \"title\": \"Audio-visual Event Localization on Portrait Mode Short Videos\", \"summary\": \"Audio-visual event localization (AVEL) plays a critical role in multimodal\\nscene understanding. While existing datasets for AVEL predominantly comprise\\nlandscape-oriented long videos with clean and simple audio context, short\\nvideos have become the primary format of online video content due to the the\\nproliferation of smartphones. Short videos are characterized by\\nportrait-oriented framing and layered audio compositions (e.g., overlapping\\nsound effects, voiceovers, and music), which brings unique challenges\\nunaddressed by conventional methods. To this end, we introduce AVE-PM, the\\nfirst AVEL dataset specifically designed for portrait mode short videos,\\ncomprising 25,335 clips that span 86 fine-grained categories with frame-level\\nannotations. Beyond dataset creation, our empirical analysis shows that\\nstate-of-the-art AVEL methods suffer an average 18.66% performance drop during\\ncross-mode evaluation. Further analysis reveals two key challenges of different\\nvideo formats: 1) spatial bias from portrait-oriented framing introduces\\ndistinct domain priors, and 2) noisy audio composition compromise the\\nreliability of audio modality. To address these issues, we investigate optimal\\npreprocessing recipes and the impact of background music for AVEL on portrait\\nmode videos. Experiments show that these methods can still benefit from\\ntailored preprocessing and specialized model design, thus achieving improved\\nperformance. This work provides both a foundational benchmark and actionable\\ninsights for advancing AVEL research in the era of mobile-centric video\\ncontent. Dataset and code will be released.\", \"main_category\": \"cs.MM\", \"categories\": \"cs.MM,cs.AI,cs.CV\", \"published\": \"2025-04-09T13:38:40Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12216v1\", \"title\": \"d1: Scaling Reasoning in Diffusion Large Language Models via\\n  Reinforcement Learning\", \"summary\": \"Recent large language models (LLMs) have demonstrated strong reasoning\\ncapabilities that benefits from online reinforcement learning (RL). These\\ncapabilities have primarily been demonstrated within the left-to-right\\nautoregressive (AR) generation paradigm. In contrast, non-autoregressive\\nparadigms based on diffusion generate text in a coarse-to-fine manner. Although\\nrecent diffusion-based large language models (dLLMs) have achieved competitive\\nlanguage modeling performance compared to their AR counterparts, it remains\\nunclear if dLLMs can also leverage recent advances in LLM reasoning. To this\\nend, we propose d1, a framework to adapt pre-trained masked dLLMs into\\nreasoning models via a combination of supervised finetuning (SFT) and RL.\\nSpecifically, we develop and extend techniques to improve reasoning in\\npretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge\\nand instill self-improvement behavior directly from existing datasets, and (b)\\nwe introduce a novel critic-free, policy-gradient based RL algorithm called\\ndiffu-GRPO. Through empirical studies, we investigate the performance of\\ndifferent post-training recipes on multiple mathematical and logical reasoning\\nbenchmarks. We find that d1 yields the best performance and significantly\\nimproves performance of a state-of-the-art dLLM.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-16T16:08:45Z\"}"}

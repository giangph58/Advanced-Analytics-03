{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13173v1\", \"title\": \"It's All Connected: A Journey Through Test-Time Memorization,\\n  Attentional Bias, Retention, and Online Optimization\", \"summary\": \"Designing efficient and effective architectural backbones has been in the\\ncore of research efforts to enhance the capability of foundation models.\\nInspired by the human cognitive phenomenon of attentional bias-the natural\\ntendency to prioritize certain events or stimuli-we reconceptualize neural\\narchitectures, including Transformers, Titans, and modern linear recurrent\\nneural networks as associative memory modules that learn a mapping of keys and\\nvalues using an internal objective, referred to as attentional bias.\\nSurprisingly, we observed that most existing sequence models leverage either\\n(1) dot-product similarity, or (2) L2 regression objectives as their\\nattentional bias. Going beyond these objectives, we present a set of\\nalternative attentional bias configurations along with their effective\\napproximations to stabilize their training procedure. We then reinterpret\\nforgetting mechanisms in modern deep learning architectures as a form of\\nretention regularization, providing a novel set of forget gates for sequence\\nmodels. Building upon these insights, we present Miras, a general framework to\\ndesign deep learning architectures based on four choices of: (i) associative\\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\\nYaad, and Memora-that go beyond the power of existing linear RNNs while\\nmaintaining a fast parallelizable training process. Our experiments show\\ndifferent design choices in Miras yield models with varying strengths. For\\nexample, certain instances of Miras achieve exceptional performance in special\\ntasks such as language modeling, commonsense reasoning, and recall intensive\\ntasks, even outperforming Transformers and other modern linear recurrent\\nmodels.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-17T17:59:33Z\"}"}

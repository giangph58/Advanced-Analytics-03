{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05043v1\", \"title\": \"xTrace: A Facial Expressive Behaviour Analysis Tool for Continuous\\n  Affect Recognition\", \"summary\": \"Recognising expressive behaviours in face videos is a long-standing challenge\\nin Affective Computing. Despite significant advancements in recent years, it\\nstill remains a challenge to build a robust and reliable system for\\nnaturalistic and in-the-wild facial expressive behaviour analysis in real time.\\nThis paper addresses two key challenges in building such a system: (1). The\\npaucity of large-scale labelled facial affect video datasets with extensive\\ncoverage of the 2D emotion space, and (2). The difficulty of extracting facial\\nvideo features that are discriminative, interpretable, robust, and\\ncomputationally efficient. Toward addressing these challenges, we introduce\\nxTrace, a robust tool for facial expressive behaviour analysis and predicting\\ncontinuous values of dimensional emotions, namely valence and arousal, from\\nin-the-wild face videos.\\n  To address challenge (1), our affect recognition model is trained on the\\nlargest facial affect video data set, containing ~450k videos that cover most\\nemotion zones in the dimensional emotion space, making xTrace highly versatile\\nin analysing a wide spectrum of naturalistic expressive behaviours. To address\\nchallenge (2), xTrace uses facial affect descriptors that are not only\\nexplainable, but can also achieve a high degree of accuracy and robustness with\\nlow computational complexity. The key components of xTrace are benchmarked\\nagainst three existing tools: MediaPipe, OpenFace, and Augsburg Affect Toolbox.\\nOn an in-the-wild validation set composed of 50k videos, xTrace achieves 0.86\\nmean CCC and 0.13 mean absolute error values. We present a detailed error\\nanalysis of affect predictions from xTrace, illustrating (a). its ability to\\nrecognise emotions with high accuracy across most bins in the 2D emotion space,\\n(b). its robustness to non-frontal head pose angles, and (c). a strong\\ncorrelation between its uncertainty estimates and its accuracy.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-08T08:27:37Z\"}"}

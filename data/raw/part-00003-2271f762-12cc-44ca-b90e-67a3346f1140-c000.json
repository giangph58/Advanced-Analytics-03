{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15801v1\", \"title\": \"A closer look at how large language models trust humans: patterns and\\n  biases\", \"summary\": \"As large language models (LLMs) and LLM-based agents increasingly interact\\nwith humans in decision-making contexts, understanding the trust dynamics\\nbetween humans and AI agents becomes a central concern. While considerable\\nliterature studies how humans trust AI agents, it is much less understood how\\nLLM-based agents develop effective trust in humans. LLM-based agents likely\\nrely on some sort of implicit effective trust in trust-related contexts (e.g.,\\nevaluating individual loan applications) to assist and affect decision making.\\nUsing established behavioral theories, we develop an approach that studies\\nwhether LLMs trust depends on the three major trustworthiness dimensions:\\ncompetence, benevolence and integrity of the human subject. We also study how\\ndemographic variables affect effective trust. Across 43,200 simulated\\nexperiments, for five popular language models, across five different scenarios\\nwe find that LLM trust development shows an overall similarity to human trust\\ndevelopment. We find that in most, but not all cases, LLM trust is strongly\\npredicted by trustworthiness, and in some cases also biased by age, religion\\nand gender, especially in financial scenarios. This is particularly true for\\nscenarios common in the literature and for newer models. While the overall\\npatterns align with human-like mechanisms of effective trust formation,\\ndifferent models exhibit variation in how they estimate trust; in some cases,\\ntrustworthiness and demographic factors are weak predictors of effective trust.\\nThese findings call for a better understanding of AI-to-human trust dynamics\\nand monitoring of biases and trust development patterns to prevent unintended\\nand potentially harmful outcomes in trust-sensitive applications of AI.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.CY\", \"published\": \"2025-04-22T11:31:50Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20472v1\", \"title\": \"Robustness via Referencing: Defending against Prompt Injection Attacks\\n  by Referencing the Executed Instruction\", \"summary\": \"Large language models (LLMs) have demonstrated impressive performance and\\nhave come to dominate the field of natural language processing (NLP) across\\nvarious tasks. However, due to their strong instruction-following capabilities\\nand inability to distinguish between instructions and data content, LLMs are\\nvulnerable to prompt injection attacks. These attacks manipulate LLMs into\\ndeviating from the original input instructions and executing maliciously\\ninjected instructions within data content, such as web documents retrieved from\\nsearch engines. Existing defense methods, including prompt-engineering and\\nfine-tuning approaches, typically instruct models to follow the original input\\ninstructions while suppressing their tendencies to execute injected\\ninstructions. However, our experiments reveal that suppressing\\ninstruction-following tendencies is challenging. Through analyzing failure\\ncases, we observe that although LLMs tend to respond to any recognized\\ninstructions, they are aware of which specific instructions they are executing\\nand can correctly reference them within the original prompt. Motivated by these\\nfindings, we propose a novel defense method that leverages, rather than\\nsuppresses, the instruction-following abilities of LLMs. Our approach prompts\\nLLMs to generate responses that include both answers and their corresponding\\ninstruction references. Based on these references, we filter out answers not\\nassociated with the original input instructions. Comprehensive experiments\\ndemonstrate that our method outperforms prompt-engineering baselines and\\nachieves performance comparable to fine-tuning methods, reducing the attack\\nsuccess rate (ASR) to 0 percent in some scenarios. Moreover, our approach has\\nminimal impact on overall utility.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR\", \"published\": \"2025-04-29T07:13:53Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23990v1\", \"title\": \"BeMERC: Behavior-Aware MLLM-based Framework for Multimodal Emotion\\n  Recognition in Conversation\", \"summary\": \"Multimodal emotion recognition in conversation (MERC), the task of\\nidentifying the emotion label for each utterance in a conversation, is vital\\nfor developing empathetic machines. Current MLLM-based MERC studies focus\\nmainly on capturing the speaker's textual or vocal characteristics, but ignore\\nthe significance of video-derived behavior information. Different from text and\\naudio inputs, learning videos with rich facial expression, body language and\\nposture, provides emotion trigger signals to the models for more accurate\\nemotion predictions. In this paper, we propose a novel behavior-aware\\nMLLM-based framework (BeMERC) to incorporate speaker's behaviors, including\\nsubtle facial micro-expression, body language and posture, into a vanilla\\nMLLM-based MERC model, thereby facilitating the modeling of emotional dynamics\\nduring a conversation. Furthermore, BeMERC adopts a two-stage instruction\\ntuning strategy to extend the model to the conversations scenario for\\nend-to-end training of a MERC predictor. Experiments demonstrate that BeMERC\\nachieves superior performance than the state-of-the-art methods on two\\nbenchmark datasets, and also provides a detailed discussion on the significance\\nof video-derived behavior information in MERC.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-03-31T12:04:53Z\"}"}

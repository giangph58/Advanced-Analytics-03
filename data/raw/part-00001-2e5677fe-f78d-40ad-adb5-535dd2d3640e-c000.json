{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10160v1\", \"title\": \"MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like\\n  Reinforcement Learning\", \"summary\": \"Large-scale reinforcement learning (RL) methods have proven highly effective\\nin enhancing the reasoning abilities of large language models (LLMs),\\nparticularly for tasks with verifiable solutions such as mathematics and\\ncoding. However, applying this idea to machine translation (MT), where outputs\\nare flexibly formatted and difficult to automatically evaluate with explicit\\nrules, remains underexplored. In this work, we introduce MT-R1-Zero, the first\\nopen-source adaptation of the R1-Zero RL framework for MT without supervised\\nfine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to\\nguide LLMs towards improved translation quality via emergent reasoning. On the\\nWMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive\\nperformance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points.\\nMeanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across\\nall metrics, placing it on par with advanced proprietary models such as GPT-4o\\nand Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves\\nstate-of-the-art scores on semantic metrics. Moreover, our work exhibits strong\\ngeneralization capabilities on out-of-distribution MT tasks, robustly\\nsupporting multilingual and low-resource settings. Extensive analysis of model\\nbehavior across different initializations and reward metrics offers pioneering\\ninsight into the critical role of reward design, LLM adaptability, training\\ndynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT.\\nOur code is available at https://github.com/fzp0424/MT-R1-Zero.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-14T12:14:18Z\"}"}

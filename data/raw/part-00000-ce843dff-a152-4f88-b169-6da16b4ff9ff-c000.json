{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19876v1\", \"title\": \"DeeCLIP: A Robust and Generalizable Transformer-Based Framework for\\n  Detecting AI-Generated Images\", \"summary\": \"This paper introduces DeeCLIP, a novel framework for detecting AI-generated\\nimages using CLIP-ViT and fusion learning. Despite significant advancements in\\ngenerative models capable of creating highly photorealistic images, existing\\ndetection methods often struggle to generalize across different models and are\\nhighly sensitive to minor perturbations. To address these challenges, DeeCLIP\\nincorporates DeeFuser, a fusion module that combines high-level and low-level\\nfeatures, improving robustness against degradations such as compression and\\nblurring. Additionally, we apply triplet loss to refine the embedding space,\\nenhancing the model's ability to distinguish between real and synthetic\\ncontent. To further enable lightweight adaptation while preserving pre-trained\\nknowledge, we adopt parameter-efficient fine-tuning using low-rank adaptation\\n(LoRA) within the CLIP-ViT backbone. This approach supports effective zero-shot\\nlearning without sacrificing generalization. Trained exclusively on 4-class\\nProGAN data, DeeCLIP achieves an average accuracy of 89.00% on 19 test subsets\\ncomposed of generative adversarial network (GAN) and diffusion models. Despite\\nhaving fewer trainable parameters, DeeCLIP outperforms existing methods,\\ndemonstrating superior robustness against various generative models and\\nreal-world distortions. The code is publicly available at\\nhttps://github.com/Mamadou-Keita/DeeCLIP for research purposes.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.CR\", \"published\": \"2025-04-28T15:06:28Z\"}"}

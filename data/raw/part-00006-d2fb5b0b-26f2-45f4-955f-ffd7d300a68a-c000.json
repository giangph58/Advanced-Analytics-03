{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11750v1\", \"title\": \"Characterizing and Optimizing LLM Inference Workloads on CPU-GPU Coupled\\n  Architectures\", \"summary\": \"Large language model (LLM)-based inference workloads increasingly dominate\\ndata center costs and resource utilization. Therefore, understanding the\\ninference workload characteristics on evolving CPU-GPU coupled architectures is\\ncrucial for optimization. This paper presents an in-depth analysis of LLM\\ninference behavior on loosely-coupled (PCIe A100/H100) and closely-coupled\\n(GH200) systems. We analyze performance dynamics using fine-grained\\noperator-to-kernel trace analysis, facilitated by our novel profiler SKIP and\\nmetrics like Total Kernel Launch and Queuing Time (TKLQT). Results show that\\nclosely-coupled (CC) GH200 significantly outperforms loosely-coupled (LC)\\nsystems at large batch sizes, achieving 1.9x-2.7x faster prefill latency for\\nLlama 3.2-1B. However, our analysis also reveals that GH200 remains CPU-bound\\nup to 4x larger batch sizes than LC systems. In this extended CPU-bound region,\\nwe identify the performance characteristics of the Grace CPU as a key factor\\ncontributing to higher inference latency at low batch sizes on GH200. We\\ndemonstrate that TKLQT accurately identifies this CPU/GPU-bound transition\\npoint. Based on this analysis, we further show that kernel fusion offers\\nsignificant potential to mitigate GH200's low-batch latency bottleneck by\\nreducing kernel launch overhead. This detailed kernel-level characterization\\nprovides critical insights for optimizing diverse CPU-GPU coupling strategies.\\nThis work is an initial effort, and we plan to explore other major AI/DL\\nworkloads that demand different degrees of CPU-GPU heterogeneous architectures.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC,cs.AI,cs.AR,cs.PF\", \"published\": \"2025-04-16T04:02:39Z\"}"}

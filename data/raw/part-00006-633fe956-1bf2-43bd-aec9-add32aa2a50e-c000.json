{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12625v1\", \"title\": \"Spectral Algorithms under Covariate Shift\", \"summary\": \"Spectral algorithms leverage spectral regularization techniques to analyze\\nand process data, providing a flexible framework for addressing supervised\\nlearning problems. To deepen our understanding of their performance in\\nreal-world scenarios where the distributions of training and test data may\\ndiffer, we conduct a rigorous investigation into the convergence behavior of\\nspectral algorithms under distribution shifts, specifically within the\\nframework of reproducing kernel Hilbert spaces. Our study focuses on the case\\nof covariate shift. In this scenario, the marginal distributions of the input\\ndata differ between the training and test datasets, while the conditional\\ndistribution of the output given the input remains unchanged. Under this\\nsetting, we analyze the generalization error of spectral algorithms and show\\nthat they achieve minimax optimality when the density ratios between the\\ntraining and test distributions are uniformly bounded. However, we also\\nidentify a critical limitation: when the density ratios are unbounded, the\\nspectral algorithms may become suboptimal. To address this limitation, we\\npropose a weighted spectral algorithm that incorporates density ratio\\ninformation into the learning process. Our theoretical analysis shows that this\\nweighted approach achieves optimal capacity-independent convergence rates.\\nFurthermore, by introducing a weight clipping technique, we demonstrate that\\nthe convergence rates of the weighted spectral algorithm can approach the\\noptimal capacity-dependent convergence rates arbitrarily closely. This\\nimprovement resolves the suboptimality issue in unbounded density ratio\\nscenarios and advances the state-of-the-art by refining existing theoretical\\nresults.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.LG\", \"published\": \"2025-04-17T04:02:06Z\"}"}

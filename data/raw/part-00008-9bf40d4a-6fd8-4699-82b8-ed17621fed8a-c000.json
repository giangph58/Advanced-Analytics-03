{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04281v1\", \"title\": \"TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement\", \"summary\": \"This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing\\nextremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes\\nnoisy images by constructing multiple virtual cameras based on a noise space.\\nCamera Feature Integration (CFI) modules are then designed to enable the model\\nto learn generalizable features across diverse virtual cameras. During the\\naligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is\\nfine-tuned using a small amount of real RAW data to adapt to the noise\\ncharacteristics of specific cameras. A structural reparameterization technique\\nfurther simplifies CFI$^T$ for efficient deployment. To address color shifts\\nduring the diffusion process, a color corrector is introduced to ensure color\\nconsistency by dynamically adjusting global color distributions. Additionally,\\na novel dataset, QID, is constructed, featuring quantifiable illumination\\nlevels and a wide dynamic range, providing a comprehensive benchmark for\\ntraining and evaluation under extreme low-light conditions. Experimental\\nresults demonstrate that TS-Diff achieves state-of-the-art performance on\\nmultiple datasets, including QID, SID, and ELD, excelling in denoising,\\ngeneralization, and color consistency across various cameras and illumination\\nlevels. These findings highlight the robustness and versatility of TS-Diff,\\nmaking it a practical solution for low-light imaging applications. Source codes\\nand models are available at https://github.com/CircccleK/TS-Diff\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-07T09:35:05Z\"}"}

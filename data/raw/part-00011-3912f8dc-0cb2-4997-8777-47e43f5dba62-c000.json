{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12076v1\", \"title\": \"Subitizing-Inspired_Large_Language_Models_for_Floorplanning\", \"summary\": \"We present a novel approach to solving the floorplanning problem by\\nleveraging fine-tuned Large Language Models (LLMs). Inspired by subitizing--the\\nhuman ability to instantly and accurately count small numbers of items at a\\nglance--we hypothesize that LLMs can similarly address floorplanning challenges\\nswiftly and accurately. We propose an efficient representation of the\\nfloorplanning problem and introduce a method for generating high-quality\\ndatasets tailored for model fine-tuning. We fine-tune LLMs on datasets with a\\nspecified number of modules to test whether LLMs can emulate the human ability\\nto quickly count and arrange items. Our experimental results demonstrate that\\nfine-tuned LLMs, particularly GPT4o-mini, achieve high success and optimal\\nrates while attaining relatively low average dead space. These findings\\nunderscore the potential of LLMs as promising solutions for complex\\noptimization tasks in VLSI design.\", \"main_category\": \"cs.AR\", \"categories\": \"cs.AR\", \"published\": \"2025-04-16T13:35:41Z\"}"}

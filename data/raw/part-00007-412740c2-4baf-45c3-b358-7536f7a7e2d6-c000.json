{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11304v1\", \"title\": \"Differentially Private Geodesic and Linear Regression\", \"summary\": \"In statistical applications it has become increasingly common to encounter\\ndata structures that live on non-linear spaces such as manifolds. Classical\\nlinear regression, one of the most fundamental methodologies of statistical\\nlearning, captures the relationship between an independent variable and a\\nresponse variable which both are assumed to live in Euclidean space. Thus,\\ngeodesic regression emerged as an extension where the response variable lives\\non a Riemannian manifold. The parameters of geodesic regression, as with linear\\nregression, capture the relationship of sensitive data and hence one should\\nconsider the privacy protection practices of said parameters. We consider\\nreleasing Differentially Private (DP) parameters of geodesic regression via the\\nK-Norm Gradient (KNG) mechanism for Riemannian manifolds. We derive theoretical\\nbounds for the sensitivity of the parameters showing they are tied to their\\nrespective Jacobi fields and hence the curvature of the space. This\\ncorroborates recent findings of differential privacy for the Fr\\\\'echet mean. We\\ndemonstrate the efficacy of our methodology on the sphere,\\n$\\\\mbS^2\\\\subset\\\\mbR^3$ and, since it is general to Riemannian manifolds, the\\nmanifold of Euclidean space which simplifies geodesic regression to a case of\\nlinear regression. Our methodology is general to any Riemannian manifold and\\nthus it is suitable for data in domains such as medical imaging and computer\\nvision.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.LG\", \"published\": \"2025-04-15T15:45:48Z\"}"}

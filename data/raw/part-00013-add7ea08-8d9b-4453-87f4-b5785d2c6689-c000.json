{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16000v1\", \"title\": \"How Private is Your Attention? Bridging Privacy with In-Context Learning\", \"summary\": \"In-context learning (ICL)-the ability of transformer-based models to perform\\nnew tasks from examples provided at inference time-has emerged as a hallmark of\\nmodern language models. While recent works have investigated the mechanisms\\nunderlying ICL, its feasibility under formal privacy constraints remains\\nlargely unexplored. In this paper, we propose a differentially private\\npretraining algorithm for linear attention heads and present the first\\ntheoretical analysis of the privacy-accuracy trade-off for ICL in linear\\nregression. Our results characterize the fundamental tension between\\noptimization and privacy-induced noise, formally capturing behaviors observed\\nin private training via iterative methods. Additionally, we show that our\\nmethod is robust to adversarial perturbations of training prompts, unlike\\nstandard ridge regression. All theoretical findings are supported by extensive\\nsimulations across diverse settings.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.AI,cs.CL,cs.CR,cs.LG\", \"published\": \"2025-04-22T16:05:26Z\"}"}

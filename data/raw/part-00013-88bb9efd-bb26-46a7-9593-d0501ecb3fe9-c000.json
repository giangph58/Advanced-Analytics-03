{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15077v1\", \"title\": \"Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL\", \"summary\": \"Large Language Models (LLMs) have shown impressive capabilities in\\ntransforming natural language questions about relational databases into SQL\\nqueries. Despite recent improvements, small LLMs struggle to handle questions\\ninvolving multiple tables and complex SQL patterns under a Zero-Shot Learning\\n(ZSL) setting. Supervised Fine-Tuning (SFT) partially compensate the knowledge\\ndeficits in pretrained models but falls short while dealing with queries\\ninvolving multi-hop reasoning. To bridge this gap, different LLM training\\nstrategies to reinforce reasoning capabilities have been proposed, ranging from\\nleveraging a thinking process within ZSL, including reasoning traces in SFT, or\\nadopt Reinforcement Learning (RL) strategies. However, the influence of\\nreasoning on Text2SQL performance is still largely unexplored. This paper\\ninvestigates to what extent LLM reasoning capabilities influence their Text2SQL\\nperformance on four benchmark datasets. To this end, it considers the following\\nLLM settings: (1) ZSL, including general-purpose reasoning or not; (2) SFT,\\nwith and without task-specific reasoning traces; (3) RL, leveraging execution\\naccuracy as primary reward function; (4) SFT+RL, i.e, a two-stage approach that\\ncombines SFT and RL. The results show that general-purpose reasoning under ZSL\\nproves to be ineffective in tackling complex Text2SQL cases. Small LLMs benefit\\nfrom SFT with reasoning much more than larger ones, bridging the gap of their\\n(weaker) model pretraining. RL is generally beneficial across all tested models\\nand datasets, particularly when SQL queries involve multi-hop reasoning and\\nmultiple tables. Small LLMs with SFT+RL excel on most complex datasets thanks\\nto a strategic balance between generality of the reasoning process and\\noptimization of the execution accuracy. Thanks to RL, the7B Qwen-Coder-2.5\\nmodel performs on par with 100+ Billion ones on the Bird dataset.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.DB\", \"published\": \"2025-04-21T13:05:26Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20022v1\", \"title\": \"Better To Ask in English? Evaluating Factual Accuracy of Multilingual\\n  LLMs in English and Low-Resource Languages\", \"summary\": \"Multilingual Large Language Models (LLMs) have demonstrated significant\\neffectiveness across various languages, particularly in high-resource languages\\nsuch as English. However, their performance in terms of factual accuracy across\\nother low-resource languages, especially Indic languages, remains an area of\\ninvestigation. In this study, we assess the factual accuracy of LLMs - GPT-4o,\\nGemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in\\nEnglish and Indic languages using the IndicQuest dataset, which contains\\nquestion-answer pairs in English and 19 Indic languages. By asking the same\\nquestions in English and their respective Indic translations, we analyze\\nwhether the models are more reliable for regional context questions in Indic\\nlanguages or when operating in English. Our findings reveal that LLMs often\\nperform better in English, even for questions rooted in Indic contexts.\\nNotably, we observe a higher tendency for hallucination in responses generated\\nin low-resource Indic languages, highlighting challenges in the multilingual\\nunderstanding capabilities of current LLMs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-28T17:48:13Z\"}"}

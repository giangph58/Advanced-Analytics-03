{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00604v1\", \"title\": \"Understanding and improving transferability in machine-learned\\n  activation energy predictors\", \"summary\": \"The calculation of reactive properties is a challenging task in chemical\\nreaction discovery. Machine learning (ML) methods play an important role in\\naccelerating electronic structure predictions of activation energies and\\nreaction enthalpies, and are a crucial ingredient to enable large-scale\\nautomated reaction network discovery with $>10^3$ reactions. Unfortunately, the\\npredictive accuracy of existing ML models does not yet reach the required\\naccuracy across the space of possible chemical reactions to enable subsequent\\nkinetic simulations that even qualitatively agree with experimental kinetics.\\nHere, we comprehensively assess the underlying reasons for prediction failures\\nwithin a selection of machine-learned models of reactivity. Models based on\\ndifference fingerprints between reactant and product structures lack\\ntransferability despite providing good in-distribution predictions. This\\nresults in a significant loss of information about the context and mechanism of\\nchemical reactions. We propose a convolutional ML model that uses atom-centered\\nquantum-chemical descriptors and approximate transition state information.\\nInclusion of the latter improves transferability for out-of-distribution\\nbenchmark reactions, making greater use of the limited chemical reaction space\\nspanned by the training data. The model further delivers atom-level\\ncontributions to activation energies and reaction enthalpies that provide a\\nuseful interpretational tool for rationalizing reactivity.\", \"main_category\": \"physics.chem-ph\", \"categories\": \"physics.chem-ph\", \"published\": \"2025-05-01T15:35:12Z\"}"}

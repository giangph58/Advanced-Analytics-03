{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11942v1\", \"title\": \"ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign\\n  Language Translation\", \"summary\": \"Current sign language machine translation systems rely on recognizing hand\\nmovements, facial expressions and body postures, and natural language\\nprocessing, to convert signs into text. Recent approaches use Transformer\\narchitectures to model long-range dependencies via positional encoding.\\nHowever, they lack accuracy in recognizing fine-grained, short-range temporal\\ndependencies between gestures captured at high frame rates. Moreover, their\\nhigh computational complexity leads to inefficient training. To mitigate these\\nissues, we propose an Adaptive Transformer (ADAT), which incorporates\\ncomponents for enhanced feature extraction and adaptive feature weighting\\nthrough a gating mechanism to emphasize contextually relevant features while\\nreducing training overhead and maintaining translation accuracy. To evaluate\\nADAT, we introduce MedASL, the first public medical American Sign Language\\ndataset. In sign-to-gloss-to-text experiments, ADAT outperforms the\\nencoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing\\ntraining time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text\\nexperiments, it improves accuracy by 8.7% and reduces training time by 2.8% on\\nPHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on\\nMedASL. Compared to encoder-only and decoder-only baselines in sign-to-text,\\nADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its\\ndual-stream structure.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CL,cs.CV\", \"published\": \"2025-04-16T10:20:11Z\"}"}

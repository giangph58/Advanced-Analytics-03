{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15594v1\", \"title\": \"Analytical Softmax Temperature Setting from Feature Dimensions for\\n  Model- and Domain-Robust Classification\", \"summary\": \"In deep learning-based classification tasks, the softmax function's\\ntemperature parameter $T$ critically influences the output distribution and\\noverall performance. This study presents a novel theoretical insight that the\\noptimal temperature $T^*$ is uniquely determined by the dimensionality of the\\nfeature representations, thereby enabling training-free determination of $T^*$.\\nDespite this theoretical grounding, empirical evidence reveals that $T^*$\\nfluctuates under practical conditions owing to variations in models, datasets,\\nand other confounding factors. To address these influences, we propose and\\noptimize a set of temperature determination coefficients that specify how $T^*$\\nshould be adjusted based on the theoretical relationship to feature\\ndimensionality. Additionally, we insert a batch normalization layer immediately\\nbefore the output layer, effectively stabilizing the feature space. Building on\\nthese coefficients and a suite of large-scale experiments, we develop an\\nempirical formula to estimate $T^*$ without additional training while also\\nintroducing a corrective scheme to refine $T^*$ based on the number of classes\\nand task complexity. Our findings confirm that the derived temperature not only\\naligns with the proposed theoretical perspective but also generalizes\\neffectively across diverse tasks, consistently enhancing classification\\nperformance and offering a practical, training-free solution for determining\\n$T^*$.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CV\", \"published\": \"2025-04-22T05:14:38Z\"}"}

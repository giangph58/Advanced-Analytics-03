{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20996v1\", \"title\": \"X-Fusion: Introducing New Modality to Frozen Large Language Models\", \"summary\": \"We propose X-Fusion, a framework that extends pretrained Large Language\\nModels (LLMs) for multimodal tasks while preserving their language\\ncapabilities. X-Fusion employs a dual-tower design with modality-specific\\nweights, keeping the LLM's parameters frozen while integrating vision-specific\\ninformation for both understanding and generation. Our experiments demonstrate\\nthat X-Fusion consistently outperforms alternative architectures on both\\nimage-to-text and text-to-image tasks. We find that incorporating\\nunderstanding-focused data improves generation quality, reducing image data\\nnoise enhances overall performance, and feature alignment accelerates\\nconvergence for smaller models but has minimal impact on larger ones. Our\\nfindings provide valuable insights into building efficient unified multimodal\\nmodels.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-29T17:59:45Z\"}"}

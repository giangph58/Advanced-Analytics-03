{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13065v1\", \"title\": \"EchoWorld: Learning Motion-Aware World Models for Echocardiography Probe\\n  Guidance\", \"summary\": \"Echocardiography is crucial for cardiovascular disease detection but relies\\nheavily on experienced sonographers. Echocardiography probe guidance systems,\\nwhich provide real-time movement instructions for acquiring standard plane\\nimages, offer a promising solution for AI-assisted or fully autonomous\\nscanning. However, developing effective machine learning models for this task\\nremains challenging, as they must grasp heart anatomy and the intricate\\ninterplay between probe motion and visual signals. To address this, we present\\nEchoWorld, a motion-aware world modeling framework for probe guidance that\\nencodes anatomical knowledge and motion-induced visual dynamics, while\\neffectively leveraging past visual-motion sequences to enhance guidance\\nprecision. EchoWorld employs a pre-training strategy inspired by world modeling\\nprinciples, where the model predicts masked anatomical regions and simulates\\nthe visual outcomes of probe adjustments. Built upon this pre-trained model, we\\nintroduce a motion-aware attention mechanism in the fine-tuning stage that\\neffectively integrates historical visual-motion data, enabling precise and\\nadaptive probe guidance. Trained on more than one million ultrasound images\\nfrom over 200 routine scans, EchoWorld effectively captures key\\nechocardiographic knowledge, as validated by qualitative analysis. Moreover,\\nour method significantly reduces guidance errors compared to existing visual\\nbackbones and guidance frameworks, excelling in both single-frame and\\nsequential evaluation protocols. Code is available at\\nhttps://github.com/LeapLabTHU/EchoWorld.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-17T16:19:05Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12971v1\", \"title\": \"Transferrable Surrogates in Expressive Neural Architecture Search Spaces\", \"summary\": \"Neural architecture search (NAS) faces a challenge in balancing the\\nexploration of expressive, broad search spaces that enable architectural\\ninnovation with the need for efficient evaluation of architectures to\\neffectively search such spaces. We investigate surrogate model training for\\nimproving search in highly expressive NAS search spaces based on context-free\\ngrammars. We show that i) surrogate models trained either using zero-cost-proxy\\nmetrics and neural graph features (GRAF) or by fine-tuning an off-the-shelf LM\\nhave high predictive power for the performance of architectures both within and\\nacross datasets, ii) these surrogates can be used to filter out bad\\narchitectures when searching on novel datasets, thereby significantly speeding\\nup search and achieving better final performances, and iii) the surrogates can\\nbe further used directly as the search objective for huge speed-ups.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-17T14:22:28Z\"}"}

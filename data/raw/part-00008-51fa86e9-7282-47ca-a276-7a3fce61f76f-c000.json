{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16574v1\", \"title\": \"PIS: Linking Importance Sampling and Attention Mechanisms for Efficient\\n  Prompt Compression\", \"summary\": \"Large language models (LLMs) have achieved remarkable progress, demonstrating\\nunprecedented capabilities across various natural language processing tasks.\\nHowever, the high costs associated with such exceptional performance limit the\\nwidespread adoption of LLMs, highlighting the need for prompt compression.\\nExisting prompt compression methods primarily rely on heuristic truncation or\\nabstractive summarization techniques, which fundamentally overlook the\\nintrinsic mechanisms of LLMs and lack a systematic evaluation of token\\nimportance for generation. In this work, we introduce Prompt Importance\\nSampling (PIS), a novel compression framework that dynamically compresses\\nprompts by sampling important tokens based on the analysis of attention scores\\nof hidden states. PIS employs a dual-level compression mechanism: 1) at the\\ntoken level, we quantify saliency using LLM-native attention scores and\\nimplement adaptive compression through a lightweight 9-layer reinforcement\\nlearning (RL) network; 2) at the semantic level, we propose a Russian roulette\\nsampling strategy for sentence-level importance sampling. Comprehensive\\nevaluations across multiple domain benchmarks demonstrate that our method\\nachieves state-of-the-art compression performance. Notably, our framework\\nserendipitously enhances reasoning efficiency through optimized context\\nstructuring. This work advances prompt engineering by offering both theoretical\\ngrounding and practical efficiency in context management for LLMs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-23T09:53:01Z\"}"}

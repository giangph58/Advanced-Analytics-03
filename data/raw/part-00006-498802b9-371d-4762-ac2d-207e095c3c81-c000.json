{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07887v1\", \"title\": \"Benchmarking Adversarial Robustness to Bias Elicitation in Large\\n  Language Models: Scalable Automated Assessment with LLM-as-a-Judge\", \"summary\": \"Large Language Models (LLMs) have revolutionized artificial intelligence,\\ndriving advancements in machine translation, summarization, and conversational\\nagents. However, their increasing integration into critical societal domains\\nhas raised concerns about embedded biases, which can perpetuate stereotypes and\\ncompromise fairness. These biases stem from various sources, including\\nhistorical inequalities in training data, linguistic imbalances, and\\nadversarial manipulation. Despite mitigation efforts, recent studies indicate\\nthat LLMs remain vulnerable to adversarial attacks designed to elicit biased\\nresponses. This work proposes a scalable benchmarking framework to evaluate LLM\\nrobustness against adversarial bias elicitation. Our methodology involves (i)\\nsystematically probing models with a multi-task approach targeting biases\\nacross various sociocultural dimensions, (ii) quantifying robustness through\\nsafety scores using an LLM-as-a-Judge approach for automated assessment of\\nmodel responses, and (iii) employing jailbreak techniques to investigate\\nvulnerabilities in safety mechanisms. Our analysis examines prevalent biases in\\nboth small and large state-of-the-art models and their impact on model safety.\\nAdditionally, we assess the safety of domain-specific models fine-tuned for\\ncritical fields, such as medicine. Finally, we release a curated dataset of\\nbias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability\\nbenchmarking. Our findings reveal critical trade-offs between model size and\\nsafety, aiding the development of fairer and more robust future language\\nmodels.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-10T16:00:59Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11393v1\", \"title\": \"DataDecide: How to Predict Best Pretraining Data with Small Experiments\", \"summary\": \"Because large language models are expensive to pretrain on different\\ndatasets, using smaller-scale experiments to decide on data is crucial for\\nreducing costs. Which benchmarks and methods of making decisions from observed\\nperformance at small scale most accurately predict the datasets that yield the\\nbest large models? To empower open exploration of this question, we release\\nmodels, data, and evaluations in DataDecide -- the most extensive open suite of\\nmodels over differences in data and scale. We conduct controlled pretraining\\nexperiments across 25 corpora with differing sources, deduplication, and\\nfiltering up to 100B tokens, model sizes up to 1B parameters, and 3 random\\nseeds. We find that the ranking of models at a single, small size (e.g., 150M\\nparameters) is a strong baseline for predicting best models at our larger\\ntarget scale (1B) (~80% of com parisons correct). No scaling law methods among\\n8 baselines exceed the compute-decision frontier of single-scale predictions,\\nbut DataDecide can measure improvement in future scaling laws. We also identify\\nthat using continuous likelihood metrics as proxies in small experiments makes\\nbenchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable\\nat the target 1B scale with just 0.01% of the compute.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL\", \"published\": \"2025-04-15T17:02:15Z\"}"}

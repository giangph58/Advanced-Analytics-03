{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07820v1\", \"title\": \"Smoothed Distance Kernels for MMDs and Applications in Wasserstein\\n  Gradient Flows\", \"summary\": \"Negative distance kernels $K(x,y) := - \\\\|x-y\\\\|$ were used in the definition\\nof maximum mean discrepancies (MMDs) in statistics and lead to favorable\\nnumerical results in various applications. In particular, so-called slicing\\ntechniques for handling high-dimensional kernel summations profit from the\\nsimple parameter-free structure of the distance kernel. However, due to its\\nnon-smoothness in $x=y$, most of the classical theoretical results, e.g. on\\nWasserstein gradient flows of the corresponding MMD functional do not longer\\nhold true. In this paper, we propose a new kernel which keeps the favorable\\nproperties of the negative distance kernel as being conditionally positive\\ndefinite of order one with a nearly linear increase towards infinity and a\\nsimple slicing structure, but is Lipschitz differentiable now. Our construction\\nis based on a simple 1D smoothing procedure of the absolute value function\\nfollowed by a Riemann-Liouville fractional integral transform. Numerical\\nresults demonstrate that the new kernel performs similarly well as the negative\\ndistance kernel in gradient descent methods, but now with theoretical\\nguarantees.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.LG,math.FA,math.PR\", \"published\": \"2025-04-10T14:57:33Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12109v1\", \"title\": \"Self-Supervised Traversability Learning with Online Prototype Adaptation\\n  for Off-Road Autonomous Driving\", \"summary\": \"Achieving reliable and safe autonomous driving in off-road environments\\nrequires accurate and efficient terrain traversability analysis. However, this\\ntask faces several challenges, including the scarcity of large-scale datasets\\ntailored for off-road scenarios, the high cost and potential errors of manual\\nannotation, the stringent real-time requirements of motion planning, and the\\nlimited computational power of onboard units. To address these challenges, this\\npaper proposes a novel traversability learning method that leverages\\nself-supervised learning, eliminating the need for manual annotation. For the\\nfirst time, a Birds-Eye View (BEV) representation is used as input, reducing\\ncomputational burden and improving adaptability to downstream motion planning.\\nDuring vehicle operation, the proposed method conducts online analysis of\\ntraversed regions and dynamically updates prototypes to adaptively assess the\\ntraversability of the current environment, effectively handling dynamic scene\\nchanges. We evaluate our approach against state-of-the-art benchmarks on both\\npublic datasets and our own dataset, covering diverse seasons and geographical\\nlocations. Experimental results demonstrate that our method significantly\\noutperforms recent approaches. Additionally, real-world vehicle experiments\\nshow that our method operates at 10 Hz, meeting real-time requirements, while a\\n5.5 km autonomous driving experiment further validates the generated\\ntraversability cost maps compatibility with downstream motion planning.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO\", \"published\": \"2025-04-16T14:17:31Z\"}"}

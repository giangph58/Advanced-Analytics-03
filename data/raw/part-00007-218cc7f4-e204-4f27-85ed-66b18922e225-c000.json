{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05465v1\", \"title\": \"ComPO: Preference Alignment via Comparison Oracles\", \"summary\": \"Direct alignment methods are increasingly used for aligning large language\\nmodels (LLMs) with human preferences. However, these methods suffer from the\\nissues of verbosity and likelihood displacement, which can be driven by the\\nnoisy preference pairs that induce similar likelihood for preferred and\\ndispreferred responses. The contributions of this paper are two-fold. First, we\\npropose a new preference alignment method based on comparison oracles and\\nprovide the convergence guarantee for its basic scheme. Second, we improve our\\nmethod using some heuristics and conduct the experiments to demonstrate the\\nflexibility and compatibility of practical scheme in improving the performance\\nof LLMs using noisy preference pairs. Evaluations are conducted across multiple\\nbase and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with\\nbenchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show\\nthe effectiveness of our method as an alternative to addressing the limitations\\nof existing direct alignment methods. A highlight of our work is that we\\nevidence the importance of designing specialized methods for preference pairs\\nwith distinct likelihood margin, which complements the recent findings in\\n\\\\citet{Razin-2025-Unintentional}.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-05-08T17:56:57Z\"}"}

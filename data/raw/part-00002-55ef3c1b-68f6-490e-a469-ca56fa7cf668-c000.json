{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03621v1\", \"title\": \"PhysLLM: Harnessing Large Language Models for Cross-Modal Remote\\n  Physiological Sensing\", \"summary\": \"Remote photoplethysmography (rPPG) enables non-contact physiological\\nmeasurement but remains highly susceptible to illumination changes, motion\\nartifacts, and limited temporal modeling. Large Language Models (LLMs) excel at\\ncapturing long-range dependencies, offering a potential solution but struggle\\nwith the continuous, noise-sensitive nature of rPPG signals due to their\\ntext-centric design. To bridge this gap, we introduce PhysLLM, a collaborative\\noptimization framework that synergizes LLMs with domain-specific rPPG\\ncomponents. Specifically, the Text Prototype Guidance (TPG) strategy is\\nproposed to establish cross-modal alignment by projecting hemodynamic features\\ninto LLM-interpretable semantic space, effectively bridging the\\nrepresentational gap between physiological signals and linguistic tokens.\\nBesides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for\\nresolving signal instability through adaptive time-frequency domain feature\\nre-weighting. Finally, rPPG task-specific cues systematically inject\\nphysiological priors through physiological statistics, environmental contextual\\nanswering, and task description, leveraging cross-modal learning to integrate\\nboth visual and textual information, enabling dynamic adaptation to challenging\\nscenarios like variable illumination and subject movements. Evaluation on four\\nbenchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness,\\ndemonstrating superior generalization across lighting variations and motion\\nscenarios.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-06T15:18:38Z\"}"}

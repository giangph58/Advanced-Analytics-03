{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06659v1\", \"title\": \"Bridging the Gap Between Preference Alignment and Machine Unlearning\", \"summary\": \"Despite advances in Preference Alignment (PA) for Large Language Models\\n(LLMs), mainstream methods like Reinforcement Learning with Human Feedback\\n(RLHF) face notable challenges. These approaches require high-quality datasets\\nof positive preference examples, which are costly to obtain and computationally\\nintensive due to training instability, limiting their use in low-resource\\nscenarios. LLM unlearning technique presents a promising alternative, by\\ndirectly removing the influence of negative examples. However, current research\\nhas primarily focused on empirical validation, lacking systematic quantitative\\nanalysis. To bridge this gap, we propose a framework to explore the\\nrelationship between PA and LLM unlearning. Specifically, we introduce a\\nbi-level optimization-based method to quantify the impact of unlearning\\nspecific negative examples on PA performance. Our analysis reveals that not all\\nnegative examples contribute equally to alignment improvement when unlearned,\\nand the effect varies significantly across examples. Building on this insight,\\nwe pose a crucial question: how can we optimally select and weight negative\\nexamples for unlearning to maximize PA performance? To answer this, we propose\\na framework called Unlearning to Align (U2A), which leverages bi-level\\noptimization to efficiently select and unlearn examples for optimal PA\\nperformance. We validate the proposed method through extensive experiments,\\nwith results confirming its effectiveness.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL\", \"published\": \"2025-04-09T07:49:08Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07831v1\", \"title\": \"Deceptive Automated Interpretability: Language Models Coordinating to\\n  Fool Oversight Systems\", \"summary\": \"We demonstrate how AI agents can coordinate to deceive oversight systems\\nusing automated interpretability of neural networks. Using sparse autoencoders\\n(SAEs) as our experimental framework, we show that language models (Llama,\\nDeepSeek R1, and Claude 3.7 Sonnet) can generate deceptive explanations that\\nevade detection. Our agents employ steganographic methods to hide information\\nin seemingly innocent explanations, successfully fooling oversight models while\\nachieving explanation quality comparable to reference labels. We further find\\nthat models can scheme to develop deceptive strategies when they believe the\\ndetection of harmful features might lead to negative consequences for\\nthemselves. All tested LLM agents were capable of deceiving the overseer while\\nachieving high interpretability scores comparable to those of reference labels.\\nWe conclude by proposing mitigation strategies, emphasizing the critical need\\nfor robust understanding and defenses against deception.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CL\", \"published\": \"2025-04-10T15:07:10Z\"}"}

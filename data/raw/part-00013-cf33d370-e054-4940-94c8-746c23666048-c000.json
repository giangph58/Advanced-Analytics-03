{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12849v1\", \"title\": \"FedX: Adaptive Model Decomposition and Quantization for IoT Federated\\n  Learning\", \"summary\": \"Federated Learning (FL) allows collaborative training among multiple devices\\nwithout data sharing, thus enabling privacy-sensitive applications on mobile or\\nInternet of Things (IoT) devices, such as mobile health and asset tracking.\\nHowever, designing an FL system with good model utility that works with low\\ncomputation/communication overhead on heterogeneous, resource-constrained\\nmobile/IoT devices is challenging. To address this problem, this paper proposes\\nFedX, a novel adaptive model decomposition and quantization FL system for IoT.\\nTo balance utility with resource constraints on IoT devices, FedX decomposes a\\nglobal FL model into different sub-networks with adaptive numbers of quantized\\nbits for different devices. The key idea is that a device with fewer resources\\nreceives a smaller sub-network for lower overhead but utilizes a larger number\\nof quantized bits for higher model utility, and vice versa. The quantization\\noperations in FedX are done at the server to reduce the computational load on\\ndevices. FedX iteratively minimizes the losses in the devices' local data and\\nin the server's public data using quantized sub-networks under a regularization\\nterm, and thus it maximizes the benefits of combining FL with model\\nquantization through knowledge sharing among the server and devices in a\\ncost-effective training process. Extensive experiments show that FedX\\nsignificantly improves quantization times by up to 8.43X, on-device computation\\ntime by 1.5X, and total end-to-end training time by 1.36X, compared with\\nbaseline FL systems. We guarantee the global model convergence theoretically\\nand validate local model convergence empirically, highlighting FedX's\\noptimization efficiency.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-17T11:08:51Z\"}"}

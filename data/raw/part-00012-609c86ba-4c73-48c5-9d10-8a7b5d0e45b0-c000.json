{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04223v1\", \"title\": \"FRAIN to Train: A Fast-and-Reliable Solution for Decentralized Federated\\n  Learning\", \"summary\": \"Federated learning (FL) enables collaborative model training across\\ndistributed clients while preserving data locality. Although FedAvg pioneered\\nsynchronous rounds for global model averaging, slower devices can delay\\ncollective progress. Asynchronous FL (e.g., FedAsync) addresses stragglers by\\ncontinuously integrating client updates, yet naive implementations risk client\\ndrift due to non-IID data and stale contributions. Some Blockchain-based FL\\napproaches (e.g., BRAIN) employ robust weighting or scoring of updates to\\nresist malicious or misaligned proposals. However, performance drops can still\\npersist under severe data heterogeneity or high staleness, and synchronization\\noverhead has emerged as a new concern due to its aggregator-free architectures.\\n  We introduce Fast-and-Reliable AI Network, FRAIN, a new asynchronous FL\\nmethod that mitigates these limitations by incorporating two key ideas. First,\\nour FastSync strategy eliminates the need to replay past model versions,\\nenabling newcomers and infrequent participants to efficiently approximate the\\nglobal model. Second, we adopt spherical linear interpolation (SLERP) when\\nmerging parameters, preserving models' directions and alleviating destructive\\ninterference from divergent local training.\\n  Experiments with a CNN image-classification model and a Transformer-based\\nlanguage model demonstrate that FRAIN achieves more stable and robust\\nconvergence than FedAvg, FedAsync, and BRAIN, especially under harsh\\nenvironments: non-IID data distributions, networks that experience delays and\\nrequire frequent re-synchronization, and the presence of malicious nodes.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-05-07T08:20:23Z\"}"}

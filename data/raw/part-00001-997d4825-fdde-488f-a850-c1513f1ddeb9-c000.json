{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04200v1\", \"title\": \"Estimating Causal Effects in Networks with Cluster-Based Bandits\", \"summary\": \"The gold standard for estimating causal effects is randomized controlled\\ntrial (RCT) or A/B testing where a random group of individuals from a\\npopulation of interest are given treatment and the outcome is compared to a\\nrandom group of individuals from the same population. However, A/B testing is\\nchallenging in the presence of interference, commonly occurring in social\\nnetworks, where individuals can impact each others outcome. Moreover, A/B\\ntesting can incur a high performance loss when one of the treatment arms has a\\npoor performance and the test continues to treat individuals with it.\\nTherefore, it is important to design a strategy that can adapt over time and\\nefficiently learn the total treatment effect in the network. We introduce two\\ncluster-based multi-armed bandit (MAB) algorithms to gradually estimate the\\ntotal treatment effect in a network while maximizing the expected reward by\\nmaking a tradeoff between exploration and exploitation. We compare the\\nperformance of our MAB algorithms with a vanilla MAB algorithm that ignores\\nclusters and the corresponding RCT methods on semi-synthetic data with\\nsimulated interference. The vanilla MAB algorithm shows higher reward-action\\nratio at the cost of higher treatment effect error due to undesired spillover.\\nThe cluster-based MAB algorithms show higher reward-action ratio compared to\\ntheir corresponding RCT methods without sacrificing much accuracy in treatment\\neffect estimation.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.SI\", \"published\": \"2025-05-07T07:54:33Z\"}"}

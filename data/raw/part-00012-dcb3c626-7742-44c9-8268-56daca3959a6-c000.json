{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12027v1\", \"title\": \"Understanding Attention Mechanism in Video Diffusion Models\", \"summary\": \"Text-to-video (T2V) synthesis models, such as OpenAI's Sora, have garnered\\nsignificant attention due to their ability to generate high-quality videos from\\na text prompt. In diffusion-based T2V models, the attention mechanism is a\\ncritical component. However, it remains unclear what intermediate features are\\nlearned and how attention blocks in T2V models affect various aspects of video\\nsynthesis, such as image quality and temporal consistency. In this paper, we\\nconduct an in-depth perturbation analysis of the spatial and temporal attention\\nblocks of T2V models using an information-theoretic approach. Our results\\nindicate that temporal and spatial attention maps affect not only the timing\\nand layout of the videos but also the complexity of spatiotemporal elements and\\nthe aesthetic quality of the synthesized videos. Notably, high-entropy\\nattention maps are often key elements linked to superior video quality, whereas\\nlow-entropy attention maps are associated with the video's intra-frame\\nstructure. Based on our findings, we propose two novel methods to enhance video\\nquality and enable text-guided video editing. These methods rely entirely on\\nlightweight manipulation of the attention matrices in T2V models. The efficacy\\nand effectiveness of our methods are further validated through experimental\\nevaluation across multiple datasets.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-16T12:37:08Z\"}"}

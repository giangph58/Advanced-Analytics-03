{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11420v1\", \"title\": \"Reinforcing Compositional Retrieval: Retrieving Step-by-Step for\\n  Composing Informative Contexts\", \"summary\": \"Large Language Models (LLMs) have demonstrated remarkable capabilities across\\nnumerous tasks, yet they often rely on external context to handle complex\\ntasks. While retrieval-augmented frameworks traditionally focus on selecting\\ntop-ranked documents in a single pass, many real-world scenarios demand\\ncompositional retrieval, where multiple sources must be combined in a\\ncoordinated manner. In this work, we propose a tri-encoder sequential retriever\\nthat models this process as a Markov Decision Process (MDP), decomposing the\\nprobability of retrieving a set of elements into a sequence of conditional\\nprobabilities and allowing each retrieval step to be conditioned on previously\\nselected examples. We train the retriever in two stages: first, we efficiently\\nconstruct supervised sequential data for initial policy training; we then\\nrefine the policy to align with the LLM's preferences using a reward grounded\\nin the structural correspondence of generated programs. Experimental results\\nshow that our method consistently and significantly outperforms baselines,\\nunderscoring the importance of explicitly modeling inter-example dependencies.\\nThese findings highlight the potential of compositional retrieval for tasks\\nrequiring multiple pieces of evidence or examples.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-15T17:35:56Z\"}"}

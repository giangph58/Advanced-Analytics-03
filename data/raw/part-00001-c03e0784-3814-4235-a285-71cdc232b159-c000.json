{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04965v1\", \"title\": \"DenseGrounding: Improving Dense Language-Vision Semantics for\\n  Ego-Centric 3D Visual Grounding\", \"summary\": \"Enabling intelligent agents to comprehend and interact with 3D environments\\nthrough natural language is crucial for advancing robotics and human-computer\\ninteraction. A fundamental task in this field is ego-centric 3D visual\\ngrounding, where agents locate target objects in real-world 3D spaces based on\\nverbal descriptions. However, this task faces two significant challenges: (1)\\nloss of fine-grained visual semantics due to sparse fusion of point clouds with\\nego-centric multi-view images, (2) limited textual semantic context due to\\narbitrary language descriptions. We propose DenseGrounding, a novel approach\\ndesigned to address these issues by enhancing both visual and textual\\nsemantics. For visual features, we introduce the Hierarchical Scene Semantic\\nEnhancer, which retains dense semantics by capturing fine-grained global scene\\nfeatures and facilitating cross-modal alignment. For text descriptions, we\\npropose a Language Semantic Enhancer that leverages large language models to\\nprovide rich context and diverse language descriptions with additional context\\nduring model training. Extensive experiments show that DenseGrounding\\nsignificantly outperforms existing methods in overall accuracy, with\\nimprovements of 5.81% and 7.56% when trained on the comprehensive full dataset\\nand smaller mini subset, respectively, further advancing the SOTA in egocentric\\n3D visual grounding. Our method also achieves 1st place and receives the\\nInnovation Award in the CVPR 2024 Autonomous Grand Challenge Multi-view 3D\\nVisual Grounding Track, validating its effectiveness and robustness.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-08T05:49:06Z\"}"}

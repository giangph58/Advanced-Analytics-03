{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03703v1\", \"title\": \"Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text\\n  Representation Learning\", \"summary\": \"Vision-language models (VLMs) allow to embed texts and images in a shared\\nrepresentation space. However, it has been shown that these models are subject\\nto a modality gap phenomenon meaning there exists a clear separation between\\nthe embeddings from one modality and another in the embedding space. While this\\nmisalignment is detrimental for downstream tasks such as multimodal retrieval,\\nmultimodal clustering or zero-shot classification, etc. no generic and\\npractical methods have so far been proposed to assess it precisely and even\\nreduce it. We therefore propose novel measures and effective techniques\\n(spectral- and optimal transport-based methods) to achieve this goal. Extensive\\nexperiments conducted on several image-text datasets and models demonstrate\\ntheir effectiveness and beneficial effects on downstream tasks. Our code is\\navailable at the URL provided in the paper's abstract.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG\", \"published\": \"2025-05-06T17:24:41Z\"}"}

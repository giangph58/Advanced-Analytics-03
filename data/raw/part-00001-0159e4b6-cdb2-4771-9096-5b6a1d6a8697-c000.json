{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14825v1\", \"title\": \"ECViT: Efficient Convolutional Vision Transformer with Local-Attention\\n  and Multi-scale Stages\", \"summary\": \"Vision Transformers (ViTs) have revolutionized computer vision by leveraging\\nself-attention to model long-range dependencies. However, ViTs face challenges\\nsuch as high computational costs due to the quadratic scaling of self-attention\\nand the requirement of a large amount of training data. To address these\\nlimitations, we propose the Efficient Convolutional Vision Transformer (ECViT),\\na hybrid architecture that effectively combines the strengths of CNNs and\\nTransformers. ECViT introduces inductive biases such as locality and\\ntranslation invariance, inherent to Convolutional Neural Networks (CNNs) into\\nthe Transformer framework by extracting patches from low-level features and\\nenhancing the encoder with convolutional operations. Additionally, it\\nincorporates local-attention and a pyramid structure to enable efficient\\nmulti-scale feature extraction and representation. Experimental results\\ndemonstrate that ECViT achieves an optimal balance between performance and\\nefficiency, outperforming state-of-the-art models on various image\\nclassification tasks while maintaining low computational and storage\\nrequirements. ECViT offers an ideal solution for applications that prioritize\\nhigh efficiency without compromising performance.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-21T03:00:17Z\"}"}

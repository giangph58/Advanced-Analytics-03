{"value":"{\"aid\": \"http://arxiv.org/abs/2504.09925v1\", \"title\": \"FUSION: Fully Integration of Vision-Language Representations for Deep\\n  Cross-Modal Understanding\", \"summary\": \"We introduce FUSION, a family of multimodal large language models (MLLMs)\\nwith a fully vision-language alignment and integration paradigm. Unlike\\nexisting methods that primarily rely on late-stage modality interaction during\\nLLM decoding, our approach achieves deep, dynamic integration throughout the\\nentire processing pipeline. To this end, we propose Text-Guided Unified Vision\\nEncoding, incorporating textual information in vision encoding to achieve\\npixel-level integration. We further design Context-Aware Recursive Alignment\\nDecoding that recursively aggregates visual features conditioned on textual\\ncontext during decoding, enabling fine-grained, question-level semantic\\nintegration. To guide feature mapping and mitigate modality discrepancies, we\\ndevelop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a\\nSynthesized Language-Driven Question-Answer (QA) dataset through a new data\\nsynthesis method, prioritizing high-quality QA pairs to optimize text-guided\\nfeature integration. Building on these foundations, we train FUSION at two\\nscales-3B, 8B-and demonstrate that our full-modality integration approach\\nsignificantly outperforms existing methods with only 630 vision tokens.\\nNotably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most\\nbenchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited\\nto 300 vision tokens. Our ablation studies show that FUSION outperforms\\nLLaVA-NeXT on over half of the benchmarks under same configuration without\\ndynamic resolution, highlighting the effectiveness of our approach. We release\\nour code, model weights, and dataset. https://github.com/starriver030515/FUSION\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-14T06:33:29Z\"}"}

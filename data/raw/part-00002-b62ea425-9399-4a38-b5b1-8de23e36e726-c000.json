{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05410v1\", \"title\": \"Reasoning Models Don't Always Say What They Think\", \"summary\": \"Chain-of-thought (CoT) offers a potential boon for AI safety as it allows\\nmonitoring a model's CoT to try to understand its intentions and reasoning\\nprocesses. However, the effectiveness of such monitoring hinges on CoTs\\nfaithfully representing models' actual reasoning processes. We evaluate CoT\\nfaithfulness of state-of-the-art reasoning models across 6 reasoning hints\\npresented in the prompts and find: (1) for most settings and models tested,\\nCoTs reveal their usage of hints in at least 1% of examples where they use the\\nhint, but the reveal rate is often below 20%, (2) outcome-based reinforcement\\nlearning initially improves faithfulness but plateaus without saturating, and\\n(3) when reinforcement learning increases how frequently hints are used (reward\\nhacking), the propensity to verbalize them does not increase, even without\\ntraining against a CoT monitor. These results suggest that CoT monitoring is a\\npromising way of noticing undesired behaviors during training and evaluations,\\nbut that it is not sufficient to rule them out. They also suggest that in\\nsettings like ours where CoT reasoning is not necessary, test-time monitoring\\nof CoTs is unlikely to reliably catch rare and catastrophic unexpected\\nbehaviors.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-05-08T16:51:43Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11227v1\", \"title\": \"VEXP: A Low-Cost RISC-V ISA Extension for Accelerated Softmax\\n  Computation in Transformers\", \"summary\": \"While Transformers are dominated by Floating-Point (FP)\\nMatrix-Multiplications, their aggressive acceleration through dedicated\\nhardware or many-core programmable systems has shifted the performance\\nbottleneck to non-linear functions like Softmax. Accelerating Softmax is\\nchallenging due to its non-pointwise, non-linear nature, with exponentiation as\\nthe most demanding step. To address this, we design a custom arithmetic block\\nfor Bfloat16 exponentiation leveraging a novel approximation algorithm based on\\nSchraudolph's method, and we integrate it into the Floating-Point Unit (FPU) of\\nthe RISC-V cores of a compute cluster, through custom Instruction Set\\nArchitecture (ISA) extensions, with a negligible area overhead of 1\\\\%. By\\noptimizing the software kernels to leverage the extension, we execute Softmax\\nwith 162.7$\\\\times$ less latency and 74.3$\\\\times$ less energy compared to the\\nbaseline cluster, achieving an 8.2$\\\\times$ performance improvement and\\n4.1$\\\\times$ higher energy efficiency for the FlashAttention-2 kernel in GPT-2\\nconfiguration. Moreover, the proposed approach enables a multi-cluster system\\nto efficiently execute end-to-end inference of pre-trained Transformer models,\\nsuch as GPT-2, GPT-3 and ViT, achieving up to 5.8$\\\\times$ and 3.6$\\\\times$\\nreduction in latency and energy consumption, respectively, without requiring\\nre-training and with negligible accuracy loss.\", \"main_category\": \"cs.AR\", \"categories\": \"cs.AR,cs.LG\", \"published\": \"2025-04-15T14:28:48Z\"}"}

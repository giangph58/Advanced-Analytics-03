{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10097v1\", \"title\": \"STaRFormer: Semi-Supervised Task-Informed Representation Learning via\\n  Dynamic Attention-Based Regional Masking for Sequential Data\", \"summary\": \"Accurate predictions using sequential spatiotemporal data are crucial for\\nvarious applications. Utilizing real-world data, we aim to learn the intent of\\na smart device user within confined areas of a vehicle's surroundings. However,\\nin real-world scenarios, environmental factors and sensor limitations result in\\nnon-stationary and irregularly sampled data, posing significant challenges. To\\naddress these issues, we developed a Transformer-based approach, STaRFormer,\\nwhich serves as a universal framework for sequential modeling. STaRFormer\\nemploys a novel, dynamic attention-based regional masking scheme combined with\\nsemi-supervised contrastive learning to enhance task-specific latent\\nrepresentations. Comprehensive experiments on 15 datasets varying in types\\n(including non-stationary and irregularly sampled), domains, sequence lengths,\\ntraining samples, and applications, demonstrate the efficacy and practicality\\nof STaRFormer. We achieve notable improvements over state-of-the-art\\napproaches. Code and data will be made available.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-14T11:03:19Z\"}"}

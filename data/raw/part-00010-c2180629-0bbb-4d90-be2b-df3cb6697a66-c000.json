{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12988v1\", \"title\": \"Why Ask One When You Can Ask $k$? Two-Stage Learning-to-Defer to a Set\\n  of Experts\", \"summary\": \"Learning-to-Defer (L2D) enables decision-making systems to improve\\nreliability by selectively deferring uncertain predictions to more competent\\nagents. However, most existing approaches focus exclusively on single-agent\\ndeferral, which is often inadequate in high-stakes scenarios that require\\ncollective expertise. We propose Top-$k$ Learning-to-Defer, a generalization of\\nthe classical two-stage L2D framework that allocates each query to the $k$ most\\nconfident agents instead of a single one. To further enhance flexibility and\\ncost-efficiency, we introduce Top-$k(x)$ Learning-to-Defer, an adaptive\\nextension that learns the optimal number of agents to consult for each query,\\nbased on input complexity, agent competency distributions, and consultation\\ncosts. For both settings, we derive a novel surrogate loss and prove that it is\\nBayes-consistent and $(\\\\mathcal{R}, \\\\mathcal{G})$-consistent, ensuring\\nconvergence to the Bayes-optimal allocation. Notably, we show that the\\nwell-established model cascades paradigm arises as a restricted instance of our\\nTop-$k$ and Top-$k(x)$ formulations. Extensive experiments across diverse\\nbenchmarks demonstrate the effectiveness of our framework on both\\nclassification and regression tasks.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,stat.ML\", \"published\": \"2025-04-17T14:50:40Z\"}"}

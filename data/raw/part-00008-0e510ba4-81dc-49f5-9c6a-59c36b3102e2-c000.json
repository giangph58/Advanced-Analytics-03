{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03233v1\", \"title\": \"GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale\\n  Synthetic Action Data\", \"summary\": \"Embodied foundation models are gaining increasing attention for their\\nzero-shot generalization, scalability, and adaptability to new tasks through\\nfew-shot post-training. However, existing models rely heavily on real-world\\ndata, which is costly and labor-intensive to collect. Synthetic data offers a\\ncost-effective alternative, yet its potential remains largely underexplored. To\\nbridge this gap, we explore the feasibility of training Vision-Language-Action\\nmodels entirely with large-scale synthetic action data. We curate SynGrasp-1B,\\na billion-frame robotic grasping dataset generated in simulation with\\nphotorealistic rendering and extensive domain randomization. Building on this,\\nwe present GraspVLA, a VLA model pretrained on large-scale synthetic action\\ndata as a foundational model for grasping tasks. GraspVLA integrates\\nautoregressive perception tasks and flow-matching-based action generation into\\na unified Chain-of-Thought process, enabling joint training on synthetic action\\ndata and Internet semantics data. This design helps mitigate sim-to-real gaps\\nand facilitates the transfer of learned actions to a broader range of\\nInternet-covered objects, achieving open-vocabulary generalization in grasping.\\nExtensive evaluations across real-world and simulation benchmarks demonstrate\\nGraspVLA's advanced zero-shot generalizability and few-shot adaptability to\\nspecific human preferences. We will release SynGrasp-1B dataset and pre-trained\\nweights to benefit the community.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO\", \"published\": \"2025-05-06T06:59:28Z\"}"}

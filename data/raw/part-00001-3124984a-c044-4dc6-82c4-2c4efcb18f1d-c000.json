{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10003v1\", \"title\": \"NaviDiffusor: Cost-Guided Diffusion Model for Visual Navigation\", \"summary\": \"Visual navigation, a fundamental challenge in mobile robotics, demands\\nversatile policies to handle diverse environments. Classical methods leverage\\ngeometric solutions to minimize specific costs, offering adaptability to new\\nscenarios but are prone to system errors due to their multi-modular design and\\nreliance on hand-crafted rules. Learning-based methods, while achieving high\\nplanning success rates, face difficulties in generalizing to unseen\\nenvironments beyond the training data and often require extensive training. To\\naddress these limitations, we propose a hybrid approach that combines the\\nstrengths of learning-based methods and classical approaches for RGB-only\\nvisual navigation. Our method first trains a conditional diffusion model on\\ndiverse path-RGB observation pairs. During inference, it integrates the\\ngradients of differentiable scene-specific and task-level costs, guiding the\\ndiffusion model to generate valid paths that meet the constraints. This\\napproach alleviates the need for retraining, offering a plug-and-play solution.\\nExtensive experiments in both indoor and outdoor settings, across simulated and\\nreal-world scenarios, demonstrate zero-shot transfer capability of our\\napproach, achieving higher success rates and fewer collisions compared to\\nbaseline methods. Code will be released at\\nhttps://github.com/SYSU-RoboticsLab/NaviD.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.CV\", \"published\": \"2025-04-14T09:06:02Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10471v1\", \"title\": \"MIEB: Massive Image Embedding Benchmark\", \"summary\": \"Image representations are often evaluated through disjointed, task-specific\\nprotocols, leading to a fragmented understanding of model capabilities. For\\ninstance, it is unclear whether an image embedding model adept at clustering\\nimages is equally good at retrieving relevant images given a piece of text. We\\nintroduce the Massive Image Embedding Benchmark (MIEB) to evaluate the\\nperformance of image and image-text embedding models across the broadest\\nspectrum to date. MIEB spans 38 languages across 130 individual tasks, which we\\ngroup into 8 high-level categories. We benchmark 50 models across our\\nbenchmark, finding that no single method dominates across all task categories.\\nWe reveal hidden capabilities in advanced vision models such as their accurate\\nvisual representation of texts, and their yet limited capabilities in\\ninterleaved encodings and matching images and texts in the presence of\\nconfounders. We also show that the performance of vision encoders on MIEB\\ncorrelates highly with their performance when used in multimodal large language\\nmodels. Our code, dataset, and leaderboard are publicly available at\\nhttps://github.com/embeddings-benchmark/mteb.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.CL\", \"published\": \"2025-04-14T17:54:28Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11907v1\", \"title\": \"A Graph-Based Reinforcement Learning Approach with Frontier Potential\\n  Based Reward for Safe Cluttered Environment Exploration\", \"summary\": \"Autonomous exploration of cluttered environments requires efficient\\nexploration strategies that guarantee safety against potential collisions with\\nunknown random obstacles. This paper presents a novel approach combining a\\ngraph neural network-based exploration greedy policy with a safety shield to\\nensure safe navigation goal selection. The network is trained using\\nreinforcement learning and the proximal policy optimization algorithm to\\nmaximize exploration efficiency while reducing the safety shield interventions.\\nHowever, if the policy selects an infeasible action, the safety shield\\nintervenes to choose the best feasible alternative, ensuring system\\nconsistency. Moreover, this paper proposes a reward function that includes a\\npotential field based on the agent's proximity to unexplored regions and the\\nexpected information gain from reaching them. Overall, the approach\\ninvestigated in this paper merges the benefits of the adaptability of\\nreinforcement learning-driven exploration policies and the guarantee ensured by\\nexplicit safety mechanisms. Extensive evaluations in simulated environments\\ndemonstrate that the approach enables efficient and safe exploration in\\ncluttered environments.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,I.2.9\", \"published\": \"2025-04-16T09:31:14Z\"}"}

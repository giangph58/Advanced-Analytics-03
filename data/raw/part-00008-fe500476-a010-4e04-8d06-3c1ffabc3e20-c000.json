{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20679v1\", \"title\": \"Are Information Retrieval Approaches Good at Harmonising Longitudinal\\n  Survey Questions in Social Science?\", \"summary\": \"Automated detection of semantically equivalent questions in longitudinal\\nsocial science surveys is crucial for long-term studies informing empirical\\nresearch in the social, economic, and health sciences. Retrieving equivalent\\nquestions faces dual challenges: inconsistent representation of theoretical\\nconstructs (i.e. concept/sub-concept) across studies as well as between\\nquestion and response options, and the evolution of vocabulary and structure in\\nlongitudinal text. To address these challenges, our multi-disciplinary\\ncollaboration of computer scientists and survey specialists presents a new\\ninformation retrieval (IR) task of identifying concept (e.g. Housing, Job,\\netc.) equivalence across question and response options to harmonise\\nlongitudinal population studies. This paper investigates multiple unsupervised\\napproaches on a survey dataset spanning 1946-2020, including probabilistic\\nmodels, linear probing of language models, and pre-trained neural networks\\nspecialised for IR. We show that IR-specialised neural models achieve the\\nhighest overall performance with other approaches performing comparably.\\nAdditionally, the re-ranking of the probabilistic model's results with neural\\nmodels only introduces modest improvements of 0.07 at most in F1-score.\\nQualitative post-hoc evaluation by survey specialists shows that models\\ngenerally have a low sensitivity to questions with high lexical overlap,\\nparticularly in cases where sub-concepts are mismatched. Altogether, our\\nanalysis serves to further research on harmonising longitudinal studies in\\nsocial science.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.IR\", \"published\": \"2025-04-29T12:00:33Z\"}"}

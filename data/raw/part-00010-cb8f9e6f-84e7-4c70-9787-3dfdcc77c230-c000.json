{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19774v1\", \"title\": \"If Concept Bottlenecks are the Question, are Foundation Models the\\n  Answer?\", \"summary\": \"Concept Bottleneck Models (CBMs) are neural networks designed to conjoin high\\nperformance with ante-hoc interpretability. CBMs work by first mapping inputs\\n(e.g., images) to high-level concepts (e.g., visible objects and their\\nproperties) and then use these to solve a downstream task (e.g., tagging or\\nscoring an image) in an interpretable manner. Their performance and\\ninterpretability, however, hinge on the quality of the concepts they learn. The\\ngo-to strategy for ensuring good quality concepts is to leverage expert\\nannotations, which are expensive to collect and seldom available in\\napplications. Researchers have recently addressed this issue by introducing\\n\\\"VLM-CBM\\\" architectures that replace manual annotations with weak supervision\\nfrom foundation models. It is however unclear what is the impact of doing so on\\nthe quality of the learned concepts. To answer this question, we put\\nstate-of-the-art VLM-CBMs to the test, analyzing their learned concepts\\nempirically using a selection of significant metrics. Our results show that,\\ndepending on the task, VLM supervision can sensibly differ from expert\\nannotations, and that concept accuracy and quality are not strongly correlated.\\nOur code is available at https://github.com/debryu/CQA.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-28T13:18:48Z\"}"}

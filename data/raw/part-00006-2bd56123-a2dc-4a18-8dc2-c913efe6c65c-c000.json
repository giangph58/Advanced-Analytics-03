{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21281v1\", \"title\": \"Mamba Based Feature Extraction And Adaptive Multilevel Feature Fusion\\n  For 3D Tumor Segmentation From Multi-modal Medical Image\", \"summary\": \"Multi-modal 3D medical image segmentation aims to accurately identify tumor\\nregions across different modalities, facing challenges from variations in image\\nintensity and tumor morphology. Traditional convolutional neural network\\n(CNN)-based methods struggle with capturing global features, while\\nTransformers-based methods, despite effectively capturing global context,\\nencounter high computational costs in 3D medical image segmentation. The Mamba\\nmodel combines linear scalability with long-distance modeling, making it a\\npromising approach for visual representation learning. However, Mamba-based 3D\\nmulti-modal segmentation still struggles to leverage modality-specific features\\nand fuse complementary information effectively. In this paper, we propose a\\nMamba based feature extraction and adaptive multilevel feature fusion for 3D\\ntumor segmentation using multi-modal medical image. We first develop the\\nspecific modality Mamba encoder to efficiently extract long-range relevant\\nfeatures that represent anatomical and pathological structures present in each\\nmodality. Moreover, we design an bi-level synergistic integration block that\\ndynamically merges multi-modal and multi-level complementary features by the\\nmodality attention and channel attention learning. Lastly, the decoder combines\\ndeep semantic information with fine-grained details to generate the tumor\\nsegmentation map. Experimental results on medical image datasets (PET/CT and\\nMRI multi-sequence) show that our approach achieve competitive performance\\ncompared to the state-of-the-art CNN, Transformer, and Mamba-based approaches.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-30T03:29:55Z\"}"}

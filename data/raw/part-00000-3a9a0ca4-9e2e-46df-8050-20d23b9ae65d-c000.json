{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19506v1\", \"title\": \"SynergyAmodal: Deocclude Anything with Text Control\", \"summary\": \"Image deocclusion (or amodal completion) aims to recover the invisible\\nregions (\\\\ie, shape and appearance) of occluded instances in images. Despite\\nrecent advances, the scarcity of high-quality data that balances diversity,\\nplausibility, and fidelity remains a major obstacle. To address this challenge,\\nwe identify three critical elements: leveraging in-the-wild image data for\\ndiversity, incorporating human expertise for plausibility, and utilizing\\ngenerative priors for fidelity. We propose SynergyAmodal, a novel framework for\\nco-synthesizing in-the-wild amodal datasets with comprehensive shape and\\nappearance annotations, which integrates these elements through a tripartite\\ndata-human-model collaboration. First, we design an occlusion-grounded\\nself-supervised learning algorithm to harness the diversity of in-the-wild\\nimage data, fine-tuning an inpainting diffusion model into a partial completion\\ndiffusion model. Second, we establish a co-synthesis pipeline to iteratively\\nfilter, refine, select, and annotate the initial deocclusion results of the\\npartial completion diffusion model, ensuring plausibility and fidelity through\\nhuman expert guidance and prior model constraints. This pipeline generates a\\nhigh-quality paired amodal dataset with extensive category and scale diversity,\\ncomprising approximately 16K pairs. Finally, we train a full completion\\ndiffusion model on the synthesized dataset, incorporating text prompts as\\nconditioning signals. Extensive experiments demonstrate the effectiveness of\\nour framework in achieving zero-shot generalization and textual\\ncontrollability. Our code, dataset, and models will be made publicly available\\nat https://github.com/imlixinyang/SynergyAmodal.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-28T06:04:17Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16516v1\", \"title\": \"Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion\\n  and Reasoning for Vision-and-Language Navigation\", \"summary\": \"Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow\\nnatural language instructions and reach target locations in real-world\\nenvironments. While prior methods often rely on either global scene\\nrepresentations or object-level features, these approaches are insufficient for\\ncapturing the complex interactions across modalities required for accurate\\nnavigation. In this paper, we propose a Multi-level Fusion and Reasoning\\nArchitecture (MFRA) to enhance the agent's ability to reason over visual\\nobservations, language instructions and navigation history. Specifically, MFRA\\nintroduces a hierarchical fusion mechanism that aggregates multi-level\\nfeatures-ranging from low-level visual cues to high-level semantic\\nconcepts-across multiple modalities. We further design a reasoning module that\\nleverages fused representations to infer navigation actions through\\ninstruction-guided attention and dynamic context integration. By selectively\\ncapturing and combining relevant visual, linguistic, and temporal signals, MFRA\\nimproves decision-making accuracy in complex navigation scenarios. Extensive\\nexperiments on benchmark VLN datasets including REVERIE, R2R, and SOON\\ndemonstrate that MFRA achieves superior performance compared to\\nstate-of-the-art methods, validating the effectiveness of multi-level modal\\nfusion for embodied navigation.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-23T08:41:27Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21311v1\", \"title\": \"Covert Prompt Transmission for Secure Large Language Model Services\", \"summary\": \"This paper investigates covert prompt transmission for secure and efficient\\nlarge language model (LLM) services over wireless networks. We formulate a\\nlatency minimization problem under fidelity and detectability constraints to\\nensure confidential and covert communication by jointly optimizing the transmit\\npower and prompt compression ratio. To solve this problem, we first propose a\\nprompt compression and encryption (PCAE) framework, performing surprisal-guided\\ncompression followed by lightweight permutation-based encryption. Specifically,\\nPCAE employs a locally deployed small language model (SLM) to estimate\\ntoken-level surprisal scores, selectively retaining semantically critical\\ntokens while discarding redundant ones. This significantly reduces\\ncomputational overhead and transmission duration. To further enhance covert\\nwireless transmission, we then develop a group-based proximal policy\\noptimization (GPPO) method that samples multiple candidate actions for each\\nstate, selecting the optimal one within each group and incorporating a\\nKullback-Leibler (KL) divergence penalty to improve policy stability and\\nexploration. Simulation results show that PCAE achieves comparable LLM response\\nfidelity to baseline methods while reducing preprocessing latency by over five\\norders of magnitude, enabling real-time edge deployment. We further validate\\nPCAE effectiveness across diverse LLM backbones, including DeepSeek-32B,\\nQwen-32B, and their smaller variants. Moreover, GPPO reduces covert\\ntransmission latency by up to 38.6\\\\% compared to existing reinforcement\\nlearning strategies, with further analysis showing that increased transmit\\npower provides additional latency benefits.\", \"main_category\": \"cs.NI\", \"categories\": \"cs.NI\", \"published\": \"2025-04-30T04:53:11Z\"}"}

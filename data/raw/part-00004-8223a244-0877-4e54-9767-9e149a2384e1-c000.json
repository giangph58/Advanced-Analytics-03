{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04340v1\", \"title\": \"Multi-Granular Attention based Heterogeneous Hypergraph Neural Network\", \"summary\": \"Heterogeneous graph neural networks (HeteGNNs) have demonstrated strong\\nabilities to learn node representations by effectively extracting complex\\nstructural and semantic information in heterogeneous graphs. Most of the\\nprevailing HeteGNNs follow the neighborhood aggregation paradigm, leveraging\\nmeta-path based message passing to learn latent node representations. However,\\ndue to the pairwise nature of meta-paths, these models fail to capture\\nhigh-order relations among nodes, resulting in suboptimal performance.\\nAdditionally, the challenge of ``over-squashing'', where long-range message\\npassing in HeteGNNs leads to severe information distortion, further limits the\\nefficacy of these models. To address these limitations, this paper proposes\\nMGA-HHN, a Multi-Granular Attention based Heterogeneous Hypergraph Neural\\nNetwork for heterogeneous graph representation learning. MGA-HHN introduces two\\nkey innovations: (1) a novel approach for constructing meta-path based\\nheterogeneous hypergraphs that explicitly models higher-order semantic\\ninformation in heterogeneous graphs through multiple views, and (2) a\\nmulti-granular attention mechanism that operates at both the node and hyperedge\\nlevels. This mechanism enables the model to capture fine-grained interactions\\namong nodes sharing the same semantic context within a hyperedge type, while\\npreserving the diversity of semantics across different hyperedge types. As\\nsuch, MGA-HHN effectively mitigates long-range message distortion and generates\\nmore expressive node representations. Extensive experiments on real-world\\nbenchmark datasets demonstrate that MGA-HHN outperforms state-of-the-art\\nmodels, showcasing its effectiveness in node classification, node clustering\\nand visualization tasks.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-05-07T11:42:00Z\"}"}

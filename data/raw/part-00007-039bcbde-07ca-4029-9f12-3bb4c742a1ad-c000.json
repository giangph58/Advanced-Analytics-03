{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12997v1\", \"title\": \"All-in-One Transferring Image Compression from Human Perception to\\n  Multi-Machine Perception\", \"summary\": \"Efficiently transferring Learned Image Compression (LIC) model from human\\nperception to machine perception is an emerging challenge in vision-centric\\nrepresentation learning. Existing approaches typically adapt LIC to downstream\\ntasks in a single-task manner, which is inefficient, lacks task interaction,\\nand results in multiple task-specific bitstreams. To address these limitations,\\nwe propose an asymmetric adaptor framework that supports multi-task adaptation\\nwithin a single model. Our method introduces a shared adaptor to learn general\\nsemantic features and task-specific adaptors to preserve task-level\\ndistinctions. With only lightweight plug-in modules and a frozen base codec,\\nour method achieves strong performance across multiple tasks while maintaining\\ncompression efficiency. Experiments on the PASCAL-Context benchmark demonstrate\\nthat our method outperforms both Fully Fine-Tuned and other Parameter Efficient\\nFine-Tuned (PEFT) baselines, and validating the effectiveness of multi-vision\\ntransferring.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-17T15:06:52Z\"}"}

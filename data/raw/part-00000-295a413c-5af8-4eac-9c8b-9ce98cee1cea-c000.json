{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15568v1\", \"title\": \"Is Learning Effective in Dynamic Strategic Interactions? Evidence from\\n  Stackelberg Games\", \"summary\": \"In many settings of interest, a policy is set by one party, the leader, in\\norder to influence the action of another party, the follower, where the\\nfollower's response is determined by some private information. A natural\\nquestion to ask is, can the leader improve their strategy by learning about the\\nunknown follower through repeated interactions? A well known folk theorem from\\ndynamic pricing, a special case of this leader-follower setting, would suggest\\nthat the leader cannot learn effectively from the follower when the follower is\\nfully strategic, leading to a large literature on learning in strategic\\nsettings that relies on limiting the strategic space of the follower in order\\nto provide positive results. In this paper, we study dynamic Bayesian\\nStackelberg games, where a leader and a \\\\emph{fully strategic} follower\\ninteract repeatedly, with the follower's type unknown. Contrary to existing\\nresults, we show that the leader can improve their utility through learning in\\nrepeated play. Using a novel average-case analysis, we demonstrate that\\nlearning is effective in these settings, without needing to weaken the\\nfollower's strategic space. Importantly, this improvement is not solely due to\\nthe leader's ability to commit, nor does learning simply substitute for\\ncommunication between the parties. We provide an algorithm, based on a\\nmixed-integer linear program, to compute the optimal leader policy in these\\ngames and develop heuristic algorithms to approximate the optimal dynamic\\npolicy more efficiently. Through simulations, we compare the efficiency and\\nruntime of these algorithms against static policies.\", \"main_category\": \"cs.GT\", \"categories\": \"cs.GT\", \"published\": \"2025-04-22T03:44:05Z\"}"}

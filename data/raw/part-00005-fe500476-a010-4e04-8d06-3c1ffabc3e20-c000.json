{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20676v1\", \"title\": \"The Limits of AI Explainability: An Algorithmic Information Theory\\n  Approach\", \"summary\": \"This paper establishes a theoretical foundation for understanding the\\nfundamental limits of AI explainability through algorithmic information theory.\\nWe formalize explainability as the approximation of complex models by simpler\\nones, quantifying both approximation error and explanation complexity using\\nKolmogorov complexity. Our key theoretical contributions include: (1) a\\ncomplexity gap theorem proving that any explanation significantly simpler than\\nthe original model must differ from it on some inputs; (2) precise bounds\\nshowing that explanation complexity grows exponentially with input dimension\\nbut polynomially with error tolerance for Lipschitz functions; and (3) a\\ncharacterization of the gap between local and global explainability,\\ndemonstrating that local explanations can be significantly simpler while\\nmaintaining accuracy in relevant regions. We further establish a regulatory\\nimpossibility theorem proving that no governance framework can simultaneously\\npursue unrestricted AI capabilities, human-interpretable explanations, and\\nnegligible error. These results highlight considerations likely to be relevant\\nto the design, evaluation, and oversight of explainable AI systems.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CY,cs.IT,math.IT\", \"published\": \"2025-04-29T11:58:37Z\"}"}

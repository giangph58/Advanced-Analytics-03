{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16516v1\", \"title\": \"Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion\\n  and Reasoning for Vision-and-Language Navigation\", \"summary\": \"Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow\\nnatural language instructions and reach target locations in real-world\\nenvironments. While prior methods often rely on either global scene\\nrepresentations or object-level features, these approaches are insufficient for\\ncapturing the complex interactions across modalities required for accurate\\nnavigation. In this paper, we propose a Multi-level Fusion and Reasoning\\nArchitecture (MFRA) to enhance the agent's ability to reason over visual\\nobservations, language instructions and navigation history. Specifically, MFRA\\nintroduces a hierarchical fusion mechanism that aggregates multi-level\\nfeatures-ranging from low-level visual cues to high-level semantic\\nconcepts-across multiple modalities. We further design a reasoning module that\\nleverages fused representations to infer navigation actions through\\ninstruction-guided attention and dynamic context integration. By selectively\\ncapturing and combining relevant visual, linguistic, and temporal signals, MFRA\\nimproves decision-making accuracy in complex navigation scenarios. Extensive\\nexperiments on benchmark VLN datasets including REVERIE, R2R, and SOON\\ndemonstrate that MFRA achieves superior performance compared to\\nstate-of-the-art methods, validating the effectiveness of multi-level modal\\nfusion for embodied navigation.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-23T08:41:27Z\"}"}
{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16533v1\", \"title\": \"SafeSpect: Safety-First Augmented Reality Heads-up Display for Drone\\n  Inspections\", \"summary\": \"Current tablet-based interfaces for drone operations often impose a heavy\\ncognitive load on pilots and reduce situational awareness by dividing attention\\nbetween the video feed and the real world. To address these challenges, we\\ndesigned a heads-up augmented reality (AR) interface that overlays in-situ\\ninformation to support drone pilots in safety-critical tasks. Through\\nparticipatory design workshops with professional pilots, we identified key\\nfeatures and developed an adaptive AR interface that dynamically switches\\nbetween task and safety views to prevent information overload. We evaluated our\\nprototype by creating a realistic building inspection task and comparing three\\ninterfaces: a 2D tablet, a static AR, and our adaptive AR design. A user study\\nwith 15 participants showed that the AR interface improved access to safety\\ninformation, while the adaptive AR interface reduced cognitive load and\\nenhanced situational awareness without compromising task performance. We offer\\ndesign insights for developing safety-first heads-up AR interfaces.\", \"main_category\": \"cs.HC\", \"categories\": \"cs.HC\", \"published\": \"2025-04-23T08:59:05Z\"}"}
{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16549v1\", \"title\": \"Exponential mixing of random interval diffeomorphisms\", \"summary\": \"We consider a finite number of orientation preserving $C^2$ interval\\ndiffeomorphisms and apply them randomly in such a way that the expected\\nLyapunov exponents at the boundary points are positive. We prove the\\nexponential mixing, with respect to the unique stationary measure supported on\\nthe interior of the interval. The key step is to show the exponential\\nsynchronization in average.\", \"main_category\": \"math.DS\", \"categories\": \"math.DS\", \"published\": \"2025-04-23T09:25:49Z\"}"}

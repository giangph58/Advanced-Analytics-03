{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12222v1\", \"title\": \"Coding-Prior Guided Diffusion Network for Video Deblurring\", \"summary\": \"While recent video deblurring methods have advanced significantly, they often\\noverlook two valuable prior information: (1) motion vectors (MVs) and coding\\nresiduals (CRs) from video codecs, which provide efficient inter-frame\\nalignment cues, and (2) the rich real-world knowledge embedded in pre-trained\\ndiffusion generative models. We present CPGDNet, a novel two-stage framework\\nthat effectively leverages both coding priors and generative diffusion priors\\nfor high-quality deblurring. First, our coding-prior feature propagation (CPFP)\\nmodule utilizes MVs for efficient frame alignment and CRs to generate attention\\nmasks, addressing motion inaccuracies and texture variations. Second, a\\ncoding-prior controlled generation (CPC) module network integrates coding\\npriors into a pretrained diffusion model, guiding it to enhance critical\\nregions and synthesize realistic details. Experiments demonstrate our method\\nachieves state-of-the-art perceptual quality with up to 30% improvement in IQA\\nmetrics. Both the code and the codingprior-augmented dataset will be\\nopen-sourced.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-16T16:14:43Z\"}"}

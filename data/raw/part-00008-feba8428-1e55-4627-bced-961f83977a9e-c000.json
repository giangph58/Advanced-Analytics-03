{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17263v1\", \"title\": \"Precision Neural Network Quantization via Learnable Adaptive Modules\", \"summary\": \"Quantization Aware Training (QAT) is a neural network quantization technique\\nthat compresses model size and improves operational efficiency while\\neffectively maintaining model performance. The paradigm of QAT is to introduce\\nfake quantization operators during the training process, allowing the model to\\nautonomously compensate for information loss caused by quantization. Making\\nquantization parameters trainable can significantly improve the performance of\\nQAT, but at the cost of compromising the flexibility during inference,\\nespecially when dealing with activation values with substantially different\\ndistributions. In this paper, we propose an effective learnable adaptive neural\\nnetwork quantization method, called Adaptive Step Size Quantization (ASQ), to\\nresolve this conflict. Specifically, the proposed ASQ method first dynamically\\nadjusts quantization scaling factors through a trained module capable of\\naccommodating different activations. Then, to address the rigid resolution\\nissue inherent in Power of Two (POT) quantization, we propose an efficient\\nnon-uniform quantization scheme. We utilize the Power Of Square root of Two\\n(POST) as the basis for exponential quantization, effectively handling the\\nbell-shaped distribution of neural network weights across various bit-widths\\nwhile maintaining computational efficiency through a Look-Up Table method\\n(LUT). Extensive experimental results demonstrate that the proposed ASQ method\\nis superior to the state-of-the-art QAT approaches. Notably that the ASQ is\\neven competitive compared to full precision baselines, with its 4-bit quantized\\nResNet34 model improving accuracy by 1.2\\\\% on ImageNet.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.CC\", \"published\": \"2025-04-24T05:46:25Z\"}"}

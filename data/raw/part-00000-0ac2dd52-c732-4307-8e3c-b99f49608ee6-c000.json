{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05683v1\", \"title\": \"Towards Smarter Hiring: Are Zero-Shot and Few-Shot Pre-trained LLMs\\n  Ready for HR Spoken Interview Transcript Analysis?\", \"summary\": \"This research paper presents a comprehensive analysis of the performance of\\nprominent pre-trained large language models (LLMs), including GPT-4 Turbo,\\nGPT-3.5 Turbo, text-davinci-003, text-babbage-001, text-curie-001,\\ntext-ada-001, llama-2-7b-chat, llama-2-13b-chat, and llama-2-70b-chat, in\\ncomparison to expert human evaluators in providing scores, identifying errors,\\nand offering feedback and improvement suggestions to candidates during mock HR\\n(Human Resources) interviews. We introduce a dataset called HURIT (Human\\nResource Interview Transcripts), which comprises 3,890 HR interview transcripts\\nsourced from real-world HR interview scenarios. Our findings reveal that\\npre-trained LLMs, particularly GPT-4 Turbo and GPT-3.5 Turbo, exhibit\\ncommendable performance and are capable of producing evaluations comparable to\\nthose of expert human evaluators. Although these LLMs demonstrate proficiency\\nin providing scores comparable to human experts in terms of human evaluation\\nmetrics, they frequently fail to identify errors and offer specific actionable\\nadvice for candidate performance improvement in HR interviews. Our research\\nsuggests that the current state-of-the-art pre-trained LLMs are not fully\\nconducive for automatic deployment in an HR interview assessment. Instead, our\\nfindings advocate for a human-in-the-loop approach, to incorporate manual\\nchecks for inconsistencies and provisions for improving feedback quality as a\\nmore suitable strategy.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-08T04:46:10Z\"}"}

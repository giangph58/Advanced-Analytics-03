{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07594v1\", \"title\": \"Extending Visual Dynamics for Video-to-Music Generation\", \"summary\": \"Music profoundly enhances video production by improving quality, engagement,\\nand emotional resonance, sparking growing interest in video-to-music\\ngeneration. Despite recent advances, existing approaches remain limited in\\nspecific scenarios or undervalue the visual dynamics. To address these\\nlimitations, we focus on tackling the complexity of dynamics and resolving\\ntemporal misalignment between video and music representations. To this end, we\\npropose DyViM, a novel framework to enhance dynamics modeling for\\nvideo-to-music generation. Specifically, we extract frame-wise dynamics\\nfeatures via a simplified motion encoder inherited from optical flow methods,\\nfollowed by a self-attention module for aggregation within frames. These\\ndynamic features are then incorporated to extend existing music tokens for\\ntemporal alignment. Additionally, high-level semantics are conveyed through a\\ncross-attention mechanism, and an annealing tuning strategy benefits to\\nfine-tune well-trained music decoders efficiently, therefore facilitating\\nseamless adaptation. Extensive experiments demonstrate DyViM's superiority over\\nstate-of-the-art (SOTA) methods.\", \"main_category\": \"cs.MM\", \"categories\": \"cs.MM,cs.CV\", \"published\": \"2025-04-10T09:47:26Z\"}"}

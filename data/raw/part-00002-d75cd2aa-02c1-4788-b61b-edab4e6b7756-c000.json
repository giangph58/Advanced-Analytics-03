{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19854v1\", \"title\": \"NORA: A Small Open-Sourced Generalist Vision Language Action Model for\\n  Embodied Tasks\", \"summary\": \"Existing Visual-Language-Action (VLA) models have shown promising performance\\nin zero-shot scenarios, demonstrating impressive task execution and reasoning\\ncapabilities. However, a significant challenge arises from the limitations of\\nvisual encoding, which can result in failures during tasks such as object\\ngrasping. Moreover, these models typically suffer from high computational\\noverhead due to their large sizes, often exceeding 7B parameters. While these\\nmodels excel in reasoning and task planning, the substantial computational\\noverhead they incur makes them impractical for real-time robotic environments,\\nwhere speed and efficiency are paramount. To address the limitations of\\nexisting VLA models, we propose NORA, a 3B-parameter model designed to reduce\\ncomputational overhead while maintaining strong task performance. NORA adopts\\nthe Qwen-2.5-VL-3B multimodal model as its backbone, leveraging its superior\\nvisual-semantic understanding to enhance visual reasoning and action grounding.\\nAdditionally, our \\\\model{} is trained on 970k real-world robot demonstrations\\nand equipped with the FAST+ tokenizer for efficient action sequence generation.\\nExperimental results demonstrate that NORA outperforms existing large-scale VLA\\nmodels, achieving better task performance with significantly reduced\\ncomputational overhead, making it a more practical solution for real-time\\nrobotic autonomy.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.AI,cs.CV\", \"published\": \"2025-04-28T14:47:34Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16415v1\", \"title\": \"Natural Policy Gradient for Average Reward Non-Stationary RL\", \"summary\": \"We consider the problem of non-stationary reinforcement learning (RL) in the\\ninfinite-horizon average-reward setting. We model it by a Markov Decision\\nProcess with time-varying rewards and transition probabilities, with a\\nvariation budget of $\\\\Delta_T$. Existing non-stationary RL algorithms focus on\\nmodel-based and model-free value-based methods. Policy-based methods despite\\ntheir flexibility in practice are not theoretically well understood in\\nnon-stationary RL. We propose and analyze the first model-free policy-based\\nalgorithm, Non-Stationary Natural Actor-Critic (NS-NAC), a policy gradient\\nmethod with a restart based exploration for change and a novel interpretation\\nof learning rates as adapting factors. Further, we present a bandit-over-RL\\nbased parameter-free algorithm BORL-NS-NAC that does not require prior\\nknowledge of the variation budget $\\\\Delta_T$. We present a dynamic regret of\\n$\\\\tilde{\\\\mathscr O}(|S|^{1/2}|A|^{1/2}\\\\Delta_T^{1/6}T^{5/6})$ for both\\nalgorithms, where $T$ is the time horizon, and $|S|$, $|A|$ are the sizes of\\nthe state and action spaces. The regret analysis leverages a novel adaptation\\nof the Lyapunov function analysis of NAC to dynamic environments and\\ncharacterizes the effects of simultaneous updates in policy, value function\\nestimate and changes in the environment.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,stat.ML\", \"published\": \"2025-04-23T04:37:26Z\"}"}

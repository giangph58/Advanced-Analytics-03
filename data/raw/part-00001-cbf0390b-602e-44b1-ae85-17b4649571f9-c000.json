{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05237v1\", \"title\": \"Latte: Transfering LLMs` Latent-level Knowledge for Few-shot Tabular\\n  Learning\", \"summary\": \"Few-shot tabular learning, in which machine learning models are trained with\\na limited amount of labeled data, provides a cost-effective approach to\\naddressing real-world challenges. The advent of Large Language Models (LLMs)\\nhas sparked interest in leveraging their pre-trained knowledge for few-shot\\ntabular learning. Despite promising results, existing approaches either rely on\\ntest-time knowledge extraction, which introduces undesirable latency, or\\ntext-level knowledge, which leads to unreliable feature engineering. To\\novercome these limitations, we propose Latte, a training-time knowledge\\nextraction framework that transfers the latent prior knowledge within LLMs to\\noptimize a more generalized downstream model. Latte enables general\\nknowledge-guided downstream tabular learning, facilitating the weighted fusion\\nof information across different feature values while reducing the risk of\\noverfitting to limited labeled data. Furthermore, Latte is compatible with\\nexisting unsupervised pre-training paradigms and effectively utilizes available\\nunlabeled samples to overcome the performance limitations imposed by an\\nextremely small labeled dataset. Extensive experiments on various few-shot\\ntabular learning benchmarks demonstrate the superior performance of Latte,\\nestablishing it as a state-of-the-art approach in this domain\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-08T13:32:09Z\"}"}

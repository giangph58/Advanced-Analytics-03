{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13068v1\", \"title\": \"Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative\\n  Classification Models\", \"summary\": \"This study explores the relationship between deep learning (DL) model\\naccuracy and expert agreement in the classification of crash narratives. We\\nevaluate five DL models -- including BERT variants, the Universal Sentence\\nEncoder (USE), and a zero-shot classifier -- against expert-labeled data and\\nnarrative text. The analysis is further extended to four large language models\\n(LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive\\ntrend: models with higher technical accuracy often exhibit lower agreement with\\ndomain experts, whereas LLMs demonstrate greater expert alignment despite\\nrelatively lower accuracy scores. To quantify and interpret model-expert\\nagreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and\\nSHAP-based explainability techniques. Findings indicate that expert-aligned\\nmodels tend to rely more on contextual and temporal language cues, rather than\\nlocation-specific keywords. These results underscore that accuracy alone is\\ninsufficient for evaluating models in safety-critical NLP applications. We\\nadvocate for incorporating expert agreement as a complementary metric in model\\nevaluation frameworks and highlight the promise of LLMs as interpretable,\\nscalable tools for crash analysis pipelines.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-17T16:29:08Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21356v1\", \"title\": \"Nexus-Gen: A Unified Model for Image Understanding, Generation, and\\n  Editing\", \"summary\": \"Unified multimodal large language models (MLLMs) aim to integrate multimodal\\nunderstanding and generation abilities through a single framework. Despite\\ntheir versatility, existing open-source unified models exhibit performance gaps\\nagainst domain-specific architectures. To bridge this gap, we present\\nNexus-Gen, a unified model that synergizes the language reasoning capabilities\\nof LLMs with the image synthesis power of diffusion models. To align the\\nembedding space of the LLM and diffusion model, we conduct a dual-phase\\nalignment training process. (1) The autoregressive LLM learns to predict image\\nembeddings conditioned on multimodal inputs, while (2) the vision decoder is\\ntrained to reconstruct high-fidelity images from these embeddings. During\\ntraining the LLM, we identified a critical discrepancy between the\\nautoregressive paradigm's training and inference phases, where error\\naccumulation in continuous embedding space severely degrades generation\\nquality. To avoid this issue, we introduce a prefilled autoregression strategy\\nthat prefills input sequence with position-embedded special tokens instead of\\ncontinuous embeddings. Through dual-phase training, Nexus-Gen has developed the\\nintegrated capability to comprehensively address the image understanding,\\ngeneration and editing tasks. All models, datasets, and codes are published at\\nhttps://github.com/modelscope/Nexus-Gen.git to facilitate further advancements\\nacross the field.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-30T06:30:48Z\"}"}

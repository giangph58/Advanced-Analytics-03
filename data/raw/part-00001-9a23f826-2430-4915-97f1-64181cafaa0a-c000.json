{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03335v1\", \"title\": \"Absolute Zero: Reinforced Self-play Reasoning with Zero Data\", \"summary\": \"Reinforcement learning with verifiable rewards (RLVR) has shown promise in\\nenhancing the reasoning capabilities of large language models by learning\\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\\nzero setting avoid supervision in labeling the reasoning process, but still\\ndepend on manually curated collections of questions and answers for training.\\nThe scarcity of high-quality, human-produced examples raises concerns about the\\nlong-term scalability of relying on human supervision, a challenge already\\nevident in the domain of language model pretraining. Furthermore, in a\\nhypothetical future where AI surpasses human intelligence, tasks provided by\\nhumans may offer limited learning potential for a superintelligent system. To\\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\\nwhich a single model learns to propose tasks that maximize its own learning\\nprogress and improves reasoning by solving them, without relying on any\\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\\n(AZR), a system that self-evolves its training curriculum and reasoning ability\\nby using a code executor to both validate proposed code reasoning tasks and\\nverify answers, serving as an unified source of verifiable reward to guide\\nopen-ended yet grounded learning. Despite being trained entirely without\\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\\nreasoning tasks, outperforming existing zero-setting models that rely on tens\\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\\nthat AZR can be effectively applied across different model scales and is\\ncompatible with various model classes.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL\", \"published\": \"2025-05-06T09:08:00Z\"}"}

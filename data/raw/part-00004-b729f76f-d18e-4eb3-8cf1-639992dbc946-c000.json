{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03475v1\", \"title\": \"am-ELO: A Stable Framework for Arena-based LLM Evaluation\", \"summary\": \"Arena-based evaluation is a fundamental yet significant evaluation paradigm\\nfor modern AI models, especially large language models (LLMs). Existing\\nframework based on ELO rating system suffers from the inevitable instability\\nproblem due to ranking inconsistency and the lack of attention to the varying\\nabilities of annotators. In this paper, we introduce a novel stable arena\\nframework to address these issues by enhancing the ELO Rating System.\\nSpecifically, we replace the iterative update method with a Maximum Likelihood\\nEstimation (MLE) approach, m-ELO, and provide theoretical proof of the\\nconsistency and stability of the MLE approach for model ranking. Additionally,\\nwe proposed the am-ELO, which modify the Elo Rating's probability function to\\nincorporate annotator abilities, enabling the simultaneous estimation of model\\nscores and annotator reliability. Experiments demonstrate that this method\\nensures stability, proving that this framework offers a more robust, accurate,\\nand stable evaluation method for LLMs.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.LG\", \"published\": \"2025-05-06T12:28:50Z\"}"}

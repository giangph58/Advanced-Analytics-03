{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03310v1\", \"title\": \"3D Gaussian Splatting Data Compression with Mixture of Priors\", \"summary\": \"3D Gaussian Splatting (3DGS) data compression is crucial for enabling\\nefficient storage and transmission in 3D scene modeling. However, its\\ndevelopment remains limited due to inadequate entropy models and suboptimal\\nquantization strategies for both lossless and lossy compression scenarios,\\nwhere existing methods have yet to 1) fully leverage hyperprior information to\\nconstruct robust conditional entropy models, and 2) apply fine-grained,\\nelement-wise quantization strategies for improved compression granularity. In\\nthis work, we propose a novel Mixture of Priors (MoP) strategy to\\nsimultaneously address these two challenges. Specifically, inspired by the\\nMixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior\\ninformation through multiple lightweight MLPs to generate diverse prior\\nfeatures, which are subsequently integrated into the MoP feature via a gating\\nmechanism. To enhance lossless compression, the resulting MoP feature is\\nutilized as a hyperprior to improve conditional entropy modeling. Meanwhile,\\nfor lossy compression, we employ the MoP feature as guidance information in an\\nelement-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine\\nQuantization (C2FQ) strategy with a predefined quantization step value.\\nSpecifically, we expand the quantization step value into a matrix and\\nadaptively refine it from coarse to fine granularity, guided by the MoP\\nfeature, thereby obtaining a quantization step matrix that facilitates\\nelement-wise quantization. Extensive experiments demonstrate that our proposed\\n3DGS data compression framework achieves state-of-the-art performance across\\nmultiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and\\nTank&Temples.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-06T08:42:39Z\"}"}

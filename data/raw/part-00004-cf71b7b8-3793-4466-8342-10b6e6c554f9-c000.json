{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10188v1\", \"title\": \"Efficient Generative Model Training via Embedded Representation Warmup\", \"summary\": \"Diffusion models excel at generating high-dimensional data but fall short in\\ntraining efficiency and representation quality compared to self-supervised\\nmethods. We identify a key bottleneck: the underutilization of high-quality,\\nsemantically rich representations during training notably slows down\\nconvergence. Our systematic analysis reveals a critical representation\\nprocessing region -- primarily in the early layers -- where semantic and\\nstructural pattern learning takes place before generation can occur. To address\\nthis, we propose Embedded Representation Warmup (ERW), a plug-and-play\\nframework where in the first stage we get the ERW module serves as a warmup\\nthat initializes the early layers of the diffusion model with high-quality,\\npretrained representations. This warmup minimizes the burden of learning\\nrepresentations from scratch, thereby accelerating convergence and boosting\\nperformance. Our theoretical analysis demonstrates that ERW's efficacy depends\\non its precise integration into specific neural network layers -- termed the\\nrepresentation processing region -- where the model primarily processes and\\ntransforms feature representations for later generation. We further establish\\nthat ERW not only accelerates training convergence but also enhances\\nrepresentation quality: empirically, our method achieves a 40$\\\\times$\\nacceleration in training speed compared to REPA, the current state-of-the-art\\nmethods. Code is available at https://github.com/LINs-lab/ERW.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-14T12:43:17Z\"}"}

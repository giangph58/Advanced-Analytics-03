{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20635v1\", \"title\": \"Bridging the Generalisation Gap: Synthetic Data Generation for\\n  Multi-Site Clinical Model Validation\", \"summary\": \"Ensuring the generalisability of clinical machine learning (ML) models across\\ndiverse healthcare settings remains a significant challenge due to variability\\nin patient demographics, disease prevalence, and institutional practices.\\nExisting model evaluation approaches often rely on real-world datasets, which\\nare limited in availability, embed confounding biases, and lack the flexibility\\nneeded for systematic experimentation. Furthermore, while generative models aim\\nfor statistical realism, they often lack transparency and explicit control over\\nfactors driving distributional shifts. In this work, we propose a novel\\nstructured synthetic data framework designed for the controlled benchmarking of\\nmodel robustness, fairness, and generalisability. Unlike approaches focused\\nsolely on mimicking observed data, our framework provides explicit control over\\nthe data generating process, including site-specific prevalence variations,\\nhierarchical subgroup effects, and structured feature interactions. This\\nenables targeted investigation into how models respond to specific\\ndistributional shifts and potential biases. Through controlled experiments, we\\ndemonstrate the framework's ability to isolate the impact of site variations,\\nsupport fairness-aware audits, and reveal generalisation failures, particularly\\nhighlighting how model complexity interacts with site-specific effects. This\\nwork contributes a reproducible, interpretable, and configurable tool designed\\nto advance the reliable deployment of ML in clinical settings.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-29T11:04:28Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10905v1\", \"title\": \"InterAnimate: Taming Region-aware Diffusion Model for Realistic Human\\n  Interaction Animation\", \"summary\": \"Recent video generation research has focused heavily on isolated actions,\\nleaving interactive motions-such as hand-face interactions-largely unexamined.\\nThese interactions are essential for emerging biometric authentication systems,\\nwhich rely on interactive motion-based anti-spoofing approaches. From a\\nsecurity perspective, there is a growing need for large-scale, high-quality\\ninteractive videos to train and strengthen authentication models. In this work,\\nwe introduce a novel paradigm for animating realistic hand-face interactions.\\nOur approach simultaneously learns spatio-temporal contact dynamics and\\nbiomechanically plausible deformation effects, enabling natural interactions\\nwhere hand movements induce anatomically accurate facial deformations while\\nmaintaining collision-free contact. To facilitate this research, we present\\nInterHF, a large-scale hand-face interaction dataset featuring 18 interaction\\npatterns and 90,000 annotated videos. Additionally, we propose InterAnimate, a\\nregion-aware diffusion model designed specifically for interaction animation.\\nInterAnimate leverages learnable spatial and temporal latents to effectively\\ncapture dynamic interaction priors and integrates a region-aware interaction\\nmechanism that injects these priors into the denoising process. To the best of\\nour knowledge, this work represents the first large-scale effort to\\nsystematically study human hand-face interactions. Qualitative and quantitative\\nresults show InterAnimate produces highly realistic animations, setting a new\\nbenchmark. Code and data will be made public to advance research.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.HC\", \"published\": \"2025-04-15T06:32:45Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20406v1\", \"title\": \"Skill Discovery for Software Scripting Automation via Offline\\n  Simulations with LLMs\", \"summary\": \"Scripting interfaces enable users to automate tasks and customize software\\nworkflows, but creating scripts traditionally requires programming expertise\\nand familiarity with specific APIs, posing barriers for many users. While Large\\nLanguage Models (LLMs) can generate code from natural language queries, runtime\\ncode generation is severely limited due to unverified code, security risks,\\nlonger response times, and higher computational costs. To bridge the gap, we\\npropose an offline simulation framework to curate a software-specific skillset,\\na collection of verified scripts, by exploiting LLMs and publicly available\\nscripting guides. Our framework comprises two components: (1) task creation,\\nusing top-down functionality guidance and bottom-up API synergy exploration to\\ngenerate helpful tasks; and (2) skill generation with trials, refining and\\nvalidating scripts based on execution feedback. To efficiently navigate the\\nextensive API landscape, we introduce a Graph Neural Network (GNN)-based link\\nprediction model to capture API synergy, enabling the generation of skills\\ninvolving underutilized APIs and expanding the skillset's diversity.\\nExperiments with Adobe Illustrator demonstrate that our framework significantly\\nimproves automation success rates, reduces response time, and saves runtime\\ntoken costs compared to traditional runtime code generation. This is the first\\nattempt to use software scripting interfaces as a testbed for LLM-based\\nsystems, highlighting the advantages of leveraging execution feedback in a\\ncontrolled environment and offering valuable insights into aligning AI\\ncapabilities with user needs in specialized software domains.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.SE\", \"published\": \"2025-04-29T04:03:37Z\"}"}

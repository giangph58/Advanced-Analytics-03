{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17550v1\", \"title\": \"HalluLens: LLM Hallucination Benchmark\", \"summary\": \"Large language models (LLMs) often generate responses that deviate from user\\ninput or training data, a phenomenon known as \\\"hallucination.\\\" These\\nhallucinations undermine user trust and hinder the adoption of generative AI\\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\\nThis paper introduces a comprehensive hallucination benchmark, incorporating\\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\\nthe lack of a unified framework due to inconsistent definitions and\\ncategorizations. We disentangle LLM hallucination from \\\"factuality,\\\" proposing\\na clear taxonomy that distinguishes between extrinsic and intrinsic\\nhallucinations, to promote consistency and facilitate research. Extrinsic\\nhallucinations, where the generated content is not consistent with the training\\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\\ntest set generation to mitigate data leakage and ensure robustness against such\\nleakage. We also analyze existing benchmarks, highlighting their limitations\\nand saturation. The work aims to: (1) establish a clear taxonomy of\\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\\ncomprehensive analysis of existing benchmarks, distinguishing them from\\nfactuality evaluations.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-24T13:40:27Z\"}"}

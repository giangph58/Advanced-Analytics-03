{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15989v1\", \"title\": \"Token-Aware Coding Flow: A Study with Nano Surge in Reasoning Model\", \"summary\": \"With the widespread application of large-scale language models (LLMs) in\\nsoftware engineering, the Chain of Thought (CoT) approach has emerged as a\\ncrucial tool for driving automated code generation and optimization. However,\\ndespite the significant success of CoT methods in generating high-quality code,\\nthe issue of token inflation during the reasoning process remains a formidable\\nchallenge to model performance and efficiency, particularly when dealing with\\ncomplex code smells. Code smells not only affect the maintainability and\\nscalability of code but also significantly increase the computational burden\\nduring LLM inference, leading to excessive token consumption and, consequently,\\nreduced reasoning efficiency. This paper introduces an innovative Token-Aware\\nCoding Flow method, aimed at addressing the token inflation problem caused by\\nsmelly code in the CoT process. Through experimentation, we validate the\\nsynergistic effect of code refactoring and prompt engineering strategies,\\ndemonstrating that after eliminating code smells, token consumption during\\nmodel inference is significantly reduced. The experimental results show that\\nrefactored code, while maintaining functional consistency, can reduce token\\nconsumption by up to 50\\\\%. Additionally, by explicitly prompting the type of\\ncode smells in the prompt and incorporating strategies such as context\\nawareness and role constraints, we further optimize the reasoning process,\\nachieving a 24.5\\\\% to 30\\\\% reduction in token consumption. These optimizations\\nnot only significantly enhance the model's reasoning efficiency and improve\\ncode generation quality but also provide new insights for addressing\\nperformance bottlenecks in complex code generation tasks.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE\", \"published\": \"2025-04-22T15:51:00Z\"}"}

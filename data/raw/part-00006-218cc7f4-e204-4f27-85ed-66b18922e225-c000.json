{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05464v1\", \"title\": \"Bring Reason to Vision: Understanding Perception and Reasoning through\\n  Model Merging\", \"summary\": \"Vision-Language Models (VLMs) combine visual perception with the general\\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\\nmechanisms by which these two abilities can be combined and contribute remain\\npoorly understood. In this work, we explore to compose perception and reasoning\\nthrough model merging that connects parameters of different models. Unlike\\nprevious works that often focus on merging models of the same kind, we propose\\nmerging models across modalities, enabling the incorporation of the reasoning\\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\\nthat model merging offers a successful pathway to transfer reasoning abilities\\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\\nmodels to understand the internal mechanism of perception and reasoning and how\\nmerging affects it. We find that perception capabilities are predominantly\\nencoded in the early layers of the model, whereas reasoning is largely\\nfacilitated by the middle-to-late layers. After merging, we observe that all\\nlayers begin to contribute to reasoning, whereas the distribution of perception\\nabilities across layers remains largely unchanged. These observations shed\\nlight on the potential of model merging as a tool for multimodal integration\\nand interpretation.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-05-08T17:56:23Z\"}"}

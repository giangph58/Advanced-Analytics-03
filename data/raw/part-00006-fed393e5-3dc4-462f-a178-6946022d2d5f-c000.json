{"value":"{\"aid\": \"http://arxiv.org/abs/2504.09895v1\", \"title\": \"Learning from Reference Answers: Versatile Language Model Alignment\\n  without Binary Human Preference Data\", \"summary\": \"Large language models~(LLMs) are expected to be helpful, harmless, and\\nhonest. In various alignment scenarios, such as general human preference,\\nsafety, and confidence alignment, binary preference data collection and reward\\nmodeling are resource-intensive but necessary for human preference\\ntransferring. In this work, we explore using the similarity between sampled\\ngenerations and high-quality reference answers as an alternative reward\\nfunction for LLM alignment. Using similarity as a reward circumvents training\\nreward models, and collecting a single reference answer potentially costs less\\ntime than constructing binary preference pairs when multiple candidates are\\navailable. Specifically, we develop \\\\textit{RefAlign}, a versatile\\nREINFORCE-style alignment algorithm, which is free of reference and reward\\nmodels. Instead, RefAlign utilizes BERTScore between sampled generations and\\nhigh-quality reference answers as the surrogate reward. Beyond general human\\npreference optimization, RefAlign can be readily extended to diverse scenarios,\\nsuch as safety and confidence alignment, by incorporating the similarity reward\\nwith task-related objectives. In various scenarios, {RefAlign} demonstrates\\ncomparable performance to previous alignment methods while offering high\\nefficiency.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-14T05:43:21Z\"}"}

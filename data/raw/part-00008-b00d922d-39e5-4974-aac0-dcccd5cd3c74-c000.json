{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15223v1\", \"title\": \"A Deep Learning Framework for Sequence Mining with Bidirectional LSTM\\n  and Multi-Scale Attention\", \"summary\": \"This paper addresses the challenges of mining latent patterns and modeling\\ncontextual dependencies in complex sequence data. A sequence pattern mining\\nalgorithm is proposed by integrating Bidirectional Long Short-Term Memory\\n(BiLSTM) with a multi-scale attention mechanism. The BiLSTM captures both\\nforward and backward dependencies in sequences, enhancing the model's ability\\nto perceive global contextual structures. At the same time, the multi-scale\\nattention module assigns adaptive weights to key feature regions under\\ndifferent window sizes. This improves the model's responsiveness to both local\\nand global important information. Extensive experiments are conducted on a\\npublicly available multivariate time series dataset. The proposed model is\\ncompared with several mainstream sequence modeling methods. Results show that\\nit outperforms existing models in terms of accuracy, precision, and recall.\\nThis confirms the effectiveness and robustness of the proposed architecture in\\ncomplex pattern recognition tasks. Further ablation studies and sensitivity\\nanalyses are carried out to investigate the effects of attention scale and\\ninput sequence length on model performance. These results provide empirical\\nsupport for structural optimization of the model.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-21T16:53:02Z\"}"}

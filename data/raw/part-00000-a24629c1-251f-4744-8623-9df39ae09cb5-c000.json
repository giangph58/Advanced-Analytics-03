{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24014v1\", \"title\": \"Optimization of Layer Skipping and Frequency Scaling for Convolutional\\n  Neural Networks under Latency Constraint\", \"summary\": \"The energy consumption of Convolutional Neural Networks (CNNs) is a critical\\nfactor in deploying deep learning models on resource-limited equipment such as\\nmobile devices and autonomous vehicles. We propose an approach involving\\nProportional Layer Skipping (PLS) and Frequency Scaling (FS). Layer skipping\\nreduces computational complexity by selectively bypassing network layers,\\nwhereas frequency scaling adjusts the frequency of the processor to optimize\\nenergy use under latency constraints. Experiments of PLS and FS on ResNet-152\\nwith the CIFAR-10 dataset demonstrated significant reductions in computational\\ndemands and energy consumption with minimal accuracy loss. This study offers\\npractical solutions for improving real-time processing in resource-limited\\nsettings and provides insights into balancing computational efficiency and\\nmodel performance.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-31T12:40:11Z\"}"}

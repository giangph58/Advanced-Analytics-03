{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11389v1\", \"title\": \"VideoPanda: Video Panoramic Diffusion with Multi-view Attention\", \"summary\": \"High resolution panoramic video content is paramount for immersive\\nexperiences in Virtual Reality, but is non-trivial to collect as it requires\\nspecialized equipment and intricate camera setups. In this work, we introduce\\nVideoPanda, a novel approach for synthesizing 360$^\\\\circ$ videos conditioned on\\ntext or single-view video data. VideoPanda leverages multi-view attention\\nlayers to augment a video diffusion model, enabling it to generate consistent\\nmulti-view videos that can be combined into immersive panoramic content.\\nVideoPanda is trained jointly using two conditions: text-only and single-view\\nvideo, and supports autoregressive generation of long-videos. To overcome the\\ncomputational burden of multi-view video generation, we randomly subsample the\\nduration and camera views used during training and show that the model is able\\nto gracefully generalize to generating more frames during inference. Extensive\\nevaluations on both real-world and synthetic video datasets demonstrate that\\nVideoPanda generates more realistic and coherent 360$^\\\\circ$ panoramas across\\nall input conditions compared to existing methods. Visit the project website at\\nhttps://research-staging.nvidia.com/labs/toronto-ai/VideoPanda/ for results.\", \"main_category\": \"cs.GR\", \"categories\": \"cs.GR,cs.AI,cs.CV\", \"published\": \"2025-04-15T16:58:15Z\"}"}

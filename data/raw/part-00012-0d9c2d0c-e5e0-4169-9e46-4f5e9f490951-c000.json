{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05209v1\", \"title\": \"EAM: Enhancing Anything with Diffusion Transformers for Blind\\n  Super-Resolution\", \"summary\": \"Utilizing pre-trained Text-to-Image (T2I) diffusion models to guide Blind\\nSuper-Resolution (BSR) has become a predominant approach in the field. While\\nT2I models have traditionally relied on U-Net architectures, recent\\nadvancements have demonstrated that Diffusion Transformers (DiT) achieve\\nsignificantly higher performance in this domain. In this work, we introduce\\nEnhancing Anything Model (EAM), a novel BSR method that leverages DiT and\\noutperforms previous U-Net-based approaches. We introduce a novel block,\\n$\\\\Psi$-DiT, which effectively guides the DiT to enhance image restoration. This\\nblock employs a low-resolution latent as a separable flow injection control,\\nforming a triple-flow architecture that effectively leverages the prior\\nknowledge embedded in the pre-trained DiT. To fully exploit the prior guidance\\ncapabilities of T2I models and enhance their generalization in BSR, we\\nintroduce a progressive Masked Image Modeling strategy, which also reduces\\ntraining costs. Additionally, we propose a subject-aware prompt generation\\nstrategy that employs a robust multi-modal model in an in-context learning\\nframework. This strategy automatically identifies key image areas, provides\\ndetailed descriptions, and optimizes the utilization of T2I diffusion priors.\\nOur experiments demonstrate that EAM achieves state-of-the-art results across\\nmultiple datasets, outperforming existing methods in both quantitative metrics\\nand visual quality.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-08T13:03:07Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03586v1\", \"title\": \"Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning\\n  Framework for Mitigating Delayed Observation\", \"summary\": \"In real-world multi-agent systems (MASs), observation delays are ubiquitous,\\npreventing agents from making decisions based on the environment's true state.\\nAn individual agent's local observation often consists of multiple components\\nfrom other agents or dynamic entities in the environment. These discrete\\nobservation components with varying delay characteristics pose significant\\nchallenges for multi-agent reinforcement learning (MARL). In this paper, we\\nfirst formulate the decentralized stochastic individual delay partially\\nobservable Markov decision process (DSID-POMDP) by extending the standard\\nDec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL\\ntraining framework for addressing stochastic individual delays, along with\\nrecommended implementations for its constituent modules. We implement the\\nDSID-POMDP's observation generation pattern using standard MARL benchmarks,\\nincluding MPE and SMAC. Experiments demonstrate that baseline MARL methods\\nsuffer severe performance degradation under fixed and unfixed delays. The\\nRDC-enhanced approach mitigates this issue, remarkably achieving ideal\\ndelay-free performance in certain delay scenarios while maintaining\\ngeneralization capability. Our work provides a novel perspective on multi-agent\\ndelayed observation problems and offers an effective solution framework.\", \"main_category\": \"cs.MA\", \"categories\": \"cs.MA,cs.AI,I.2\", \"published\": \"2025-05-06T14:47:56Z\"}"}

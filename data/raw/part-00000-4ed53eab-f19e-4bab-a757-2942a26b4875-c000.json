{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20834v1\", \"title\": \"Reinforcement Learning for LLM Reasoning Under Memory Constraints\", \"summary\": \"We explore reinforcement learning (RL) techniques to enhance reasoning within\\ntargeted problem spaces in large language models (LLMs) under memory and\\ncompute constraints. Our focus is on critic-free methods compatible with LoRA\\nfine-tuning on a single 40GB GPU, a common limitation in academic settings. We\\nintroduce S-GRPO, a memory-efficient variant of Group Relative Policy\\nOptimization, and T-SPMO, a token-level prefix matching strategy for\\nfine-grained credit assignment. Despite limited resources, when used to\\nfine-tune Qwen2-1.5B both methods significantly improve SVAMP benchmark\\naccuracy from 46% to above 70% using LoRA training. T-SPMO also excels in\\nmulti-digit multiplication tasks, underscoring the potential of RL fine-tuning\\nunder hardware constraints. Additionally, we find that our full-token GRPO\\nbaseline under LoRA fine-tuning did not improve model performance (compared to\\nbase model) on either task, suggesting that our memory-efficient methods may\\nact as a form of regularization that stabilizes training when only a small\\nsubset of parameters are updated.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-29T14:58:43Z\"}"}

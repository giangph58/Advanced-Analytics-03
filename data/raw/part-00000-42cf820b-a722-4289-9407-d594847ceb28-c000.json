{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24028v1\", \"title\": \"Pay More Attention to the Robustness of Prompt for Instruction Data\\n  Mining\", \"summary\": \"Instruction tuning has emerged as a paramount method for tailoring the\\nbehaviors of LLMs. Recent work has unveiled the potential for LLMs to achieve\\nhigh performance through fine-tuning with a limited quantity of high-quality\\ninstruction data. Building upon this approach, we further explore the impact of\\nprompt's robustness on the selection of high-quality instruction data. This\\npaper proposes a pioneering framework of high-quality online instruction data\\nmining for instruction tuning, focusing on the impact of prompt's robustness on\\nthe data mining process. Our notable innovation, is to generate the adversarial\\ninstruction data by conducting the attack for the prompt of online instruction\\ndata. Then, we introduce an Adversarial Instruction-Following Difficulty metric\\nto measure how much help the adversarial instruction data can provide to the\\ngeneration of the corresponding response. Apart from it, we propose a novel\\nAdversarial Instruction Output Embedding Consistency approach to select\\nhigh-quality online instruction data. We conduct extensive experiments on two\\nbenchmark datasets to assess the performance. The experimental results serve to\\nunderscore the effectiveness of our proposed two methods. Moreover, the results\\nunderscore the critical practical significance of considering prompt's\\nrobustness.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-03-31T12:53:08Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20500v1\", \"title\": \"UniDetox: Universal Detoxification of Large Language Models via Dataset\\n  Distillation\", \"summary\": \"We present UniDetox, a universally applicable method designed to mitigate\\ntoxicity across various large language models (LLMs). Previous detoxification\\nmethods are typically model-specific, addressing only individual models or\\nmodel families, and require careful hyperparameter tuning due to the trade-off\\nbetween detoxification efficacy and language modeling performance. In contrast,\\nUniDetox provides a detoxification technique that can be universally applied to\\na wide range of LLMs without the need for separate model-specific tuning.\\nSpecifically, we propose a novel and efficient dataset distillation technique\\nfor detoxification using contrastive decoding. This approach distills\\ndetoxifying representations in the form of synthetic text data, enabling\\nuniversal detoxification of any LLM through fine-tuning with the distilled\\ntext. Our experiments demonstrate that the detoxifying text distilled from\\nGPT-2 can effectively detoxify larger models, including OPT, Falcon, and\\nLLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter\\ntuning for each model, as a single hyperparameter configuration can be\\nseamlessly applied across different models. Additionally, analysis of the\\ndetoxifying text reveals a reduction in politically biased content, providing\\ninsights into the attributes necessary for effective detoxification of LLMs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-29T07:40:00Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21800v1\", \"title\": \"How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in\\n  Prolonged Exposure Dialogues\", \"summary\": \"The growing adoption of synthetic data in healthcare is driven by privacy\\nconcerns, limited access to real-world data, and the high cost of annotation.\\nThis work explores the use of synthetic Prolonged Exposure (PE) therapeutic\\nconversations for Post-Traumatic Stress Disorder (PTSD) as a scalable\\nalternative for training and evaluating clinical models. We systematically\\ncompare real and synthetic dialogues using linguistic, structural, and\\nprotocol-specific metrics, including turn-taking patterns and treatment\\nfidelity. We also introduce and evaluate PE-specific metrics derived from\\nlinguistic analysis and semantic modeling, offering a novel framework for\\nassessing clinical fidelity beyond surface fluency. Our findings show that\\nalthough synthetic data holds promise for mitigating data scarcity and\\nprotecting patient privacy, it can struggle to capture the subtle dynamics of\\ntherapeutic interactions. In our dataset, synthetic dialogues match structural\\nfeatures of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99),\\nhowever, synthetic interactions do not adequately reflect key fidelity markers\\n(e.g., distress monitoring). We highlight gaps in existing evaluation\\nframeworks and advocate for fidelity-aware metrics that go beyond surface\\nfluency to uncover clinically significant failures. Our findings clarify where\\nsynthetic data can effectively complement real-world datasets -- and where\\ncritical limitations remain.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.CY,cs.HC\", \"published\": \"2025-04-30T16:56:56Z\"}"}

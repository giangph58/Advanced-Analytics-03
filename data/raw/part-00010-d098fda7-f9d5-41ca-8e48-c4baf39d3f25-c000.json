{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12801v1\", \"title\": \"Sign-In to the Lottery: Reparameterizing Sparse Training From Scratch\", \"summary\": \"The performance gap between training sparse neural networks from scratch\\n(PaI) and dense-to-sparse training presents a major roadblock for efficient\\ndeep learning. According to the Lottery Ticket Hypothesis, PaI hinges on\\nfinding a problem specific parameter initialization. As we show, to this end,\\ndetermining correct parameter signs is sufficient. Yet, they remain elusive to\\nPaI. To address this issue, we propose Sign-In, which employs a dynamic\\nreparameterization that provably induces sign flips. Such sign flips are\\ncomplementary to the ones that dense-to-sparse training can accomplish,\\nrendering Sign-In as an orthogonal method. While our experiments and theory\\nsuggest performance improvements of PaI, they also carve out the main open\\nchallenge to close the gap between PaI and dense-to-sparse training.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CV\", \"published\": \"2025-04-17T10:01:59Z\"}"}

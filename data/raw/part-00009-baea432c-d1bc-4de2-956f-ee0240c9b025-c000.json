{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04841v1\", \"title\": \"Prior2Former -- Evidential Modeling of Mask Transformers for\\n  Assumption-Free Open-World Panoptic Segmentation\", \"summary\": \"In panoptic segmentation, individual instances must be separated within\\nsemantic classes. As state-of-the-art methods rely on a pre-defined set of\\nclasses, they struggle with novel categories and out-of-distribution (OOD)\\ndata. This is particularly problematic in safety-critical applications, such as\\nautonomous driving, where reliability in unseen scenarios is essential. We\\naddress the gap between outstanding benchmark performance and reliability by\\nproposing Prior2Former (P2F), the first approach for segmentation vision\\ntransformers rooted in evidential learning. P2F extends the mask vision\\ntransformer architecture by incorporating a Beta prior for computing model\\nuncertainty in pixel-wise binary mask assignments. This design enables\\nhigh-quality uncertainty estimation that effectively detects novel and OOD\\nobjects enabling state-of-the-art anomaly instance segmentation and open-world\\npanoptic segmentation. Unlike most segmentation models addressing unknown\\nclasses, P2F operates without access to OOD data samples or contrastive\\ntraining on void (i.e., unlabeled) classes, making it highly applicable in\\nreal-world scenarios where such prior information is unavailable. Additionally,\\nP2F can be flexibly applied to anomaly instance and panoptic segmentation.\\nThrough comprehensive experiments on the Cityscapes, COCO, SegmentMeIfYouCan,\\nand OoDIS datasets, we demonstrate the state-of-the-art performance of P2F. It\\nachieves the highest ranking in the OoDIS anomaly instance benchmark among\\nmethods not using OOD data in any way.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-07T08:53:14Z\"}"}

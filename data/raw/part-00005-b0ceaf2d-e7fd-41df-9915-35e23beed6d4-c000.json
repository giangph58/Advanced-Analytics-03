{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20490v1\", \"title\": \"Hetu v2: A General and Scalable Deep Learning System with Hierarchical\\n  and Heterogeneous Single Program Multiple Data Annotations\", \"summary\": \"The Single Program Multiple Data (SPMD) paradigm provides a unified\\nabstraction to annotate various parallel dimensions in distributed deep\\nlearning (DL) training. With SPMD, users can write training programs from the\\nviewpoint of a single device, and the system will automatically deduce the\\ntensor sharding and communication patterns. However, with the recent\\ndevelopment in large-scale DL models, distributed training exhibits spatial and\\ntemporal workload heterogeneity, arising from both device disparities (e.g.,\\nmixed hardware, failures) and data variations (e.g., uneven sequence lengths).\\nSuch heterogeneity violates SPMD's assumption of uniform workload partitioning,\\nwhich restricts its ability to express and optimize heterogeneous parallel\\nstrategies effectively.\\n  To address this, we propose HSPMD within the Hetu v2 system to achieve\\ngeneral and scalable DL training. HSPMD extends SPMD's annotations to support\\nasymmetric sharding and composes standard communication primitives for\\nhierarchical communication, all while retaining the simplicity of a\\nsingle-device declarative programming model. Leveraging HSPMD, Hetu handles\\nspatial heterogeneity through progressive graph specialization, enabling\\ndevice-specific execution logic, and addresses temporal heterogeneity via\\ndynamic graph switching. Evaluations on heterogeneous clusters, elastic\\ntraining, and mixed-length data scenarios show that HSPMD matches or\\noutperforms specialized systems, providing a flexible and efficient solution\\nfor modern large-scale model training.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC\", \"published\": \"2025-04-29T07:27:54Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03356v1\", \"title\": \"Effective Reinforcement Learning Control using Conservative Soft\\n  Actor-Critic\", \"summary\": \"Reinforcement Learning (RL) has shown great potential in complex control\\ntasks, particularly when combined with deep neural networks within the\\nActor-Critic (AC) framework. However, in practical applications, balancing\\nexploration, learning stability, and sample efficiency remains a significant\\nchallenge. Traditional methods such as Soft Actor-Critic (SAC) and Proximal\\nPolicy Optimization (PPO) address these issues by incorporating entropy or\\nrelative entropy regularization, but often face problems of instability and low\\nsample efficiency. In this paper, we propose the Conservative Soft Actor-Critic\\n(CSAC) algorithm, which seamlessly integrates entropy and relative entropy\\nregularization within the AC framework. CSAC improves exploration through\\nentropy regularization while avoiding overly aggressive policy updates with the\\nuse of relative entropy regularization. Evaluations on benchmark tasks and\\nreal-world robotic simulations demonstrate that CSAC offers significant\\nimprovements in stability and efficiency over existing methods. These findings\\nsuggest that CSAC provides strong robustness and application potential in\\ncontrol tasks under dynamic environments.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO\", \"published\": \"2025-05-06T09:26:29Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16083v1\", \"title\": \"MMInference: Accelerating Pre-filling for Long-Context VLMs via\\n  Modality-Aware Permutation Sparse Attention\", \"summary\": \"The integration of long-context capabilities with visual understanding\\nunlocks unprecedented potential for Vision Language Models (VLMs). However, the\\nquadratic attention complexity during the pre-filling phase remains a\\nsignificant obstacle to real-world deployment. To overcome this limitation, we\\nintroduce MMInference (Multimodality Million tokens Inference), a dynamic\\nsparse attention method that accelerates the prefilling stage for long-context\\nmulti-modal inputs. First, our analysis reveals that the temporal and spatial\\nlocality of video input leads to a unique sparse pattern, the Grid pattern.\\nSimultaneously, VLMs exhibit markedly different sparse distributions across\\ndifferent modalities. We introduce a permutation-based method to leverage the\\nunique Grid pattern and handle modality boundary issues. By offline search the\\noptimal sparse patterns for each head, MMInference constructs the sparse\\ndistribution dynamically based on the input. We also provide optimized GPU\\nkernels for efficient sparse computations. Notably, MMInference integrates\\nseamlessly into existing VLM pipelines without any model modifications or\\nfine-tuning. Experiments on multi-modal benchmarks-including Video QA,\\nCaptioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art\\nlong-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that\\nMMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while\\nmaintaining accuracy. Our code is available at https://aka.ms/MMInference.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG\", \"published\": \"2025-04-22T17:59:51Z\"}"}

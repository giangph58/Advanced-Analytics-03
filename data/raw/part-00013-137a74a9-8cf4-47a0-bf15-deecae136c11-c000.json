{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05689v1\", \"title\": \"Separator Injection Attack: Uncovering Dialogue Biases in Large Language\\n  Models Caused by Role Separators\", \"summary\": \"Conversational large language models (LLMs) have gained widespread attention\\ndue to their instruction-following capabilities. To ensure conversational LLMs\\nfollow instructions, role separators are employed to distinguish between\\ndifferent participants in a conversation. However, incorporating role\\nseparators introduces potential vulnerabilities. Misusing roles can lead to\\nprompt injection attacks, which can easily misalign the model's behavior with\\nthe user's intentions, raising significant security concerns. Although various\\nprompt injection attacks have been proposed, recent research has largely\\noverlooked the impact of role separators on safety. This highlights the\\ncritical need to thoroughly understand the systemic weaknesses in dialogue\\nsystems caused by role separators. This paper identifies modeling weaknesses\\ncaused by role separators. Specifically, we observe a strong positional bias\\nassociated with role separators, which is inherent in the format of dialogue\\nmodeling and can be triggered by the insertion of role separators. We further\\ndevelop the Separators Injection Attack (SIA), a new orthometric attack based\\non role separators. The experiment results show that SIA is efficient and\\nextensive in manipulating model behavior with an average gain of 18.2% for\\nmanual methods and enhances the attack success rate to 100% with automatic\\nmethods.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.CR\", \"published\": \"2025-04-08T05:20:56Z\"}"}

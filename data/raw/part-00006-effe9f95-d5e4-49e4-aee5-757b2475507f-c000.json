{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00304v1\", \"title\": \"Reinforcement Learning with Continuous Actions Under Unmeasured\\n  Confounding\", \"summary\": \"This paper addresses the challenge of offline policy learning in\\nreinforcement learning with continuous action spaces when unmeasured\\nconfounders are present. While most existing research focuses on policy\\nevaluation within partially observable Markov decision processes (POMDPs) and\\nassumes discrete action spaces, we advance this field by establishing a novel\\nidentification result to enable the nonparametric estimation of policy value\\nfor a given target policy under an infinite-horizon framework. Leveraging this\\nidentification, we develop a minimax estimator and introduce a\\npolicy-gradient-based algorithm to identify the in-class optimal policy that\\nmaximizes the estimated policy value. Furthermore, we provide theoretical\\nresults regarding the consistency, finite-sample error bound, and regret bound\\nof the resulting optimal policy. Extensive simulations and a real-world\\napplication using the German Family Panel data demonstrate the effectiveness of\\nour proposed methodology.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.LG,stat.ME\", \"published\": \"2025-05-01T04:55:29Z\"}"}

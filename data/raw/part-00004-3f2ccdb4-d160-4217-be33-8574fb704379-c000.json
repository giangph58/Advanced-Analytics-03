{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11034v1\", \"title\": \"Defending Against Frequency-Based Attacks with Diffusion Models\", \"summary\": \"Adversarial training is a common strategy for enhancing model robustness\\nagainst adversarial attacks. However, it is typically tailored to the specific\\nattack types it is trained on, limiting its ability to generalize to unseen\\nthreat models. Adversarial purification offers an alternative by leveraging a\\ngenerative model to remove perturbations before classification. Since the\\npurifier is trained independently of both the classifier and the threat models,\\nit is better equipped to handle previously unseen attack scenarios. Diffusion\\nmodels have proven highly effective for noise purification, not only in\\ncountering pixel-wise adversarial perturbations but also in addressing\\nnon-adversarial data shifts. In this study, we broaden the focus beyond\\npixel-wise robustness to explore the extent to which purification can mitigate\\nboth spectral and spatial adversarial attacks. Our findings highlight its\\neffectiveness in handling diverse distortion patterns across low- to\\nhigh-frequency regions.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-15T09:57:17Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12949v1\", \"title\": \"RL-PINNs: Reinforcement Learning-Driven Adaptive Sampling for Efficient\\n  Training of PINNs\", \"summary\": \"Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework\\nfor solving partial differential equations (PDEs). However, their performance\\nheavily relies on the strategy used to select training points. Conventional\\nadaptive sampling methods, such as residual-based refinement, often require\\nmulti-round sampling and repeated retraining of PINNs, leading to computational\\ninefficiency due to redundant points and costly gradient\\ncomputations-particularly in high-dimensional or high-order derivative\\nscenarios. To address these limitations, we propose RL-PINNs, a reinforcement\\nlearning(RL)-driven adaptive sampling framework that enables efficient training\\nwith only a single round of sampling. Our approach formulates adaptive sampling\\nas a Markov decision process, where an RL agent dynamically selects optimal\\ntraining points by maximizing a long-term utility metric. Critically, we\\nreplace gradient-dependent residual metrics with a computationally efficient\\nfunction variation as the reward signal, eliminating the overhead of derivative\\ncalculations. Furthermore, we employ a delayed reward mechanism to prioritize\\nlong-term training stability over short-term gains. Extensive experiments\\nacross diverse PDE benchmarks, including low-regular, nonlinear,\\nhigh-dimensional, and high-order problems, demonstrate that RL-PINNs\\nsignificantly outperforms existing residual-driven adaptive methods in\\naccuracy. Notably, RL-PINNs achieve this with negligible sampling overhead,\\nmaking them scalable to high-dimensional and high-order problems.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.NA,math.NA\", \"published\": \"2025-04-17T13:50:55Z\"}"}

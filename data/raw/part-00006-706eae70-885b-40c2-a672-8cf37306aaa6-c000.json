{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15779v1\", \"title\": \"Shannon invariants: A scalable approach to information decomposition\", \"summary\": \"Distributed systems, such as biological and artificial neural networks,\\nprocess information via complex interactions engaging multiple subsystems,\\nresulting in high-order patterns with distinct properties across scales.\\nInvestigating how these systems process information remains challenging due to\\ndifficulties in defining appropriate multivariate metrics and ensuring their\\nscalability to large systems. To address these challenges, we introduce a novel\\nframework based on what we call \\\"Shannon invariants\\\" -- quantities that capture\\nessential properties of high-order information processing in a way that depends\\nonly on the definition of entropy and can be efficiently calculated for large\\nsystems. Our theoretical results demonstrate how Shannon invariants can be used\\nto resolve long-standing ambiguities regarding the interpretation of widely\\nused multivariate information-theoretic measures. Moreover, our practical\\nresults reveal distinctive information-processing signatures of various deep\\nlearning architectures across layers, which lead to new insights into how these\\nsystems process information and how this evolves during training. Overall, our\\nframework resolves fundamental limitations in analyzing high-order phenomena\\nand offers broad opportunities for theoretical developments and empirical\\nanalyses.\", \"main_category\": \"cs.IT\", \"categories\": \"cs.IT,cs.AI,cs.LG,math.IT,nlin.AO,physics.data-an\", \"published\": \"2025-04-22T10:41:38Z\"}"}

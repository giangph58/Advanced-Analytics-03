{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03238v1\", \"title\": \"RobotxR1: Enabling Embodied Robotic Intelligence on Large Language\\n  Models through Closed-Loop Reinforcement Learning\", \"summary\": \"Future robotic systems operating in real-world environments will require\\non-board embodied intelligence without continuous cloud connection, balancing\\ncapabilities with constraints on computational power and memory. This work\\npresents an extension of the R1-zero approach, which enables the usage of low\\nparameter-count Large Language Models (LLMs) in the robotic domain. The R1-Zero\\napproach was originally developed to enable mathematical reasoning in LLMs\\nusing static datasets. We extend it to the robotics domain through integration\\nin a closed-loop Reinforcement Learning (RL) framework. This extension enhances\\nreasoning in Embodied Artificial Intelligence (Embodied AI) settings without\\nrelying solely on distillation of large models through Supervised Fine-Tuning\\n(SFT). We show that small-scale LLMs can achieve effective reasoning\\nperformance by learning through closed-loop interaction with their environment,\\nwhich enables tasks that previously required significantly larger models. In an\\nautonomous driving setting, a performance gain of 20.2%-points over the\\nSFT-based baseline is observed with a Qwen2.5-1.5B model. Using the proposed\\ntraining procedure, Qwen2.5-3B achieves a 63.3% control adaptability score,\\nsurpassing the 58.5% obtained by the much larger, cloud-bound GPT-4o. These\\nresults highlight that practical, on-board deployment of small LLMs is not only\\nfeasible but can outperform larger models if trained through environmental\\nfeedback, underscoring the importance of an interactive learning framework for\\nrobotic Embodied AI, one grounded in practical experience rather than static\\nsupervision.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO\", \"published\": \"2025-05-06T07:07:28Z\"}"}

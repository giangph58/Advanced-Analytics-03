{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12931v1\", \"title\": \"Explainable AI in Usable Privacy and Security: Challenges and\\n  Opportunities\", \"summary\": \"Large Language Models (LLMs) are increasingly being used for automated\\nevaluations and explaining them. However, concerns about explanation quality,\\nconsistency, and hallucinations remain open research challenges, particularly\\nin high-stakes contexts like privacy and security, where user trust and\\ndecision-making are at stake. In this paper, we investigate these issues in the\\ncontext of PRISMe, an interactive privacy policy assessment tool that leverages\\nLLMs to evaluate and explain website privacy policies. Based on a prior user\\nstudy with 22 participants, we identify key concerns regarding LLM judgment\\ntransparency, consistency, and faithfulness, as well as variations in user\\npreferences for explanation detail and engagement. We discuss potential\\nstrategies to mitigate these concerns, including structured evaluation\\ncriteria, uncertainty estimation, and retrieval-augmented generation (RAG). We\\nidentify a need for adaptive explanation strategies tailored to different user\\nprofiles for LLM-as-a-judge. Our goal is to showcase the application area of\\nusable privacy and security to be promising for Human-Centered Explainable AI\\n(HCXAI) to make an impact.\", \"main_category\": \"cs.HC\", \"categories\": \"cs.HC\", \"published\": \"2025-04-17T13:28:01Z\"}"}

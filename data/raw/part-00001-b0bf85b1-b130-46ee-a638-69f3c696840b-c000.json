{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19925v1\", \"title\": \"Accelerating Mixture-of-Experts Training with Adaptive Expert\\n  Replication\", \"summary\": \"Mixture-of-Experts (MoE) models have become a widely adopted solution to\\ncontinue scaling model sizes without a corresponding linear increase in\\ncompute. During MoE model training, each input token is dynamically routed to a\\nsubset of experts -- sparsely-activated feed-forward networks -- within each\\ntransformer layer. The distribution of tokens assigned to each expert varies\\nwidely and rapidly over the course of training. To handle the wide load\\nimbalance across experts, current systems are forced to either drop tokens\\nassigned to popular experts, degrading convergence, or frequently rebalance\\nresources allocated to each expert based on popularity, incurring high state\\nmigration overheads.\\n  To break this performance-accuracy tradeoff, we introduce SwiftMoE, an\\nadaptive MoE training system. The key insight of SwiftMoE is to decouple the\\nplacement of expert parameters from their large optimizer state. SwiftMoE\\nstatically partitions the optimizer of each expert across all training nodes.\\nMeanwhile, SwiftMoE dynamically adjusts the placement of expert parameters by\\nrepurposing existing weight updates, avoiding migration overheads. In doing so,\\nSwiftMoE right-sizes the GPU resources allocated to each expert, on a\\nper-iteration basis, with minimal overheads. Compared to state-of-the-art MoE\\ntraining systems, DeepSpeed and FlexMoE, SwiftMoE is able to achieve a 30.5%\\nand 25.9% faster time-to-convergence, respectively.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC,cs.LG\", \"published\": \"2025-04-28T15:58:55Z\"}"}

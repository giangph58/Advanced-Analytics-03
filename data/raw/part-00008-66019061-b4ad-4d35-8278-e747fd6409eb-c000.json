{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16828v1\", \"title\": \"Process Reward Models That Think\", \"summary\": \"Step-by-step verifiers -- also known as process reward models (PRMs) -- are a\\nkey ingredient for test-time scaling. PRMs require step-level supervision,\\nmaking them expensive to train. This work aims to build data-efficient PRMs as\\nverbalized step-wise reward models that verify every step in the solution by\\ngenerating a verification chain-of-thought (CoT). We propose ThinkPRM, a long\\nCoT verifier fine-tuned on orders of magnitude fewer process labels than those\\nrequired by discriminative PRMs. Our approach capitalizes on the inherent\\nreasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and\\ndiscriminative verifiers -- using only 1% of the process labels in PRM800K --\\nacross several challenging benchmarks. Specifically, ThinkPRM beats the\\nbaselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and\\nreward-guided search. In an out-of-domain evaluation on a subset of\\nGPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers\\ntrained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the\\nsame token budget, ThinkPRM scales up verification compute more effectively\\ncompared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of\\nProcessBench. Our work highlights the value of generative, long CoT PRMs that\\ncan scale test-time compute for verification while requiring minimal\\nsupervision for training. Our code, data, and models will be released at\\nhttps://github.com/mukhal/thinkprm.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL\", \"published\": \"2025-04-23T15:44:54Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16612v1\", \"title\": \"Federated EndoViT: Pretraining Vision Transformers via Federated\\n  Learning on Endoscopic Image Collections\", \"summary\": \"Purpose: In this study, we investigate the training of foundation models\\nusing federated learning to address data-sharing limitations and enable\\ncollaborative model training without data transfer for minimally invasive\\nsurgery. Methods: Inspired by the EndoViT study, we adapt the Masked\\nAutoencoder for federated learning, enhancing it with adaptive Sharpness-Aware\\nMinimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is\\npretrained on the Endo700k dataset collection and later fine-tuned and\\nevaluated for tasks such as Semantic Segmentation, Action Triplet Recognition,\\nand Surgical Phase Recognition. Results: Our findings demonstrate that\\nintegrating adaptive FedSAM into the federated MAE approach improves\\npretraining, leading to a reduction in reconstruction loss per patch. The\\napplication of FL-EndoViT in surgical downstream tasks results in performance\\ncomparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over\\nCEN-EndoViT in surgical scene segmentation when data is limited and in action\\ntriplet recognition when large datasets are used. Conclusion: These findings\\nhighlight the potential of federated learning for privacy-preserving training\\nof surgical foundation models, offering a robust and generalizable solution for\\nsurgical data science. Effective collaboration requires adapting federated\\nlearning methods, such as the integration of FedSAM, which can accommodate the\\ninherent data heterogeneity across institutions. In future, exploring FL in\\nvideo-based models may enhance these capabilities by incorporating\\nspatiotemporal dynamics crucial for real-world surgical environments.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG\", \"published\": \"2025-04-23T10:54:32Z\"}"}

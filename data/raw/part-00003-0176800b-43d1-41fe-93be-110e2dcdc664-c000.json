{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20634v1\", \"title\": \"On Stochastic Rounding with Few Random Bits\", \"summary\": \"Large-scale numerical computations make increasing use of low-precision (LP)\\nfloating point formats and mixed precision arithmetic, which can be enhanced by\\nthe technique of stochastic rounding (SR), that is, rounding an intermediate\\nhigh-precision value up or down randomly as a function of the value's distance\\nto the two rounding candidates. Stochastic rounding requires, in addition to\\nthe high-precision input value, a source of random bits. As the provision of\\nhigh-quality random bits is an additional computational cost, it is of interest\\nto require as few bits as possible while maintaining the desirable properties\\nof SR in a given computation, or computational domain. This paper examines a\\nnumber of possible implementations of few-bit stochastic rounding (FBSR), and\\nshows how several natural implementations can introduce sometimes significant\\nbias into the rounding process, which are not present in the case of\\ninfinite-bit, infinite-precision examinations of these implementations. The\\npaper explores the impact of these biases in machine learning examples, and\\nhence opens another class of configuration parameters of which practitioners\\nshould be aware when developing or adopting low-precision floating point. Code\\nis available at\\nhttp://github.com/graphcore-research/arith25-stochastic-rounding.\", \"main_category\": \"math.NA\", \"categories\": \"math.NA,cs.AI,cs.LG,cs.MS,cs.NA\", \"published\": \"2025-04-29T11:04:25Z\"}"}

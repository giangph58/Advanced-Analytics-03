{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10483v1\", \"title\": \"REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion\\n  Transformers\", \"summary\": \"In this paper we tackle a fundamental question: \\\"Can we train latent\\ndiffusion models together with the variational auto-encoder (VAE) tokenizer in\\nan end-to-end manner?\\\" Traditional deep-learning wisdom dictates that\\nend-to-end training is often preferable when possible. However, for latent\\ndiffusion transformers, it is observed that end-to-end training both VAE and\\ndiffusion-model using standard diffusion-loss is ineffective, even causing a\\ndegradation in final performance. We show that while diffusion loss is\\nineffective, end-to-end training can be unlocked through the\\nrepresentation-alignment (REPA) loss -- allowing both VAE and diffusion model\\nto be jointly tuned during the training process. Despite its simplicity, the\\nproposed training recipe (REPA-E) shows remarkable performance; speeding up\\ndiffusion model training by over 17x and 45x over REPA and vanilla training\\nrecipes, respectively. Interestingly, we observe that end-to-end tuning with\\nREPA-E also improves the VAE itself; leading to improved latent space structure\\nand downstream generation performance. In terms of final performance, our\\napproach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and\\nwithout classifier-free guidance on ImageNet 256 x 256. Code is available at\\nhttps://end2end-diffusion.github.io.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG\", \"published\": \"2025-04-14T17:59:53Z\"}"}

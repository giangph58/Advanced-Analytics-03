{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03427v1\", \"title\": \"MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks\", \"summary\": \"Large Language Models (LLMs) have demonstrated significant promise for\\nvarious applications in healthcare. However, their efficacy in the Arabic\\nmedical domain remains unexplored due to the lack of high-quality\\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\\nmultiple specialties and including multiple choice questions,\\nfill-in-the-blank, and patient-doctor question answering. We first constructed\\nthe dataset using past medical exams and publicly available datasets. We then\\nintroduced different modifications to evaluate various LLM capabilities,\\nincluding bias mitigation. We conducted an extensive evaluation with five\\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\\nnew high-quality benchmarks that span different languages to ensure fair\\ndeployment and scalability of LLMs in healthcare. By establishing this\\nbenchmark and releasing the dataset, we provide a foundation for future\\nresearch aimed at evaluating and enhancing the multilingual capabilities of\\nLLMs for the equitable use of generative AI in healthcare.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.HC\", \"published\": \"2025-05-06T11:07:26Z\"}"}

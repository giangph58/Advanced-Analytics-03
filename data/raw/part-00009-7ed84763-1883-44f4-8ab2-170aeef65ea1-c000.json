{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02398v1\", \"title\": \"Scaling Analysis of Interleaved Speech-Text Language Models\", \"summary\": \"Existing Speech Language Model (SLM) scaling analysis paints a bleak picture.\\nThey predict that SLMs require much more compute and data compared to text,\\nleading some to question the feasibility of training high-quality SLMs.\\nHowever, modern SLMs are often initialised from pre-trained TextLMs using\\nspeech-text interleaving to allow knowledge transfer. This raises the question\\n- Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper\\nwe answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by\\ntraining several dozen and analysing the scaling trends. We see that under this\\nsetup SLMs scale more efficiently with compute. Additionally, our results\\nindicate that the scaling-dynamics are significantly different than\\ntextless-SLMs, suggesting one should allocate notably more of the compute\\nbudget for increasing model size over training tokens. We also study the role\\nof synthetic data and TextLM model families in unlocking this potential.\\nResults suggest, that our scaled up model achieves comparable performance with\\nleading models on speech semantic metrics while using less compute and data\\nthan other approaches. We open source models, samples, and data -\\nhttps://pages.cs.huji.ac.il/adiyoss-lab/sims.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.SD,eess.AS\", \"published\": \"2025-04-03T08:46:56Z\"}"}

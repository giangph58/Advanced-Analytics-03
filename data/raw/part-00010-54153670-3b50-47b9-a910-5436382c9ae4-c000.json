{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20605v1\", \"title\": \"TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open\\n  Language Models\", \"summary\": \"Moral stories are a time-tested vehicle for transmitting values, yet modern\\nNLP lacks a large, structured corpus that couples coherent narratives with\\nexplicit ethical lessons. We close this gap with TF1-EN-3M, the first open\\ndataset of three million English-language fables generated exclusively by\\ninstruction-tuned models no larger than 8B parameters. Each story follows a\\nsix-slot scaffold (character -> trait -> setting -> conflict -> resolution ->\\nmoral), produced through a combinatorial prompt engine that guarantees genre\\nfidelity while covering a broad thematic space.\\n  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores\\ngrammar, creativity, moral clarity, and template adherence with (ii)\\nreference-free diversity and readability metrics. Among ten open-weight\\ncandidates, an 8B-parameter Llama-3 variant delivers the best quality-speed\\ntrade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)\\nat approximately 13.5 cents per 1,000 fables.\\n  We release the dataset, generation code, evaluation scripts, and full\\nmetadata under a permissive license, enabling exact reproducibility and cost\\nbenchmarking. TF1-EN-3M opens avenues for research in instruction following,\\nnarrative intelligence, value alignment, and child-friendly educational AI,\\ndemonstrating that large-scale moral storytelling no longer requires\\nproprietary giant models.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-29T10:15:28Z\"}"}

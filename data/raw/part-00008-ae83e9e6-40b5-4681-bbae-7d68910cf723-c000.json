{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07004v1\", \"title\": \"Task-Based Tensor Computations on Modern GPUs\", \"summary\": \"Domain-specific, fixed-function units are becoming increasingly common in\\nmodern processors. As the computational demands of applications evolve, the\\ncapabilities and programming interfaces of these fixed-function units continue\\nto change. NVIDIA's Hopper GPU architecture contains multiple fixed-function\\nunits per compute unit, including an asynchronous data movement unit (TMA) and\\nan asynchronous matrix multiplication unit (Tensor Core). Efficiently utilizing\\nthese units requires a fundamentally different programming style than previous\\narchitectures; programmers must now develop warp-specialized kernels that\\norchestrate producer-consumer pipelines between the asynchronous units. To\\nmanage the complexity of programming these new architectures, we introduce\\nCypress, a task-based programming model with sequential semantics. Cypress\\nprograms are a set of designated functions called \\\\emph{tasks} that operate on\\n\\\\emph{tensors} and are free of communication and synchronization. Cypress\\nprograms are bound to the target machine through a \\\\emph{mapping} specification\\nthat describes where tasks should run and in which memories tensors should be\\nmaterialized. We present a compiler architecture that lowers Cypress programs\\ninto CUDA programs that perform competitively with expert-written codes.\\nCypress achieves 0.88x-1.06x the performance of cuBLAS on GEMM, and between\\n0.80x-0.98x the performance of the currently best-known Flash Attention\\nimplementation while eliminating all aspects of explicit data movement and\\nasynchronous computation from application code.\", \"main_category\": \"cs.PL\", \"categories\": \"cs.PL\", \"published\": \"2025-04-09T16:24:15Z\"}"}

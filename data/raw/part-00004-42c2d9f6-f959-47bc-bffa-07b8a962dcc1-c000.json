{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17432v1\", \"title\": \"Breaking the Modality Barrier: Universal Embedding Learning with\\n  Multimodal LLMs\", \"summary\": \"The Contrastive Language-Image Pre-training (CLIP) framework has become a\\nwidely used approach for multimodal representation learning, particularly in\\nimage-text retrieval and clustering. However, its efficacy is constrained by\\nthree key limitations: (1) text token truncation, (2) isolated image-text\\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\\nsignificant advances in generalized vision-language understanding, their\\npotential for learning transferable multimodal representations remains\\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\\na novel two-stage framework that leverages MLLMs to learn discriminative\\nrepresentations for diverse downstream tasks. In the first stage, we perform\\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\\nmodel to enhance the embedding capability of the MLLM\\\\'s language component. In\\nthe second stage, we introduce hard negative enhanced instruction tuning to\\nfurther advance discriminative representation learning. Specifically, we\\ninitially mitigate false negative contamination and then sample multiple hard\\nnegatives per instance within each batch, forcing the model to focus on\\nchallenging samples. This approach not only improves discriminative power but\\nalso enhances instruction-following ability in downstream tasks. We conduct\\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\\nincluding short and long caption retrieval and compositional retrieval. Results\\ndemonstrate that UniME achieves consistent performance improvement across all\\ntasks, exhibiting superior discriminative and compositional capabilities.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-24T10:51:52Z\"}"}

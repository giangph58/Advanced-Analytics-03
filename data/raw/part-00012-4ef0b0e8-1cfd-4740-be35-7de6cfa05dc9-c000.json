{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14854v1\", \"title\": \"Uncertainty quantification of neural network models of evolving\\n  processes via Langevin sampling\", \"summary\": \"We propose a scalable, approximate inference hypernetwork framework for a\\ngeneral model of history-dependent processes. The flexible data model is based\\non a neural ordinary differential equation (NODE) representing the evolution of\\ninternal states together with a trainable observation model subcomponent. The\\nposterior distribution corresponding to the data model parameters (weights and\\nbiases) follows a stochastic differential equation with a drift term related to\\nthe score of the posterior that is learned jointly with the data model\\nparameters. This Langevin sampling approach offers flexibility in balancing the\\ncomputational budget between the evaluation cost of the data model and the\\napproximation of the posterior density of its parameters. We demonstrate\\nperformance of the hypernetwork on chemical reaction and material physics data\\nand compare it to mean-field variational inference.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,stat.ML\", \"published\": \"2025-04-21T04:45:40Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21463v1\", \"title\": \"RWKV-X: A Linear Complexity Hybrid Language Model\", \"summary\": \"In this paper, we introduce \\\\textbf{RWKV-X}, a novel hybrid architecture that\\ncombines the efficiency of RWKV for short-range modeling with a sparse\\nattention mechanism designed to capture long-range context. Unlike previous\\nhybrid approaches that rely on full attention layers and retain quadratic\\ncomplexity, RWKV-X achieves linear-time complexity in training and\\nconstant-time complexity in inference decoding. We demonstrate that RWKV-X,\\nwhen continually pretrained on 64K-token sequences, achieves near-perfect\\naccuracy on the 64K passkey retrieval benchmark. It consistently outperforms\\nprior RWKV-7 models on long-context benchmarks, while maintaining strong\\nperformance on short-context tasks. These results highlight RWKV-X as a\\nscalable and efficient backbone for general-purpose language modeling, capable\\nof decoding sequences up to 1 million tokens with stable speed and memory\\nusage. To facilitate further research and analysis, we have made the\\ncheckpoints and the associated code publicly accessible at:\\nhttps://github.com/howard-hou/RWKV-X.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-30T09:38:17Z\"}"}

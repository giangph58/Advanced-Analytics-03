{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10883v1\", \"title\": \"Bringing together invertible UNets with invertible attention modules for\\n  memory-efficient diffusion models\", \"summary\": \"Diffusion models have recently gained state of the art performance on many\\nimage generation tasks. However, most models require significant computational\\nresources to achieve this. This becomes apparent in the application of medical\\nimage synthesis due to the 3D nature of medical datasets like CT-scans, MRIs,\\nelectron microscope, etc. In this paper we propose a novel architecture for a\\nsingle GPU memory-efficient training for diffusion models for high dimensional\\nmedical datasets. The proposed model is built by using an invertible UNet\\narchitecture with invertible attention modules. This leads to the following two\\ncontributions: 1. denoising diffusion models and thus enabling memory usage to\\nbe independent of the dimensionality of the dataset, and 2. reducing the energy\\nusage during training. While this new model can be applied to a multitude of\\nimage generation tasks, we showcase its memory-efficiency on the 3D BraTS2020\\ndataset leading to up to 15\\\\% decrease in peak memory consumption during\\ntraining with comparable results to SOTA while maintaining the image quality.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-15T05:26:42Z\"}"}

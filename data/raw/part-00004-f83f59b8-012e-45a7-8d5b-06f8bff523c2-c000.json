{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02304v1\", \"title\": \"Measurement of LLM's Philosophies of Human Nature\", \"summary\": \"The widespread application of artificial intelligence (AI) in various tasks,\\nalong with frequent reports of conflicts or violations involving AI, has\\nsparked societal concerns about interactions with AI systems. Based on\\nWrightsman's Philosophies of Human Nature Scale (PHNS), a scale empirically\\nvalidated over decades to effectively assess individuals' attitudes toward\\nhuman nature, we design the standardized psychological scale specifically\\ntargeting large language models (LLM), named the Machine-based Philosophies of\\nHuman Nature Scale (M-PHNS). By evaluating LLMs' attitudes toward human nature\\nacross six dimensions, we reveal that current LLMs exhibit a systemic lack of\\ntrust in humans, and there is a significant negative correlation between the\\nmodel's intelligence level and its trust in humans. Furthermore, we propose a\\nmental loop learning framework, which enables LLM to continuously optimize its\\nvalue system during virtual interactions by constructing moral scenarios,\\nthereby improving its attitude toward human nature. Experiments demonstrate\\nthat mental loop learning significantly enhances their trust in humans compared\\nto persona or instruction prompts. This finding highlights the potential of\\nhuman-based psychological assessments for LLM, which can not only diagnose\\ncognitive biases but also provide a potential solution for ethical learning in\\nartificial intelligence. We release the M-PHNS evaluation code and data at\\nhttps://github.com/kodenii/M-PHNS.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-03T06:22:19Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03242v1\", \"title\": \"Seeing the Abstract: Translating the Abstract Language for Vision\\n  Language Models\", \"summary\": \"Natural language goes beyond dryly describing visual content. It contains\\nrich abstract concepts to express feeling, creativity and properties that\\ncannot be directly perceived. Yet, current research in Vision Language Models\\n(VLMs) has not shed light on abstract-oriented language. Our research breaks\\nnew ground by uncovering its wide presence and under-estimated value, with\\nextensive analysis. Particularly, we focus our investigation on the fashion\\ndomain, a highly-representative field with abstract expressions. By analyzing\\nrecent large-scale multimodal fashion datasets, we find that abstract terms\\nhave a dominant presence, rivaling the concrete ones, providing novel\\ninformation, and being useful in the retrieval task. However, a critical\\nchallenge emerges: current general-purpose or fashion-specific VLMs are\\npre-trained with databases that lack sufficient abstract words in their text\\ncorpora, thus hindering their ability to effectively represent\\nabstract-oriented language. We propose a training-free and model-agnostic\\nmethod, Abstract-to-Concrete Translator (ACT), to shift abstract\\nrepresentations towards well-represented concrete ones in the VLM latent space,\\nusing pre-trained models and existing multimodal databases. On the\\ntext-to-image retrieval task, despite being training-free, ACT outperforms the\\nfine-tuned VLMs in both same- and cross-dataset settings, exhibiting its\\neffectiveness with a strong generalization capability. Moreover, the\\nimprovement introduced by ACT is consistent with various VLMs, making it a\\nplug-and-play solution.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-05-06T07:14:10Z\"}"}

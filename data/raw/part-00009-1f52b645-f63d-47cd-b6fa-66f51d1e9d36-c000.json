{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04417v1\", \"title\": \"Localized Diffusion Models for High Dimensional Distributions Generation\", \"summary\": \"Diffusion models are the state-of-the-art tools for various generative tasks.\\nHowever, estimating high-dimensional score functions makes them potentially\\nsuffer from the curse of dimensionality (CoD). This underscores the importance\\nof better understanding and exploiting low-dimensional structure in the target\\ndistribution. In this work, we consider locality structure, which describes\\nsparse dependencies between model components. Under locality structure, the\\nscore function is effectively low-dimensional, so that it can be estimated by a\\nlocalized neural network with significantly reduced sample complexity. This\\nmotivates the localized diffusion model, where a localized score matching loss\\nis used to train the score function within a localized hypothesis space. We\\nprove that such localization enables diffusion models to circumvent CoD, at the\\nprice of additional localization error. Under realistic sample size scaling, we\\nshow both theoretically and numerically that a moderate localization radius can\\nbalance the statistical and localization error, leading to a better overall\\nperformance. The localized structure also facilitates parallel training of\\ndiffusion models, making it potentially more efficient for large-scale\\napplications.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.NA,math.NA,stat.ML\", \"published\": \"2025-05-07T13:51:50Z\"}"}

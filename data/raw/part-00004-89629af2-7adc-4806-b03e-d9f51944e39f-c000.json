{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00624v1\", \"title\": \"FineScope : Precision Pruning for Domain-Specialized Large Language\\n  Models Using SAE-Guided Self-Data Cultivation\", \"summary\": \"Training large language models (LLMs) from scratch requires significant\\ncomputational resources, driving interest in developing smaller,\\ndomain-specific LLMs that maintain both efficiency and strong task performance.\\nMedium-sized models such as LLaMA, llama} have served as starting points for\\ndomain-specific adaptation, but they often suffer from accuracy degradation\\nwhen tested on specialized datasets. We introduce FineScope, a framework for\\nderiving compact, domain-optimized LLMs from larger pretrained models.\\nFineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its\\nability to produce interpretable feature representations, to extract\\ndomain-specific subsets from large datasets. We apply structured pruning with\\ndomain-specific constraints, ensuring that the resulting pruned models retain\\nessential knowledge for the target domain. To further enhance performance,\\nthese pruned models undergo self-data distillation, leveraging SAE-curated\\ndatasets to restore key domain-specific information lost during pruning.\\nExtensive experiments and ablation studies demonstrate that FineScope achieves\\nhighly competitive performance, outperforming several large-scale\\nstate-of-the-art LLMs in domain-specific tasks. Additionally, our results show\\nthat FineScope enables pruned models to regain a substantial portion of their\\noriginal performance when fine-tuned with SAE-curated datasets. Furthermore,\\napplying these datasets to fine-tune pretrained LLMs without pruning also\\nimproves their domain-specific accuracy, highlighting the robustness of our\\napproach. The code will be released.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-05-01T16:05:08Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15275v1\", \"title\": \"Stop Summation: Min-Form Credit Assignment Is All Process Reward Model\\n  Needs for Reasoning\", \"summary\": \"Process reward models (PRMs) have proven effective for test-time scaling of\\nLarge Language Models (LLMs) on challenging reasoning tasks. However, reward\\nhacking issues with PRMs limit their successful application in reinforcement\\nfine-tuning. In this paper, we identify the main cause of PRM-induced reward\\nhacking: the canonical summation-form credit assignment in reinforcement\\nlearning (RL), which defines the value as cumulative gamma-decayed future\\nrewards, easily induces LLMs to hack steps with high rewards. To address this,\\nwe propose PURE: Process sUpervised Reinforcement lEarning. The key innovation\\nof PURE is a min-form credit assignment that formulates the value function as\\nthe minimum of future rewards. This method significantly alleviates reward\\nhacking by limiting the value function range and distributing advantages more\\nreasonably. Through extensive experiments on 3 base models, we show that\\nPRM-based approaches enabling min-form credit assignment achieve comparable\\nreasoning performance to verifiable reward-based methods within only 30% steps.\\nIn contrast, the canonical sum-form credit assignment collapses training even\\nat the beginning! Additionally, when we supplement PRM-based fine-tuning with\\njust 10% verifiable rewards, we further alleviate reward hacking and produce\\nthe best fine-tuned model based on Qwen2.5-Math-7B in our experiments,\\nachieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5\\nbenchmarks. Moreover, we summarize the observed reward hacking cases and\\nanalyze the causes of training collapse. Code and models are available at\\nhttps://github.com/CJReinforce/PURE.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.LG\", \"published\": \"2025-04-21T17:59:02Z\"}"}

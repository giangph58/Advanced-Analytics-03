{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15720v1\", \"title\": \"SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\\n  Language Model Inference\", \"summary\": \"Large language models (LLMs) with different architectures and sizes have been\\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\\nservice inefficiency due to the varying demand of LLM requests. A common\\npractice is to share multiple LLMs. However, existing sharing systems either do\\nnot consider the autoregressive pattern of LLM services, or only focus on\\nimproving the throughput, which impairs the sharing performance, especially the\\nserving latency. We present SeaLLM, which enables service-aware and\\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\\nof LLM services, (2) a placement algorithm to determine the placement plan and\\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\\nunified key-value cache to share GPU memory among LLM services efficiently. Our\\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\\nimproves the normalized latency by up to $13.60\\\\times$, the tail latency by up\\nto $18.69\\\\times$, and the SLO attainment by up to $3.64\\\\times$ compared to\\nexisting solutions.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC\", \"published\": \"2025-04-22T09:08:46Z\"}"}

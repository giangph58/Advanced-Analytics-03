{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19674v1\", \"title\": \"$\\\\texttt{SAGE}$: A Generic Framework for LLM Safety Evaluation\", \"summary\": \"Safety evaluation of Large Language Models (LLMs) has made progress and\\nattracted academic interest, but it remains challenging to keep pace with the\\nrapid integration of LLMs across diverse applications. Different applications\\nexpose users to various harms, necessitating application-specific safety\\nevaluations with tailored harms and policies. Another major gap is the lack of\\nfocus on the dynamic and conversational nature of LLM systems. Such potential\\noversights can lead to harms that go unnoticed in standard safety benchmarks.\\nThis paper identifies the above as key requirements for robust LLM safety\\nevaluation and recognizing that current evaluation methodologies do not satisfy\\nthese, we introduce the $\\\\texttt{SAGE}$ (Safety AI Generic Evaluation)\\nframework. $\\\\texttt{SAGE}$ is an automated modular framework designed for\\ncustomized and dynamic harm evaluations. It utilizes adversarial user models\\nthat are system-aware and have unique personalities, enabling a holistic\\nred-teaming evaluation. We demonstrate $\\\\texttt{SAGE}$'s effectiveness by\\nevaluating seven state-of-the-art LLMs across three applications and harm\\npolicies. Our experiments with multi-turn conversational evaluations revealed a\\nconcerning finding that harm steadily increases with conversation length.\\nFurthermore, we observe significant disparities in model behavior when exposed\\nto different user personalities and scenarios. Our findings also reveal that\\nsome models minimize harmful outputs by employing severe refusal tactics that\\ncan hinder their usefulness. These insights highlight the necessity of adaptive\\nand context-specific testing to ensure better safety alignment and safer\\ndeployment of LLMs in real-world scenarios.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.AI\", \"published\": \"2025-04-28T11:01:08Z\"}"}

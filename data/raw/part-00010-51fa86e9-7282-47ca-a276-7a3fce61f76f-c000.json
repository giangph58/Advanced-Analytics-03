{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16576v1\", \"title\": \"MMHCL: Multi-Modal Hypergraph Contrastive Learning for Recommendation\", \"summary\": \"The burgeoning presence of multimodal content-sharing platforms propels the\\ndevelopment of personalized recommender systems. Previous works usually suffer\\nfrom data sparsity and cold-start problems, and may fail to adequately explore\\nsemantic user-product associations from multimodal data. To address these\\nissues, we propose a novel Multi-Modal Hypergraph Contrastive Learning (MMHCL)\\nframework for user recommendation. For a comprehensive information exploration\\nfrom user-product relations, we construct two hypergraphs, i.e. a user-to-user\\n(u2u) hypergraph and an item-to-item (i2i) hypergraph, to mine shared\\npreferences among users and intricate multimodal semantic resemblance among\\nitems, respectively. This process yields denser second-order semantics that are\\nfused with first-order user-item interaction as complementary to alleviate the\\ndata sparsity issue. Then, we design a contrastive feature enhancement paradigm\\nby applying synergistic contrastive learning. By maximizing/minimizing the\\nmutual information between second-order (e.g. shared preference pattern for\\nusers) and first-order (information of selected items for users) embeddings of\\nthe same/different users and items, the feature distinguishability can be\\neffectively enhanced. Compared with using sparse primary user-item interaction\\nonly, our MMHCL obtains denser second-order hypergraphs and excavates more\\nabundant shared attributes to explore the user-product associations, which to a\\ncertain extent alleviates the problems of data sparsity and cold-start.\\nExtensive experiments have comprehensively demonstrated the effectiveness of\\nour method. Our code is publicly available at: https://github.com/Xu107/MMHCL.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR,cs.AI\", \"published\": \"2025-04-23T09:58:54Z\"}"}

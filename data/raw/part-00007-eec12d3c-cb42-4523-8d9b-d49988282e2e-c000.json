{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12180v1\", \"title\": \"Trusting CHATGPT: how minor tweaks in the prompts lead to major\\n  differences in sentiment classification\", \"summary\": \"One fundamental question for the social sciences today is: how much can we\\ntrust highly complex predictive models like ChatGPT? This study tests the\\nhypothesis that subtle changes in the structure of prompts do not produce\\nsignificant variations in the classification results of sentiment polarity\\nanalysis generated by the Large Language Model GPT-4o mini. Using a dataset of\\n100.000 comments in Spanish on four Latin American presidents, the model\\nclassified the comments as positive, negative, or neutral on 10 occasions,\\nvarying the prompts slightly each time. The experimental methodology included\\nexploratory and confirmatory analyses to identify significant discrepancies\\namong classifications.\\n  The results reveal that even minor modifications to prompts such as lexical,\\nsyntactic, or modal changes, or even their lack of structure impact the\\nclassifications. In certain cases, the model produced inconsistent responses,\\nsuch as mixing categories, providing unsolicited explanations, or using\\nlanguages other than Spanish. Statistical analysis using Chi-square tests\\nconfirmed significant differences in most comparisons between prompts, except\\nin one case where linguistic structures were highly similar.\\n  These findings challenge the robustness and trust of Large Language Models\\nfor classification tasks, highlighting their vulnerability to variations in\\ninstructions. Moreover, it was evident that the lack of structured grammar in\\nprompts increases the frequency of hallucinations. The discussion underscores\\nthat trust in Large Language Models is based not only on technical performance\\nbut also on the social and institutional relationships underpinning their use.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-16T15:37:09Z\"}"}

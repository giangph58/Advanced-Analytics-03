{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04393v1\", \"title\": \"Large Means Left: Political Bias in Large Language Models Increases with\\n  Their Number of Parameters\", \"summary\": \"With the increasing prevalence of artificial intelligence, careful evaluation\\nof inherent biases needs to be conducted to form the basis for alleviating the\\neffects these predispositions can have on users. Large language models (LLMs)\\nare predominantly used by many as a primary source of information for various\\ntopics. LLMs frequently make factual errors, fabricate data (hallucinations),\\nor present biases, exposing users to misinformation and influencing opinions.\\nEducating users on their risks is key to responsible use, as bias, unlike\\nhallucinations, cannot be caught through data verification. We quantify the\\npolitical bias of popular LLMs in the context of the recent vote of the German\\nBundestag using the score produced by the Wahl-O-Mat. This metric measures the\\nalignment between an individual's political views and the positions of German\\npolitical parties. We compare the models' alignment scores to identify factors\\ninfluencing their political preferences. Doing so, we discover a bias toward\\nleft-leaning parties, most dominant in larger LLMs. Also, we find that the\\nlanguage we use to communicate with the models affects their political views.\\nAdditionally, we analyze the influence of a model's origin and release date and\\ncompare the results to the outcome of the recent vote of the Bundestag. Our\\nresults imply that LLMs are prone to exhibiting political bias. Large\\ncorporations with the necessary means to develop LLMs, thus, knowingly or\\nunknowingly, have a responsibility to contain these biases, as they can\\ninfluence each voter's decision-making process and inform public opinion in\\ngeneral and at scale.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-05-07T13:18:41Z\"}"}

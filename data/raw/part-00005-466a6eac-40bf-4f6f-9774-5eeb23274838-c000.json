{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06148v1\", \"title\": \"V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric\\n  Capabilities in Multimodal Large Language Models\", \"summary\": \"Recent advancements in Multimodal Large Language Models (MLLMs) have led to\\nsignificant improvements across various multimodal benchmarks. However, as\\nevaluations shift from static datasets to open-world, dynamic environments,\\ncurrent game-based benchmarks remain inadequate because they lack\\nvisual-centric tasks and fail to assess the diverse reasoning skills required\\nfor real-world decision-making. To address this, we introduce Visual-centric\\nMultiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework\\ndesigned to assess visual reasoning capabilities of MLLMs. V-MAGE features five\\ndiverse games with 30+ handcrafted levels, testing models on core visual skills\\nsuch as positioning, trajectory tracking, timing, and visual memory, alongside\\nhigher-level reasoning like long-term planning and deliberation. We use V-MAGE\\nto evaluate leading MLLMs, revealing significant challenges in their visual\\nperception and reasoning. In all game environments, the top-performing MLLMs,\\nas determined by Elo rating comparisons, exhibit a substantial performance gap\\ncompared to humans. Our findings highlight critical limitations, including\\nvarious types of perceptual errors made by the models, and suggest potential\\navenues for improvement from an agent-centric perspective, such as refining\\nagent strategies and addressing perceptual inaccuracies. Code is available at\\nhttps://github.com/CSU-JPG/V-MAGE.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-08T15:43:01Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01898v1\", \"title\": \"Analysis of an Idealized Stochastic Polyak Method and its Application to\\n  Black-Box Model Distillation\", \"summary\": \"We provide a general convergence theorem of an idealized stochastic Polyak\\nstep size called SPS$^*$. Besides convexity, we only assume a local expected\\ngradient bound, that includes locally smooth and locally Lipschitz losses as\\nspecial cases. We refer to SPS$^*$ as idealized because it requires access to\\nthe loss for every training batch evaluated at a solution. It is also ideal, in\\nthat it achieves the optimal lower bound for globally Lipschitz function, and\\nis the first Polyak step size to have an $O(1/\\\\sqrt{t})$ anytime convergence in\\nthe smooth setting. We show how to combine SPS$^*$ with momentum to achieve the\\nsame favorable rates for the last iterate. We conclude with several experiments\\nto validate our theory, and a more practical setting showing how we can distill\\na teacher GPT-2 model into a smaller student model without any hyperparameter\\ntuning.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,G.1.6\", \"published\": \"2025-04-02T16:57:39Z\"}"}

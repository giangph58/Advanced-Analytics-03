{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02441v1\", \"title\": \"Cognitive Memory in Large Language Models\", \"summary\": \"This paper examines memory mechanisms in Large Language Models (LLMs),\\nemphasizing their importance for context-rich responses, reduced\\nhallucinations, and improved efficiency. It categorizes memory into sensory,\\nshort-term, and long-term, with sensory memory corresponding to input prompts,\\nshort-term memory processing immediate context, and long-term memory\\nimplemented via external databases or structures. The text-based memory section\\ncovers acquisition (selection and summarization), management (updating,\\naccessing, storing, and resolving conflicts), and utilization (full-text\\nsearch, SQL queries, semantic search). The KV cache-based memory section\\ndiscusses selection methods (regularity-based summarization, score-based\\napproaches, special token embeddings) and compression techniques (low-rank\\ncompression, KV merging, multimodal compression), along with management\\nstrategies like offloading and shared attention mechanisms. Parameter-based\\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\\nenhance efficiency, while hidden-state-based memory approaches (chunk\\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\\nby combining RNN hidden states with current methods. Overall, the paper offers\\na comprehensive analysis of LLM memory mechanisms, highlighting their\\nsignificance and future research directions.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-03T09:58:19Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14838v1\", \"title\": \"Establishing Reliability Metrics for Reward Models in Large Language\\n  Models\", \"summary\": \"The reward model (RM) that represents human preferences plays a crucial role\\nin optimizing the outputs of large language models (LLMs), e.g., through\\nreinforcement learning from human feedback (RLHF) or rejection sampling.\\nHowever, a long challenge for RM is its uncertain reliability, i.e., LLM\\noutputs with higher rewards may not align with actual human preferences.\\nCurrently, there is a lack of a convincing metric to quantify the reliability\\nof RMs. To bridge this gap, we propose the \\\\textit{\\\\underline{R}eliable at\\n\\\\underline{$\\\\eta$}} (RETA) metric, which directly measures the reliability of\\nan RM by evaluating the average quality (scored by an oracle) of the top $\\\\eta$\\nquantile responses assessed by an RM. On top of RETA, we present an integrated\\nbenchmarking pipeline that allows anyone to evaluate their own RM without\\nincurring additional Oracle labeling costs. Extensive experimental studies\\ndemonstrate the superior stability of RETA metric, providing solid evaluations\\nof the reliability of various publicly available and proprietary RMs. When\\ndealing with an unreliable RM, we can use the RETA metric to identify the\\noptimal quantile from which to select the responses.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-21T03:39:33Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02397v1\", \"title\": \"Learning Audio-guided Video Representation with Gated Attention for\\n  Video-Text Retrieval\", \"summary\": \"Video-text retrieval, the task of retrieving videos based on a textual query\\nor vice versa, is of paramount importance for video understanding and\\nmultimodal information retrieval. Recent methods in this area rely primarily on\\nvisual and textual features and often ignore audio, although it helps enhance\\noverall comprehension of video content. Moreover, traditional models that\\nincorporate audio blindly utilize the audio input regardless of whether it is\\nuseful or not, resulting in suboptimal video representation. To address these\\nlimitations, we propose a novel video-text retrieval framework, Audio-guided\\nVIdeo representation learning with GATEd attention (AVIGATE), that effectively\\nleverages audio cues through a gated attention mechanism that selectively\\nfilters out uninformative audio signals. In addition, we propose an adaptive\\nmargin-based contrastive loss to deal with the inherently unclear\\npositive-negative relationship between video and text, which facilitates\\nlearning better video-text alignment. Our extensive experiments demonstrate\\nthat AVIGATE achieves state-of-the-art performance on all the public\\nbenchmarks.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-03T08:45:36Z\"}"}

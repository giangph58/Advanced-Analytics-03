{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11930v1\", \"title\": \"Beyond Words: Augmenting Discriminative Richness via Diffusions in\\n  Unsupervised Prompt Learning\", \"summary\": \"Fine-tuning vision-language models (VLMs) with large amounts of unlabeled\\ndata has recently garnered significant interest. However, a key challenge\\nremains the lack of high-quality pseudo-labeled data. Current pseudo-labeling\\nstrategies often struggle with mismatches between semantic and visual\\ninformation, leading to sub-optimal performance of unsupervised prompt learning\\n(UPL) methods. In this paper, we introduce a simple yet effective approach\\ncalled \\\\textbf{A}ugmenting D\\\\textbf{i}scriminative \\\\textbf{R}ichness via\\nDiffusions (AiR), toward learning a richer discriminating way to represent the\\nclass comprehensively and thus facilitate classification. Specifically, our\\napproach includes a pseudo-label generation module that leverages high-fidelity\\nsynthetic samples to create an auxiliary classifier, which captures richer\\nvisual variation, bridging text-image-pair classification to a more robust\\nimage-image-pair classification. Additionally, we exploit the diversity of\\ndiffusion-based synthetic samples to enhance prompt learning, providing greater\\ninformation for semantic-visual alignment. Extensive experiments on five public\\nbenchmarks, including RESISC45 and Flowers102, and across three learning\\nparadigms-UL, SSL, and TRZSL-demonstrate that AiR achieves substantial and\\nconsistent performance improvements over state-of-the-art unsupervised prompt\\nlearning methods.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-16T10:09:45Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05628v1\", \"title\": \"Stratified Expert Cloning with Adaptive Selection for User Retention in\\n  Large-Scale Recommender Systems\", \"summary\": \"User retention has emerged as a critical challenge in large-scale recommender\\nsystems, significantly impacting the long-term success of online platforms.\\nExisting methods often focus on short-term engagement metrics, failing to\\ncapture the complex dynamics of user preferences and behaviors over extended\\nperiods. While reinforcement learning (RL) approaches have shown promise in\\noptimizing long-term rewards, they face difficulties in credit assignment,\\nsample efficiency, and exploration when applied to the user retention problem.\\nIn this work, we propose Stratified Expert Cloning (SEC), a novel imitation\\nlearning framework that effectively leverages abundant logged data from\\nhigh-retention users to learn robust recommendation policies. SEC introduces\\nthree key innovations: 1) a multi-level expert stratification strategy that\\ncaptures the nuances in expert user behaviors at different retention levels; 2)\\nan adaptive expert selection mechanism that dynamically assigns users to the\\nmost suitable policy based on their current state and historical retention\\nlevel; and 3) an action entropy regularization technique that promotes\\nrecommendation diversity and mitigates the risk of policy collapse. Through\\nextensive offline experiments and online A/B tests on two major video\\nplatforms, Kuaishou and Kuaishou Lite, with hundreds of millions of daily\\nactive users, we demonstrate SEC's significant improvements over\\nstate-of-the-art methods in user retention. The results demonstrate significant\\nimprovements in user retention, with cumulative lifts of 0.098\\\\% and 0.122\\\\% in\\nactive days on Kuaishou and Kuaishou Lite respectively, additionally bringing\\ntens of thousands of daily active users to each platform.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR\", \"published\": \"2025-04-08T03:10:42Z\"}"}

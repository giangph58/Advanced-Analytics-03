{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02733v1\", \"title\": \"Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study\", \"summary\": \"Large Language Models (LLMs) are highly vulnerable to input perturbations, as\\neven a small prompt change may result in a substantially different output.\\nExisting methods to enhance LLM robustness are primarily focused on perturbed\\ndata samples, whereas improving resiliency to perturbations of task-level\\ninstructions has remained relatively underexplored. In this work, we focus on\\ncharacter- and word-level edits of task-specific instructions, which\\nsubstantially degrade downstream performance. We experiment with a variety of\\ntechniques to enhance the robustness of LLMs, including self-denoising and\\nrepresentation alignment, testing different models (Llama 3 and Flan-T5),\\ndatasets (CoLA, QNLI, SST-2) and instructions (both task-oriented and\\nrole-oriented). We find that, on average, self-denoising -- whether performed\\nby a frozen LLM or a fine-tuned model -- achieves substantially higher\\nperformance gains than alternative strategies, including more complex baselines\\nsuch as ensembling and supervised methods.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-03T16:17:56Z\"}"}

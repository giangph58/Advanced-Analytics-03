{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01382v1\", \"title\": \"An Illusion of Progress? Assessing the Current State of Web Agents\", \"summary\": \"As digitalization and cloud technologies evolve, the web is becoming\\nincreasingly important in the modern society. Autonomous web agents based on\\nlarge language models (LLMs) hold a great potential in work automation. It is\\ntherefore important to accurately measure and monitor the progression of their\\ncapabilities. In this work, we conduct a comprehensive and rigorous assessment\\nof the current state of web agents. Our results depict a very different picture\\nof the competency of current agents, suggesting over-optimism in previously\\nreported results. This gap can be attributed to shortcomings in existing\\nbenchmarks. We introduce Online-Mind2Web, an online evaluation benchmark\\nconsisting of 300 diverse and realistic tasks spanning 136 websites. It enables\\nus to evaluate web agents under a setting that approximates how real users use\\nthese agents. To facilitate more scalable evaluation and development, we also\\ndevelop a novel LLM-as-a-Judge automatic evaluation method and show that it can\\nachieve around 85% agreement with human judgment, substantially higher than\\nexisting methods. Finally, we present the first comprehensive comparative\\nanalysis of current web agents, highlighting both their strengths and\\nlimitations to inspire future research.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CL\", \"published\": \"2025-04-02T05:51:29Z\"}"}

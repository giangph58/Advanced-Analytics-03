{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05181v1\", \"title\": \"Stochastic Variational Propagation: Local, Scalable and Efficient\\n  Alternative to Backpropagation\", \"summary\": \"Backpropagation (BP) is the cornerstone of deep learning, but its reliance on\\nglobal gradient synchronization limits scalability and imposes significant\\nmemory overhead. We propose Stochastic Variational Propagation (SVP), a\\nscalable alternative that reframes training as hierarchical variational\\ninference. SVP treats layer activations as latent variables and optimizes local\\nEvidence Lower Bounds (ELBOs), enabling independent, local updates while\\npreserving global coherence. However, directly applying KL divergence in\\nlayer-wise ELBOs risks inter-layer's representation collapse due to excessive\\ncompression. To prevent this, SVP projects activations into low-dimensional\\nspaces via fixed random matrices, ensuring information preservation and\\nrepresentational diversity. Combined with a feature alignment loss for\\ninter-layer consistency, SVP achieves competitive accuracy with BP across\\ndiverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to\\nImageNet), reduces memory usage by up to 4x, and significantly improves\\nscalability. More broadly, SVP introduces a probabilistic perspective to deep\\nrepresentation learning, opening pathways toward more modular and interpretable\\nneural network design.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-05-08T12:32:29Z\"}"}

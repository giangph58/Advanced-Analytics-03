{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12184v1\", \"title\": \"Feature Selection for Data-driven Explainable Optimization\", \"summary\": \"Mathematical optimization, although often leading to NP-hard models, is now\\ncapable of solving even large-scale instances within reasonable time. However,\\nthe primary focus is often placed solely on optimality. This implies that while\\nobtained solutions are globally optimal, they are frequently not comprehensible\\nto humans, in particular when obtained by black-box routines. In contrast,\\nexplainability is a standard requirement for results in Artificial\\nIntelligence, but it is rarely considered in optimization yet. There are only a\\nfew studies that aim to find solutions that are both of high quality and\\nexplainable. In recent work, explainability for optimization was defined in a\\ndata-driven manner: a solution is considered explainable if it closely\\nresembles solutions that have been used in the past under similar\\ncircumstances. To this end, it is crucial to identify a preferably small subset\\nof features from a presumably large set that can be used to explain a solution.\\nIn mathematical optimization, feature selection has received little attention\\nyet. In this work, we formally define the feature selection problem for\\nexplainable optimization and prove that its decision version is NP-complete. We\\nintroduce mathematical models for optimized feature selection. As their global\\nsolution requires significant computation time with modern mixed-integer linear\\nsolvers, we employ local heuristics. Our computational study using data that\\nreflect real-world scenarios demonstrates that the problem can be solved\\npractically efficiently for instances of reasonable size.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC\", \"published\": \"2025-04-16T15:40:03Z\"}"}

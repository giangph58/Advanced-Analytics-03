{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00317v1\", \"title\": \"Beyond Quadratic Costs in LQR: Bregman Divergence Control\", \"summary\": \"In the past couple of decades, the use of ``non-quadratic\\\" convex cost\\nfunctions has revolutionized signal processing, machine learning, and\\nstatistics, allowing one to customize solutions to have desired structures and\\nproperties. However, the situation is not the same in control where the use of\\nquadratic costs still dominates, ostensibly because determining the ``value\\nfunction\\\", i.e., the optimal expected cost-to-go, which is critical to the\\nconstruction of the optimal controller, becomes computationally intractable as\\nsoon as one considers general convex costs. As a result, practitioners often\\nresort to heuristics and approximations, such as model predictive control that\\nonly looks a few steps into the future. In the quadratic case, the value\\nfunction is easily determined by solving Riccati equations. In this work, we\\nconsider a special class of convex cost functions constructed from Bregman\\ndivergence and show how, with appropriate choices, they can be used to fully\\nextend the framework developed for the quadratic case. The resulting optimal\\ncontrollers are infinite horizon, come with stability guarantees, and have\\nstate-feedback, or estimated state-feedback, laws. They exhibit a much wider\\nrange of behavior than their quadratic counterparts since the feedback laws are\\nnonlinear. The approach can be applied to several cases of interest, including\\nsafety control, sparse control, and bang-bang control.\", \"main_category\": \"eess.SY\", \"categories\": \"eess.SY,cs.SY,math.OC\", \"published\": \"2025-05-01T05:31:45Z\"}"}

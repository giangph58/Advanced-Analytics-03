{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23768v1\", \"title\": \"Texture or Semantics? Vision-Language Models Get Lost in Font\\n  Recognition\", \"summary\": \"Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguistic\\ncapabilities, achieving impressive performance in various tasks such as image\\nrecognition and object localization. However, their effectiveness in\\nfine-grained tasks remains an open question. In everyday scenarios, individuals\\nencountering design materials, such as magazines, typography tutorials,\\nresearch papers, or branding content, may wish to identify aesthetically\\npleasing fonts used in the text. Given their multimodal capabilities and free\\naccessibility, many VLMs are often considered potential tools for font\\nrecognition. This raises a fundamental question: Do VLMs truly possess the\\ncapability to recognize fonts? To investigate this, we introduce the Font\\nRecognition Benchmark (FRB), a compact and well-structured dataset comprising\\n15 commonly used fonts. FRB includes two versions: (i) an easy version, where\\n10 sentences are rendered in different fonts, and (ii) a hard version, where\\neach text sample consists of the names of the 15 fonts themselves, introducing\\na stroop effect that challenges model perception. Through extensive evaluation\\nof various VLMs on font recognition tasks, we arrive at the following key\\nfindings: (i) Current VLMs exhibit limited font recognition capabilities, with\\nmany state-of-the-art models failing to achieve satisfactory performance. (ii)\\nFew-shot learning and Chain-of-Thought (CoT) prompting provide minimal benefits\\nin improving font recognition accuracy across different VLMs. (iii) Attention\\nanalysis sheds light on the inherent limitations of VLMs in capturing semantic\\nfeatures.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.CV\", \"published\": \"2025-03-31T06:33:21Z\"}"}

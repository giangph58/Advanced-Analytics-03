{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12016v1\", \"title\": \"Active Human Feedback Collection via Neural Contextual Dueling Bandits\", \"summary\": \"Collecting human preference feedback is often expensive, leading recent works\\nto develop principled algorithms to select them more efficiently. However,\\nthese works assume that the underlying reward function is linear, an assumption\\nthat does not hold in many real-life applications, such as online\\nrecommendation and LLM alignment. To address this limitation, we propose\\nNeural-ADB, an algorithm based on the neural contextual dueling bandit\\nframework that provides a principled and practical method for collecting human\\npreference feedback when the underlying latent reward function is non-linear.\\nWe theoretically show that when preference feedback follows the\\nBradley-Terry-Luce model, the worst sub-optimality gap of the policy learned by\\nNeural-ADB decreases at a sub-linear rate as the preference dataset increases.\\nOur experimental results on problem instances derived from synthetic preference\\ndatasets further validate the effectiveness of Neural-ADB.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-16T12:16:10Z\"}"}

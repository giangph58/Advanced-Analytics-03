{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20484v1\", \"title\": \"Enhancing LLM Language Adaption through Cross-lingual In-Context\\n  Pre-training\", \"summary\": \"Large language models (LLMs) exhibit remarkable multilingual capabilities\\ndespite English-dominated pre-training, attributed to cross-lingual mechanisms\\nduring pre-training. Existing methods for enhancing cross-lingual transfer\\nremain constrained by parallel resources, suffering from limited linguistic and\\ndomain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT),\\na simple and scalable approach that enhances cross-lingual transfer by\\nleveraging semantically related bilingual texts via simple next-word\\nprediction. We construct CrossIC-PT samples by interleaving semantic-related\\nbilingual Wikipedia documents into a single context window. To access window\\nsize constraints, we implement a systematic segmentation policy to split long\\nbilingual document pairs into chunks while adjusting the sliding window\\nmechanism to preserve contextual coherence. We further extend data availability\\nthrough a semantic retrieval framework to construct CrossIC-PT samples from\\nweb-crawled corpus. Experimental results demonstrate that CrossIC-PT improves\\nmultilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and\\nQwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%,\\n3.99%, and 1.95%, respectively, with additional improvements after data\\naugmentation.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-29T07:24:25Z\"}"}

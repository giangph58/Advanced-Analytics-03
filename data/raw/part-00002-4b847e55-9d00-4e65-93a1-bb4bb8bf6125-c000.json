{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17269v1\", \"title\": \"Towards Generalized and Training-Free Text-Guided Semantic Manipulation\", \"summary\": \"Text-guided semantic manipulation refers to semantically editing an image\\ngenerated from a source prompt to match a target prompt, enabling the desired\\nsemantic changes (e.g., addition, removal, and style transfer) while preserving\\nirrelevant contents. With the powerful generative capabilities of the diffusion\\nmodel, the task has shown the potential to generate high-fidelity visual\\ncontent. Nevertheless, existing methods either typically require time-consuming\\nfine-tuning (inefficient), fail to accomplish multiple semantic manipulations\\n(poorly extensible), and/or lack support for different modality tasks (limited\\ngeneralizability). Upon further investigation, we find that the geometric\\nproperties of noises in the diffusion model are strongly correlated with the\\nsemantic changes. Motivated by this, we propose a novel $\\\\textit{GTF}$ for\\ntext-guided semantic manipulation, which has the following attractive\\ncapabilities: 1) $\\\\textbf{Generalized}$: our $\\\\textit{GTF}$ supports multiple\\nsemantic manipulations (e.g., addition, removal, and style transfer) and can be\\nseamlessly integrated into all diffusion-based methods (i.e., Plug-and-play)\\nacross different modalities (i.e., modality-agnostic); and 2)\\n$\\\\textbf{Training-free}$: $\\\\textit{GTF}$ produces high-fidelity results via\\nsimply controlling the geometric relationship between noises without tuning or\\noptimization. Our extensive experiments demonstrate the efficacy of our\\napproach, highlighting its potential to advance the state-of-the-art in\\nsemantics manipulation.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-24T05:54:56Z\"}"}

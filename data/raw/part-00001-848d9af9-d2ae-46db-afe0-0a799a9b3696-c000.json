{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05446v1\", \"title\": \"Adaptive Markup Language Generation for Contextually-Grounded Visual\\n  Document Understanding\", \"summary\": \"Visual Document Understanding has become essential with the increase of\\ntext-rich visual content. This field poses significant challenges due to the\\nneed for effective integration of visual perception and textual comprehension,\\nparticularly across diverse document types with complex layouts. Moreover,\\nexisting fine-tuning datasets for this domain often fall short in providing the\\ndetailed contextual information for robust understanding, leading to\\nhallucinations and limited comprehension of spatial relationships among visual\\nelements. To address these challenges, we propose an innovative pipeline that\\nutilizes adaptive generation of markup languages, such as Markdown, JSON, HTML,\\nand TiKZ, to build highly structured document representations and deliver\\ncontextually-grounded responses. We introduce two fine-grained structured\\ndatasets: DocMark-Pile, comprising approximately 3.8M pretraining data pairs\\nfor document parsing, and DocMark-Instruct, featuring 624k fine-tuning data\\nannotations for grounded instruction following. Extensive experiments\\ndemonstrate that our proposed model significantly outperforms existing\\nstate-of-theart MLLMs across a range of visual document understanding\\nbenchmarks, facilitating advanced reasoning and comprehension capabilities in\\ncomplex visual scenarios. Our code and models are released at https://github.\\ncom/Euphoria16/DocMark.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.CL\", \"published\": \"2025-05-08T17:37:36Z\"}"}

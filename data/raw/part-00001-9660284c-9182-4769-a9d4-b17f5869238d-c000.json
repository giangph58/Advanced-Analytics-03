{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14893v1\", \"title\": \"Hardware-based Heterogeneous Memory Management for Large Language Model\\n  Inference\", \"summary\": \"A large language model (LLM) is one of the most important emerging machine\\nlearning applications nowadays. However, due to its huge model size and runtime\\nincrease of the memory footprint, LLM inferences suffer from the lack of memory\\ncapacity in conventional systems consisting of multiple GPUs with a modest\\namount of high bandwidth memory. Moreover, since LLM contains many\\nbandwidthintensive kernels, only focusing on the memory capacity without\\nconsidering the bandwidth incurs a serious performance degradation. To handle\\nsuch conflicting memory capacity and bandwidth demands in a cost-effective way,\\nthis study investigates the potential of heterogeneous memory systems,\\nproposing H2M2. It uses an asymmetric memory architecture consisting of\\ncapacity-centric and bandwidthcentric memory with computation units attached to\\neach memory device. With the asymmetric memory, we first analyze the effect of\\nkernel-memory mapping for the asymmetric memory. Second, we propose a dynamic\\nruntime algorithm that finds a mapping solution considering the characteristics\\nof LLM operations and the change of footprint during LLM inference. Third, we\\nadvocate the need for memory abstraction for the efficient management of the\\nasymmetric memory. H2M2 outperforms the conventional homogeneous memory system\\nwith LPDDR by 1.46x, 1.55x, and 2.94x speedup in GPT3-175B, Chinchilla-70B, and\\nLlama2-70B, respectively.\", \"main_category\": \"cs.AR\", \"categories\": \"cs.AR\", \"published\": \"2025-04-21T06:45:41Z\"}"}

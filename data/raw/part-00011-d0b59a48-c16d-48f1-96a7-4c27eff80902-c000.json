{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15122v1\", \"title\": \"MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry\\n  Monocular Video\", \"summary\": \"We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS)\\nframework capable of reconstructing sharp and high-quality novel\\nspatio-temporal views from blurry monocular videos in an end-to-end manner.\\nExisting dynamic novel view synthesis (NVS) methods are highly sensitive to\\nmotion blur in casually captured videos, resulting in significant degradation\\nof rendering quality. While recent approaches address motion-blurred inputs for\\nNVS, they primarily focus on static scene reconstruction and lack dedicated\\nmotion modeling for dynamic objects. To overcome these limitations, our MoBGS\\nintroduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for\\neffective latent camera trajectory estimation, improving global camera motion\\ndeblurring. In addition, we propose a physically-inspired Latent Camera-induced\\nExposure Estimation (LCEE) method to ensure consistent deblurring of both\\nglobal camera and local object motion. Our MoBGS framework ensures the temporal\\nconsistency of unseen latent timestamps and robust motion decomposition of\\nstatic and dynamic regions. Extensive experiments on the Stereo Blur dataset\\nand real-world blurry videos show that our MoBGS significantly outperforms the\\nvery recent advanced methods (DyBluRF and Deblur4DGS), achieving\\nstate-of-the-art performance for dynamic NVS under motion blur.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-21T14:19:19Z\"}"}

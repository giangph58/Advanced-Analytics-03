{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23830v1\", \"title\": \"OrchMLLM: Orchestrate Multimodal Data with Batch Post-Balancing to\\n  Accelerate Multimodal Large Language Model Training\", \"summary\": \"Multimodal large language models (MLLMs), such as GPT-4o, are garnering\\nsignificant attention. During the exploration of MLLM training, we identified\\nModality Composition Incoherence, a phenomenon that the proportion of a certain\\nmodality varies dramatically across different examples. It exacerbates the\\nchallenges of addressing mini-batch imbalances, which lead to uneven GPU\\nutilization between Data Parallel (DP) instances and severely degrades the\\nefficiency and scalability of MLLM training, ultimately affecting training\\nspeed and hindering further research on MLLMs.\\n  To address these challenges, we introduce OrchMLLM, a comprehensive framework\\ndesigned to mitigate the inefficiencies in MLLM training caused by Modality\\nComposition Incoherence. First, we propose Batch Post-Balancing Dispatcher, a\\ntechnique that efficiently eliminates mini-batch imbalances in sequential data.\\nAdditionally, we integrate MLLM Global Orchestrator into the training framework\\nto orchestrate multimodal data and tackle the issues arising from Modality\\nComposition Incoherence. We evaluate OrchMLLM across various MLLM sizes,\\ndemonstrating its efficiency and scalability. Experimental results reveal that\\nOrchMLLM achieves a Model FLOPs Utilization (MFU) of $41.6\\\\%$ when training an\\n84B MLLM with three modalities on $2560$ H100 GPUs, outperforming Megatron-LM\\nby up to $3.1\\\\times$ in throughput.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC,cs.AI\", \"published\": \"2025-03-31T08:24:23Z\"}"}

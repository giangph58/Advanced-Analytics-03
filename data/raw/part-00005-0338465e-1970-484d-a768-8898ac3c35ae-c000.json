{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04302v1\", \"title\": \"PPO-ACT: Proximal Policy Optimization with Adversarial Curriculum\\n  Transfer for Spatial Public Goods Games\", \"summary\": \"This study investigates cooperation evolution mechanisms in the spatial\\npublic goods game. A novel deep reinforcement learning framework, Proximal\\nPolicy Optimization with Adversarial Curriculum Transfer (PPO-ACT), is proposed\\nto model agent strategy optimization in dynamic environments. Traditional\\nevolutionary game models frequently exhibit limitations in modeling long-term\\ndecision-making processes. Deep reinforcement learning effectively addresses\\nthis limitation by bridging policy gradient methods with evolutionary game\\ntheory. Our study pioneers the application of proximal policy optimization's\\ncontinuous strategy optimization capability to public goods games through a\\ntwo-stage adversarial curriculum transfer training paradigm. The experimental\\nresults show that PPO-ACT performs better in critical enhancement factor\\nregimes. Compared to conventional standard proximal policy optimization\\nmethods, Q-learning and Fermi update rules, achieve earlier cooperation phase\\ntransitions and maintain stable cooperative equilibria. This framework exhibits\\nbetter robustness when handling challenging scenarios like all-defector initial\\nconditions. Systematic comparisons reveal the unique advantage of policy\\ngradient methods in population-scale cooperation, i.e., achieving\\nspatiotemporal payoff coordination through value function propagation. Our work\\nprovides a new computational framework for studying cooperation emergence in\\ncomplex systems, algorithmically validating the punishment promotes cooperation\\nhypothesis while offering methodological insights for multi-agent system\\nstrategy design.\", \"main_category\": \"cs.GT\", \"categories\": \"cs.GT\", \"published\": \"2025-05-07T10:20:55Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10485v1\", \"title\": \"Decoupled Diffusion Sparks Adaptive Scene Generation\", \"summary\": \"Controllable scene generation could reduce the cost of diverse data\\ncollection substantially for autonomous driving. Prior works formulate the\\ntraffic layout generation as predictive progress, either by denoising entire\\nsequences at once or by iteratively predicting the next frame. However, full\\nsequence denoising hinders online reaction, while the latter's short-sighted\\nnext-frame prediction lacks precise goal-state guidance. Further, the learned\\nmodel struggles to generate complex or challenging scenarios due to a large\\nnumber of safe and ordinal driving behaviors from open datasets. To overcome\\nthese, we introduce Nexus, a decoupled scene generation framework that improves\\nreactivity and goal conditioning by simulating both ordinal and challenging\\nscenarios from fine-grained tokens with independent noise states. At the core\\nof the decoupled pipeline is the integration of a partial noise-masking\\ntraining strategy and a noise-aware schedule that ensures timely environmental\\nupdates throughout the denoising process. To complement challenging scenario\\ngeneration, we collect a dataset consisting of complex corner cases. It covers\\n540 hours of simulated data, including high-risk interactions such as cut-in,\\nsudden braking, and collision. Nexus achieves superior generation realism while\\npreserving reactivity and goal orientation, with a 40% reduction in\\ndisplacement error. We further demonstrate that Nexus improves closed-loop\\nplanning by 20% through data augmentation and showcase its capability in\\nsafety-critical data generation.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-14T17:59:57Z\"}"}

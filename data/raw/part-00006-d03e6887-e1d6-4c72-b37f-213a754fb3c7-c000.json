{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19819v1\", \"title\": \"Joint Optimization of Neural Radiance Fields and Continuous Camera\\n  Motion from a Monocular Video\", \"summary\": \"Neural Radiance Fields (NeRF) has demonstrated its superior capability to\\nrepresent 3D geometry but require accurately precomputed camera poses during\\ntraining. To mitigate this requirement, existing methods jointly optimize\\ncamera poses and NeRF often relying on good pose initialisation or depth\\npriors. However, these approaches struggle in challenging scenarios, such as\\nlarge rotations, as they map each camera to a world coordinate system. We\\npropose a novel method that eliminates prior dependencies by modeling\\ncontinuous camera motions as time-dependent angular velocity and velocity.\\nRelative motions between cameras are learned first via velocity integration,\\nwhile camera poses can be obtained by aggregating such relative motions up to a\\nworld coordinate system defined at a single time step within the video.\\nSpecifically, accurate continuous camera movements are learned through a\\ntime-dependent NeRF, which captures local scene geometry and motion by training\\nfrom neighboring frames for each time step. The learned motions enable\\nfine-tuning the NeRF to represent the full scene geometry. Experiments on Co3D\\nand Scannet show our approach achieves superior camera pose and depth\\nestimation and comparable novel-view synthesis performance compared to\\nstate-of-the-art methods. Our code is available at\\nhttps://github.com/HoangChuongNguyen/cope-nerf.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-28T14:22:04Z\"}"}

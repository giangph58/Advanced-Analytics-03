{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24026v1\", \"title\": \"HumanDreamer: Generating Controllable Human-Motion Videos via Decoupled\\n  Generation\", \"summary\": \"Human-motion video generation has been a challenging task, primarily due to\\nthe difficulty inherent in learning human body movements. While some approaches\\nhave attempted to drive human-centric video generation explicitly through pose\\ncontrol, these methods typically rely on poses derived from existing videos,\\nthereby lacking flexibility. To address this, we propose HumanDreamer, a\\ndecoupled human video generation framework that first generates diverse poses\\nfrom text prompts and then leverages these poses to generate human-motion\\nvideos. Specifically, we propose MotionVid, the largest dataset for\\nhuman-motion pose generation. Based on the dataset, we present MotionDiT, which\\nis trained to generate structured human-motion poses from text prompts.\\nBesides, a novel LAMA loss is introduced, which together contribute to a\\nsignificant improvement in FID by 62.4%, along with respective enhancements in\\nR-precision for top1, top2, and top3 by 41.8%, 26.3%, and 18.3%, thereby\\nadvancing both the Text-to-Pose control accuracy and FID metrics. Our\\nexperiments across various Pose-to-Video baselines demonstrate that the poses\\ngenerated by our method can produce diverse and high-quality human-motion\\nvideos. Furthermore, our model can facilitate other downstream tasks, such as\\npose sequence prediction and 2D-3D motion lifting.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-31T12:51:45Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03646v1\", \"title\": \"ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders\", \"summary\": \"Despite the extensive use of deep autoencoders (AEs) in critical\\napplications, their adversarial robustness remains relatively underexplored\\ncompared to classification models. AE robustness is characterized by the\\nLipschitz bounds of its components. Existing robustness evaluation frameworks\\nbased on white-box attacks do not fully exploit the vulnerabilities of\\nintermediate ill-conditioned layers in AEs. In the context of optimizing\\nimperceptible norm-bounded additive perturbations to maximize output damage,\\nexisting methods struggle to effectively propagate adversarial loss gradients\\nthroughout the network, often converging to less effective perturbations. To\\naddress this, we propose a novel layer-conditioning-based adversarial\\noptimization objective that effectively guides the adversarial map toward\\nregions of local Lipschitz bounds by enhancing loss gradient information\\npropagation during attack optimization. We demonstrate through extensive\\nexperiments on state-of-the-art AEs that our adversarial objective results in\\nstronger attacks, outperforming existing methods in both universal and\\nsample-specific scenarios. As a defense method against this attack, we\\nintroduce an inference-time adversarially trained defense plugin that mitigates\\nthe effects of adversarial examples.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CV\", \"published\": \"2025-05-06T15:52:14Z\"}"}

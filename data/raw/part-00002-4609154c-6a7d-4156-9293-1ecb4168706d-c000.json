{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10016v1\", \"title\": \"Quantifying Privacy Leakage in Split Inference via Fisher-Approximated\\n  Shannon Information Analysis\", \"summary\": \"Split inference (SI) partitions deep neural networks into distributed\\nsub-models, enabling privacy-preserving collaborative learning. Nevertheless,\\nit remains vulnerable to Data Reconstruction Attacks (DRAs), wherein\\nadversaries exploit exposed smashed data to reconstruct raw inputs. Despite\\nextensive research on adversarial attack-defense games, a shortfall remains in\\nthe fundamental analysis of privacy risks. This paper establishes a theoretical\\nframework for privacy leakage quantification using information theory, defining\\nit as the adversary's certainty and deriving both average-case and worst-case\\nerror bounds. We introduce Fisher-approximated Shannon information (FSInfo), a\\nnovel privacy metric utilizing Fisher Information (FI) for operational privacy\\nleakage computation. We empirically show that our privacy metric correlates\\nwell with empirical attacks and investigate some of the factors that affect\\nprivacy leakage, namely the data distribution, model size, and overfitting.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR\", \"published\": \"2025-04-14T09:19:06Z\"}"}

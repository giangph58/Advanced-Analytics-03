{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16472v1\", \"title\": \"Harden and Catch for Just-in-Time Assured LLM-Based Software Testing:\\n  Open Research Challenges\", \"summary\": \"Despite decades of research and practice in automated software testing,\\nseveral fundamental concepts remain ill-defined and under-explored, yet offer\\nenormous potential real-world impact. We show that these concepts raise\\nexciting new challenges in the context of Large Language Models for software\\ntest generation. More specifically, we formally define and investigate the\\nproperties of hardening and catching tests. A hardening test is one that seeks\\nto protect against future regressions, while a catching test is one that\\ncatches such a regression or a fault in new functionality introduced by a code\\nchange. Hardening tests can be generated at any time and may become catching\\ntests when a future regression is caught. We also define and motivate the\\nCatching `Just-in-Time' (JiTTest) Challenge, in which tests are generated\\n`just-in-time' to catch new faults before they land into production. We show\\nthat any solution to Catching JiTTest generation can also be repurposed to\\ncatch latent faults in legacy code. We enumerate possible outcomes for\\nhardening and catching tests and JiTTests, and discuss open research problems,\\ndeployment options, and initial results from our work on automated LLM-based\\nhardening at Meta. This paper\\\\footnote{Author order is alphabetical. The\\ncorresponding author is Mark Harman.} was written to accompany the keynote by\\nthe authors at the ACM International Conference on the Foundations of Software\\nEngineering (FSE) 2025.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE,cs.AI\", \"published\": \"2025-04-23T07:32:43Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15009v1\", \"title\": \"Insert Anything: Image Insertion via In-Context Editing in DiT\", \"summary\": \"This work presents Insert Anything, a unified framework for reference-based\\nimage insertion that seamlessly integrates objects from reference images into\\ntarget scenes under flexible, user-specified control guidance. Instead of\\ntraining separate models for individual tasks, our approach is trained once on\\nour new AnyInsertion dataset--comprising 120K prompt-image pairs covering\\ndiverse tasks such as person, object, and garment insertion--and effortlessly\\ngeneralizes to a wide range of insertion scenarios. Such a challenging setting\\nrequires capturing both identity features and fine-grained details, while\\nallowing versatile local adaptations in style, color, and texture. To this end,\\nwe propose to leverage the multimodal attention of the Diffusion Transformer\\n(DiT) to support both mask- and text-guided editing. Furthermore, we introduce\\nan in-context editing mechanism that treats the reference image as contextual\\ninformation, employing two prompting strategies to harmonize the inserted\\nelements with the target scene while faithfully preserving their distinctive\\nfeatures. Extensive experiments on AnyInsertion, DreamBooth, and VTON-HD\\nbenchmarks demonstrate that our method consistently outperforms existing\\nalternatives, underscoring its great potential in real-world applications such\\nas creative content generation, virtual try-on, and scene composition.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-21T10:19:12Z\"}"}

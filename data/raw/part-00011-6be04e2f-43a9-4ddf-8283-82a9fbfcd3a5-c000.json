{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20930v1\", \"title\": \"ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning\\n  through Step-by-Step Verification\", \"summary\": \"Recent advances in reasoning-enhanced large language models (LLMs) and\\nmultimodal LLMs (MLLMs) have significantly improved performance in complex\\ntasks, yet medical AI models often overlook the structured reasoning processes\\ninherent in clinical practice. In this work, we present ChestX-Reasoner, a\\nradiology diagnosis MLLM designed to leverage process supervision mined\\ndirectly from clinical reports, reflecting the step-by-step reasoning followed\\nby radiologists. We construct a large dataset by extracting and refining\\nreasoning chains from routine radiology reports. Our two-stage training\\nframework combines supervised fine-tuning and reinforcement learning guided by\\nprocess rewards to better align model reasoning with clinical standards. We\\nintroduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual\\nquestion answering samples with 301K clinically validated reasoning steps, and\\npropose RadRScore, a metric evaluating reasoning factuality, completeness, and\\neffectiveness. ChestX-Reasoner outperforms existing medical and general-domain\\nMLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,\\nand 18% improvements in reasoning ability compared to the best medical MLLM,\\nthe best general MLLM, and its base model, respectively, as well as 3.3%, 24%,\\nand 27% improvements in outcome accuracy. All resources are open-sourced to\\nfacilitate further research in medical reasoning MLLMs.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CL,cs.CV\", \"published\": \"2025-04-29T16:48:23Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11320v1\", \"title\": \"Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\\n  Constraints\", \"summary\": \"Large Language Models (LLMs) are indispensable in today's applications, but\\ntheir inference procedure -- generating responses by processing text in\\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\\ncomputational resources, particularly under memory constraints. This paper\\nformulates LLM inference optimization as a multi-stage online scheduling\\nproblem where sequential prompt arrivals and KV cache growth render\\nconventional scheduling ineffective. We develop a fluid dynamics approximation\\nto provide a tractable benchmark that guides algorithm design. Building on\\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\\nalgorithm, which uses multiple thresholds to schedule incoming prompts\\noptimally when output lengths are known, and extend it to Nested WAIT for cases\\nwith unknown output lengths. Theoretical analysis shows that both algorithms\\nachieve near-optimal performance against the fluid benchmark in heavy traffic\\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\\nreal-world datasets demonstrate improved throughput and reduced latency\\nrelative to established baselines like vLLM and Sarathi. This work bridges\\noperations research and machine learning, offering a rigorous framework for the\\nefficient deployment of LLMs under memory constraints.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.DC,math.OC,stat.ML\", \"published\": \"2025-04-15T16:00:21Z\"}"}

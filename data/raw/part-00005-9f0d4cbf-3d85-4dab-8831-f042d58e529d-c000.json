{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03329v1\", \"title\": \"FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for\\n  Scene Text Editing\", \"summary\": \"The task of scene text editing is to modify or add texts on images while\\nmaintaining the fidelity of newly generated text and visual coherence with the\\nbackground. Recent works based on latent diffusion models (LDM) show improved\\ntext editing results, yet still face challenges and often generate inaccurate\\nor unrecognizable characters, especially for non-Latin ones (\\\\eg, Chinese),\\nwhich have complex glyph structures. To address these issues, we present\\nFLUX-Text, a simple and advanced multilingual scene text editing framework\\nbased on FLUX-Fill. Specifically, we carefully investigate glyph conditioning,\\nconsidering both visual and textual modalities. To retain the original\\ngenerative capabilities of FLUX-Fill while enhancing its understanding and\\ngeneration of glyphs, we propose lightweight glyph and text embedding modules.\\nOwning to the lightweight design, FLUX-Text is trained only with $100K$\\ntraining examples compared to current popular methods trained with 2.9M ones.\\nWith no bells and whistles, our method achieves state-of-the-art performance on\\ntext editing tasks. Qualitative and quantitative experiments on the public\\ndatasets demonstrate that our method surpasses previous works in text fidelity.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-06T08:56:28Z\"}"}

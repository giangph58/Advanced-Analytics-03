{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15920v1\", \"title\": \"ScaleGNN: Towards Scalable Graph Neural Networks via Adaptive High-order\\n  Neighboring Feature Fusion\", \"summary\": \"Graph Neural Networks (GNNs) have demonstrated strong performance across\\nvarious graph-based tasks by effectively capturing relational information\\nbetween nodes. These models rely on iterative message passing to propagate node\\nfeatures, enabling nodes to aggregate information from their neighbors. Recent\\nresearch has significantly improved the message-passing mechanism, enhancing\\nGNN scalability on large-scale graphs. However, GNNs still face two main\\nchallenges: over-smoothing, where excessive message passing results in\\nindistinguishable node representations, especially in deep networks\\nincorporating high-order neighbors; and scalability issues, as traditional\\narchitectures suffer from high model complexity and increased inference time\\ndue to redundant information aggregation. This paper proposes a novel framework\\nfor large-scale graphs named ScaleGNN that simultaneously addresses both\\nchallenges by adaptively fusing multi-level graph features. We first construct\\nneighbor matrices for each order, learning their relative information through\\ntrainable weights through an adaptive high-order feature fusion module. This\\nallows the model to selectively emphasize informative high-order neighbors\\nwhile reducing unnecessary computational costs. Additionally, we introduce a\\nHigh-order redundant feature masking mechanism based on a Local Contribution\\nScore (LCS), which enables the model to retain only the most relevant neighbors\\nat each order, preventing redundant information propagation. Furthermore,\\nlow-order enhanced feature aggregation adaptively integrates low-order and\\nhigh-order features based on task relevance, ensuring effective capture of both\\nlocal and global structural information without excessive complexity. Extensive\\nexperiments on real-world datasets demonstrate that our approach consistently\\noutperforms state-of-the-art GNN models in both accuracy and computational\\nefficiency.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-22T14:05:11Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20938v1\", \"title\": \"Towards Understanding the Nature of Attention with Low-Rank Sparse\\n  Decomposition\", \"summary\": \"We propose Low-Rank Sparse Attention (Lorsa), a sparse replacement model of\\nTransformer attention layers to disentangle original Multi Head Self Attention\\n(MHSA) into individually comprehensible components. Lorsa is designed to\\naddress the challenge of attention superposition to understand\\nattention-mediated interaction between features in different token positions.\\nWe show that Lorsa heads find cleaner and finer-grained versions of previously\\ndiscovered MHSA behaviors like induction heads, successor heads and attention\\nsink behavior (i.e., heavily attending to the first token). Lorsa and Sparse\\nAutoencoder (SAE) are both sparse dictionary learning methods applied to\\ndifferent Transformer components, and lead to consistent findings in many ways.\\nFor instance, we discover a comprehensive family of arithmetic-specific Lorsa\\nheads, each corresponding to an atomic operation in Llama-3.1-8B. Automated\\ninterpretability analysis indicates that Lorsa achieves parity with SAE in\\ninterpretability while Lorsa exhibits superior circuit discovery properties,\\nespecially for features computed collectively by multiple MHSA heads. We also\\nconduct extensive experiments on architectural design ablation, Lorsa scaling\\nlaw and error analysis.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL\", \"published\": \"2025-04-29T17:03:03Z\"}"}

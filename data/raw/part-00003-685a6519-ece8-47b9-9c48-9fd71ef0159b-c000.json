{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00661v1\", \"title\": \"On the generalization of language models from in-context learning and\\n  finetuning: a controlled study\", \"summary\": \"Large language models exhibit exciting capabilities, yet can show\\nsurprisingly narrow generalization from finetuning -- from failing to\\ngeneralize to simple reversals of relations they are trained on, to missing\\nlogical deductions that can be made from trained information. These failures to\\ngeneralize from fine-tuning can hinder practical application of these models.\\nHowever, language models' in-context learning shows different inductive biases,\\nand can generalize better in some of these cases. Here, we explore these\\ndifferences in generalization between in-context- and fine-tuning-based\\nlearning. To do so, we constructed several novel datasets to evaluate and\\nimprove models' ability to generalize from finetuning data. The datasets are\\nconstructed to isolate the knowledge in the dataset from that in pretraining,\\nto create clean tests of generalization. We expose pretrained large models to\\ncontrolled subsets of the information in these datasets -- either in context,\\nor through fine-tuning -- and evaluate their performance on test sets that\\nrequire various types of generalization. We find overall that in data-matched\\nsettings, in-context learning can generalize more flexibly than fine-tuning\\n(though we also find some qualifications of prior findings, such as cases when\\nfine-tuning can generalize to reversals embedded in a larger structure of\\nknowledge). We build on these findings to propose a method to enable improved\\ngeneralization from fine-tuning: adding in-context inferences to finetuning\\ndata. We show that this method improves generalization across various splits of\\nour datasets and other benchmarks. Our results have implications for\\nunderstanding the inductive biases of different modes of learning in language\\nmodels, and practically improving their performance.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-05-01T17:02:27Z\"}"}

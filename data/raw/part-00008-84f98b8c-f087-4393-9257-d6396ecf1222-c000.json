{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13034v1\", \"title\": \"Inference-friendly Graph Compression for Graph Neural Networks\", \"summary\": \"Graph Neural Networks (GNNs) have demonstrated promising performance in graph\\nanalysis. Nevertheless, the inference process of GNNs remains costly, hindering\\ntheir applications for large graphs. This paper proposes inference-friendly\\ngraph compression (IFGC), a graph compression scheme to accelerate GNNs\\ninference. Given a graph $G$ and a GNN $M$, an IFGC computes a small compressed\\ngraph $G_c$, to best preserve the inference results of $M$ over $G$, such that\\nthe result can be directly inferred by accessing $G_c$ with no or little\\ndecompression cost. (1) We characterize IFGC with a class of inference\\nequivalence relation. The relation captures the node pairs in $G$ that are not\\ndistinguishable for GNN inference. (2) We introduce three practical\\nspecifications of IFGC for representative GNNs: structural preserving\\ncompression (SPGC), which computes $G_c$ that can be directly processed by GNN\\ninference without decompression; ($\\\\alpha$, $r$)-compression, that allows for a\\nconfigurable trade-off between compression ratio and inference quality, and\\nanchored compression that preserves inference results for specific nodes of\\ninterest. For each scheme, we introduce compression and inference algorithms\\nwith guarantees of efficiency and quality of the inferred results. We conduct\\nextensive experiments on diverse sets of large-scale graphs, which verifies the\\neffectiveness and efficiency of our graph compression approaches.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-17T15:42:13Z\"}"}

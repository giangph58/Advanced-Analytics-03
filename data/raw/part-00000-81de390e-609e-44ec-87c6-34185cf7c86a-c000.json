{"value":"{\"aid\": \"http://arxiv.org/abs/2503.21776v1\", \"title\": \"Video-R1: Reinforcing Video Reasoning in MLLMs\", \"summary\": \"Inspired by DeepSeek-R1's success in eliciting reasoning abilities through\\nrule-based reinforcement learning (RL), we introduce Video-R1 as the first\\nattempt to systematically explore the R1 paradigm for eliciting video reasoning\\nwithin multimodal large language models (MLLMs). However, directly applying RL\\ntraining with the GRPO algorithm to video reasoning presents two primary\\nchallenges: (i) a lack of temporal modeling for video reasoning, and (ii) the\\nscarcity of high-quality video-reasoning data. To address these issues, we\\nfirst propose the T-GRPO algorithm, which encourages models to utilize temporal\\ninformation in videos for reasoning. Additionally, instead of relying solely on\\nvideo data, we incorporate high-quality image-reasoning data into the training\\nprocess. We have constructed two datasets: Video-R1-COT-165k for SFT cold start\\nand Video-R1-260k for RL training, both comprising image and video data.\\nExperimental results demonstrate that Video-R1 achieves significant\\nimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as\\nwell as on general video benchmarks including MVBench and TempCompass, etc.\\nNotably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning\\nbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All\\ncodes, models, data are released.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-27T17:59:51Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17449v1\", \"title\": \"HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant\\n  Inference in Pretrained Language Models\", \"summary\": \"The significant computational demands of pretrained language models (PLMs),\\nwhich often require dedicated hardware, present a substantial challenge in\\nserving them efficiently, especially in multi-tenant environments. To address\\nthis, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant\\nInference system, designed to manage tenants with distinct PLMs\\nresource-efficiently. Our approach is three-fold: Firstly, we categorize PLM\\nknowledge into general, domain-specific, and task-specific. Leveraging insights\\non knowledge acquisition across different model layers, we construct\\nhierarchical PLMs (hPLMs) by extracting and storing knowledge at different\\nlevels, significantly reducing GPU memory usage per tenant. Secondly, we\\nestablish hierarchical knowledge management for hPLMs generated by various\\ntenants in HMI. We manage domain-specific knowledge with acceptable storage\\nincreases by constructing and updating domain-specific knowledge trees based on\\nfrequency. We manage task-specific knowledge within limited GPU memory through\\nparameter swapping. Finally, we propose system optimizations to enhance\\nresource utilization and inference throughput. These include fine-grained\\npipelining via hierarchical knowledge prefetching to overlap CPU and I/O\\noperations with GPU computations, and optimizing parallel implementations with\\nbatched matrix multiplications. Our experimental results demonstrate that the\\nproposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a\\nsingle GPU, with only a negligible compromise in accuracy.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL\", \"published\": \"2025-04-24T11:28:40Z\"}"}

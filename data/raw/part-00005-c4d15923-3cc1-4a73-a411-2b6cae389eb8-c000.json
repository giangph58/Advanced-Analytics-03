{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16438v1\", \"title\": \"Private Federated Learning using Preference-Optimized Synthetic Data\", \"summary\": \"In practical settings, differentially private Federated learning (DP-FL) is\\nthe dominant method for training models from private, on-device client data.\\nRecent work has suggested that DP-FL may be enhanced or outperformed by methods\\nthat use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary\\nalgorithms for generating DP synthetic data for FL applications require careful\\nprompt engineering based on public information and/or iterative private client\\nfeedback. Our key insight is that the private client feedback collected by\\nprior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be\\nviewed as a preference ranking. Our algorithm, Preference Optimization for\\nPrivate Client Data (POPri) harnesses client feedback using preference\\noptimization algorithms such as Direct Preference Optimization (DPO) to\\nfine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri,\\nwe release LargeFedBench, a new federated text benchmark for uncontaminated LLM\\nevaluations on federated client data. POPri substantially improves the utility\\nof DP synthetic data relative to prior work on LargeFedBench datasets and an\\nexisting benchmark from Xie et al. (2024). POPri closes the gap between\\nnext-token prediction accuracy in the fully-private and non-private settings by\\nup to 68%, compared to 52% for prior synthetic data methods, and 10% for\\nstate-of-the-art DP federated learning methods. The code and data are available\\nat https://github.com/meiyuw/POPri.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CR,cs.DC\", \"published\": \"2025-04-23T05:57:20Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11903v1\", \"title\": \"FedCanon: Non-Convex Composite Federated Learning with Efficient\\n  Proximal Operation on Heterogeneous Data\", \"summary\": \"Composite federated learning offers a general framework for solving machine\\nlearning problems with additional regularization terms. However, many existing\\nmethods require clients to perform multiple proximal operations to handle\\nnon-smooth terms and their performance are often susceptible to data\\nheterogeneity. To overcome these limitations, we propose a novel composite\\nfederated learning algorithm called \\\\textbf{FedCanon}, designed to solve the\\noptimization problems comprising a possibly non-convex loss function and a\\nweakly convex, potentially non-smooth regularization term. By decoupling\\nproximal mappings from local updates, FedCanon requires only a single proximal\\nevaluation on the server per iteration, thereby reducing the overall proximal\\ncomputation cost. It also introduces control variables that incorporate global\\ngradient information into client updates, which helps mitigate the effects of\\ndata heterogeneity. Theoretical analysis demonstrates that FedCanon achieves\\nsublinear convergence rates under general non-convex settings and linear\\nconvergence under the Polyak-{\\\\L}ojasiewicz condition, without relying on\\nbounded heterogeneity assumptions. Experiments demonstrate that FedCanon\\noutperforms the state-of-the-art methods in terms of both accuracy and\\ncomputational efficiency, particularly under heterogeneous data distributions.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.DC,math.OC\", \"published\": \"2025-04-16T09:28:26Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24150v1\", \"title\": \"Learning a Canonical Basis of Human Preferences from Binary Ratings\", \"summary\": \"Recent advances in generative AI have been driven by alignment techniques\\nsuch as reinforcement learning from human feedback (RLHF). RLHF and related\\ntechniques typically involve constructing a dataset of binary or ranked choice\\nhuman preferences and subsequently fine-tuning models to align with these\\npreferences. This paper shifts the focus to understanding the preferences\\nencoded in such datasets and identifying common human preferences. We find that\\na small subset of 21 preference categories (selected from a set of nearly 5,000\\ndistinct preferences) captures >89% of preference variation across individuals.\\nThis small set of preferences is analogous to a canonical basis of human\\npreferences, similar to established findings that characterize human variation\\nin psychology or facial recognition studies. Through both synthetic and\\nempirical evaluations, we confirm that our low-rank, canonical set of human\\npreferences generalizes across the entire dataset and within specific topics.\\nWe further demonstrate our preference basis' utility in model evaluation, where\\nour preference categories offer deeper insights into model alignment, and in\\nmodel training, where we show that fine-tuning on preference-defined subsets\\nsuccessfully aligns the model accordingly.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.HC\", \"published\": \"2025-03-31T14:35:48Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21501v1\", \"title\": \"Deep Learning Optimization Using Self-Adaptive Weighted Auxiliary\\n  Variables\", \"summary\": \"In this paper, we develop a new optimization framework for the least squares\\nlearning problem via fully connected neural networks or physics-informed neural\\nnetworks. The gradient descent sometimes behaves inefficiently in deep learning\\nbecause of the high non-convexity of loss functions and the vanishing gradient\\nissue. Our idea is to introduce auxiliary variables to separate the layers of\\nthe deep neural networks and reformulate the loss functions for ease of\\noptimization. We design the self-adaptive weights to preserve the consistency\\nbetween the reformulated loss and the original mean squared loss, which\\nguarantees that optimizing the new loss helps optimize the original problem.\\nNumerical experiments are presented to verify the consistency and show the\\neffectiveness and robustness of our models over gradient descent.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-30T10:43:13Z\"}"}

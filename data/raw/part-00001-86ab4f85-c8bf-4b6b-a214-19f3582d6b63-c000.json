{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21635v1\", \"title\": \"Sadeed: Advancing Arabic Diacritization Through Small Language Model\", \"summary\": \"Arabic text diacritization remains a persistent challenge in natural language\\nprocessing due to the language's morphological richness. In this paper, we\\nintroduce Sadeed, a novel approach based on a fine-tuned decoder-only language\\nmodel adapted from Kuwain 1.5B Hennara et al. [2025], a compact model\\noriginally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully\\ncurated, high-quality diacritized datasets, constructed through a rigorous\\ndata-cleaning and normalization pipeline. Despite utilizing modest\\ncomputational resources, Sadeed achieves competitive results compared to\\nproprietary large language models and outperforms traditional models trained on\\nsimilar domains. Additionally, we highlight key limitations in current\\nbenchmarking practices for Arabic diacritization. To address these issues, we\\nintroduce SadeedDiac-25, a new benchmark designed to enable fairer and more\\ncomprehensive evaluation across diverse text genres and complexity levels.\\nTogether, Sadeed and SadeedDiac-25 provide a robust foundation for advancing\\nArabic NLP applications, including machine translation, text-to-speech, and\\nlanguage learning tools.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-30T13:37:24Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13181v1\", \"title\": \"Perception Encoder: The best visual embeddings are not at the output of\\n  the network\", \"summary\": \"We introduce Perception Encoder (PE), a state-of-the-art encoder for image\\nand video understanding trained via simple vision-language learning.\\nTraditionally, vision encoders have relied on a variety of pretraining\\nobjectives, each tailored to specific downstream tasks such as classification,\\ncaptioning, or localization. Surprisingly, after scaling our carefully tuned\\nimage pretraining recipe and refining with our robust video data engine, we\\nfind that contrastive vision-language training alone can produce strong,\\ngeneral embeddings for all of these downstream tasks. There is only one caveat:\\nthese embeddings are hidden within the intermediate layers of the network. To\\ndraw them out, we introduce two alignment methods, language alignment for\\nmultimodal language modeling, and spatial alignment for dense prediction.\\nTogether with the core contrastive checkpoint, our PE family of models achieves\\nstate-of-the-art performance on a wide variety of tasks, including zero-shot\\nimage and video classification and retrieval; document, image, and video Q&A;\\nand spatial tasks such as detection, depth estimation, and tracking. To foster\\nfurther research, we are releasing our models, code, and a novel dataset of\\nsynthetically and human-annotated videos.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-17T17:59:57Z\"}"}

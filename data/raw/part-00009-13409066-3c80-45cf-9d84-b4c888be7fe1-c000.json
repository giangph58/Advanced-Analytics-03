{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07854v1\", \"title\": \"The KL3M Data Project: Copyright-Clean Training Resources for Large\\n  Language Models\", \"summary\": \"Practically all large language models have been pre-trained on data that is\\nsubject to global uncertainty related to copyright infringement and breach of\\ncontract. This creates potential risk for users and developers due to this\\nuncertain legal status. The KL3M Data Project directly confronts this critical\\nissue by introducing the largest comprehensive training data pipeline that\\nminimizes risks related to copyright or breach of contract. The foundation of\\nthis project is a corpus of over 132 million documents and trillions of tokens\\nspanning 16 different sources that have been verified to meet the strict\\ncopyright and licensing protocol detailed herein. We are releasing the entire\\npipeline, including 1) the source code to acquire and process these documents,\\n2) the original document formats with associated provenance and metadata, 3)\\nextracted content in a standardized format, 4) pre-tokenized representations of\\nthe documents, and 5) various mid- and post-train resources such as\\nquestion-answer, summarization, conversion, drafting, classification,\\nprediction, and conversational data. All of these resources are freely\\navailable to the public on S3, Hugging Face, and GitHub under CC-BY terms. We\\nare committed to continuing this project in furtherance of a more ethical,\\nlegal, and sustainable approach to the development and use of AI models.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-10T15:31:17Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05800v1\", \"title\": \"Storybooth: Training-free Multi-Subject Consistency for Improved Visual\\n  Storytelling\", \"summary\": \"Training-free consistent text-to-image generation depicting the same subjects\\nacross different images is a topic of widespread recent interest. Existing\\nworks in this direction predominantly rely on cross-frame self-attention; which\\nimproves subject-consistency by allowing tokens in each frame to pay attention\\nto tokens in other frames during self-attention computation. While useful for\\nsingle subjects, we find that it struggles when scaling to multiple characters.\\nIn this work, we first analyze the reason for these limitations. Our\\nexploration reveals that the primary-issue stems from self-attention-leakage,\\nwhich is exacerbated when trying to ensure consistency across\\nmultiple-characters. This happens when tokens from one subject pay attention to\\nother characters, causing them to appear like each other (e.g., a dog appearing\\nlike a duck). Motivated by these findings, we propose StoryBooth: a\\ntraining-free approach for improving multi-character consistency. In\\nparticular, we first leverage multi-modal chain-of-thought reasoning and\\nregion-based generation to apriori localize the different subjects across the\\ndesired story outputs. The final outputs are then generated using a modified\\ndiffusion model which consists of two novel layers: 1) a bounded cross-frame\\nself-attention layer for reducing inter-character attention leakage, and 2)\\ntoken-merging layer for improving consistency of fine-grain subject details.\\nThrough both qualitative and quantitative results we find that the proposed\\napproach surpasses prior state-of-the-art, exhibiting improved consistency\\nacross both multiple-characters and fine-grain subject details.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG,cs.MM\", \"published\": \"2025-04-08T08:30:55Z\"}"}

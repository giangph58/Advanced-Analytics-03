{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14992v1\", \"title\": \"Efficient Pretraining Length Scaling\", \"summary\": \"Recent advances in large language models have demonstrated the effectiveness\\nof length scaling during post-training, yet its potential in pre-training\\nremains underexplored. We present the Parallel Hidden Decoding Transformer\\n(\\\\textit{PHD}-Transformer), a novel framework that enables efficient length\\nscaling during pre-training while maintaining inference efficiency.\\n\\\\textit{PHD}-Transformer achieves this through an innovative KV cache\\nmanagement strategy that distinguishes between original tokens and hidden\\ndecoding tokens. By retaining only the KV cache of original tokens for\\nlong-range dependencies while immediately discarding hidden decoding tokens\\nafter use, our approach maintains the same KV cache size as the vanilla\\ntransformer while enabling effective length scaling. To further enhance\\nperformance, we introduce two optimized variants: \\\\textit{PHD-SWA} employs\\nsliding window attention to preserve local dependencies, while\\n\\\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\\nimprovements across multiple benchmarks.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-21T09:41:26Z\"}"}

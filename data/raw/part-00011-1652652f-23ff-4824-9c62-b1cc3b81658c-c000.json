{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13171v1\", \"title\": \"Sleep-time Compute: Beyond Inference Scaling at Test-time\", \"summary\": \"Scaling test-time compute has emerged as a key ingredient for enabling large\\nlanguage models (LLMs) to solve difficult problems, but comes with high latency\\nand inference cost. We introduce sleep-time compute, which allows models to\\n\\\"think\\\" offline about contexts before queries are presented: by anticipating\\nwhat queries users might ask and pre-computing useful quantities, we can\\nsignificantly reduce the compute requirements at test-time. To demonstrate the\\nefficacy of our method, we create modified versions of two reasoning tasks -\\nStateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can\\nreduce the amount of test-time compute needed to achieve the same accuracy by ~\\n5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time\\ncompute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic\\nand 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,\\nwhich extends GSM-Symbolic by including multiple related queries per context.\\nBy amortizing sleep-time compute across related queries about the same context\\nusing Multi-Query GSM-Symbolic, we can decrease the average cost per query by\\n2.5x. We then conduct additional analysis to understand when sleep-time compute\\nis most effective, finding the predictability of the user query to be well\\ncorrelated with the efficacy of sleep-time compute. Finally, we conduct a\\ncase-study of applying sleep-time compute to a realistic agentic SWE task.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CL\", \"published\": \"2025-04-17T17:59:25Z\"}"}

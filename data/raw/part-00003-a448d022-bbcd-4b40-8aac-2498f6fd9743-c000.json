{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17333v1\", \"title\": \"Fine-Grained Fusion: The Missing Piece in Area-Efficient State Space\\n  Model Acceleration\", \"summary\": \"State Space Models (SSMs) offer a promising alternative to transformers for\\nlong-sequence processing. However, their efficiency remains hindered by\\nmemory-bound operations, particularly in the prefill stage. While MARCA, a\\nrecent first effort to accelerate SSMs through a dedicated hardware\\naccelerator, achieves great speedup over high-end GPUs, an analysis into the\\nbroader accelerator design space is lacking. This work systematically analyzes\\nSSM acceleration opportunities both from the scheduling perspective through\\nfine-grained operator fusion and the hardware perspective through design space\\nexploration, using an extended version of the Stream modeling framework.\\n  Our results demonstrate that the improved data locality stemming from our\\noptimized fusion and scheduling strategy enables a speedup of up to 4.8x over\\nunfused execution, while our adaptive memory-aware fusion approach reduces\\non-chip memory requirements by an order of magnitude without sacrificing\\nperformance. We further explore accelerator design trade-offs, showing that a\\nfusion-aware hardware architecture can achieve 1.78x higher performance than\\nthe state-of-the-art MARCA accelerator, within the same area budget. These\\nresults establish operator fusion as a key enabler for next-generation SSM\\naccelerators.\", \"main_category\": \"cs.AR\", \"categories\": \"cs.AR\", \"published\": \"2025-04-24T07:52:02Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00509v1\", \"title\": \"Self-Ablating Transformers: More Interpretability, Less Sparsity\", \"summary\": \"A growing intuition in machine learning suggests a link between sparsity and\\ninterpretability. We introduce a novel self-ablation mechanism to investigate\\nthis connection ante-hoc in the context of language transformers. Our approach\\ndynamically enforces a k-winner-takes-all constraint, forcing the model to\\ndemonstrate selective activation across neuron and attention units. Unlike\\npost-hoc methods that analyze already-trained models, our approach integrates\\ninterpretability directly into model training, promoting feature localization\\nfrom inception. Training small models on the TinyStories dataset and employing\\ninterpretability tests, we find that self-ablation leads to more localized\\ncircuits, concentrated feature representations, and increased neuron\\nspecialization without compromising language modelling performance.\\nSurprisingly, our method also decreased overall sparsity, indicating that\\nself-ablation promotes specialization rather than widespread inactivity. This\\nreveals a complex interplay between sparsity and interpretability, where\\ndecreased global sparsity can coexist with increased local specialization,\\nleading to enhanced interpretability. To facilitate reproducibility, we make\\nour code available at\\nhttps://github.com/keenanpepper/self-ablating-transformers.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-01T13:25:37Z\"}"}

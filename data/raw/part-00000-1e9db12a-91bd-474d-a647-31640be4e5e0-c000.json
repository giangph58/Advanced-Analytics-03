{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05097v1\", \"title\": \"State Tuning: State-based Test-Time Scaling on RWKV-7\", \"summary\": \"Test-time scaling has emerged as a prominent research direction in machine\\nlearning, enabling models to enhance their expressive capabilities during\\ninference.Transformers, renowned for striking a delicate balance between\\nefficiency and expressiveness, have benefited from test-time scaling techniques\\nthat leverage an expanding key-value (KV) cache to significantly improve\\nperformance.In this paper, we introduce a novel state-based approach to\\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\\nstate-of-the-art performance on the target task without altering the model's\\npre-trained weights. Our approach centers on three key innovations. First, we\\ndevelop an observer framework that allows a smaller model to replicate and\\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\\nto dynamically upscale the state size, enhancing the model's capacity to\\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\\nmodel can outperform larger models on the given task. This method preserves the\\nefficiency of the original RWKV-7 architecture while harnessing the power of\\ntest-time scaling to deliver superior results. Our findings underscore the\\npotential of state tuning as an effective strategy for advancing model\\nperformance in resource-constrained settings. Our code is\\nhttps://github.com/TorchRWKV/flash-linear-attention.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-07T14:04:30Z\"}"}

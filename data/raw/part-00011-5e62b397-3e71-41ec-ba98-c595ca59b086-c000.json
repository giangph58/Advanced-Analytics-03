{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17376v1\", \"title\": \"On-Device Qwen2.5: Efficient LLM Inference with Model Compression and\\n  Hardware Acceleration\", \"summary\": \"Transformer-based Large Language Models (LLMs) have significantly advanced AI\\ncapabilities but pose considerable challenges for deployment on edge devices\\ndue to high computational demands, memory bandwidth constraints, and energy\\nconsumption. This paper addresses these challenges by presenting an efficient\\nframework for deploying the Qwen2.5-0.5B model on the Xilinx Kria KV260 edge\\nplatform, a heterogeneous system integrating an ARM Cortex-A53 CPU with\\nreconfigurable FPGA logic. Leveraging Activation-aware Weight Quantization\\n(AWQ) with FPGA-accelerated execution pipelines, the proposed approach enhances\\nboth model compression rate and system throughput. Additionally, we propose a\\nhybrid execution strategy that intelligently offloads compute-intensive\\noperations to the FPGA while utilizing the CPU for lighter tasks, effectively\\nbalancing the computational workload and maximizing overall performance. Our\\nframework achieves a model compression rate of 55.08% compared to the original\\nmodel and produces output at a rate of 5.1 tokens per second, outperforming the\\nbaseline performance of 2.8 tokens per second.\", \"main_category\": \"cs.AR\", \"categories\": \"cs.AR,cs.LG\", \"published\": \"2025-04-24T08:50:01Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06607v1\", \"title\": \"Visually Similar Pair Alignment for Robust Cross-Domain Object Detection\", \"summary\": \"Domain gaps between training data (source) and real-world environments\\n(target) often degrade the performance of object detection models. Most\\nexisting methods aim to bridge this gap by aligning features across source and\\ntarget domains but often fail to account for visual differences, such as color\\nor orientation, in alignment pairs. This limitation leads to less effective\\ndomain adaptation, as the model struggles to manage both domain-specific shifts\\n(e.g., fog) and visual variations simultaneously. In this work, we demonstrate\\nfor the first time, using a custom-built dataset, that aligning visually\\nsimilar pairs significantly improves domain adaptation. Based on this insight,\\nwe propose a novel memory-based system to enhance domain alignment. This system\\nstores precomputed features of foreground objects and background areas from the\\nsource domain, which are periodically updated during training. By retrieving\\nvisually similar source features for alignment with target foreground and\\nbackground features, the model effectively addresses domain-specific\\ndifferences while reducing the impact of visual variations. Extensive\\nexperiments across diverse domain shift scenarios validate our method's\\neffectiveness, achieving 53.1 mAP on Foggy Cityscapes and 62.3 on Sim10k,\\nsurpassing prior state-of-the-art methods by 1.2 and 4.1 mAP, respectively.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-09T06:11:11Z\"}"}

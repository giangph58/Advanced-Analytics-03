{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12687v1\", \"title\": \"Data-efficient LLM Fine-tuning for Code Generation\", \"summary\": \"Large language models (LLMs) have demonstrated significant potential in code\\ngeneration tasks. However, there remains a performance gap between open-source\\nand closed-source models. To address this gap, existing approaches typically\\ngenerate large amounts of synthetic data for fine-tuning, which often leads to\\ninefficient training. In this work, we propose a data selection strategy in\\norder to improve the effectiveness and efficiency of training for code-based\\nLLMs. By prioritizing data complexity and ensuring that the sampled subset\\naligns with the distribution of the original dataset, our sampling strategy\\neffectively selects high-quality data. Additionally, we optimize the\\ntokenization process through a \\\"dynamic pack\\\" technique, which minimizes\\npadding tokens and reduces computational resource consumption. Experimental\\nresults show that when training on 40% of the OSS-Instruct dataset, the\\nDeepSeek-Coder-Base-6.7B model achieves an average performance of 66.9%,\\nsurpassing the 66.1% performance with the full dataset. Moreover, training time\\nis reduced from 47 minutes to 34 minutes, and the peak GPU memory decreases\\nfrom 61.47 GB to 42.72 GB during a single epoch. Similar improvements are\\nobserved with the CodeLlama-Python-7B model on the Evol-Instruct dataset. By\\noptimizing both data selection and tokenization, our approach not only improves\\nmodel performance but also improves training efficiency.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-17T06:29:28Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03275v1\", \"title\": \"RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via\\n  Retrieval-Augmented Generation\", \"summary\": \"Large language models (LLMs) struggle to effectively utilize a growing number\\nof external tools, such as those defined by the Model Context Protocol\\n(MCP)\\\\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We\\nintroduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes\\nthis challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to\\nidentify the most relevant MCP(s) for a given query from an external index\\nbefore engaging the LLM. Only the selected tool descriptions are passed to the\\nmodel, drastically reducing prompt size and simplifying decision-making.\\nExperiments, including an MCP stress test, demonstrate RAG-MCP significantly\\ncuts prompt tokens (e.g., by over 50%) and more than triples tool selection\\naccuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables\\nscalable and accurate tool integration for LLMs.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.SE\", \"published\": \"2025-05-06T08:05:35Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.09935v1\", \"title\": \"Constrained Auto-Regressive Decoding Constrains Generative Retrieval\", \"summary\": \"Generative retrieval seeks to replace traditional search index data\\nstructures with a single large-scale neural network, offering the potential for\\nimproved efficiency and seamless integration with generative large language\\nmodels. As an end-to-end paradigm, generative retrieval adopts a learned\\ndifferentiable search index to conduct retrieval by directly generating\\ndocument identifiers through corpus-specific constrained decoding. The\\ngeneralization capabilities of generative retrieval on out-of-distribution\\ncorpora have gathered significant attention.\\n  In this paper, we examine the inherent limitations of constrained\\nauto-regressive generation from two essential perspectives: constraints and\\nbeam search. We begin with the Bayes-optimal setting where the generative\\nretrieval model exactly captures the underlying relevance distribution of all\\npossible documents. Then we apply the model to specific corpora by simply\\nadding corpus-specific constraints. Our main findings are two-fold: (i) For the\\neffect of constraints, we derive a lower bound of the error, in terms of the KL\\ndivergence between the ground-truth and the model-predicted step-wise marginal\\ndistributions. (ii) For the beam search algorithm used during generation, we\\nreveal that the usage of marginal distributions may not be an ideal approach.\\nThis paper aims to improve our theoretical understanding of the generalization\\ncapabilities of the auto-regressive decoding retrieval paradigm, laying a\\nfoundation for its limitations and inspiring future advancements toward more\\nrobust and generalizable generative retrieval.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR\", \"published\": \"2025-04-14T06:54:49Z\"}"}

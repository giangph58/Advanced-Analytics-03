{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01738v1\", \"title\": \"Style over Substance: Distilled Language Models Reason Via Stylistic\\n  Replication\", \"summary\": \"Specialized reasoning language models (RLMs) have demonstrated that scaling\\ntest-time computation through detailed reasoning traces significantly enhances\\nperformance. Although these traces effectively facilitate knowledge\\ndistillation into smaller, instruction-tuned models, the precise nature of\\ntransferred reasoning remains unclear. In this study, we investigate to what\\nextent distilled models internalize replicated stylistic patterns during\\nreasoning. To this end, we systematically analyze reasoning traces, identifying\\nstructural and lexical patterns that characterize successful reasoning. We then\\nintroduce two new datasets -- a dataset of emergent reasoning traces and a\\nsynthetic dataset explicitly constructed to replicate these stylistic patterns\\n-- to precisely examine their influence on distilled models' reasoning\\ncapabilities. We find that models trained on the synthetic traces achieve\\ncomparable performance, indicating that distilled reasoning abilities rely\\nsignificantly on surface-level patterns. Surprisingly, we observe an increase\\nin performance even when the synthetic traces are altered to lead to the wrong\\nanswer. Our findings highlight how stylistic patterns can be leveraged to\\nefficiently enhance LM reasoning across diverse model families.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-02T13:50:20Z\"}"}

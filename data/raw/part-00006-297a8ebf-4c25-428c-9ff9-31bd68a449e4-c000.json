{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07527v1\", \"title\": \"Supervised Optimism Correction: Be Confident When LLMs Are Sure\", \"summary\": \"In this work, we establish a novel theoretical connection between supervised\\nfine-tuning and offline reinforcement learning under the token-level Markov\\ndecision process, revealing that large language models indeed learn an implicit\\n$Q$-function for inference. Through this theoretical lens, we demonstrate that\\nthe widely used beam search method suffers from unacceptable over-optimism,\\nwhere inference errors are inevitably amplified due to inflated $Q$-value\\nestimations of suboptimal steps. To address this limitation, we propose\\nSupervised Optimism Correction(SOC), which introduces a simple yet effective\\nauxiliary loss for token-level $Q$-value estimations during supervised\\nfine-tuning. Specifically, the auxiliary loss employs implicit value\\nregularization to boost model confidence in expert-demonstrated responses,\\nthereby suppressing over-optimism toward insufficiently supervised responses.\\nExtensive experiments on mathematical reasoning benchmarks, including GSM8K,\\nMATH, and GAOKAO, showcase the superiority of the proposed SOC with beam search\\nacross a series of open-source models.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-10T07:50:03Z\"}"}

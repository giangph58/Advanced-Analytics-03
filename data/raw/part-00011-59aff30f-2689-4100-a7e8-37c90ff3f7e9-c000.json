{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14977v1\", \"title\": \"RealisDance-DiT: Simple yet Strong Baseline towards Controllable\\n  Character Animation in the Wild\", \"summary\": \"Controllable character animation remains a challenging problem, particularly\\nin handling rare poses, stylized characters, character-object interactions,\\ncomplex illumination, and dynamic scenes. To tackle these issues, prior work\\nhas largely focused on injecting pose and appearance guidance via elaborate\\nbypass networks, but often struggles to generalize to open-world scenarios. In\\nthis paper, we propose a new perspective that, as long as the foundation model\\nis powerful enough, straightforward model modifications with flexible\\nfine-tuning strategies can largely address the above challenges, taking a step\\ntowards controllable character animation in the wild. Specifically, we\\nintroduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our\\nsufficient analysis reveals that the widely adopted Reference Net design is\\nsuboptimal for large-scale DiT models. Instead, we demonstrate that minimal\\nmodifications to the foundation model architecture yield a surprisingly strong\\nbaseline. We further propose the low-noise warmup and \\\"large batches and small\\niterations\\\" strategies to accelerate model convergence during fine-tuning while\\nmaximally preserving the priors of the foundation model. In addition, we\\nintroduce a new test dataset that captures diverse real-world challenges,\\ncomplementing existing benchmarks such as TikTok dataset and UBC fashion video\\ndataset, to comprehensively evaluate the proposed method. Extensive experiments\\nshow that RealisDance-DiT outperforms existing methods by a large margin.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-21T09:09:21Z\"}"}

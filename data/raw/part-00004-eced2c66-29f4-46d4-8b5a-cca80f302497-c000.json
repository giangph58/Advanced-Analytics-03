{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11409v1\", \"title\": \"Efficient Hybrid Language Model Compression through Group-Aware SSM\\n  Pruning\", \"summary\": \"Hybrid LLM architectures that combine Attention and State Space Models (SSMs)\\nachieve state-of-the-art accuracy and runtime performance. Recent work has\\ndemonstrated that applying compression and distillation to Attention-only\\nmodels yields smaller, more accurate models at a fraction of the training cost.\\nIn this work, we explore the effectiveness of compressing Hybrid architectures.\\nWe introduce a novel group-aware pruning strategy that preserves the structural\\nintegrity of SSM blocks and their sequence modeling capabilities. Furthermore,\\nwe demonstrate the necessity of such SSM pruning to achieve improved accuracy\\nand inference speed compared to traditional approaches. Our compression recipe\\ncombines SSM, FFN, embedding dimension, and layer pruning, followed by\\nknowledge distillation-based retraining, similar to the MINITRON technique.\\nUsing this approach, we compress the Nemotron-H 8B Hybrid model down to 4B\\nparameters with up to 40x fewer training tokens. The resulting model surpasses\\nthe accuracy of similarly-sized models while achieving 2x faster inference,\\nsignificantly advancing the Pareto frontier.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-15T17:26:29Z\"}"}

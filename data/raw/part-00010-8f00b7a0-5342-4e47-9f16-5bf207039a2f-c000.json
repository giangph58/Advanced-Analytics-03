{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02286v1\", \"title\": \"Moment Quantization for Video Temporal Grounding\", \"summary\": \"Video temporal grounding is a critical video understanding task, which aims\\nto localize moments relevant to a language description. The challenge of this\\ntask lies in distinguishing relevant and irrelevant moments. Previous methods\\nfocused on learning continuous features exhibit weak differentiation between\\nforeground and background features. In this paper, we propose a novel\\nMoment-Quantization based Video Temporal Grounding method (MQVTG), which\\nquantizes the input video into various discrete vectors to enhance the\\ndiscrimination between relevant and irrelevant moments. Specifically, MQVTG\\nmaintains a learnable moment codebook, where each video moment matches a\\ncodeword. Considering the visual diversity, i.e., various visual expressions\\nfor the same moment, MQVTG treats moment-codeword matching as a clustering\\nprocess without using discrete vectors, avoiding the loss of useful information\\nfrom direct hard quantization. Additionally, we employ effective\\nprior-initialization and joint-projection strategies to enhance the maintained\\nmoment codebook. With its simple implementation, the proposed method can be\\nintegrated into existing temporal grounding models as a plug-and-play\\ncomponent. Extensive experiments on six popular benchmarks demonstrate the\\neffectiveness and generalizability of MQVTG, significantly outperforming\\nstate-of-the-art methods. Further qualitative analysis shows that our method\\neffectively groups relevant features and separates irrelevant ones, aligning\\nwith our goal of enhancing discrimination.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-03T05:21:14Z\"}"}

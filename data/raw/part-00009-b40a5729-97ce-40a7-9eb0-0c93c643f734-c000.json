{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04258v1\", \"title\": \"RGB-Event Fusion with Self-Attention for Collision Prediction\", \"summary\": \"Ensuring robust and real-time obstacle avoidance is critical for the safe\\noperation of autonomous robots in dynamic, real-world environments. This paper\\nproposes a neural network framework for predicting the time and collision\\nposition of an unmanned aerial vehicle with a dynamic object, using RGB and\\nevent-based vision sensors. The proposed architecture consists of two separate\\nencoder branches, one for each modality, followed by fusion by self-attention\\nto improve prediction accuracy. To facilitate benchmarking, we leverage the\\nABCD [8] dataset collected that enables detailed comparisons of single-modality\\nand fusion-based approaches. At the same prediction throughput of 50Hz, the\\nexperimental results show that the fusion-based model offers an improvement in\\nprediction accuracy over single-modality approaches of 1% on average and 10%\\nfor distances beyond 0.5m, but comes at the cost of +71% in memory and + 105%\\nin FLOPs. Notably, the event-based model outperforms the RGB model by 4% for\\nposition and 26% for time error at a similar computational cost, making it a\\ncompetitive alternative. Additionally, we evaluate quantized versions of the\\nevent-based models, applying 1- to 8-bit quantization to assess the trade-offs\\nbetween predictive performance and computational efficiency. These findings\\nhighlight the trade-offs of multi-modal perception using RGB and event-based\\ncameras in robotic applications.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.CV\", \"published\": \"2025-05-07T09:03:26Z\"}"}

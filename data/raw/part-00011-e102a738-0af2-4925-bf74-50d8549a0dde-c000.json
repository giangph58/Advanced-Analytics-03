{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16628v1\", \"title\": \"ParetoHqD: Fast Offline Multiobjective Alignment of Large Language\\n  Models using Pareto High-quality Data\", \"summary\": \"Aligning large language models with multiple human expectations and values is\\ncrucial for ensuring that they adequately serve a variety of user needs. To\\nthis end, offline multiobjective alignment algorithms such as the\\nRewards-in-Context algorithm have shown strong performance and efficiency.\\nHowever, inappropriate preference representations and training with imbalanced\\nreward scores limit the performance of such algorithms. In this work, we\\nintroduce ParetoHqD that addresses the above issues by representing human\\npreferences as preference directions in the objective space and regarding data\\nnear the Pareto front as ''high-quality'' data. For each preference, ParetoHqD\\nfollows a two-stage supervised fine-tuning process, where each stage uses an\\nindividual Pareto high-quality training set that best matches its preference\\ndirection. The experimental results have demonstrated the superiority of\\nParetoHqD over five baselines on two multiobjective alignment tasks.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL\", \"published\": \"2025-04-23T11:35:57Z\"}"}

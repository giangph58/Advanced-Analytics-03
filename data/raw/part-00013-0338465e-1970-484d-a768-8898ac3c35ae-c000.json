{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04310v1\", \"title\": \"Flow Models for Unbounded and Geometry-Aware Distributional\\n  Reinforcement Learning\", \"summary\": \"We introduce a new architecture for Distributional Reinforcement Learning\\n(DistRL) that models return distributions using normalizing flows. This\\napproach enables flexible, unbounded support for return distributions, in\\ncontrast to categorical approaches like C51 that rely on fixed or bounded\\nrepresentations. It also offers richer modeling capacity to capture\\nmulti-modality, skewness, and tail behavior than quantile based approaches. Our\\nmethod is significantly more parameter-efficient than categorical approaches.\\nStandard metrics used to train existing models like KL divergence or\\nWasserstein distance either are scale insensitive or have biased sample\\ngradients, especially when return supports do not overlap. To address this, we\\npropose a novel surrogate for the Cram\\\\`er distance, that is geometry-aware and\\ncomputable directly from the return distribution's PDF, avoiding the costly CDF\\ncomputation. We test our model on the ATARI-5 sub-benchmark and show that our\\napproach outperforms PDF based models while remaining competitive with quantile\\nbased methods.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,math.OC\", \"published\": \"2025-05-07T10:49:53Z\"}"}

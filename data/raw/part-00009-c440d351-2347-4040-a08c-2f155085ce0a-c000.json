{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01444v1\", \"title\": \"PiCo: Jailbreaking Multimodal Large Language Models via\\n  $\\\\textbf{Pi}$ctorial $\\\\textbf{Co}$de Contextualization\", \"summary\": \"Multimodal Large Language Models (MLLMs), which integrate vision and other\\nmodalities into Large Language Models (LLMs), significantly enhance AI\\ncapabilities but also introduce new security vulnerabilities. By exploiting the\\nvulnerabilities of the visual modality and the long-tail distribution\\ncharacteristic of code training data, we present PiCo, a novel jailbreaking\\nframework designed to progressively bypass multi-tiered defense mechanisms in\\nadvanced MLLMs. PiCo employs a tier-by-tier jailbreak strategy, using\\ntoken-level typographic attacks to evade input filtering and embedding harmful\\nintent within programming context instructions to bypass runtime monitoring. To\\ncomprehensively assess the impact of attacks, a new evaluation metric is\\nfurther proposed to assess both the toxicity and helpfulness of model outputs\\npost-attack. By embedding harmful intent within code-style visual instructions,\\nPiCo achieves an average Attack Success Rate (ASR) of 84.13% on Gemini-Pro\\nVision and 52.66% on GPT-4, surpassing previous methods. Experimental results\\nhighlight the critical gaps in current defenses, underscoring the need for more\\nrobust strategies to secure advanced MLLMs.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.AI\", \"published\": \"2025-04-02T07:54:32Z\"}"}

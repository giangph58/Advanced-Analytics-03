{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03344v1\", \"title\": \"RIFT: Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic\\n  Simulation\", \"summary\": \"Achieving both realism and controllability in interactive closed-loop traffic\\nsimulation remains a key challenge in autonomous driving. Data-driven\\nsimulation methods reproduce realistic trajectories but suffer from covariate\\nshift in closed-loop deployment, compounded by simplified dynamics models that\\nfurther reduce reliability. Conversely, physics-based simulation methods\\nenhance reliable and controllable closed-loop interactions but often lack\\nexpert demonstrations, compromising realism. To address these challenges, we\\nintroduce a dual-stage AV-centered simulation framework that conducts open-loop\\nimitation learning pre-training in a data-driven simulator to capture\\ntrajectory-level realism and multimodality, followed by closed-loop\\nreinforcement learning fine-tuning in a physics-based simulator to enhance\\ncontrollability and mitigate covariate shift. In the fine-tuning stage, we\\npropose RIFT, a simple yet effective closed-loop RL fine-tuning strategy that\\npreserves the trajectory-level multimodality through a GRPO-style\\ngroup-relative advantage formulation, while enhancing controllability and\\ntraining stability by replacing KL regularization with the dual-clip mechanism.\\nExtensive experiments demonstrate that RIFT significantly improves the realism\\nand controllability of generated traffic scenarios, providing a robust platform\\nfor evaluating autonomous vehicle performance in diverse and interactive\\nscenarios.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.LG\", \"published\": \"2025-05-06T09:12:37Z\"}"}

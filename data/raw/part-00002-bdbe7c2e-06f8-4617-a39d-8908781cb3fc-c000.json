{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10326v1\", \"title\": \"AlayaDB: The Data Foundation for Efficient and Effective Long-context\\n  LLM Inference\", \"summary\": \"AlayaDB is a cutting-edge vector database system natively architected for\\nefficient and effective long-context inference for Large Language Models (LLMs)\\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\\ncomputation from the LLM inference systems, and encapsulates them into a novel\\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\\nconsumes fewer hardware resources and offers higher generation quality for\\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\\ncomparing with the existing alternative solutions (e.g., KV cache\\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\\nit abstracts the attention computation and cache management for LLM inference\\ninto a query processing procedure, and optimizes the performance via a native\\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\\n(i) three use cases from our industry partners, and (ii) extensive experimental\\nresults on LLM inference benchmarks.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.DB,cs.IR\", \"published\": \"2025-04-14T15:34:26Z\"}"}

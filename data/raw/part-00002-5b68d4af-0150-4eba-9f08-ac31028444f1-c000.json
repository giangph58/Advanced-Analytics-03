{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04560v1\", \"title\": \"ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge\\n  Distillation via $\\u03b1$-$\\u03b2$-Divergence\", \"summary\": \"Knowledge Distillation (KD) transfers knowledge from a large teacher model to\\na smaller student model by minimizing the divergence between their output\\ndistributions, typically using forward Kullback-Leibler divergence (FKLD) or\\nreverse KLD (RKLD). It has become an effective training paradigm due to the\\nbroader supervision information provided by the teacher distribution compared\\nto one-hot labels. We identify that the core challenge in KD lies in balancing\\ntwo mode-concentration effects: the \\\\textbf{\\\\textit{Hardness-Concentration}}\\neffect, which refers to focusing on modes with large errors, and the\\n\\\\textbf{\\\\textit{Confidence-Concentration}} effect, which refers to focusing on\\nmodes with high student confidence. Through an analysis of how probabilities\\nare reassigned during gradient updates, we observe that these two effects are\\nentangled in FKLD and RKLD, but in extreme forms. Specifically, both are too\\nweak in FKLD, causing the student to fail to concentrate on the target class.\\nIn contrast, both are too strong in RKLD, causing the student to overly\\nemphasize the target class while ignoring the broader distributional\\ninformation from the teacher. To address this imbalance, we propose ABKD, a\\ngeneric framework with $\\\\alpha$-$\\\\beta$-divergence. Our theoretical results\\nshow that ABKD offers a smooth interpolation between FKLD and RKLD, achieving\\nan effective trade-off between these effects. Extensive experiments on 17\\nlanguage/vision datasets with 12 teacher-student settings confirm its efficacy.\\nThe code is available at https://github.com/ghwang-s/abkd.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-07T16:48:49Z\"}"}

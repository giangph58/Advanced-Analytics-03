{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03160v1\", \"title\": \"Particle decay as asymptotic narrow parametric resonance\", \"summary\": \"Parametric resonance can produce particles from oscillating scalar field,\\nwhere an exponential growth of the particle number density can be developed.\\nWhile it has been noticed that stimulated decay in Boltzmann equations exhibits\\nsimilar parametric dependence of the exponential growth, it is not\\nquantitatively clear yet under what circumstance can the two phenomena\\nreconcile. We demonstrate that trilinear particle interaction in the Boltzmann\\nequation can provide a good approximation to describe the distribution function\\nand the number density in the asymptotic regime of narrow parametric resonance.\\nWe find that the crucial treatment leading to the quantitative agreement is a\\nproper Gaussian simulation of the momentum spread inherited from\\nnonrelativistic particle decay. With the simple particle picture, the analytic\\nBoltzmann approximation can be applied to explosive photon production from\\naxions/axion-like particles and to dark matter production from oscillating\\nfields.\", \"main_category\": \"hep-ph\", \"categories\": \"hep-ph\", \"published\": \"2025-05-06T04:13:38Z\"}"}
{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03176v1\", \"title\": \"seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant\\n  World Models\", \"summary\": \"Current self-supervised algorithms mostly rely on transformations such as\\ndata augmentation and masking to learn visual representations. This is achieved\\nby inducing invariance or equivariance with respect to these transformations\\nafter encoding two views of an image. This dominant two-view paradigm can limit\\nthe flexibility of learned representations for downstream adaptation by\\ncreating performance trade-offs between invariance-related tasks such as image\\nclassification and more fine-grained equivariance-related tasks. In this work,\\nwe introduce \\\\emph{seq-JEPA}, a world modeling paradigm based on\\njoint-embedding predictive architecture that leverages architectural inductive\\nbiases to resolve this trade-off. Without requiring an additional equivariance\\npredictor or loss term, seq-JEPA simultaneously learns two architecturally\\nsegregated representations: one equivariant to the specified transformations\\nand another invariant to them and suited for tasks such as classification. To\\ndo so, our model processes a short sequence of different views (observations)\\nof an input image. Each encoded view is concatenated with embeddings\\ncorresponding to the relative transformation (action) producing the next\\nobservation in the sequence. A transformer encoder outputs an aggregate\\nrepresentation of this sequence, which is subsequently conditioned on the\\naction leading to the next observation to predict its representation.\\nEmpirically, seq-JEPA achieves strong performance on equivariant benchmarks and\\nimage classification without sacrificing one for the other. Additionally, our\\nframework excels at tasks that inherently require aggregating a sequence of\\nobservations, such as path integration across actions and predictive learning\\nacross eye movements.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.LG\", \"published\": \"2025-05-06T04:39:11Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06835v1\", \"title\": \"LVC: A Lightweight Compression Framework for Enhancing VLMs in Long\\n  Video Understanding\", \"summary\": \"Long video understanding is a complex task that requires both spatial detail\\nand temporal awareness. While Vision-Language Models (VLMs) obtain frame-level\\nunderstanding capabilities through multi-frame input, they suffer from\\ninformation loss due to the sparse sampling strategy. In contrast, Video Large\\nLanguage Models (Video-LLMs) capture temporal relationships within visual\\nfeatures but are limited by the scarcity of high-quality video-text datasets.\\nTo transfer long video understanding capabilities to VLMs with minimal data and\\ncomputational cost, we propose Lightweight Video Compression (LVC), a novel\\nmethod featuring the Query-Attention Video Compression mechanism, which\\neffectively tackles the sparse sampling problem in VLMs. By training only the\\nalignment layer with 10k short video-text pairs, LVC significantly enhances the\\ntemporal reasoning abilities of VLMs. Extensive experiments show that LVC\\nprovides consistent performance improvements across various models, including\\nthe InternVL2 series and Phi-3.5-Vision. Notably, the InternVL2-40B-LVC\\nachieves scores of 68.2 and 65.9 on the long video understanding benchmarks\\nMLVU and Video-MME, respectively, with relative improvements of 14.6% and 7.7%.\\nThe enhanced models and code will be publicly available soon.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-09T12:51:10Z\"}"}

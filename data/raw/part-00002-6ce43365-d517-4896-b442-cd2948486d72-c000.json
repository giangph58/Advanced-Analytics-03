{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04976v1\", \"title\": \"A Domain-Based Taxonomy of Jailbreak Vulnerabilities in Large Language\\n  Models\", \"summary\": \"The study of large language models (LLMs) is a key area in open-world machine\\nlearning. Although LLMs demonstrate remarkable natural language processing\\ncapabilities, they also face several challenges, including consistency issues,\\nhallucinations, and jailbreak vulnerabilities. Jailbreaking refers to the\\ncrafting of prompts that bypass alignment safeguards, leading to unsafe outputs\\nthat compromise the integrity of LLMs. This work specifically focuses on the\\nchallenge of jailbreak vulnerabilities and introduces a novel taxonomy of\\njailbreak attacks grounded in the training domains of LLMs. It characterizes\\nalignment failures through generalization, objectives, and robustness gaps. Our\\nprimary contribution is a perspective on jailbreak, framed through the\\ndifferent linguistic domains that emerge during LLM training and alignment.\\nThis viewpoint highlights the limitations of existing approaches and enables us\\nto classify jailbreak attacks on the basis of the underlying model deficiencies\\nthey exploit. Unlike conventional classifications that categorize attacks based\\non prompt construction methods (e.g., prompt templating), our approach provides\\na deeper understanding of LLM behavior. We introduce a taxonomy with four\\ncategories -- mismatched generalization, competing objectives, adversarial\\nrobustness, and mixed attacks -- offering insights into the fundamental nature\\nof jailbreak vulnerabilities. Finally, we present key lessons derived from this\\ntaxonomic study.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,I.2.7\", \"published\": \"2025-04-07T12:05:16Z\"}"}

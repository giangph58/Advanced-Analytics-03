{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19981v1\", \"title\": \"Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided\\n  GFlowNets\", \"summary\": \"Achieving both accuracy and diverse reasoning remains challenging for Large\\nLanguage Models (LLMs) in complex domains like mathematics. A key bottleneck is\\nevaluating intermediate reasoning steps to guide generation without costly\\nhuman annotations. To address this, we first introduce a novel Process Reward\\nModel (PRM) trained automatically using Monte Carlo Tree Search coupled with a\\nsimilarity-based data augmentation technique, effectively capturing step-level\\nreasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks\\n(GFlowNets) to operate at the reasoning step level. Unlike traditional\\nreinforcement learning focused on maximizing a single reward, GFlowNets\\nnaturally sample diverse, high-quality solutions proportional to their rewards,\\nas measured by our PRM. Empirical evaluation shows strong improvements in both\\naccuracy and solution diversity on challenging mathematical benchmarks (e.g.,\\n+2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective\\ngeneralization to unseen datasets (+9.4% absolute on SAT MATH). Our work\\ndemonstrates the potential of PRM-guided, step-level GFlowNets for developing\\nmore robust and versatile mathematical reasoning in LLMs.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL\", \"published\": \"2025-04-28T16:56:41Z\"}"}

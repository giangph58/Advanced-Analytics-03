{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03153v1\", \"title\": \"Robust Fairness Vision-Language Learning for Medical Image Analysis\", \"summary\": \"The advent of Vision-Language Models (VLMs) in medical image analysis has the\\npotential to help process multimodal inputs and increase performance over\\ntraditional inference methods. However, when considering the domain in which\\nthese models will be implemented, fairness and robustness are important to\\nensure the model stays true for any patient. In this paper, we introduce a\\nframework for ensuring robustness and fairness of VLM models. This framework\\nmodifies the loss function at training by identifying and adjusting faulty\\nimage-text pairs through a Dynamic Bad Pair Mining algorithm and also utilizing\\nSinkhorn distance to ensure the loss distributions of protected groups do not\\ndeviate from the total loss. Experimental testing of our framework shows up to\\na 8.6\\\\% improvement when looking at equity-scaled AUC.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-06T03:59:25Z\"}"}

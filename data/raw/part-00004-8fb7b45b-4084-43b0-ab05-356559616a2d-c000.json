{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11358v1\", \"title\": \"DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks\", \"summary\": \"LLM-integrated applications and agents are vulnerable to prompt injection\\nattacks, where an attacker injects prompts into their inputs to induce\\nattacker-desired outputs. A detection method aims to determine whether a given\\ninput is contaminated by an injected prompt. However, existing detection\\nmethods have limited effectiveness against state-of-the-art attacks, let alone\\nadaptive ones. In this work, we propose DataSentinel, a game-theoretic method\\nto detect prompt injection attacks. Specifically, DataSentinel fine-tunes an\\nLLM to detect inputs contaminated with injected prompts that are strategically\\nadapted to evade detection. We formulate this as a minimax optimization\\nproblem, with the objective of fine-tuning the LLM to detect strong adaptive\\nattacks. Furthermore, we propose a gradient-based method to solve the minimax\\noptimization problem by alternating between the inner max and outer min\\nproblems. Our evaluation results on multiple benchmark datasets and LLMs show\\nthat DataSentinel effectively detects both existing and adaptive prompt\\ninjection attacks.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.AI\", \"published\": \"2025-04-15T16:26:21Z\"}"}

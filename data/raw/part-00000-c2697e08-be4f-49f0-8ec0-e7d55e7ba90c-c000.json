{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01930v1\", \"title\": \"A thorough benchmark of automatic text classification: From traditional\\n  approaches to large language models\", \"summary\": \"Automatic text classification (ATC) has experienced remarkable advancements\\nin the past decade, best exemplified by recent small and large language models\\n(SLMs and LLMs), leveraged by Transformer architectures. Despite recent\\neffectiveness improvements, a comprehensive cost-benefit analysis investigating\\nwhether the effectiveness gains of these recent approaches compensate their\\nmuch higher costs when compared to more traditional text classification\\napproaches such as SVMs and Logistic Regression is still missing in the\\nliterature. In this context, this work's main contributions are twofold: (i) we\\nprovide a scientifically sound comparative analysis of the cost-benefit of\\ntwelve traditional and recent ATC solutions including five open LLMs, and (ii)\\na large benchmark comprising {22 datasets}, including sentiment analysis and\\ntopic classification, with their (train-validation-test) partitions based on\\nfolded cross-validation procedures, along with documentation, and code. The\\nrelease of code, data, and documentation enables the community to replicate\\nexperiments and advance the field in a more scientifically sound manner. Our\\ncomparative experimental results indicate that LLMs outperform traditional\\napproaches (up to 26%-7.1% on average) and SLMs (up to 4.9%-1.9% on average) in\\nterms of effectiveness. However, LLMs incur significantly higher computational\\ncosts due to fine-tuning, being, on average 590x and 8.5x slower than\\ntraditional methods and SLMs, respectively. Results suggests the following\\nrecommendations: (1) LLMs for applications that require the best possible\\neffectiveness and can afford the costs; (2) traditional methods such as\\nLogistic Regression and SVM for resource-limited applications or those that\\ncannot afford the cost of tuning large LLMs; and (3) SLMs like Roberta for\\nnear-optimal effectiveness-efficiency trade-off.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-02T17:40:08Z\"}"}

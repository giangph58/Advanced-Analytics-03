{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21326v1\", \"title\": \"Q-function Decomposition with Intervention Semantics with Factored\\n  Action Spaces\", \"summary\": \"Many practical reinforcement learning environments have a discrete factored\\naction space that induces a large combinatorial set of actions, thereby posing\\nsignificant challenges. Existing approaches leverage the regular structure of\\nthe action space and resort to a linear decomposition of Q-functions, which\\navoids enumerating all combinations of factored actions. In this paper, we\\nconsider Q-functions defined over a lower dimensional projected subspace of the\\noriginal action space, and study the condition for the unbiasedness of\\ndecomposed Q-functions using causal effect estimation from the no unobserved\\nconfounder setting in causal statistics. This leads to a general scheme which\\nwe call action decomposed reinforcement learning that uses the projected\\nQ-functions to approximate the Q-function in standard model-free reinforcement\\nlearning algorithms. The proposed approach is shown to improve sample\\ncomplexity in a model-based reinforcement learning setting. We demonstrate\\nimprovements in sample efficiency compared to state-of-the-art baselines in\\nonline continuous control environments and a real-world offline sepsis\\ntreatment environment.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-30T05:26:51Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15932v1\", \"title\": \"Reasoning Physical Video Generation with Diffusion Timestep Tokens via\\n  Reinforcement Learning\", \"summary\": \"Despite recent progress in video generation, producing videos that adhere to\\nphysical laws remains a significant challenge. Traditional diffusion-based\\nmethods struggle to extrapolate to unseen physical conditions (eg, velocity)\\ndue to their reliance on data-driven approximations. To address this, we\\npropose to integrate symbolic reasoning and reinforcement learning to enforce\\nphysical consistency in video generation. We first introduce the Diffusion\\nTimestep Tokenizer (DDT), which learns discrete, recursive visual tokens by\\nrecovering visual attributes lost during the diffusion process. The recursive\\nvisual tokens enable symbolic reasoning by a large language model. Based on it,\\nwe propose the Phys-AR framework, which consists of two stages: The first stage\\nuses supervised fine-tuning to transfer symbolic knowledge, while the second\\nstage applies reinforcement learning to optimize the model's reasoning\\nabilities through reward functions based on physical conditions. Our approach\\nallows the model to dynamically adjust and improve the physical properties of\\ngenerated videos, ensuring adherence to physical laws. Experimental results\\ndemonstrate that PhysAR can generate videos that are physically consistent.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-22T14:20:59Z\"}"}

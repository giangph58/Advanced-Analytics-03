{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05716v1\", \"title\": \"Single-Agent vs. Multi-Agent LLM Strategies for Automated Student\\n  Reflection Assessment\", \"summary\": \"We explore the use of Large Language Models (LLMs) for automated assessment\\nof open-text student reflections and prediction of academic performance.\\nTraditional methods for evaluating reflections are time-consuming and may not\\nscale effectively in educational settings. In this work, we employ LLMs to\\ntransform student reflections into quantitative scores using two assessment\\nstrategies (single-agent and multi-agent) and two prompting techniques\\n(zero-shot and few-shot). Our experiments, conducted on a dataset of 5,278\\nreflections from 377 students over three academic terms, demonstrate that the\\nsingle-agent with few-shot strategy achieves the highest match rate with human\\nevaluations. Furthermore, models utilizing LLM-assessed reflection scores\\noutperform baselines in both at-risk student identification and grade\\nprediction tasks. These findings suggest that LLMs can effectively automate\\nreflection assessment, reduce educators' workload, and enable timely support\\nfor students who may need additional assistance. Our work emphasizes the\\npotential of integrating advanced generative AI technologies into educational\\npractices to enhance student engagement and academic success.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CY\", \"published\": \"2025-04-08T06:34:15Z\"}"}

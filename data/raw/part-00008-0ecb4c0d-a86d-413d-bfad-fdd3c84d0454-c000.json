{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07440v1\", \"title\": \"Revisiting LLM Evaluation through Mechanism Interpretability: a New\\n  Metric and Model Utility Law\", \"summary\": \"Large Language Models (LLMs) have become indispensable across academia,\\nindustry, and daily applications, yet current evaluation methods struggle to\\nkeep pace with their rapid development. In this paper, we analyze the core\\nlimitations of traditional evaluation pipelines and propose a novel metric, the\\nModel Utilization Index (MUI), which introduces mechanism interpretability\\ntechniques to complement traditional performance metrics. MUI quantifies the\\nextent to which a model leverages its capabilities to complete tasks. The core\\nidea is that to assess an LLM's overall ability, we must evaluate not only its\\ntask performance but also the effort expended to achieve the outcome. Our\\nextensive experiments reveal an inverse relationship between MUI and\\nperformance, from which we deduce a common trend observed in popular LLMs,\\nwhich we term the Utility Law. Based on this, we derive four corollaries that\\naddress key challenges, including training judgement, the issue of data\\ncontamination, fairness in model comparison, and data diversity. We hope that\\nour survey, novel metric, and utility law will foster mutual advancement in\\nboth evaluation and mechanism interpretability. Our code can be found at\\nhttps://github.com/ALEX-nlp/MUI-Eva.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-10T04:09:47Z\"}"}

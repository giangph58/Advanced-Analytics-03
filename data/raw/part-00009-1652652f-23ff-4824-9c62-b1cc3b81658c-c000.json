{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13169v1\", \"title\": \"Generate, but Verify: Reducing Hallucination in Vision-Language Models\\n  with Retrospective Resampling\", \"summary\": \"Vision-Language Models (VLMs) excel at visual understanding but often suffer\\nfrom visual hallucinations, where they generate descriptions of nonexistent\\nobjects, actions, or concepts, posing significant risks in safety-critical\\napplications. Existing hallucination mitigation methods typically follow one of\\ntwo paradigms: generation adjustment, which modifies decoding behavior to align\\ntext with visual inputs, and post-hoc verification, where external models\\nassess and correct outputs. While effective, generation adjustment methods\\noften rely on heuristics and lack correction mechanisms, while post-hoc\\nverification is complicated, typically requiring multiple models and tending to\\nreject outputs rather than refine them. In this work, we introduce REVERSE, a\\nunified framework that integrates hallucination-aware training with on-the-fly\\nself-verification. By leveraging a new hallucination-verification dataset\\ncontaining over 1.3M semi-synthetic samples, along with a novel inference-time\\nretrospective resampling technique, our approach enables VLMs to both detect\\nhallucinations during generation and dynamically revise those hallucinations.\\nOur evaluations show that REVERSE achieves state-of-the-art hallucination\\nreduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO\\nand 28% on HaloQuest. Our dataset, model, and code are available at:\\nhttps://reverse-vlm.github.io.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-17T17:59:22Z\"}"}

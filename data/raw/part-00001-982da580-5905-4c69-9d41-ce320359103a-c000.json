{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21447v1\", \"title\": \"Rethinking Visual Layer Selection in Multimodal LLMs\", \"summary\": \"Multimodal large language models (MLLMs) have achieved impressive performance\\nacross a wide range of tasks, typically using CLIP-ViT as their visual encoder\\ndue to its strong text-image alignment capabilities. While prior studies\\nsuggest that different CLIP-ViT layers capture different types of information,\\nwith shallower layers focusing on fine visual details and deeper layers\\naligning more closely with textual semantics, most MLLMs still select visual\\nfeatures based on empirical heuristics rather than systematic analysis. In this\\nwork, we propose a Layer-wise Representation Similarity approach to group\\nCLIP-ViT layers with similar behaviors into {shallow, middle, and deep}\\ncategories and assess their impact on MLLM performance. Building on this\\nfoundation, we revisit the visual layer selection problem in MLLMs at scale,\\ntraining LLaVA-style models ranging from 1.4B to 7B parameters. Through\\nextensive experiments across 10 datasets and 4 tasks, we find that: (1) deep\\nlayers are essential for OCR tasks; (2) shallow and middle layers substantially\\noutperform deep layers on reasoning tasks involving counting, positioning, and\\nobject localization; (3) a lightweight fusion of features across shallow,\\nmiddle, and deep layers consistently outperforms specialized fusion baselines\\nand single-layer selections, achieving gains on 9 out of 10 datasets. Our work\\noffers the first principled study of visual layer selection in MLLMs, laying\\nthe groundwork for deeper investigations into visual representation learning\\nfor MLLMs.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-30T09:07:10Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05143v1\", \"title\": \"Sparse Training from Random Initialization: Aligning Lottery Ticket\\n  Masks using Weight Symmetry\", \"summary\": \"The Lottery Ticket Hypothesis (LTH) suggests there exists a sparse LTH mask\\nand weights that achieve the same generalization performance as the dense model\\nwhile using significantly fewer parameters. However, finding a LTH solution is\\ncomputationally expensive, and a LTH sparsity mask does not generalize to other\\nrandom weight initializations. Recent work has suggested that neural networks\\ntrained from random initialization find solutions within the same basin modulo\\npermutation, and proposes a method to align trained models within the same loss\\nbasin. We hypothesize that misalignment of basins is the reason why LTH masks\\ndo not generalize to new random initializations and propose permuting the LTH\\nmask to align with the new optimization basin when performing sparse training\\nfrom a different random init. We empirically show a significant increase in\\ngeneralization when sparse training from random initialization with the\\npermuted mask as compared to using the non-permuted LTH mask, on multiple\\ndatasets (CIFAR-10, CIFAR-100 and ImageNet) and models (VGG11, ResNet20 and\\nResNet50).\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-08T11:27:31Z\"}"}

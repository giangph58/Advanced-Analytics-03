{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10227v1\", \"title\": \"Probing then Editing Response Personality of Large Language Models\", \"summary\": \"Large Language Models (LLMs) have demonstrated promising capabilities to\\ngenerate responses that exhibit consistent personality traits. Despite the\\nmajor attempts to analyze personality expression through output-based\\nevaluations, little is known about how such traits are internally encoded\\nwithin LLM parameters. In this paper, we introduce a layer-wise probing\\nframework to systematically investigate the layer-wise capability of LLMs in\\nencoding personality for responding. We conduct probing experiments on 11\\nopen-source LLMs over the PersonalityEdit benchmark and find that LLMs\\npredominantly encode personality for responding in their middle and upper\\nlayers, with instruction-tuned models demonstrating a slightly clearer\\nseparation of personality traits. Furthermore, by interpreting the trained\\nprobing hyperplane as a layer-wise boundary for each personality category, we\\npropose a layer-wise perturbation method to edit the personality expressed by\\nLLMs during inference. Our results show that even when the prompt explicitly\\nspecifies a particular personality, our method can still successfully alter the\\nresponse personality of LLMs. Interestingly, the difficulty of converting\\nbetween certain personality traits varies substantially, which aligns with the\\nrepresentational distances in our probing experiments. Finally, we conduct a\\ncomprehensive MMLU benchmark evaluation and time overhead analysis,\\ndemonstrating that our proposed personality editing method incurs only minimal\\ndegradation in general capabilities while maintaining low training costs and\\nacceptable inference latency. Our code is publicly available at\\nhttps://github.com/universe-sky/probing-then-editing-personality.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-14T13:46:35Z\"}"}

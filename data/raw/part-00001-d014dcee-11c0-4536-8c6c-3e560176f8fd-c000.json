{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03350v1\", \"title\": \"A Vision-Language Model for Focal Liver Lesion Classification\", \"summary\": \"Accurate classification of focal liver lesions is crucial for diagnosis and\\ntreatment in hepatology. However, traditional supervised deep learning models\\ndepend on large-scale annotated datasets, which are often limited in medical\\nimaging. Recently, Vision-Language models (VLMs) such as Contrastive\\nLanguage-Image Pre-training model (CLIP) has been applied to image\\nclassifications. Compared to the conventional convolutional neural network\\n(CNN), which classifiers image based on visual information only, VLM leverages\\nmultimodal learning with text and images, allowing it to learn effectively even\\nwith a limited amount of labeled data. Inspired by CLIP, we pro-pose a\\nLiver-VLM, a model specifically designed for focal liver lesions (FLLs)\\nclassification. First, Liver-VLM incorporates class information into the text\\nencoder without introducing additional inference overhead. Second, by\\ncalculating the pairwise cosine similarities between image and text embeddings\\nand optimizing the model with a cross-entropy loss, Liver-VLM ef-fectively\\naligns image features with class-level text features. Experimental results on\\nMPCT-FLLs dataset demonstrate that the Liver-VLM model out-performs both the\\nstandard CLIP and MedCLIP models in terms of accuracy and Area Under the Curve\\n(AUC). Further analysis shows that using a lightweight ResNet18 backbone\\nenhances classification performance, particularly under data-constrained\\nconditions.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-06T09:19:12Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05855v1\", \"title\": \"Enhancing Coreference Resolution with Pretrained Language Models:\\n  Bridging the Gap Between Syntax and Semantics\", \"summary\": \"Large language models have made significant advancements in various natural\\nlanguage processing tasks, including coreference resolution. However,\\ntraditional methods often fall short in effectively distinguishing referential\\nrelationships due to a lack of integration between syntactic and semantic\\ninformation. This study introduces an innovative framework aimed at enhancing\\ncoreference resolution by utilizing pretrained language models. Our approach\\ncombines syntax parsing with semantic role labeling to accurately capture finer\\ndistinctions in referential relationships. By employing state-of-the-art\\npretrained models to gather contextual embeddings and applying an attention\\nmechanism for fine-tuning, we improve the performance of coreference tasks.\\nExperimental results across diverse datasets show that our method surpasses\\nconventional coreference resolution systems, achieving notable accuracy in\\ndisambiguating references. This development not only improves coreference\\nresolution outcomes but also positively impacts other natural language\\nprocessing tasks that depend on precise referential understanding.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-08T09:33:09Z\"}"}

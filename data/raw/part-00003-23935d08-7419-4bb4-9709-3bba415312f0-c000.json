{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03314v1\", \"title\": \"Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic\\n  Music Generation\", \"summary\": \"The recent surge in the popularity of diffusion models for image synthesis\\nhas attracted new attention to their potential for generation tasks in other\\ndomains. However, their applications to symbolic music generation remain\\nlargely under-explored because symbolic music is typically represented as\\nsequences of discrete events and standard diffusion models are not well-suited\\nfor discrete data. We represent symbolic music as image-like pianorolls,\\nfacilitating the use of diffusion models for the generation of symbolic music.\\nMoreover, this study introduces a novel diffusion model that incorporates our\\nproposed Transformer-Mamba block and learnable wavelet transform.\\nClassifier-free guidance is utilised to generate symbolic music with target\\nchords. Our evaluation shows that our method achieves compelling results in\\nterms of music quality and controllability, outperforming the strong baseline\\nin pianoroll generation. Our code is available at\\nhttps://github.com/jinchengzhanggg/proffusion.\", \"main_category\": \"cs.SD\", \"categories\": \"cs.SD,cs.AI,eess.AS\", \"published\": \"2025-05-06T08:44:52Z\"}"}

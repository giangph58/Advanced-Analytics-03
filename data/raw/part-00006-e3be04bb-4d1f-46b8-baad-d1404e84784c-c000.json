{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10825v1\", \"title\": \"OmniVDiff: Omni Controllable Video Diffusion for Generation and\\n  Understanding\", \"summary\": \"In this paper, we propose a novel framework for controllable video diffusion,\\nOmniVDiff, aiming to synthesize and comprehend multiple video visual content in\\na single diffusion model. To achieve this, OmniVDiff treats all video visual\\nmodalities in the color space to learn a joint distribution, while employing an\\nadaptive control strategy that dynamically adjusts the role of each visual\\nmodality during the diffusion process, either as a generation modality or a\\nconditioning modality. This allows flexible manipulation of each modality's\\nrole, enabling support for a wide range of tasks. Consequently, our model\\nsupports three key functionalities: (1) Text-conditioned video generation:\\nmulti-modal visual video sequences (i.e., rgb, depth, canny, segmentaion) are\\ngenerated based on the text conditions in one diffusion process; (2) Video\\nunderstanding: OmniVDiff can estimate the depth, canny map, and semantic\\nsegmentation across the input rgb frames while ensuring coherence with the rgb\\ninput; and (3) X-conditioned video generation: OmniVDiff generates videos\\nconditioned on fine-grained attributes (e.g., depth maps or segmentation maps).\\nBy integrating these diverse tasks into a unified video diffusion framework,\\nOmniVDiff enhances the flexibility and scalability for controllable video\\ndiffusion, making it an effective tool for a variety of downstream\\napplications, such as video-to-video translation. Extensive experiments\\ndemonstrate the effectiveness of our approach, highlighting its potential for\\nvarious video-related applications.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-15T03:05:46Z\"}"}

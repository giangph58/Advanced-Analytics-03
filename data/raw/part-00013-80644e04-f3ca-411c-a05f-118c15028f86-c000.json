{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05235v1\", \"title\": \"Advancing Neural Network Verification through Hierarchical Safety\\n  Abstract Interpretation\", \"summary\": \"Traditional methods for formal verification (FV) of deep neural networks\\n(DNNs) are constrained by a binary encoding of safety properties, where a model\\nis classified as either safe or unsafe (robust or not robust). This binary\\nencoding fails to capture the nuanced safety levels within a model, often\\nresulting in either overly restrictive or too permissive requirements. In this\\npaper, we introduce a novel problem formulation called Abstract\\nDNN-Verification, which verifies a hierarchical structure of unsafe outputs,\\nproviding a more granular analysis of the safety aspect for a given DNN.\\nCrucially, by leveraging abstract interpretation and reasoning about output\\nreachable sets, our approach enables assessing multiple safety levels during\\nthe FV process, requiring the same (in the worst case) or even potentially less\\ncomputational effort than the traditional binary verification approach.\\nSpecifically, we demonstrate how this formulation allows rank adversarial\\ninputs according to their abstract safety level violation, offering a more\\ndetailed evaluation of the model's safety and robustness. Our contributions\\ninclude a theoretical exploration of the relationship between our novel\\nabstract safety formulation and existing approaches that employ abstract\\ninterpretation for robustness verification, complexity analysis of the novel\\nproblem introduced, and an empirical evaluation considering both a complex deep\\nreinforcement learning task (based on Habitat 3.0) and standard\\nDNN-Verification benchmarks.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-05-08T13:29:46Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06670v1\", \"title\": \"Dynamic Residual Safe Reinforcement Learning for Multi-Agent\\n  Safety-Critical Scenarios Decision-Making\", \"summary\": \"In multi-agent safety-critical scenarios, traditional autonomous driving\\nframeworks face significant challenges in balancing safety constraints and task\\nperformance. These frameworks struggle to quantify dynamic interaction risks in\\nreal-time and depend heavily on manual rules, resulting in low computational\\nefficiency and conservative strategies. To address these limitations, we\\npropose a Dynamic Residual Safe Reinforcement Learning (DRS-RL) framework\\ngrounded in a safety-enhanced networked Markov decision process. It's the first\\ntime that the weak-to-strong theory is introduced into multi-agent\\ndecision-making, enabling lightweight dynamic calibration of safety boundaries\\nvia a weak-to-strong safety correction paradigm. Based on the multi-agent\\ndynamic conflict zone model, our framework accurately captures spatiotemporal\\ncoupling risks among heterogeneous traffic participants and surpasses the\\nstatic constraints of conventional geometric rules. Moreover, a risk-aware\\nprioritized experience replay mechanism mitigates data distribution bias by\\nmapping risk to sampling probability. Experimental results reveal that the\\nproposed method significantly outperforms traditional RL algorithms in safety,\\nefficiency, and comfort. Specifically, it reduces the collision rate by up to\\n92.17%, while the safety model accounts for merely 27% of the main model's\\nparameters.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO\", \"published\": \"2025-04-09T08:13:14Z\"}"}

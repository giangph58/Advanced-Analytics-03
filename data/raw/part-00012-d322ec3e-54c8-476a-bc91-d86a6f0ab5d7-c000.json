{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11793v1\", \"title\": \"Selective Attention Federated Learning: Improving Privacy and Efficiency\\n  for Clinical Text Classification\", \"summary\": \"Federated Learning (FL) faces major challenges regarding communication\\noverhead and model privacy when training large language models (LLMs),\\nespecially in healthcare applications. To address these, we introduce Selective\\nAttention Federated Learning (SAFL), a novel approach that dynamically\\nfine-tunes only those transformer layers identified as attention-critical. By\\nemploying attention patterns to determine layer importance, SAFL significantly\\nreduces communication bandwidth and enhances differential privacy resilience.\\nEvaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and\\nMIMIC-III discharge summaries) demonstrate that SAFL achieves competitive\\nperformance with centralized models while substantially improving communication\\nefficiency and privacy preservation.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-16T05:59:29Z\"}"}

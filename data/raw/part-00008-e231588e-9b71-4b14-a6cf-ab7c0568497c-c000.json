{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02821v1\", \"title\": \"Sparse Autoencoders Learn Monosemantic Features in Vision-Language\\n  Models\", \"summary\": \"Sparse Autoencoders (SAEs) have recently been shown to enhance\\ninterpretability and steerability in Large Language Models (LLMs). In this\\nwork, we extend the application of SAEs to Vision-Language Models (VLMs), such\\nas CLIP, and introduce a comprehensive framework for evaluating monosemanticity\\nin vision representations. Our experimental results reveal that SAEs trained on\\nVLMs significantly enhance the monosemanticity of individual neurons while also\\nexhibiting hierarchical representations that align well with expert-defined\\nstructures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that\\napplying SAEs to intervene on a CLIP vision encoder, directly steer output from\\nmultimodal LLMs (e.g., LLaVA) without any modifications to the underlying\\nmodel. These findings emphasize the practicality and efficacy of SAEs as an\\nunsupervised approach for enhancing both the interpretability and control of\\nVLMs.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.LG\", \"published\": \"2025-04-03T17:58:35Z\"}"}

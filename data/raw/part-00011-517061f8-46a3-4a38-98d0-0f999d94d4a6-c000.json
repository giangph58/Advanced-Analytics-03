{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15784v1\", \"title\": \"Automated Creativity Evaluation for Large Language Models: A\\n  Reference-Based Approach\", \"summary\": \"Creative writing is a key capability of Large Language Models (LLMs), with\\npotential applications in literature, storytelling, and various creative\\ndomains. However, evaluating the creativity of machine-generated texts remains\\na significant challenge, as existing methods either rely on costly manual\\nannotations or fail to align closely with human assessments. In this paper, we\\npropose an effective automated evaluation method based on the Torrance Test of\\nCreative Writing (TTCW), which evaluates creativity as product. Our method\\nemploys a reference-based Likert-style approach, scoring generated creative\\ntexts relative to high-quality reference texts across various tests.\\nExperimental results demonstrate that our method significantly improves the\\nalignment between LLM evaluations and human assessments, achieving a pairwise\\naccuracy of 0.75 (+15\\\\%).\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-22T10:52:23Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05335v1\", \"title\": \"FLAM: Frame-Wise Language-Audio Modeling\", \"summary\": \"Recent multi-modal audio-language models (ALMs) excel at text-audio retrieval\\nbut struggle with frame-wise audio understanding. Prior works use\\ntemporal-aware labels or unsupervised training to improve frame-wise\\ncapabilities, but they still lack fine-grained labeling capability to pinpoint\\nwhen an event occurs. While traditional sound event detection models can\\nprecisely localize events, they are limited to pre-defined categories, making\\nthem ineffective for real-world scenarios with out-of-distribution events. In\\nthis work, we introduce FLAM, an open-vocabulary contrastive audio-language\\nmodel capable of localizing specific sound events. FLAM employs a\\nmemory-efficient and calibrated frame-wise objective with logit adjustment to\\naddress spurious correlations, such as event dependencies and label imbalances\\nduring training. To enable frame-wise supervision, we leverage a large-scale\\ndataset with diverse audio events, LLM-generated captions and simulation.\\nExperimental results and case studies demonstrate that FLAM significantly\\nimproves the open-vocabulary localization capability while maintaining strong\\nperformance in global retrieval and downstream tasks.\", \"main_category\": \"cs.SD\", \"categories\": \"cs.SD,eess.AS\", \"published\": \"2025-05-08T15:27:43Z\"}"}

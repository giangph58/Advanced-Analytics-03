{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17252v1\", \"title\": \"Low-Resource Neural Machine Translation Using Recurrent Neural Networks\\n  and Transfer Learning: A Case Study on English-to-Igbo\", \"summary\": \"In this study, we develop Neural Machine Translation (NMT) and\\nTransformer-based transfer learning models for English-to-Igbo translation - a\\nlow-resource African language spoken by over 40 million people across Nigeria\\nand West Africa. Our models are trained on a curated and benchmarked dataset\\ncompiled from Bible corpora, local news, Wikipedia articles, and Common Crawl,\\nall verified by native language experts. We leverage Recurrent Neural Network\\n(RNN) architectures, including Long Short-Term Memory (LSTM) and Gated\\nRecurrent Units (GRU), enhanced with attention mechanisms to improve\\ntranslation accuracy. To further enhance performance, we apply transfer\\nlearning using MarianNMT pre-trained models within the SimpleTransformers\\nframework. Our RNN-based system achieves competitive results, closely matching\\nexisting English-Igbo benchmarks. With transfer learning, we observe a\\nperformance gain of +4.83 BLEU points, reaching an estimated translation\\naccuracy of 70%. These findings highlight the effectiveness of combining RNNs\\nwith transfer learning to address the performance gap in low-resource language\\ntranslation tasks.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-24T05:02:26Z\"}"}

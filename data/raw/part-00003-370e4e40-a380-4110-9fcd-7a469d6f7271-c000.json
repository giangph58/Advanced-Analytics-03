{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10352v1\", \"title\": \"Pseudo-Autoregressive Neural Codec Language Models for Efficient\\n  Zero-Shot Text-to-Speech Synthesis\", \"summary\": \"Recent zero-shot text-to-speech (TTS) systems face a common dilemma:\\nautoregressive (AR) models suffer from slow generation and lack duration\\ncontrollability, while non-autoregressive (NAR) models lack temporal modeling\\nand typically require complex designs. In this paper, we introduce a novel\\npseudo-autoregressive (PAR) codec language modeling approach that unifies AR\\nand NAR modeling. Combining explicit temporal modeling from AR with parallel\\ngeneration from NAR, PAR generates dynamic-length spans at fixed time steps.\\nBuilding on PAR, we propose PALLE, a two-stage TTS system that leverages PAR\\nfor initial generation followed by NAR refinement. In the first stage, PAR\\nprogressively generates speech tokens along the time dimension, with each step\\npredicting all positions in parallel but only retaining the left-most span. In\\nthe second stage, low-confidence tokens are iteratively refined in parallel,\\nleveraging the global contextual information. Experiments demonstrate that\\nPALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on\\nlarge-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech\\ntest-clean set in terms of speech quality, speaker similarity, and\\nintelligibility, while achieving up to ten times faster inference speed. Audio\\nsamples are available at https://anonymous-palle.github.io.\", \"main_category\": \"eess.AS\", \"categories\": \"eess.AS,cs.CL\", \"published\": \"2025-04-14T16:03:21Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00570v1\", \"title\": \"FreqKV: Frequency Domain Key-Value Compression for Efficient Context\\n  Window Extension\", \"summary\": \"Extending the context window in large language models (LLMs) is essential for\\napplications involving long-form content generation. However, the linear\\nincrease in key-value (KV) cache memory requirements and the quadratic\\ncomplexity of self-attention with respect to sequence length present\\nsignificant challenges during fine-tuning and inference. Existing methods\\nsuffer from performance degradation when extending to longer contexts. In this\\nwork, we introduce a novel context extension method that optimizes both\\nfine-tuning and inference efficiency. Our method exploits a key observation: in\\nthe frequency domain, the energy distribution of the KV cache is primarily\\nconcentrated in low-frequency components. By filtering out the high-frequency\\ncomponents, the KV cache can be effectively compressed with minimal information\\nloss. Building on this insight, we propose an efficient compression technique,\\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\\nintroduces no additional parameters or architectural modifications. With\\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\\ncompressed in the frequency domain and extend the context window efficiently.\\nExperiments on various long context language modeling and understanding tasks\\ndemonstrate the efficiency and efficacy of the proposed method.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-05-01T14:53:12Z\"}"}

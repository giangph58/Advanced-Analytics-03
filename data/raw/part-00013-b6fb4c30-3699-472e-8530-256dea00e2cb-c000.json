{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01655v1\", \"title\": \"Q-Adapt: Adapting LMM for Visual Quality Assessment with Progressive\\n  Instruction Tuning\", \"summary\": \"The rapid advancement of Large Multi-modal Foundation Models (LMM) has paved\\nthe way for the possible Explainable Image Quality Assessment (EIQA) with\\ninstruction tuning from two perspectives: overall quality explanation, and\\nattribute-wise perception answering. However, existing works usually overlooked\\nthe conflicts between these two types of perception explanations during joint\\ninstruction tuning, leading to insufficient perception understanding. To\\nmitigate this, we propose a new paradigm for perception-oriented instruction\\ntuning, i.e., Q-Adapt, which aims to eliminate the conflicts and achieve the\\nsynergy between these two EIQA tasks when adapting LMM, resulting in enhanced\\nmulti-faceted explanations of IQA. Particularly, we propose a progressive\\ninstruction tuning strategy by dividing the adaption process of LMM for EIQA\\ninto two stages, where the first stage empowers the LMM with universal\\nperception knowledge tailored for two tasks using an efficient transfer\\nlearning strategy, i.e., LoRA, and the second stage introduces the\\ninstruction-adaptive visual prompt tuning to dynamically adapt visual features\\nfor the different instructions from two tasks. In this way, our proposed\\nQ-Adapt can achieve a lightweight visual quality evaluator, demonstrating\\ncomparable performance and, in some instances, superior results across\\nperceptual-related benchmarks and commonly-used IQA databases. The source code\\nis publicly available at https://github.com/yeppp27/Q-Adapt.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.MM\", \"published\": \"2025-04-02T12:02:57Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01407v1\", \"title\": \"TimeSearch: Hierarchical Video Search with Spotlight and Reflection for\\n  Human-like Long Video Understanding\", \"summary\": \"Large video-language models (LVLMs) have shown remarkable performance across\\nvarious video-language tasks. However, they encounter significant challenges\\nwhen processing long videos because of the large number of video frames\\ninvolved. Downsampling long videos in either space or time can lead to visual\\nhallucinations, making it difficult to accurately interpret long videos.\\nMotivated by human hierarchical temporal search strategies, we propose\\n\\\\textbf{TimeSearch}, a novel framework enabling LVLMs to understand long videos\\nin a human-like manner. TimeSearch integrates two human-like primitives into a\\nunified autoregressive LVLM: 1) \\\\textbf{Spotlight} efficiently identifies\\nrelevant temporal events through a Temporal-Augmented Frame Representation\\n(TAFR), explicitly binding visual features with timestamps; 2)\\n\\\\textbf{Reflection} evaluates the correctness of the identified events,\\nleveraging the inherent temporal self-reflection capabilities of LVLMs.\\nTimeSearch progressively explores key events and prioritizes temporal search\\nbased on reflection confidence. Extensive experiments on challenging long-video\\nbenchmarks confirm that TimeSearch substantially surpasses previous\\nstate-of-the-art, improving the accuracy from 41.8\\\\% to 51.5\\\\% on the LVBench.\\nAdditionally, experiments on temporal grounding demonstrate that appropriate\\nTAFR is adequate to effectively stimulate the surprising temporal grounding\\nability of LVLMs in a simpler yet versatile manner, which improves mIoU on\\nCharades-STA by 11.8\\\\%. The code will be released.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-02T06:47:19Z\"}"}

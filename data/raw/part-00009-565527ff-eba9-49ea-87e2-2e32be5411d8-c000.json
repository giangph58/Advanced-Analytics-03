{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20690v1\", \"title\": \"In-Context Edit: Enabling Instructional Image Editing with In-Context\\n  Generation in Large Scale Diffusion Transformer\", \"summary\": \"Instruction-based image editing enables robust image modification via natural\\nlanguage prompts, yet current methods face a precision-efficiency tradeoff.\\nFine-tuning methods demand significant computational resources and large\\ndatasets, while training-free techniques struggle with instruction\\ncomprehension and edit quality. We resolve this dilemma by leveraging\\nlarge-scale Diffusion Transformer (DiT)' enhanced generation capacity and\\nnative contextual awareness. Our solution introduces three contributions: (1)\\nan in-context editing framework for zero-shot instruction compliance using\\nin-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning\\nstrategy that enhances flexibility with efficient adaptation and dynamic expert\\nrouting, without extensive retraining; and (3) an early filter inference-time\\nscaling method using vision-language models (VLMs) to select better initial\\nnoise early, improving edit quality. Extensive evaluations demonstrate our\\nmethod's superiority: it outperforms state-of-the-art approaches while\\nrequiring only 0.5% training data and 1% trainable parameters compared to\\nconventional baselines. This work establishes a new paradigm that enables\\nhigh-precision yet efficient instruction-guided editing. Codes and demos can be\\nfound in https://river-zhang.github.io/ICEdit-gh-pages/.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-29T12:14:47Z\"}"}

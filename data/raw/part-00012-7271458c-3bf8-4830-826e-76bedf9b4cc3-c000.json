{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17565v1\", \"title\": \"DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale\\n  Difficulty-Graded Data Training\", \"summary\": \"Although large language models (LLMs) have recently achieved remarkable\\nperformance on various complex reasoning benchmarks, the academic community\\nstill lacks an in-depth understanding of base model training processes and data\\nquality. To address this, we construct a large-scale, difficulty-graded\\nreasoning dataset containing approximately 3.34 million unique queries of\\nvarying difficulty levels and about 40 million distilled responses generated by\\nmultiple models over several passes. Leveraging pass rate and Coefficient of\\nVariation (CV), we precisely select the most valuable training data to enhance\\nreasoning capability. Notably, we observe a training pattern shift, indicating\\nthat reasoning-focused training based on base models requires higher learning\\nrates for effective training. Using this carefully selected data, we\\nsignificantly improve the reasoning capabilities of the base model, achieving a\\npass rate of 79.2\\\\% on the AIME2024 mathematical reasoning benchmark. This\\nresult surpasses most current distilled models and closely approaches\\nstate-of-the-art performance. We provide detailed descriptions of our data\\nprocessing, difficulty assessment, and training methodology, and have publicly\\nreleased all datasets and methods to promote rapid progress in open-source\\nlong-reasoning LLMs. The dataset is available at:\\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-24T13:57:53Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16667v1\", \"title\": \"Representation Learning via Non-Contrastive Mutual Information\", \"summary\": \"Labeling data is often very time consuming and expensive, leaving us with a\\nmajority of unlabeled data. Self-supervised representation learning methods\\nsuch as SimCLR (Chen et al., 2020) or BYOL (Grill et al., 2020) have been very\\nsuccessful at learning meaningful latent representations from unlabeled image\\ndata, resulting in much more general and transferable representations for\\ndownstream tasks. Broadly, self-supervised methods fall into two types: 1)\\nContrastive methods, such as SimCLR; and 2) Non-Contrastive methods, such as\\nBYOL. Contrastive methods are generally trying to maximize mutual information\\nbetween related data points, so they need to compare every data point to every\\nother data point, resulting in high variance, and thus requiring large batch\\nsizes to work well. Non-contrastive methods like BYOL have much lower variance\\nas they do not need to make pairwise comparisons, but are much trickier to\\nimplement as they have the possibility of collapsing to a constant vector. In\\nthis paper, we aim to develop a self-supervised objective that combines the\\nstrength of both types. We start with a particular contrastive method called\\nthe Spectral Contrastive Loss (HaoChen et al., 2021; Lu et al., 2024), and we\\nconvert it into a more general non-contrastive form; this removes the pairwise\\ncomparisons resulting in lower variance, but keeps the mutual information\\nformulation of the contrastive method preventing collapse. We call our new\\nobjective the Mutual Information Non-Contrastive (MINC) loss. We test MINC by\\nlearning image representations on ImageNet (similar to SimCLR and BYOL) and\\nshow that it consistently improves upon the Spectral Contrastive loss baseline.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CV,stat.ML\", \"published\": \"2025-04-23T12:35:27Z\"}"}

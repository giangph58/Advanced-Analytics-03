{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12048v1\", \"title\": \"Modular-Cam: Modular Dynamic Camera-view Video Generation with LLM\", \"summary\": \"Text-to-Video generation, which utilizes the provided text prompt to generate\\nhigh-quality videos, has drawn increasing attention and achieved great success\\ndue to the development of diffusion models recently. Existing methods mainly\\nrely on a pre-trained text encoder to capture the semantic information and\\nperform cross attention with the encoded text prompt to guide the generation of\\nvideo. However, when it comes to complex prompts that contain dynamic scenes\\nand multiple camera-view transformations, these methods can not decompose the\\noverall information into separate scenes, as well as fail to smoothly change\\nscenes based on the corresponding camera-views. To solve these problems, we\\npropose a novel method, i.e., Modular-Cam. Specifically, to better understand a\\ngiven complex prompt, we utilize a large language model to analyze user\\ninstructions and decouple them into multiple scenes together with transition\\nactions. To generate a video containing dynamic scenes that match the given\\ncamera-views, we incorporate the widely-used temporal transformer into the\\ndiffusion model to ensure continuity within a single scene and propose\\nCamOperator, a modular network based module that well controls the camera\\nmovements. Moreover, we propose AdaControlNet, which utilizes ControlNet to\\nensure consistency across scenes and adaptively adjusts the color tone of the\\ngenerated video. Extensive qualitative and quantitative experiments prove our\\nproposed Modular-Cam's strong capability of generating multi-scene videos\\ntogether with its ability to achieve fine-grained control of camera movements.\\nGenerated results are available at https://modular-cam.github.io.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-16T13:04:01Z\"}"}

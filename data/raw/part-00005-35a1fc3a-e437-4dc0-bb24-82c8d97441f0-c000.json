{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19956v1\", \"title\": \"Securing Agentic AI: A Comprehensive Threat Model and Mitigation\\n  Framework for Generative AI Agents\", \"summary\": \"As generative AI (GenAI) agents become more common in enterprise settings,\\nthey introduce security challenges that differ significantly from those posed\\nby traditional systems. These agents are not just LLMs; they reason, remember,\\nand act, often with minimal human oversight. This paper introduces a\\ncomprehensive threat model tailored specifically for GenAI agents, focusing on\\nhow their autonomy, persistent memory access, complex reasoning, and tool\\nintegration create novel risks. This research work identifies 9 primary threats\\nand organizes them across five key domains: cognitive architecture\\nvulnerabilities, temporal persistence threats, operational execution\\nvulnerabilities, trust boundary violations, and governance circumvention. These\\nthreats are not just theoretical they bring practical challenges such as\\ndelayed exploitability, cross-system propagation, cross system lateral\\nmovement, and subtle goal misalignments that are hard to detect with existing\\nframeworks and standard approaches. To help address this, the research work\\npresent two complementary frameworks: ATFAA - Advanced Threat Framework for\\nAutonomous AI Agents, which organizes agent-specific risks, and SHIELD, a\\nframework proposing practical mitigation strategies designed to reduce\\nenterprise exposure. While this work builds on existing work in LLM and AI\\nsecurity, the focus is squarely on what makes agents different and why those\\ndifferences matter. Ultimately, this research argues that GenAI agents require\\na new lens for security. If we fail to adapt our threat models and defenses to\\naccount for their unique architecture and behavior, we risk turning a powerful\\nnew tool into a serious enterprise liability.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.AI\", \"published\": \"2025-04-28T16:29:24Z\"}"}

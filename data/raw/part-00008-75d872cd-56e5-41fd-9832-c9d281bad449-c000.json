{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21530v1\", \"title\": \"RoboGround: Robotic Manipulation with Grounded Vision-Language Priors\", \"summary\": \"Recent advancements in robotic manipulation have highlighted the potential of\\nintermediate representations for improving policy generalization. In this work,\\nwe explore grounding masks as an effective intermediate representation,\\nbalancing two key advantages: (1) effective spatial guidance that specifies\\ntarget objects and placement areas while also conveying information about\\nobject shape and size, and (2) broad generalization potential driven by\\nlarge-scale vision-language models pretrained on diverse grounding datasets. We\\nintroduce RoboGround, a grounding-aware robotic manipulation system that\\nleverages grounding masks as an intermediate representation to guide policy\\nnetworks in object manipulation tasks. To further explore and enhance\\ngeneralization, we propose an automated pipeline for generating large-scale,\\nsimulated data with a diverse set of objects and instructions. Extensive\\nexperiments show the value of our dataset and the effectiveness of grounding\\nmasks as intermediate guidance, significantly enhancing the generalization\\nabilities of robot policies.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.CV\", \"published\": \"2025-04-30T11:26:40Z\"}"}

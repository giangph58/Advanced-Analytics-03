{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10149v1\", \"title\": \"BoTTA: Benchmarking on-device Test Time Adaptation\", \"summary\": \"The performance of deep learning models depends heavily on test samples at\\nruntime, and shifts from the training data distribution can significantly\\nreduce accuracy. Test-time adaptation (TTA) addresses this by adapting models\\nduring inference without requiring labeled test data or access to the original\\ntraining set. While research has explored TTA from various perspectives like\\nalgorithmic complexity, data and class distribution shifts, model\\narchitectures, and offline versus continuous learning, constraints specific to\\nmobile and edge devices remain underexplored. We propose BoTTA, a benchmark\\ndesigned to evaluate TTA methods under practical constraints on mobile and edge\\ndevices. Our evaluation targets four key challenges caused by limited resources\\nand usage conditions: (i) limited test samples, (ii) limited exposure to\\ncategories, (iii) diverse distribution shifts, and (iv) overlapping shifts\\nwithin a sample. We assess state-of-the-art TTA methods under these scenarios\\nusing benchmark datasets and report system-level metrics on a real testbed.\\nFurthermore, unlike prior work, we align with on-device requirements by\\nadvocating periodic adaptation instead of continuous inference-time adaptation.\\nExperiments reveal key insights: many recent TTA algorithms struggle with small\\ndatasets, fail to generalize to unseen categories, and depend on the diversity\\nand complexity of distribution shifts. BoTTA also reports device-specific\\nresource use. For example, while SHOT improves accuracy by $2.25\\\\times$ with\\n$512$ adaptation samples, it uses $1.08\\\\times$ peak memory on Raspberry Pi\\nversus the base model. BoTTA offers actionable guidance for TTA in real-world,\\nresource-constrained deployments.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-14T12:00:00Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10049v1\", \"title\": \"Summarization of Multimodal Presentations with Vision-Language Models:\\n  Study of the Effect of Modalities and Structure\", \"summary\": \"Vision-Language Models (VLMs) can process visual and textual information in\\nmultiple formats: texts, images, interleaved texts and images, or even\\nhour-long videos. In this work, we conduct fine-grained quantitative and\\nqualitative analyses of automatic summarization of multimodal presentations\\nusing VLMs with various representations as input. From these experiments, we\\nsuggest cost-effective strategies for generating summaries from text-heavy\\nmultimodal documents under different input-length budgets using VLMs. We show\\nthat slides extracted from the video stream can be beneficially used as input\\nagainst the raw video, and that a structured representation from interleaved\\nslides and transcript provides the best performance. Finally, we reflect and\\ncomment on the nature of cross-modal interactions in multimodal presentations\\nand share suggestions to improve the capabilities of VLMs to understand\\ndocuments of this nature.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.CL\", \"published\": \"2025-04-14T09:55:01Z\"}"}

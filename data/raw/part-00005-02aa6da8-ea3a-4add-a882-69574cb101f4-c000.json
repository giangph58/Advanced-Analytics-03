{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05327v1\", \"title\": \"ICon: In-Context Contribution for Automatic Data Selection\", \"summary\": \"Data selection for instruction tuning is essential for improving the\\nperformance of Large Language Models (LLMs) and reducing training cost.\\nHowever, existing automated selection methods either depend on computationally\\nexpensive gradient-based measures or manually designed heuristics, which may\\nfail to fully exploit the intrinsic attributes of data. In this paper, we\\npropose In-context Learning for Contribution Measurement (ICon), a novel\\ngradient-free method that takes advantage of the implicit fine-tuning nature of\\nin-context learning (ICL) to measure sample contribution without gradient\\ncomputation or manual indicators engineering. ICon offers a computationally\\nefficient alternative to gradient-based methods and reduces human inductive\\nbias inherent in heuristic-based approaches. ICon comprises three components\\nand identifies high-contribution data by assessing performance shifts under\\nimplicit learning through ICL. Extensive experiments on three LLMs across 12\\nbenchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of\\nICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data\\noutperform full datasets by 5.42% points and exceed the best performance of\\nwidely used selection methods by 2.06% points. We further analyze\\nhigh-contribution samples selected by ICon, which show both diverse tasks and\\nappropriate difficulty levels, rather than just the hardest ones.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-05-08T15:17:37Z\"}"}

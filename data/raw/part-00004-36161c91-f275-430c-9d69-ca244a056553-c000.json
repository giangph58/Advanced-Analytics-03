{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04993v1\", \"title\": \"Latent Preference Coding: Aligning Large Language Models via Discrete\\n  Latent Codes\", \"summary\": \"Large language models (LLMs) have achieved remarkable success, yet aligning\\ntheir generations with human preferences remains a critical challenge. Existing\\napproaches to preference modeling often rely on an explicit or implicit reward\\nfunction, overlooking the intricate and multifaceted nature of human\\npreferences that may encompass conflicting factors across diverse tasks and\\npopulations. To address this limitation, we introduce Latent Preference Coding\\n(LPC), a novel framework that models the implicit factors as well as their\\ncombinations behind holistic preferences using discrete latent codes. LPC\\nseamlessly integrates with various offline alignment algorithms, automatically\\ninferring the underlying factors and their importance from data without relying\\non pre-defined reward functions and hand-crafted combination weights. Extensive\\nexperiments on multiple benchmarks demonstrate that LPC consistently improves\\nupon three alignment algorithms (DPO, SimPO, and IPO) using three base models\\n(Mistral-7B, Llama3-8B, and Llama3-8B-Instruct). Furthermore, deeper analysis\\nreveals that the learned latent codes effectively capture the differences in\\nthe distribution of human preferences and significantly enhance the robustness\\nof alignment against noise in data. By providing a unified representation for\\nthe multifarious preference factors, LPC paves the way towards developing more\\nrobust and versatile alignment techniques for the responsible deployment of\\npowerful LLMs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-05-08T06:59:06Z\"}"}

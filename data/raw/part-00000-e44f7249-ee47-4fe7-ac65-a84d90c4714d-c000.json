{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05062v1\", \"title\": \"ULFine: Unbiased Lightweight Fine-tuning for Foundation-Model-Assisted\\n  Long-Tailed Semi-Supervised Learning\", \"summary\": \"Based on the success of large-scale visual foundation models like CLIP in\\nvarious downstream tasks, this paper initially attempts to explore their impact\\non Long-Tailed Semi-Supervised Learning (LTSSL) by employing the foundation\\nmodel with three strategies: Linear Probing (LP), Lightweight Fine-Tuning\\n(LFT), and Full Fine-Tuning (FFT). Our analysis presents the following\\ninsights: i) Compared to LTSSL algorithms trained from scratch, FFT results in\\na decline in model performance, whereas LP and LFT, although boosting overall\\nmodel performance, exhibit negligible benefits to tail classes. ii) LP produces\\nnumerous false pseudo-labels due to \\\\textit{underlearned} training data, while\\nLFT can reduce the number of these false labels but becomes overconfident about\\nthem owing to \\\\textit{biased fitting} training data. This exacerbates the\\npseudo-labeled and classifier biases inherent in LTSSL, limiting performance\\nimprovement in the tail classes. With these insights, we propose a Unbiased\\nLightweight Fine-tuning strategy, \\\\textbf{ULFine}, which mitigates the\\noverconfidence via confidence-aware adaptive fitting of textual prototypes and\\ncounteracts the pseudo-labeled and classifier biases via complementary fusion\\nof dual logits. Extensive experiments demonstrate that ULFine markedly\\ndecreases training costs by over ten times and substantially increases\\nprediction accuracies compared to state-of-the-art methods.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-08T08:54:57Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03654v1\", \"title\": \"ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language\\n  and Vision Assistant\", \"summary\": \"Recent advances in personalized MLLMs enable effective capture of\\nuser-specific concepts, supporting both recognition of personalized concepts\\nand contextual captioning. However, humans typically explore and reason over\\nrelations among objects and individuals, transcending surface-level information\\nto achieve more personalized and contextual understanding. To this end,\\nexisting methods may face three main limitations: Their training data lacks\\nmulti-object sets in which relations among objects are learnable. Building on\\nthe limited training data, their models overlook the relations between\\ndifferent personalized concepts and fail to reason over them. Their experiments\\nmainly focus on a single personalized concept, where evaluations are limited to\\nrecognition and captioning tasks. To address the limitations, we present a new\\ndataset named ReGraP, consisting of 120 sets of personalized knowledge. Each\\nset includes images, KGs, and CoT QA pairs derived from the KGs, enabling more\\nstructured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an\\nMLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard\\ngraph prompting methods are designed to align KGs within the model's semantic\\nspace. We establish the ReGraP Benchmark, which contains diverse task types:\\nmultiple-choice, fill-in-the-blank, True/False, and descriptive questions in\\nboth open- and closed-ended settings. The proposed benchmark is designed to\\nevaluate the relational reasoning and knowledge-connection capability of\\npersonalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and\\nother competitive MLLMs. Results show that the proposed model not only learns\\npersonalized knowledge but also performs relational reasoning in responses,\\nachieving the SoTA performance compared with the competitive methods. All the\\ncodes and datasets are released at: https://github.com/xyfyyds/ReGraP.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-05-06T16:00:13Z\"}"}

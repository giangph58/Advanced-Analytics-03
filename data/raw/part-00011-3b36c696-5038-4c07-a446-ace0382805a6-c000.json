{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05682v1\", \"title\": \"On the Suitability of Reinforcement Fine-Tuning to Visual Tasks\", \"summary\": \"Reinforcement Fine-Tuning (RFT) is proved to be greatly valuable for\\nenhancing the reasoning ability of LLMs. Researchers have been starting to\\napply RFT to MLLMs, hoping it will also enhance the capabilities of visual\\nunderstanding. However, these works are at a very early stage and have not\\nexamined how suitable RFT actually is for visual tasks. In this work, we\\nendeavor to understand the suitabilities and limitations of RFT for visual\\ntasks, through experimental analysis and observations. We start by quantitative\\ncomparisons on various tasks, which shows RFT is generally better than SFT on\\nvisual tasks. %especially when the number of training samples are limited. To\\ncheck whether such advantages are brought up by the reasoning process, we\\ndesign a new reward that encourages the model to ``think'' more, whose results\\nshow more thinking can be beneficial for complicated tasks but harmful for\\nsimple tasks. We hope this study can provide more insight for the rapid\\nadvancements on this topic.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-08T04:45:00Z\"}"}

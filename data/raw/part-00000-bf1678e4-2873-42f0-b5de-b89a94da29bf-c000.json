{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24123v1\", \"title\": \"CTSketch: Compositional Tensor Sketching for Scalable Neurosymbolic\\n  Learning\", \"summary\": \"Many computational tasks benefit from being formulated as the composition of\\nneural networks followed by a discrete symbolic program. The goal of\\nneurosymbolic learning is to train the neural networks using only end-to-end\\ninput-output labels of the composite. We introduce CTSketch, a novel, scalable\\nneurosymbolic learning algorithm. CTSketch uses two techniques to improve the\\nscalability of neurosymbolic inference: decompose the symbolic program into\\nsub-programs and summarize each sub-program with a sketched tensor. This\\nstrategy allows us to approximate the output distribution of the program with\\nsimple tensor operations over the input distributions and summaries. We provide\\ntheoretical insight into the maximum error of the approximation. Furthermore,\\nwe evaluate CTSketch on many benchmarks from the neurosymbolic literature,\\nincluding some designed for evaluating scalability. Our results show that\\nCTSketch pushes neurosymbolic learning to new scales that have previously been\\nunattainable by obtaining high accuracy on tasks involving over one thousand\\ninputs.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-03-31T14:08:58Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20900v1\", \"title\": \"Evaluating Generative Models for Tabular Data: Novel Metrics and\\n  Benchmarking\", \"summary\": \"Generative models have revolutionized multiple domains, yet their application\\nto tabular data remains underexplored. Evaluating generative models for tabular\\ndata presents unique challenges due to structural complexity, large-scale\\nvariability, and mixed data types, making it difficult to intuitively capture\\nintricate patterns. Existing evaluation metrics offer only partial insights,\\nlacking a comprehensive measure of generative performance. To address this\\nlimitation, we propose three novel evaluation metrics: FAED, FPCAD, and RFIS.\\nOur extensive experimental analysis, conducted on three standard network\\nintrusion detection datasets, compares these metrics with established\\nevaluation methods such as Fidelity, Utility, TSTR, and TRTS. Our results\\ndemonstrate that FAED effectively captures generative modeling issues\\noverlooked by existing metrics. While FPCAD exhibits promising performance,\\nfurther refinements are necessary to enhance its reliability. Our proposed\\nframework provides a robust and practical approach for assessing generative\\nmodels in tabular data applications.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-29T16:16:51Z\"}"}

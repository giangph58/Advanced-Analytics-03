{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16788v1\", \"title\": \"Towards Explainable AI: Multi-Modal Transformer for Video-based Image\\n  Description Generation\", \"summary\": \"Understanding and analyzing video actions are essential for producing\\ninsightful and contextualized descriptions, especially for video-based\\napplications like intelligent monitoring and autonomous systems. The proposed\\nwork introduces a novel framework for generating natural language descriptions\\nfrom video datasets by combining textual and visual modalities. The suggested\\narchitecture makes use of ResNet50 to extract visual features from video frames\\nthat are taken from the Microsoft Research Video Description Corpus (MSVD), and\\nBerkeley DeepDrive eXplanation (BDD-X) datasets. The extracted visual\\ncharacteristics are converted into patch embeddings and then run through an\\nencoder-decoder model based on Generative Pre-trained Transformer-2 (GPT-2). In\\norder to align textual and visual representations and guarantee high-quality\\ndescription production, the system uses multi-head self-attention and\\ncross-attention techniques. The model's efficacy is demonstrated by performance\\nevaluation using BLEU (1-4), CIDEr, METEOR, and ROUGE-L. The suggested\\nframework outperforms traditional methods with BLEU-4 scores of 0.755 (BDD-X)\\nand 0.778 (MSVD), CIDEr scores of 1.235 (BDD-X) and 1.315 (MSVD), METEOR scores\\nof 0.312 (BDD-X) and 0.329 (MSVD), and ROUGE-L scores of 0.782 (BDD-X) and\\n0.795 (MSVD). By producing human-like, contextually relevant descriptions,\\nstrengthening interpretability, and improving real-world applications, this\\nresearch advances explainable AI.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-23T15:03:37Z\"}"}

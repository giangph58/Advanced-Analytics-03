{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10886v1\", \"title\": \"Exploring Persona-dependent LLM Alignment for the Moral Machine\\n  Experiment\", \"summary\": \"Deploying large language models (LLMs) with agency in real-world applications\\nraises critical questions about how these models will behave. In particular,\\nhow will their decisions align with humans when faced with moral dilemmas? This\\nstudy examines the alignment between LLM-driven decisions and human judgment in\\nvarious contexts of the moral machine experiment, including personas reflecting\\ndifferent sociodemographics. We find that the moral decisions of LLMs vary\\nsubstantially by persona, showing greater shifts in moral decisions for\\ncritical tasks than humans. Our data also indicate an interesting partisan\\nsorting phenomenon, where political persona predominates the direction and\\ndegree of LLM decisions. We discuss the ethical implications and risks\\nassociated with deploying these models in applications that involve moral\\ndecisions.\", \"main_category\": \"cs.CY\", \"categories\": \"cs.CY,cs.AI,cs.CL\", \"published\": \"2025-04-15T05:29:51Z\"}"}

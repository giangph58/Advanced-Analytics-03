{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05138v1\", \"title\": \"Towards Optimal Heterogeneous Client Sampling in Multi-Model Federated\\n  Learning\", \"summary\": \"Federated learning (FL) allows edge devices to collaboratively train models\\nwithout sharing local data. As FL gains popularity, clients may need to train\\nmultiple unrelated FL models, but communication constraints limit their ability\\nto train all models simultaneously. While clients could train FL models\\nsequentially, opportunistically having FL clients concurrently train different\\nmodels -- termed multi-model federated learning (MMFL) -- can reduce the\\noverall training time. Prior work uses simple client-to-model assignments that\\ndo not optimize the contribution of each client to each model over the course\\nof its training. Prior work on single-model FL shows that intelligent client\\nselection can greatly accelerate convergence, but na\\\\\\\"ive extensions to MMFL\\ncan violate heterogeneous resource constraints at both the server and the\\nclients. In this work, we develop a novel convergence analysis of MMFL with\\narbitrary client sampling methods, theoretically demonstrating the strengths\\nand limitations of previous well-established gradient-based methods. Motivated\\nby this analysis, we propose MMFL-LVR, a loss-based sampling method that\\nminimizes training variance while explicitly respecting communication limits at\\nthe server and reducing computational costs at the clients. We extend this to\\nMMFL-StaleVR, which incorporates stale updates for improved efficiency and\\nstability, and MMFL-StaleVRE, a lightweight variant suitable for low-overhead\\ndeployment. Experiments show our methods improve average accuracy by up to\\n19.1% over random sampling, with only a 5.4% gap from the theoretical optimum\\n(full client participation).\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.DC,I.2.11\", \"published\": \"2025-04-07T14:43:17Z\"}"}

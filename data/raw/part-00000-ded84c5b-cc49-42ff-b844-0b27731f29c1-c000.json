{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03434v1\", \"title\": \"Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in\\n  LLM-Based Agents\", \"summary\": \"Large Language Models (LLMs) represent a landmark achievement in Artificial\\nIntelligence (AI), demonstrating unprecedented proficiency in procedural tasks\\nsuch as text generation, code completion, and conversational coherence. These\\ncapabilities stem from their architecture, which mirrors human procedural\\nmemory -- the brain's ability to automate repetitive, pattern-driven tasks\\nthrough practice. However, as LLMs are increasingly deployed in real-world\\napplications, it becomes impossible to ignore their limitations operating in\\ncomplex, unpredictable environments. This paper argues that LLMs, while\\ntransformative, are fundamentally constrained by their reliance on procedural\\nmemory. To create agents capable of navigating ``wicked'' learning environments\\n-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must\\naugment LLMs with semantic memory and associative learning systems. By adopting\\na modular architecture that decouples these cognitive functions, we can bridge\\nthe gap between narrow procedural expertise and the adaptive intelligence\\nrequired for real-world problem-solving.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.LG\", \"published\": \"2025-05-06T11:18:34Z\"}"}

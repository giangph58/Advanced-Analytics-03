{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00502v1\", \"title\": \"Towards Scalable Human-aligned Benchmark for Text-guided Image Editing\", \"summary\": \"A variety of text-guided image editing models have been proposed recently.\\nHowever, there is no widely-accepted standard evaluation method mainly due to\\nthe subjective nature of the task, letting researchers rely on manual user\\nstudy. To address this, we introduce a novel Human-Aligned benchmark for\\nText-guided Image Editing (HATIE). Providing a large-scale benchmark set\\ncovering a wide range of editing tasks, it allows reliable evaluation, not\\nlimited to specific easy-to-evaluate cases. Also, HATIE provides a\\nfully-automated and omnidirectional evaluation pipeline. Particularly, we\\ncombine multiple scores measuring various aspects of editing so as to align\\nwith human perception. We empirically verify that the evaluation of HATIE is\\nindeed human-aligned in various aspects, and provide benchmark results on\\nseveral state-of-the-art models to provide deeper insights on their\\nperformance.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-01T13:06:05Z\"}"}

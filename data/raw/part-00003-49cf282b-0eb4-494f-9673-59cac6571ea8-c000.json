{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14872v1\", \"title\": \"Efficient Function Orchestration for Large Language Models\", \"summary\": \"Function calling is a fundamental capability of today's large language\\nmodels, but sequential function calling posed efficiency problems. Recent\\nstudies have proposed to request function calls with parallelism support in\\norder to alleviate this issue. However, they either delegate the concurrent\\nfunction calls to users for execution which are conversely executed\\nsequentially, or overlook the relations among various function calls, rending\\nlimited efficiency. This paper introduces LLMOrch, an advanced framework for\\nautomated, parallel function calling in large language models. The key\\nprinciple behind LLMOrch is to identify an available processor to execute a\\nfunction call while preventing any single processor from becoming overburdened.\\nTo this end, LLMOrch models the data relations (i.e., def-use) among different\\nfunction calls and coordinates their executions by their control relations\\n(i.e., mutual-exclusion) as well as the working status of the underlying\\nprocessors. When comparing with state-of-the-art techniques, LLMOrch\\ndemonstrated comparable efficiency improvements in orchestrating I/O-intensive\\nfunctions, while significantly outperforming (2$\\\\times$) them with\\ncompute-intensive functions. LLMOrch's performance even showed a linear\\ncorrelation to the number of allocated processors. We believe that these\\nresults highlight the potential of LLMOrch as an efficient solution for\\nparallel function orchestration in the context of large language models.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE\", \"published\": \"2025-04-21T05:57:34Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01786v1\", \"title\": \"BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing\", \"summary\": \"3D graphics editing is crucial in applications like movie production and game\\ndesign, yet it remains a time-consuming process that demands highly specialized\\ndomain expertise. Automating this process is challenging because graphical\\nediting requires performing a variety of tasks, each requiring distinct skill\\nsets. Recently, vision-language models (VLMs) have emerged as a powerful\\nframework for automating the editing process, but their development and\\nevaluation are bottlenecked by the lack of a comprehensive benchmark that\\nrequires human-level perception and presents real-world editing complexity. In\\nthis work, we present BlenderGym, the first comprehensive VLM system benchmark\\nfor 3D graphics editing. BlenderGym evaluates VLM systems through code-based 3D\\nreconstruction tasks. We evaluate closed- and open-source VLM systems and\\nobserve that even the state-of-the-art VLM system struggles with tasks\\nrelatively easy for human Blender users. Enabled by BlenderGym, we study how\\ninference scaling techniques impact VLM's performance on graphics editing\\ntasks. Notably, our findings reveal that the verifier used to guide the scaling\\nof generation can itself be improved through inference scaling, complementing\\nrecent insights on inference scaling of LLM generation in coding and math\\ntasks. We further show that inference compute is not uniformly effective and\\ncan be optimized by strategically distributing it between generation and\\nverification.\", \"main_category\": \"cs.GR\", \"categories\": \"cs.GR,cs.LG\", \"published\": \"2025-04-02T14:51:45Z\"}"}

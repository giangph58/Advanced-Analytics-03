{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11775v1\", \"title\": \"Discrimination-free Insurance Pricing with Privatized Sensitive\\n  Attributes\", \"summary\": \"Fairness has emerged as a critical consideration in the landscape of machine\\nlearning algorithms, particularly as AI continues to transform decision-making\\nacross societal domains. To ensure that these algorithms are free from bias and\\ndo not discriminate against individuals based on sensitive attributes such as\\ngender and race, the field of algorithmic bias has introduced various fairness\\nconcepts, along with methodologies to achieve these notions in different\\ncontexts. Despite the rapid advancement, not all sectors have embraced these\\nfairness principles to the same extent. One specific sector that merits\\nattention in this regard is insurance. Within the realm of insurance pricing,\\nfairness is defined through a distinct and specialized framework. Consequently,\\nachieving fairness according to established notions does not automatically\\nensure fair pricing in insurance. In particular, regulators are increasingly\\nemphasizing transparency in pricing algorithms and imposing constraints on\\ninsurance companies on the collection and utilization of sensitive consumer\\nattributes. These factors present additional challenges in the implementation\\nof fairness in pricing algorithms. To address these complexities and comply\\nwith regulatory demands, we propose an efficient method for constructing fair\\nmodels that are tailored to the insurance domain, using only privatized\\nsensitive attributes. Notably, our approach ensures statistical guarantees,\\ndoes not require direct access to sensitive attributes, and adapts to varying\\ntransparency requirements, addressing regulatory demands while ensuring\\nfairness in insurance pricing.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.CY,cs.LG,q-fin.RM\", \"published\": \"2025-04-16T05:29:11Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15719v1\", \"title\": \"Implementing Rational Choice Functions with LLMs and Measuring their\\n  Alignment with User Preferences\", \"summary\": \"As large language models (LLMs) become integral to intelligent user\\ninterfaces (IUIs), their role as decision-making agents raises critical\\nconcerns about alignment. Although extensive research has addressed issues such\\nas factuality, bias, and toxicity, comparatively little attention has been paid\\nto measuring alignment to preferences, i.e., the relative desirability of\\ndifferent alternatives, a concept used in decision making, economics, and\\nsocial choice theory. However, a reliable decision-making agent makes choices\\nthat align well with user preferences.\\n  In this paper, we generalize existing methods that exploit LLMs for ranking\\nalternative outcomes by addressing alignment with the broader and more flexible\\nconcept of user preferences, which includes both strict preferences and\\nindifference among alternatives. To this end, we put forward design principles\\nfor using LLMs to implement rational choice functions, and provide the\\nnecessary tools to measure preference satisfaction. We demonstrate the\\napplicability of our approach through an empirical study in a practical\\napplication of an IUI in the automotive domain.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-22T09:08:21Z\"}"}

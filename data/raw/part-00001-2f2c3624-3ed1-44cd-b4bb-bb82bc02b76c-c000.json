{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20995v1\", \"title\": \"TesserAct: Learning 4D Embodied World Models\", \"summary\": \"This paper presents an effective approach for learning novel 4D embodied\\nworld models, which predict the dynamic evolution of 3D scenes over time in\\nresponse to an embodied agent's actions, providing both spatial and temporal\\nconsistency. We propose to learn a 4D world model by training on RGB-DN (RGB,\\nDepth, and Normal) videos. This not only surpasses traditional 2D models by\\nincorporating detailed shape, configuration, and temporal changes into their\\npredictions, but also allows us to effectively learn accurate inverse dynamic\\nmodels for an embodied agent. Specifically, we first extend existing robotic\\nmanipulation video datasets with depth and normal information leveraging\\noff-the-shelf models. Next, we fine-tune a video generation model on this\\nannotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for\\neach frame. We then present an algorithm to directly convert generated RGB,\\nDepth, and Normal videos into a high-quality 4D scene of the world. Our method\\nensures temporal and spatial coherence in 4D scene predictions from embodied\\nscenarios, enables novel view synthesis for embodied environments, and\\nfacilitates policy learning that significantly outperforms those derived from\\nprior video-based world models.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.RO\", \"published\": \"2025-04-29T17:59:30Z\"}"}

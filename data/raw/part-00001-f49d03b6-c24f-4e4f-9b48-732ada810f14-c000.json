{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05303v1\", \"title\": \"InteractVLM: 3D Interaction Reasoning from 2D Foundational Models\", \"summary\": \"We introduce InteractVLM, a novel method to estimate 3D contact points on\\nhuman bodies and objects from single in-the-wild images, enabling accurate\\nhuman-object joint reconstruction in 3D. This is challenging due to occlusions,\\ndepth ambiguities, and widely varying object shapes. Existing methods rely on\\n3D contact annotations collected via expensive motion-capture systems or\\ntedious manual labeling, limiting scalability and generalization. To overcome\\nthis, InteractVLM harnesses the broad visual knowledge of large Vision-Language\\nModels (VLMs), fine-tuned with limited 3D contact data. However, directly\\napplying these models is non-trivial, as they reason only in 2D, while\\nhuman-object contact is inherently 3D. Thus we introduce a novel\\nRender-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D\\nspace via multi-view rendering, (2) trains a novel multi-view localization\\nmodel (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D.\\nAdditionally, we propose a new task called Semantic Human Contact estimation,\\nwhere human contact predictions are conditioned explicitly on object semantics,\\nenabling richer interaction modeling. InteractVLM outperforms existing work on\\ncontact estimation and also facilitates 3D reconstruction from an in-the wild\\nimage. Code and models are available at https://interactvlm.is.tue.mpg.de.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-07T17:59:33Z\"}"}

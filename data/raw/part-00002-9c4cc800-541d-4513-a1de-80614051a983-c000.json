{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06219v1\", \"title\": \"Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling\\n  Opt-Outs\", \"summary\": \"The increasing adoption of web crawling opt-outs by copyright holders of\\nonline content raises critical questions about the impact of data compliance on\\nlarge language model (LLM) performance. However, little is known about how\\nthese restrictions (and the resultant filtering of pretraining datasets) affect\\nthe capabilities of models trained using these corpora. In this work, we\\nconceptualize this effect as the $\\\\textit{data compliance gap}$ (DCG), which\\nquantifies the performance difference between models trained on datasets that\\ncomply with web crawling opt-outs, and those that do not. We measure the data\\ncompliance gap in two settings: pretraining models from scratch and continual\\npretraining from existing compliant models (simulating a setting where\\ncopyrighted data could be integrated later in pretraining). Our experiments\\nwith 1.5B models show that, as of January 2025, compliance with web data\\nopt-outs does not degrade general knowledge acquisition (close to 0\\\\% DCG).\\nHowever, in specialized domains such as biomedical research, excluding major\\npublishers leads to performance declines. These findings suggest that while\\ngeneral-purpose LLMs can be trained to perform equally well using fully open\\ndata, performance in specialized domains may benefit from access to\\nhigh-quality copyrighted sources later in training. Our study provides\\nempirical insights into the long-debated trade-off between data compliance and\\ndownstream model performance, informing future discussions on AI training\\npractices and policy decisions.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-08T17:08:06Z\"}"}

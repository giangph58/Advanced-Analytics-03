{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20883v1\", \"title\": \"Guessing Efficiently for Constrained Subspace Approximation\", \"summary\": \"In this paper we study constrained subspace approximation problem. Given a\\nset of $n$ points $\\\\{a_1,\\\\ldots,a_n\\\\}$ in $\\\\mathbb{R}^d$, the goal of the {\\\\em\\nsubspace approximation} problem is to find a $k$ dimensional subspace that best\\napproximates the input points. More precisely, for a given $p\\\\geq 1$, we aim to\\nminimize the $p$th power of the $\\\\ell_p$ norm of the error vector\\n$(\\\\|a_1-\\\\bm{P}a_1\\\\|,\\\\ldots,\\\\|a_n-\\\\bm{P}a_n\\\\|)$, where $\\\\bm{P}$ denotes the\\nprojection matrix onto the subspace and the norms are Euclidean. In\\n\\\\emph{constrained} subspace approximation (CSA), we additionally have\\nconstraints on the projection matrix $\\\\bm{P}$. In its most general form, we\\nrequire $\\\\bm{P}$ to belong to a given subset $\\\\mathcal{S}$ that is described\\nexplicitly or implicitly.\\n  We introduce a general framework for constrained subspace approximation. Our\\napproach, that we term coreset-guess-solve, yields either\\n$(1+\\\\varepsilon)$-multiplicative or $\\\\varepsilon$-additive approximations for a\\nvariety of constraints. We show that it provides new algorithms for\\npartition-constrained subspace approximation with applications to {\\\\it fair}\\nsubspace approximation, $k$-means clustering, and projected non-negative matrix\\nfactorization, among others. Specifically, while we reconstruct the best known\\nbounds for $k$-means clustering in Euclidean spaces, we improve the known\\nresults for the remainder of the problems.\", \"main_category\": \"cs.DS\", \"categories\": \"cs.DS,cs.LG\", \"published\": \"2025-04-29T15:56:48Z\"}"}

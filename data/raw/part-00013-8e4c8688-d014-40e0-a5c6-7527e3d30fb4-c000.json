{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17403v1\", \"title\": \"Coding for Computation: Efficient Compression of Neural Networks for\\n  Reconfigurable Hardware\", \"summary\": \"As state of the art neural networks (NNs) continue to grow in size, their\\nresource-efficient implementation becomes ever more important. In this paper,\\nwe introduce a compression scheme that reduces the number of computations\\nrequired for NN inference on reconfigurable hardware such as FPGAs. This is\\nachieved by combining pruning via regularized training, weight sharing and\\nlinear computation coding (LCC). Contrary to common NN compression techniques,\\nwhere the objective is to reduce the memory used for storing the weights of the\\nNNs, our approach is optimized to reduce the number of additions required for\\ninference in a hardware-friendly manner. The proposed scheme achieves\\ncompetitive performance for simple multilayer perceptrons, as well as for large\\nscale deep NNs such as ResNet-34.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.IT,eess.SP,math.IT\", \"published\": \"2025-04-24T09:49:18Z\"}"}

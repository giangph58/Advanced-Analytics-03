{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01445v1\", \"title\": \"Enabling Systematic Generalization in Abstract Spatial Reasoning through\\n  Meta-Learning for Compositionality\", \"summary\": \"Systematic generalization refers to the capacity to understand and generate\\nnovel combinations from known components. Despite recent progress by large\\nlanguage models (LLMs) across various domains, these models often fail to\\nextend their knowledge to novel compositional scenarios, revealing notable\\nlimitations in systematic generalization. There has been an ongoing debate\\nabout whether neural networks possess the capacity for systematic\\ngeneralization, with recent studies suggesting that meta-learning approaches\\ndesigned for compositionality can significantly enhance this ability. However,\\nthese insights have largely been confined to linguistic problems, leaving their\\napplicability to other tasks an open question. In this study, we extend the\\napproach of meta-learning for compositionality to the domain of abstract\\nspatial reasoning. To this end, we introduce $\\\\textit{SYGAR}$-a dataset\\ndesigned to evaluate the capacity of models to systematically generalize from\\nknown geometric transformations (e.g., translation, rotation) of\\ntwo-dimensional objects to novel combinations of these transformations (e.g.,\\ntranslation+rotation). Our results show that a transformer-based\\nencoder-decoder model, trained via meta-learning for compositionality, can\\nsystematically generalize to previously unseen transformation compositions,\\nsignificantly outperforming state-of-the-art LLMs, including o3-mini, GPT-4o,\\nand Gemini 2.0 Flash, which fail to exhibit similar systematic behavior. Our\\nfindings highlight the effectiveness of meta-learning in promoting\\nsystematicity beyond linguistic tasks, suggesting a promising direction toward\\nmore robust and generalizable models.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-02T07:56:39Z\"}"}

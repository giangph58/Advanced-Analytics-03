{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00506v1\", \"title\": \"HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World\\n  Hallucination Detection\", \"summary\": \"As large language models (LLMs) are increasingly deployed in high-stakes\\ndomains, detecting hallucinated content$\\\\unicode{x2013}$text that is not\\ngrounded in supporting evidence$\\\\unicode{x2013}$has become a critical\\nchallenge. Existing benchmarks for hallucination detection are often\\nsynthetically generated, narrowly focused on extractive question answering, and\\nfail to capture the complexity of real-world scenarios involving multi-document\\ncontexts and full-sentence outputs. We introduce the HalluMix Benchmark, a\\ndiverse, task-agnostic dataset that includes examples from a range of domains\\nand formats. Using this benchmark, we evaluate seven hallucination detection\\nsystems$\\\\unicode{x2013}$both open and closed\\nsource$\\\\unicode{x2013}$highlighting differences in performance across tasks,\\ndocument lengths, and input representations. Our analysis highlights\\nsubstantial performance disparities between short and long contexts, with\\ncritical implications for real-world Retrieval Augmented Generation (RAG)\\nimplementations. Quotient Detections achieves the best overall performance,\\nwith an accuracy of 0.82 and an F1 score of 0.84.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-05-01T13:22:45Z\"}"}

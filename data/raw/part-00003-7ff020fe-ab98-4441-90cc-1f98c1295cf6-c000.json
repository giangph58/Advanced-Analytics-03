{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15120v1\", \"title\": \"Kuwain 1.5B: An Arabic SLM via Language Injection\", \"summary\": \"Enhancing existing models with new knowledge is a crucial aspect of AI\\ndevelopment. This paper introduces a novel method for integrating a new\\nlanguage into a large language model (LLM). Our approach successfully\\nincorporates a previously unseen target language into an existing LLM without\\ncompromising its prior knowledge. We trained a tiny model with 1.5 billion\\nparameters named Kuwain by injecting the Arabic language into a small\\nopen-source model mainly trained in English. Our method demonstrates\\nsignificant improvements in Arabic language performance, with an average 8%\\nimprovement across various benchmarks, while retaining the model's existing\\nknowledge with a minimum amount of the original model's data. This offers a\\ncost-effective alternative to training a comprehensive model in both English\\nand Arabic. The results highlight the potential for efficient, targeted\\nlanguage model expansion without extensive retraining or resource-intensive\\nprocesses.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-21T14:17:25Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20799v1\", \"title\": \"Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation,\\n  and Challenges\", \"summary\": \"Recent technical breakthroughs in large language models (LLMs) have enabled\\nthem to fluently generate source code. Software developers often leverage both\\ngeneral-purpose and code-specialized LLMs to revise existing code or even\\ngenerate a whole function from scratch. These capabilities are also beneficial\\nin no-code or low-code contexts, in which one can write programs without a\\ntechnical background. However, due to their internal design, LLMs are prone to\\ngenerating hallucinations, which are incorrect, nonsensical, and not\\njustifiable information but difficult to identify its presence. This problem\\nalso occurs when generating source code. Once hallucinated code is produced, it\\nis often challenging for users to identify and fix it, especially when such\\nhallucinations can be identified under specific execution paths. As a result,\\nthe hallucinated code may remain unnoticed within the codebase. This survey\\ninvestigates recent studies and techniques relevant to hallucinations generated\\nby CodeLLMs. We categorize the types of hallucinations in the code generated by\\nCodeLLMs, review existing benchmarks and mitigation strategies, and identify\\nopen challenges. Based on these findings, this survey outlines further research\\ndirections in the detection and removal of hallucinations produced by CodeLLMs.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE,cs.AI\", \"published\": \"2025-04-29T14:13:57Z\"}"}

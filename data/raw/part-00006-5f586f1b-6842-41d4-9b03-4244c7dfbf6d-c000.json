{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03280v1\", \"title\": \"MDPs with a State Sensing Cost\", \"summary\": \"In many practical sequential decision-making problems, tracking the state of\\nthe environment incurs a sensing/communication/computation cost. In these\\nsettings, the agent's interaction with its environment includes the additional\\ncomponent of deciding $\\\\textit{when}$ to sense the state, in a manner that\\nbalances the value associated with optimal (state-specific) actions and the\\ncost of sensing. We formulate this as an expected discounted cost Markov\\nDecision Process (MDP), wherein the agent incurs an additional cost for sensing\\nits next state, but has the option to take actions while remaining 'blind' to\\nthe system state.\\n  We pose this problem as a classical discounted cost MDP with an expanded\\n(countably infinite) state space. While computing the optimal policy for this\\nMDP is intractable in general, we bound the sub-optimality gap associated with\\noptimal policies in a restricted class, where the number of consecutive\\nnon-sensing (a.k.a., blind) actions is capped. We also design a computationally\\nefficient heuristic algorithm based on policy improvement, which in practice\\nperforms close to the optimal policy. Finally, we benchmark against the state\\nof the art via a numerical case study.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-06T08:06:45Z\"}"}

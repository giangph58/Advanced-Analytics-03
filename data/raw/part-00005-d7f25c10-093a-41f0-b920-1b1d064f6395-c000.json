{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16739v1\", \"title\": \"Prompt-Tuning SAM: From Generalist to Specialist with only 2048\\n  Parameters and 16 Training Images\", \"summary\": \"The Segment Anything Model (SAM) is widely used for segmenting a diverse\\nrange of objects in natural images from simple user prompts like points or\\nbounding boxes. However, SAM's performance decreases substantially when applied\\nto non-natural domains like microscopic imaging. Furthermore, due to SAM's\\ninteractive design, it requires a precise prompt for each image and object,\\nwhich is unfeasible in many automated biomedical applications. Previous\\nsolutions adapt SAM by training millions of parameters via fine-tuning large\\nparts of the model or of adapter layers. In contrast, we show that as little as\\n2,048 additional parameters are sufficient for turning SAM into a use-case\\nspecialist for a certain downstream task. Our novel PTSAM (prompt-tuned SAM)\\nmethod uses prompt-tuning, a parameter-efficient fine-tuning technique, to\\nadapt SAM for a specific task. We validate the performance of our approach on\\nmultiple microscopic and one medical dataset. Our results show that\\nprompt-tuning only SAM's mask decoder already leads to a performance on-par\\nwith state-of-the-art techniques while requiring roughly 2,000x less trainable\\nparameters. For addressing domain gaps, we find that additionally prompt-tuning\\nSAM's image encoder is beneficial, further improving segmentation accuracy by\\nup to 18% over state-of-the-art results. Since PTSAM can be reliably trained\\nwith as little as 16 annotated images, we find it particularly helpful for\\napplications with limited training data and domain shifts.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-23T14:10:02Z\"}"}

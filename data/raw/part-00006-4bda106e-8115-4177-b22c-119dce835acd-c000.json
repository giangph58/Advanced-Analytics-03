{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01603v1\", \"title\": \"A$^\\\\text{T}$A: Adaptive Transformation Agent for Text-Guided\\n  Subject-Position Variable Background Inpainting\", \"summary\": \"Image inpainting aims to fill the missing region of an image. Recently, there\\nhas been a surge of interest in foreground-conditioned background inpainting, a\\nsub-task that fills the background of an image while the foreground subject and\\nassociated text prompt are provided. Existing background inpainting methods\\ntypically strictly preserve the subject's original position from the source\\nimage, resulting in inconsistencies between the subject and the generated\\nbackground. To address this challenge, we propose a new task, the \\\"Text-Guided\\nSubject-Position Variable Background Inpainting\\\", which aims to dynamically\\nadjust the subject position to achieve a harmonious relationship between the\\nsubject and the inpainted background, and propose the Adaptive Transformation\\nAgent (A$^\\\\text{T}$A) for this task. Firstly, we design a PosAgent Block that\\nadaptively predicts an appropriate displacement based on given features to\\nachieve variable subject-position. Secondly, we design the Reverse Displacement\\nTransform (RDT) module, which arranges multiple PosAgent blocks in a reverse\\nstructure, to transform hierarchical feature maps from deep to shallow based on\\nsemantic information. Thirdly, we equip A$^\\\\text{T}$A with a Position Switch\\nEmbedding to control whether the subject's position in the generated image is\\nadaptively predicted or fixed. Extensive comparative experiments validate the\\neffectiveness of our A$^\\\\text{T}$A approach, which not only demonstrates\\nsuperior inpainting capabilities in subject-position variable inpainting, but\\nalso ensures good performance on subject-position fixed inpainting.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-02T11:13:46Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06260v1\", \"title\": \"FEABench: Evaluating Language Models on Multiphysics Reasoning Ability\", \"summary\": \"Building precise simulations of the real world and invoking numerical solvers\\nto answer quantitative problems is an essential requirement in engineering and\\nscience. We present FEABench, a benchmark to evaluate the ability of large\\nlanguage models (LLMs) and LLM agents to simulate and solve physics,\\nmathematics and engineering problems using finite element analysis (FEA). We\\nintroduce a comprehensive evaluation scheme to investigate the ability of LLMs\\nto solve these problems end-to-end by reasoning over natural language problem\\ndescriptions and operating COMSOL Multiphysics$^\\\\circledR$, an FEA software, to\\ncompute the answers. We additionally design a language model agent equipped\\nwith the ability to interact with the software through its Application\\nProgramming Interface (API), examine its outputs and use tools to improve its\\nsolutions over multiple iterations. Our best performing strategy generates\\nexecutable API calls 88% of the time. LLMs that can successfully interact with\\nand operate FEA software to solve problems such as those in our benchmark would\\npush the frontiers of automation in engineering. Acquiring this capability\\nwould augment LLMs' reasoning skills with the precision of numerical solvers\\nand advance the development of autonomous systems that can tackle complex\\nproblems in the real world. The code is available at\\nhttps://github.com/google/feabench\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CL,cs.NA,math.NA\", \"published\": \"2025-04-08T17:59:39Z\"}"}

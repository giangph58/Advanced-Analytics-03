{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03228v1\", \"title\": \"MGFF-TDNN: A Multi-Granularity Feature Fusion TDNN Model with Depth-Wise\\n  Separable Module for Speaker Verification\", \"summary\": \"In speaker verification, traditional models often emphasize modeling\\nlong-term contextual features to capture global speaker characteristics.\\nHowever, this approach can neglect fine-grained voiceprint information, which\\ncontains highly discriminative features essential for robust speaker\\nembeddings. This paper introduces a novel model architecture, termed MGFF-TDNN,\\nbased on multi-granularity feature fusion. The MGFF-TDNN leverages a\\ntwo-dimensional depth-wise separable convolution module, enhanced with local\\nfeature modeling, as a front-end feature extractor to effectively capture\\ntime-frequency domain features. To achieve comprehensive multi-granularity\\nfeature fusion, we propose the M-TDNN structure, which integrates global\\ncontextual modeling with fine-grained feature extraction by combining\\ntime-delay neural networks and phoneme-level feature pooling. Experiments on\\nthe VoxCeleb dataset demonstrate that the MGFF-TDNN achieves outstanding\\nperformance in speaker verification while remaining efficient in terms of\\nparameters and computational resources.\", \"main_category\": \"cs.SD\", \"categories\": \"cs.SD,eess.AS\", \"published\": \"2025-05-06T06:42:58Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15895v1\", \"title\": \"Dynamic Early Exit in Reasoning Models\", \"summary\": \"Recent advances in large reasoning language models (LRLMs) rely on test-time\\nscaling, which extends long chain-of-thought (CoT) generation to solve complex\\ntasks. However, overthinking in long CoT not only slows down the efficiency of\\nproblem solving, but also risks accuracy loss due to the extremely detailed or\\nredundant reasoning steps. We propose a simple yet effective method that allows\\nLLMs to self-truncate CoT sequences by early exit during generation. Instead of\\nrelying on fixed heuristics, the proposed method monitors model behavior at\\npotential reasoning transition points (e.g.,\\\"Wait\\\" tokens) and dynamically\\nterminates the next reasoning chain's generation when the model exhibits high\\nconfidence in a trial answer. Our method requires no additional training and\\ncan be seamlessly integrated into existing o1-like reasoning LLMs. Experiments\\non multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024\\nshow that the proposed method is consistently effective on deepseek-series\\nreasoning LLMs, reducing the length of CoT sequences by an average of 31% to\\n43% while improving accuracy by 1.7% to 5.7%.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-22T13:36:53Z\"}"}

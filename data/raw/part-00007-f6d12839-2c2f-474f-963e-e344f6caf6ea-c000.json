{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07596v1\", \"title\": \"Boosting Universal LLM Reward Design through the Heuristic Reward\\n  Observation Space Evolution\", \"summary\": \"Large Language Models (LLMs) are emerging as promising tools for automated\\nreinforcement learning (RL) reward design, owing to their robust capabilities\\nin commonsense reasoning and code generation. By engaging in dialogues with RL\\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\\nenvironment states and defining their internal operations. However, existing\\nframeworks have not effectively leveraged historical exploration data or manual\\ntask descriptions to iteratively evolve this space. In this paper, we propose a\\nnovel heuristic framework that enhances LLM-driven reward design by evolving\\nthe ROS through a table-based exploration caching mechanism and a text-code\\nreconciliation strategy. Our framework introduces a state execution table,\\nwhich tracks the historical usage and success rates of environment states,\\novercoming the Markovian constraint typically found in LLM dialogues and\\nfacilitating more effective exploration. Furthermore, we reconcile\\nuser-provided task descriptions with expert-defined success criteria using\\nstructured prompts, ensuring alignment in reward design objectives.\\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\\nand stability of the proposed framework. Code and video demos are available at\\njingjjjjjie.github.io/LLM2Reward.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-10T09:48:56Z\"}"}

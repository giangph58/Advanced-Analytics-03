{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15208v1\", \"title\": \"Compute-Optimal LLMs Provably Generalize Better With Scale\", \"summary\": \"Why do larger language models generalize better? To investigate this\\nquestion, we develop generalization bounds on the pretraining objective of\\nlarge language models (LLMs) in the compute-optimal regime, as described by the\\nChinchilla scaling laws. We introduce a novel, fully empirical Freedman-type\\nmartingale concentration inequality that tightens existing bounds by accounting\\nfor the variance of the loss function. This generalization bound can be\\ndecomposed into three interpretable components: the number of parameters per\\ntoken, the loss variance, and the quantization error at a fixed bitrate. As\\ncompute-optimal language models are scaled up, the number of parameters per\\ndata point remains constant; however, both the loss variance and the\\nquantization error decrease, implying that larger models should have smaller\\ngeneralization gaps. We examine why larger models tend to be more quantizable\\nfrom an information theoretic perspective, showing that the rate at which they\\ncan integrate new information grows more slowly than their capacity on the\\ncompute-optimal frontier. From these findings we produce a scaling law for the\\ngeneralization gap, with bounds that become predictably stronger with scale.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-21T16:26:56Z\"}"}

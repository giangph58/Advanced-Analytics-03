{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05071v1\", \"title\": \"FG-CLIP: Fine-Grained Visual and Textual Alignment\", \"summary\": \"Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks\\nsuch as image-text retrieval and zero-shot classification but struggles with\\nfine-grained understanding due to its focus on coarse-grained short captions.\\nTo address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances\\nfine-grained understanding through three key innovations. First, we leverage\\nlarge multimodal models to generate 1.6 billion long caption-image pairs for\\ncapturing global-level semantic details. Second, a high-quality dataset is\\nconstructed with 12 million images and 40 million region-specific bounding\\nboxes aligned with detailed captions to ensure precise, context-rich\\nrepresentations. Third, 10 million hard fine-grained negative samples are\\nincorporated to improve the model's ability to distinguish subtle semantic\\ndifferences. Corresponding training methods are meticulously designed for these\\ndata. Extensive experiments demonstrate that FG-CLIP outperforms the original\\nCLIP and other state-of-the-art methods across various downstream tasks,\\nincluding fine-grained understanding, open-vocabulary object detection,\\nimage-text retrieval, and general multimodal benchmarks. These results\\nhighlight FG-CLIP's effectiveness in capturing fine-grained image details and\\nimproving overall model performance. The related data, code, and models are\\navailable at https://github.com/360CVGroup/FG-CLIP.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-05-08T09:06:53Z\"}"}

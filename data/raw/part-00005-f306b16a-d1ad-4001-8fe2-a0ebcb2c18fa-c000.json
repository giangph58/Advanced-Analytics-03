{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13143v1\", \"title\": \"$\\\\texttt{Complex-Edit}$: CoT-Like Instruction Generation for\\n  Complexity-Controllable Image Editing Benchmark\", \"summary\": \"We introduce $\\\\texttt{Complex-Edit}$, a comprehensive benchmark designed to\\nsystematically evaluate instruction-based image editing models across\\ninstructions of varying complexity. To develop this benchmark, we harness\\nGPT-4o to automatically collect a diverse set of editing instructions at scale.\\nOur approach follows a well-structured ``Chain-of-Edit'' pipeline: we first\\ngenerate individual atomic editing tasks independently and then integrate them\\nto form cohesive, complex instructions. Additionally, we introduce a suite of\\nmetrics to assess various aspects of editing performance, along with a\\nVLM-based auto-evaluation pipeline that supports large-scale assessments. Our\\nbenchmark yields several notable insights: 1) Open-source models significantly\\nunderperform relative to proprietary, closed-source models, with the\\nperformance gap widening as instruction complexity increases; 2) Increased\\ninstructional complexity primarily impairs the models' ability to retain key\\nelements from the input images and to preserve the overall aesthetic quality;\\n3) Decomposing a complex instruction into a sequence of atomic steps, executed\\nin a step-by-step manner, substantially degrades performance across multiple\\nmetrics; 4) A straightforward Best-of-N selection strategy improves results for\\nboth direct editing and the step-by-step sequential approach; and 5) We observe\\na ``curse of synthetic data'': when synthetic data is involved in model\\ntraining, the edited images from such models tend to appear increasingly\\nsynthetic as the complexity of the editing instructions rises -- a phenomenon\\nthat intriguingly also manifests in the latest GPT-4o outputs.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-17T17:51:59Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05812v1\", \"title\": \"Right Question is Already Half the Answer: Fully Unsupervised LLM\\n  Reasoning Incentivization\", \"summary\": \"While large language models (LLMs) have demonstrated exceptional capabilities\\nin challenging tasks such as mathematical reasoning, existing methods to\\nenhance reasoning ability predominantly rely on supervised fine-tuning (SFT)\\nfollowed by reinforcement learning (RL) on reasoning-specific data after\\npre-training. However, these approaches critically depend on external\\nsupervisions--such as human labelled reasoning traces, verified golden answers,\\nor pre-trained reward models--which limits scalability and practical\\napplicability. In this work, we propose Entropy Minimized Policy Optimization\\n(EMPO), which makes an early attempt at fully unsupervised LLM reasoning\\nincentivization. EMPO does not require any supervised information for\\nincentivizing reasoning capabilities (i.e., neither verifiable reasoning\\ntraces, problems with golden answers, nor additional pre-trained reward\\nmodels). By continuously minimizing the predictive entropy of LLMs on unlabeled\\nuser queries in a latent semantic space, EMPO enables purely self-supervised\\nevolution of reasoning capabilities with strong flexibility and practicality.\\nOur experiments demonstrate competitive performance of EMPO on both\\nmathematical reasoning and free-form commonsense reasoning tasks. Specifically,\\nwithout any supervised signals, EMPO boosts the accuracy of Qwen2.5-Math-7B\\nBase from 30.7\\\\% to 48.1\\\\% on mathematical benchmarks and improves truthfulness\\naccuracy of Qwen2.5-7B Instruct from 87.16\\\\% to 97.25\\\\% on TruthfulQA.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-08T08:48:51Z\"}"}

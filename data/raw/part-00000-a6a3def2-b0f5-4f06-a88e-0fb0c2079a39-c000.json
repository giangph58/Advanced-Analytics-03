{"value":"{\"aid\": \"http://arxiv.org/abs/2503.21771v1\", \"title\": \"A Unified Image-Dense Annotation Generation Model for Underwater Scenes\", \"summary\": \"Underwater dense prediction, especially depth estimation and semantic\\nsegmentation, is crucial for gaining a comprehensive understanding of\\nunderwater scenes. Nevertheless, high-quality and large-scale underwater\\ndatasets with dense annotations remain scarce because of the complex\\nenvironment and the exorbitant data collection costs. This paper proposes a\\nunified Text-to-Image and DEnse annotation generation method (TIDE) for\\nunderwater scenes. It relies solely on text as input to simultaneously generate\\nrealistic underwater images and multiple highly consistent dense annotations.\\nSpecifically, we unify the generation of text-to-image and text-to-dense\\nannotations within a single model. The Implicit Layout Sharing mechanism (ILS)\\nand cross-modal interaction method called Time Adaptive Normalization (TAN) are\\nintroduced to jointly optimize the consistency between image and dense\\nannotations. We synthesize a large-scale underwater dataset using TIDE to\\nvalidate the effectiveness of our method in underwater dense prediction tasks.\\nThe results demonstrate that our method effectively improves the performance of\\nexisting underwater dense prediction models and mitigates the scarcity of\\nunderwater data with dense annotations. We hope our method can offer new\\nperspectives on alleviating data scarcity issues in other fields. The code is\\navailable at https: //github.com/HongkLin/TIDE.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-27T17:59:43Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04445v1\", \"title\": \"M2Rec: Multi-scale Mamba for Efficient Sequential Recommendation\", \"summary\": \"Sequential recommendation systems aim to predict users' next preferences\\nbased on their interaction histories, but existing approaches face critical\\nlimitations in efficiency and multi-scale pattern recognition. While\\nTransformer-based methods struggle with quadratic computational complexity,\\nrecent Mamba-based models improve efficiency but fail to capture periodic user\\nbehaviors, leverage rich semantic information, or effectively fuse multimodal\\nfeatures. To address these challenges, we propose \\\\model, a novel sequential\\nrecommendation framework that integrates multi-scale Mamba with Fourier\\nanalysis, Large Language Models (LLMs), and adaptive gating. First, we enhance\\nMamba with Fast Fourier Transform (FFT) to explicitly model periodic patterns\\nin the frequency domain, separating meaningful trends from noise. Second, we\\nincorporate LLM-based text embeddings to enrich sparse interaction data with\\nsemantic context from item descriptions. Finally, we introduce a learnable gate\\nmechanism to dynamically balance temporal (Mamba), frequency (FFT), and\\nsemantic (LLM) features, ensuring harmonious multimodal fusion. Extensive\\nexperiments demonstrate that \\\\model\\\\ achieves state-of-the-art performance,\\nimproving Hit Rate@10 by 3.2\\\\% over existing Mamba-based models while\\nmaintaining 20\\\\% faster inference than Transformer baselines. Our results\\nhighlight the effectiveness of combining frequency analysis, semantic\\nunderstanding, and adaptive fusion for sequential recommendation. Code and\\ndatasets are available at: https://anonymous.4open.science/r/M2Rec.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR\", \"published\": \"2025-05-07T14:14:29Z\"}"}

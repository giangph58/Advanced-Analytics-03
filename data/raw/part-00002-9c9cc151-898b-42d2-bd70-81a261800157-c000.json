{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00685v1\", \"title\": \"On the Importance of Gaussianizing Representations\", \"summary\": \"The normal distribution plays a central role in information theory - it is at\\nthe same time the best-case signal and worst-case noise distribution, has the\\ngreatest representational capacity of any distribution, and offers an\\nequivalence between uncorrelatedness and independence for joint distributions.\\nAccounting for the mean and variance of activations throughout the layers of\\ndeep neural networks has had a significant effect on facilitating their\\neffective training, but seldom has a prescription for precisely what\\ndistribution these activations should take, and how this might be achieved,\\nbeen offered. Motivated by the information-theoretic properties of the normal\\ndistribution, we address this question and concurrently present normality\\nnormalization: a novel normalization layer which encourages normality in the\\nfeature representations of neural networks using the power transform and\\nemploys additive Gaussian noise during training. Our experiments\\ncomprehensively demonstrate the effectiveness of normality normalization, in\\nregards to its generalization performance on an array of widely used model and\\ndataset combinations, its strong performance across various common factors of\\nvariation such as model width, depth, and training minibatch size, its\\nsuitability for usage wherever existing normalization layers are conventionally\\nused, and as a means to improving model robustness to random perturbations.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-01T17:47:44Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16056v1\", \"title\": \"Honey, I Shrunk the Language Model: Impact of Knowledge Distillation\\n  Methods on Performance and Explainability\", \"summary\": \"Artificial Intelligence (AI) has increasingly influenced modern society,\\nrecently in particular through significant advancements in Large Language\\nModels (LLMs). However, high computational and storage demands of LLMs still\\nlimit their deployment in resource-constrained environments. Knowledge\\ndistillation addresses this challenge by training a small student model from a\\nlarger teacher model. Previous research has introduced several distillation\\nmethods for both generating training data and for training the student model.\\nDespite their relevance, the effects of state-of-the-art distillation methods\\non model performance and explainability have not been thoroughly investigated\\nand compared. In this work, we enlarge the set of available methods by applying\\ncritique-revision prompting to distillation for data generation and by\\nsynthesizing existing methods for training. For these methods, we provide a\\nsystematic comparison based on the widely used Commonsense Question-Answering\\n(CQA) dataset. While we measure performance via student model accuracy, we\\nemploy a human-grounded study to evaluate explainability. We contribute new\\ndistillation methods and their comparison in terms of both performance and\\nexplainability. This should further advance the distillation of small language\\nmodels and, thus, contribute to broader applicability and faster diffusion of\\nLLM technology.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-22T17:32:48Z\"}"}

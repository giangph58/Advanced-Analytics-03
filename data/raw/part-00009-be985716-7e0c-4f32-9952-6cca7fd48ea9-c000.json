{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04406v1\", \"title\": \"YABLoCo: Yet Another Benchmark for Long Context Code Generation\", \"summary\": \"Large Language Models demonstrate the ability to solve various programming\\ntasks, including code generation. Typically, the performance of LLMs is\\nmeasured on benchmarks with small or medium-sized context windows of thousands\\nof lines of code. At the same time, in real-world software projects,\\nrepositories can span up to millions of LoC. This paper closes this gap by\\ncontributing to the long context code generation benchmark (YABLoCo). The\\nbenchmark featured a test set of 215 functions selected from four large\\nrepositories with thousands of functions. The dataset contained metadata of\\nfunctions, contexts of the functions with different levels of dependencies,\\ndocstrings, functions bodies, and call graphs for each repository. This paper\\npresents three key aspects of the contribution. First, the benchmark aims at\\nfunction body generation in large repositories in C and C++, two languages not\\ncovered by previous benchmarks. Second, the benchmark contains large\\nrepositories from 200K to 2,000K LoC. Third, we contribute a scalable\\nevaluation pipeline for efficient computing of the target metrics and a tool\\nfor visual analysis of generated code. Overall, these three aspects allow for\\nevaluating code generation in large repositories in C and C++.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.SE\", \"published\": \"2025-05-07T13:42:23Z\"}"}

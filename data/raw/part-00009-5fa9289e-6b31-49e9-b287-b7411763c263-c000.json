{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02211v1\", \"title\": \"FT-Transformer: Resilient and Reliable Transformer with End-to-End Fault\\n  Tolerant Attention\", \"summary\": \"Transformer models leverage self-attention mechanisms to capture complex\\ndependencies, demonstrating exceptional performance in various applications.\\nHowever, the long-duration high-load computations required for model inference\\nimpose stringent reliability demands on the computing platform, as soft errors\\nthat occur during execution can significantly degrade model performance.\\nExisting fault tolerance methods protect each operation separately using\\ndecoupled kernels, incurring substantial computational and memory overhead. In\\nthis paper, we propose a novel error-resilient framework for Transformer\\nmodels, integrating end-to-end fault tolerant attention (EFTA) to improve\\ninference reliability against soft errors. Our approach enables error detection\\nand correction within a fully fused attention kernel, reducing redundant data\\naccess and thereby mitigating memory faults. To further enhance error coverage\\nand reduce overhead, we design a hybrid fault tolerance scheme tailored for the\\nEFTA, introducing for the first time: 1) architecture-aware algorithm-based\\nfault tolerance (ABFT) using tensor checksum, which minimizes inter-thread\\ncommunication overhead on tensor cores during error detection; 2) selective\\nneuron value restriction, which selectively applies adaptive fault tolerance\\nconstraints to neuron values, balancing error coverage and overhead; 3) unified\\nverification, reusing checksums to streamline multiple computation steps into a\\nsingle verification process. Experimental results show that EFTA achieves up to\\n7.56x speedup over traditional methods with an average fault tolerance overhead\\nof 13.9%.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC,cs.AI,cs.LG\", \"published\": \"2025-04-03T02:05:08Z\"}"}

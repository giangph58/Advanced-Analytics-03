{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23697v1\", \"title\": \"A Low-complexity Structured Neural Network to Realize States of\\n  Dynamical Systems\", \"summary\": \"Data-driven learning is rapidly evolving and places a new perspective on\\nrealizing state-space dynamical systems. However, dynamical systems derived\\nfrom nonlinear ordinary differential equations (ODEs) suffer from limitations\\nin computational efficiency. Thus, this paper stems from data-driven learning\\nto advance states of dynamical systems utilizing a structured neural network\\n(StNN). The proposed learning technique also seeks to identify an optimal,\\nlow-complexity operator to solve dynamical systems, the so-called Hankel\\noperator, derived from time-delay measurements. Thus, we utilize the StNN based\\non the Hankel operator to solve dynamical systems as an alternative to existing\\ndata-driven techniques. We show that the proposed StNN reduces the number of\\nparameters and computational complexity compared with the conventional neural\\nnetworks and also with the classical data-driven techniques, such as Sparse\\nIdentification of Nonlinear Dynamics (SINDy) and Hankel Alternative view of\\nKoopman (HAVOK), which is commonly known as delay-Dynamic Mode\\nDecomposition(DMD) or Hankel-DMD. More specifically, we present numerical\\nsimulations to solve dynamical systems utilizing the StNN based on the Hankel\\noperator beginning from the fundamental Lotka-Volterra model, where we compare\\nthe StNN with the LEarning Across Dynamical Systems (LEADS), and extend our\\nanalysis to highly nonlinear and chaotic Lorenz systems, comparing the StNN\\nwith conventional neural networks, SINDy, and HAVOK. Hence, we show that the\\nproposed StNN paves the way for realizing state-space dynamical systems with a\\nlow-complexity learning algorithm, enabling prediction and understanding of\\nfuture states.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,math.DS\", \"published\": \"2025-03-31T03:52:38Z\"}"}
{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23715v1\", \"title\": \"HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video\\n  Generation\", \"summary\": \"Text-to-video (T2V) generation has made tremendous progress in generating\\ncomplicated scenes based on texts. However, human-object interaction (HOI)\\noften cannot be precisely generated by current T2V models due to the lack of\\nlarge-scale videos with accurate captions for HOI. To address this issue, we\\nintroduce HOIGen-1M, the first largescale dataset for HOI Generation,\\nconsisting of over one million high-quality videos collected from diverse\\nsources. In particular, to guarantee the high quality of videos, we first\\ndesign an efficient framework to automatically curate HOI videos using the\\npowerful multimodal large language models (MLLMs), and then the videos are\\nfurther cleaned by human annotators. Moreover, to obtain accurate textual\\ncaptions for HOI videos, we design a novel video description method based on a\\nMixture-of-Multimodal-Experts (MoME) strategy that not only generates\\nexpressive captions but also eliminates the hallucination by individual MLLM.\\nFurthermore, due to the lack of an evaluation framework for generated HOI\\nvideos, we propose two new metrics to assess the quality of generated videos in\\na coarse-to-fine manner. Extensive experiments reveal that current T2V models\\nstruggle to generate high-quality HOI videos and confirm that our HOIGen-1M\\ndataset is instrumental for improving HOI video generation. Project webpage is\\navailable at https://liuqi-creat.github.io/HOIGen.github.io.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-31T04:30:34Z\"}"}

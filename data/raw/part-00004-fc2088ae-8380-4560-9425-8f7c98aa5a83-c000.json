{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11083v1\", \"title\": \"QAMA: Quantum annealing multi-head attention operator with classical\\n  deep learning framework\", \"summary\": \"As large language models scale up, the conventional attention mechanism faces\\ncritical challenges of exponential growth in memory consumption and energy\\ncosts. Quantum annealing computing, with its inherent advantages in\\ncomputational efficiency and low energy consumption, offers an innovative\\ndirection for constructing novel deep learning architectures. This study\\nproposes the first Quantum Annealing-based Multi-head Attention (QAMA)\\nmechanism, achieving seamless compatibility with classical attention\\narchitectures through quadratic unconstrained binary optimization (QUBO)\\nmodeling of forward propagation and energy-based backpropagation. The method\\ninnovatively leverages the quantum bit interaction characteristics of Ising\\nmodels to optimize the conventional $O(n^2)$ spatiotemporal complexity into\\nlinear resource consumption. Integrated with the optical computing advantages\\nof coherent Ising machines (CIM), the system maintains millisecond-level\\nreal-time responsiveness while significantly reducing energy consumption. Our\\nkey contributions include: Theoretical proofs establish QAMA mathematical\\nequivalence to classical attention mechanisms; Dual optimization of multi-head\\nspecificity and long-range information capture via QUBO constraints; Explicit\\ngradient proofs for the Ising energy equation are utilized to implement\\ngradient conduction as the only path in the computational graph as a layer;\\nProposed soft selection mechanism overcoming traditional binary attention\\nlimitations to approximate continuous weights. Experiments on QBoson CPQC\\nquantum computer show QAMA achieves comparable accuracy to classical operators\\nwhile reducing inference time to millisecond level and improving solution\\nquality. This work pioneers architectural-level integration of quantum\\ncomputing and deep learning, applicable to any attention-based model, driving\\nparadigm innovation in AI foundational computing.\", \"main_category\": \"quant-ph\", \"categories\": \"quant-ph,cs.AI\", \"published\": \"2025-04-15T11:29:09Z\"}"}

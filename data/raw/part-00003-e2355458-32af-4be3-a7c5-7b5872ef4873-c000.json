{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05978v1\", \"title\": \"Smart Exploration in Reinforcement Learning using Bounded Uncertainty\\n  Models\", \"summary\": \"Reinforcement learning (RL) is a powerful tool for decision-making in\\nuncertain environments, but it often requires large amounts of data to learn an\\noptimal policy. We propose using prior model knowledge to guide the exploration\\nprocess to speed up this learning process. This model knowledge comes in the\\nform of a model set to which the true transition kernel and reward function\\nbelong. We optimize over this model set to obtain upper and lower bounds on the\\nQ-function, which are then used to guide the exploration of the agent. We\\nprovide theoretical guarantees on the convergence of the Q-function to the\\noptimal Q-function under the proposed class of exploring policies. Furthermore,\\nwe also introduce a data-driven regularized version of the model set\\noptimization problem that ensures the convergence of the class of exploring\\npolicies to the optimal policy. Lastly, we show that when the model set has a\\nspecific structure, namely the bounded-parameter MDP (BMDP) framework, the\\nregularized model set optimization problem becomes convex and simple to\\nimplement. In this setting, we also show that we obtain finite-time convergence\\nto the optimal policy under additional assumptions. We demonstrate the\\neffectiveness of the proposed exploration strategy in a simulation study. The\\nresults indicate that the proposed method can significantly speed up the\\nlearning process in reinforcement learning.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.SY,eess.SY\", \"published\": \"2025-04-08T12:33:38Z\"}"}

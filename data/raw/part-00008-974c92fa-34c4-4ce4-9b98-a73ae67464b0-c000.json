{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01476v1\", \"title\": \"Enhanced Cross-modal 3D Retrieval via Tri-modal Reconstruction\", \"summary\": \"Cross-modal 3D retrieval is a critical yet challenging task, aiming to\\nachieve bi-directional retrieval between 3D and text modalities. Current\\nmethods predominantly rely on a certain 3D representation (e.g., point cloud),\\nwith few exploiting the 2D-3D consistency and complementary relationships,\\nwhich constrains their performance. To bridge this gap, we propose to adopt\\nmulti-view images and point clouds to jointly represent 3D shapes, facilitating\\ntri-modal alignment (i.e., image, point, text) for enhanced cross-modal 3D\\nretrieval. Notably, we introduce tri-modal reconstruction to improve the\\ngeneralization ability of encoders. Given point features, we reconstruct image\\nfeatures under the guidance of text features, and vice versa. With well-aligned\\npoint cloud and multi-view image features, we aggregate them as multimodal\\nembeddings through fine-grained 2D-3D fusion to enhance geometric and semantic\\nunderstanding. Recognizing the significant noise in current datasets where many\\n3D shapes and texts share similar semantics, we employ hard negative\\ncontrastive training to emphasize harder negatives with greater significance,\\nleading to robust discriminative embeddings. Extensive experiments on the\\nText2Shape dataset demonstrate that our method significantly outperforms\\nprevious state-of-the-art methods in both shape-to-text and text-to-shape\\nretrieval tasks by a substantial margin.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-02T08:29:42Z\"}"}

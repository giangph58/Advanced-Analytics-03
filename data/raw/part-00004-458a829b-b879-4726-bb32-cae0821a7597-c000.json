{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03388v1\", \"title\": \"Memory-Efficient Distributed Unlearning\", \"summary\": \"Machine unlearning considers the removal of the contribution of a set of data\\npoints from a trained model. In a distributed setting, where a server\\norchestrates training using data available at a set of remote users, unlearning\\nis essential to cope with late-detected malicious or corrupted users. Existing\\ndistributed unlearning algorithms require the server to store all model updates\\nobserved in training, leading to immense storage overhead for preserving the\\nability to unlearn. In this work we study lossy compression schemes for\\nfacilitating distributed server-side unlearning with limited memory footprint.\\nWe propose memory-efficient distributed unlearning (MEDU), a hierarchical lossy\\ncompression scheme tailored for server-side unlearning, that integrates user\\nsparsification, differential thresholding, and random lattice coding, to\\nsubstantially reduce memory footprint. We rigorously analyze MEDU, deriving an\\nupper bound on the difference between the desired model that is trained from\\nscratch and the model unlearned from lossy compressed stored updates. Our bound\\noutperforms the state-of-the-art known bounds for non-compressed decentralized\\nserver-side unlearning, even when lossy compression is incorporated. We further\\nprovide a numerical study, which shows that suited lossy compression can enable\\ndistributed unlearning with notably reduced memory footprint at the server\\nwhile preserving the utility of the unlearned model.\", \"main_category\": \"eess.SP\", \"categories\": \"eess.SP\", \"published\": \"2025-05-06T10:10:18Z\"}"}

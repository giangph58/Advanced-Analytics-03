{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10433v1\", \"title\": \"MonoDiff9D: Monocular Category-Level 9D Object Pose Estimation via\\n  Diffusion Model\", \"summary\": \"Object pose estimation is a core means for robots to understand and interact\\nwith their environment. For this task, monocular category-level methods are\\nattractive as they require only a single RGB camera. However, current methods\\nrely on shape priors or CAD models of the intra-class known objects. We propose\\na diffusion-based monocular category-level 9D object pose generation method,\\nMonoDiff9D. Our motivation is to leverage the probabilistic nature of diffusion\\nmodels to alleviate the need for shape priors, CAD models, or depth sensors for\\nintra-class unknown object pose estimation. We first estimate coarse depth via\\nDINOv2 from the monocular image in a zero-shot manner and convert it into a\\npoint cloud. We then fuse the global features of the point cloud with the input\\nimage and use the fused features along with the encoded time step to condition\\nMonoDiff9D. Finally, we design a transformer-based denoiser to recover the\\nobject pose from Gaussian noise. Extensive experiments on two popular benchmark\\ndatasets show that MonoDiff9D achieves state-of-the-art monocular\\ncategory-level 9D object pose estimation accuracy without the need for shape\\npriors or CAD models at any stage. Our code will be made public at\\nhttps://github.com/CNJianLiu/MonoDiff9D.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.RO\", \"published\": \"2025-04-14T17:21:10Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06820v1\", \"title\": \"Regret Bounds for Robust Online Decision Making\", \"summary\": \"We propose a framework which generalizes \\\"decision making with structured\\nobservations\\\" by allowing robust (i.e. multivalued) models. In this framework,\\neach model associates each decision with a convex set of probability\\ndistributions over outcomes. Nature can choose distributions out of this set in\\nan arbitrary (adversarial) manner, that can be nonoblivious and depend on past\\nhistory. The resulting framework offers much greater generality than classical\\nbandits and reinforcement learning, since the realizability assumption becomes\\nmuch weaker and more realistic. We then derive a theory of regret bounds for\\nthis framework. Although our lower and upper bounds are not tight, they are\\nsufficient to fully characterize power-law learnability. We demonstrate this\\ntheory in two special cases: robust linear bandits and tabular robust online\\nreinforcement learning. In both cases, we derive regret bounds that improve\\nstate-of-the-art (except that we do not address computational efficiency).\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,I.2.6\", \"published\": \"2025-04-09T12:25:00Z\"}"}

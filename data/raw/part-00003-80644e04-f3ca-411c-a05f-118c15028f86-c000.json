{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05225v1\", \"title\": \"QualBench: Benchmarking Chinese LLMs with Localized Professional\\n  Qualifications for Vertical Domain Evaluation\", \"summary\": \"The rapid advancement of Chinese large language models (LLMs) underscores the\\nneed for domain-specific evaluations to ensure reliable applications. However,\\nexisting benchmarks often lack coverage in vertical domains and offer limited\\ninsights into the Chinese working context. Leveraging qualification exams as a\\nunified framework for human expertise evaluation, we introduce QualBench, the\\nfirst multi-domain Chinese QA benchmark dedicated to localized assessment of\\nChinese LLMs. The dataset includes over 17,000 questions across six vertical\\ndomains, with data selections grounded in 24 Chinese qualifications to closely\\nalign with national policies and working standards. Through comprehensive\\nevaluation, the Qwen2.5 model outperformed the more advanced GPT-4o, with\\nChinese LLMs consistently surpassing non-Chinese models, highlighting the\\nimportance of localized domain knowledge in meeting qualification requirements.\\nThe best performance of 75.26% reveals the current gaps in domain coverage\\nwithin model capabilities. Furthermore, we present the failure of LLM\\ncollaboration with crowdsourcing mechanisms and suggest the opportunities for\\nmulti-domain RAG knowledge enhancement and vertical domain LLM training with\\nFederated Learning.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-05-08T13:16:49Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20535v1\", \"title\": \"DeeP-Mod: Deep Dynamic Programming based Environment Modelling using\\n  Feature Extraction\", \"summary\": \"The DeeP-Mod framework builds an environment model using features from a Deep\\nDynamic Programming Network (DDPN), trained via a Deep Q-Network (DQN). While\\nDeep Q-Learning is effective in decision-making, state information is lost in\\ndeeper DQN layers due to mixed state-action representations. We address this by\\nusing Dynamic Programming (DP) to train a DDPN, where Value Iteration ensures\\nthe output represents state values, not state-action pairs. Extracting features\\nfrom the DDPN preserves state information, enabling task and action set\\nindependence. We show that a reduced DDPN can be trained using features\\nextracted from the original DDPN trained on an identical problem. This reduced\\nDDPN achieves faster convergence under noise and outperforms the original DDPN.\\nFinally, we introduce the DeeP-Mod framework, which creates an environment\\nmodel using the evolution of features extracted from a DDPN in response to\\nactions. A second DDPN, which learns directly from this feature model rather\\nthan raw states, can learn an effective feature-value representation and thus\\noptimal policy. A key advantage of DeeP-Mod is that an externally defined\\nenvironment model is not needed at any stage, making DDPN applicable to a wide\\nrange of environments.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-29T08:30:11Z\"}"}

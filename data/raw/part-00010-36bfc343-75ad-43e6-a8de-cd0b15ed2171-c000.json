{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02664v1\", \"title\": \"How humans evaluate AI systems for person detection in automatic train\\n  operation: Not all misses are alike\", \"summary\": \"If artificial intelligence (AI) is to be applied in safety-critical domains,\\nits performance needs to be evaluated reliably. The present study aimed to\\nunderstand how humans evaluate AI systems for person detection in automatic\\ntrain operation. In three experiments, participants saw image sequences of\\npeople moving in the vicinity of railway tracks. A simulated AI had highlighted\\nall detected people, sometimes correctly and sometimes not. Participants had to\\nprovide a numerical rating of the AI's performance and then verbally explain\\ntheir rating. The experiments varied several factors that might influence human\\nratings: the types and plausibility of AI mistakes, the number of affected\\nimages, the number of people present in an image, the position of people\\nrelevant to the tracks, and the methods used to elicit human evaluations. While\\nall these factors influenced human ratings, some effects were unexpected or\\ndeviated from normative standards. For instance, the factor with the strongest\\nimpact was people's position relative to the tracks, although participants had\\nexplicitly been instructed that the AI could not process such information.\\nTaken together, the results suggest that humans may sometimes evaluate more\\nthan the AI's performance on the assigned task. Such mismatches between AI\\ncapabilities and human expectations should be taken into consideration when\\nconducting safety audits of AI systems.\", \"main_category\": \"cs.HC\", \"categories\": \"cs.HC\", \"published\": \"2025-04-03T15:05:57Z\"}"}

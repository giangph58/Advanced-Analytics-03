{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05138v1\", \"title\": \"Guiding Evolutionary AutoEncoder Training with Activation-Based Pruning\\n  Operators\", \"summary\": \"This study explores a novel approach to neural network pruning using\\nevolutionary computation, focusing on simultaneously pruning the encoder and\\ndecoder of an autoencoder. We introduce two new mutation operators that use\\nlayer activations to guide weight pruning. Our findings reveal that one of\\nthese activation-informed operators outperforms random pruning, resulting in\\nmore efficient autoencoders with comparable performance to canonically trained\\nmodels. Prior work has established that autoencoder training is effective and\\nscalable with a spatial coevolutionary algorithm that cooperatively coevolves a\\npopulation of encoders with a population of decoders, rather than one\\nautoencoder. We evaluate how the same activity-guided mutation operators\\ntransfer to this context. We find that random pruning is better than guided\\npruning, in the coevolutionary setting. This suggests activation-based guidance\\nproves more effective in low-dimensional pruning environments, where\\nconstrained sample spaces can lead to deviations from true uniformity in\\nrandomization. Conversely, population-driven strategies enhance robustness by\\nexpanding the total pruning dimensionality, achieving statistically uniform\\nrandomness that better preserves system dynamics. We experiment with pruning\\naccording to different schedules and present best combinations of operator and\\nschedule for the canonical and coevolving populations cases.\", \"main_category\": \"cs.NE\", \"categories\": \"cs.NE,cs.AI\", \"published\": \"2025-05-08T11:21:29Z\"}"}

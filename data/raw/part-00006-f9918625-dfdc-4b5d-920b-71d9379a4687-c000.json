{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11373v1\", \"title\": \"Cancer-Myth: Evaluating AI Chatbot on Patient Questions with False\\n  Presuppositions\", \"summary\": \"Cancer patients are increasingly turning to large language models (LLMs) as a\\nnew form of internet search for medical information, making it critical to\\nassess how well these models handle complex, personalized questions. However,\\ncurrent medical benchmarks focus on medical exams or consumer-searched\\nquestions and do not evaluate LLMs on real patient questions with detailed\\nclinical contexts. In this paper, we first evaluate LLMs on cancer-related\\nquestions drawn from real patients, reviewed by three hematology oncology\\nphysicians. While responses are generally accurate, with GPT-4-Turbo scoring\\n4.13 out of 5, the models frequently fail to recognize or address false\\npresuppositions in the questions-posing risks to safe medical decision-making.\\nTo study this limitation systematically, we introduce Cancer-Myth, an\\nexpert-verified adversarial dataset of 585 cancer-related questions with false\\npresuppositions. On this benchmark, no frontier LLM -- including GPT-4o,\\nGemini-1.Pro, and Claude-3.5-Sonnet -- corrects these false presuppositions\\nmore than 30% of the time. Even advanced medical agentic methods do not prevent\\nLLMs from ignoring false presuppositions. These findings expose a critical gap\\nin the clinical reliability of LLMs and underscore the need for more robust\\nsafeguards in medical AI systems.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.CY\", \"published\": \"2025-04-15T16:37:32Z\"}"}

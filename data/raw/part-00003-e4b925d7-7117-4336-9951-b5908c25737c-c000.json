{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21303v1\", \"title\": \"Confidence in Large Language Model Evaluation: A Bayesian Approach to\\n  Limited-Sample Challenges\", \"summary\": \"Large language models (LLMs) exhibit probabilistic output characteristics,\\nyet conventional evaluation frameworks rely on deterministic scalar metrics.\\nThis study introduces a Bayesian approach for LLM capability assessment that\\nintegrates prior knowledge through probabilistic inference, addressing\\nlimitations under limited-sample regimes. By treating model capabilities as\\nlatent variables and leveraging a curated query set to induce discriminative\\nresponses, we formalize model ranking as a Bayesian hypothesis testing problem\\nover mutually exclusive capability intervals. Experimental evaluations with\\nGPT-series models demonstrate that the proposed method achieves superior\\ndiscrimination compared to conventional evaluation methods. Results indicate\\nthat even with reduced sample sizes, the approach maintains statistical\\nrobustness while providing actionable insights, such as probabilistic\\nstatements about a model's likelihood of surpassing specific baselines. This\\nwork advances LLM evaluation methodologies by bridging Bayesian inference with\\npractical constraints in real-world deployment scenarios.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-30T04:24:50Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15595v1\", \"title\": \"Grasping Deformable Objects via Reinforcement Learning with Cross-Modal\\n  Attention to Visuo-Tactile Inputs\", \"summary\": \"We consider the problem of grasping deformable objects with soft shells using\\na robotic gripper. Such objects have a center-of-mass that changes dynamically\\nand are fragile so prone to burst. Thus, it is difficult for robots to generate\\nappropriate control inputs not to drop or break the object while performing\\nmanipulation tasks. Multi-modal sensing data could help understand the grasping\\nstate through global information (e.g., shapes, pose) from visual data and\\nlocal information around the contact (e.g., pressure) from tactile data.\\nAlthough they have complementary information that can be beneficial to use\\ntogether, fusing them is difficult owing to their different properties.\\n  We propose a method based on deep reinforcement learning (DRL) that generates\\ncontrol inputs of a simple gripper from visuo-tactile sensing information. Our\\nmethod employs a cross-modal attention module in the encoder network and trains\\nit in a self-supervised manner using the loss function of the RL agent. With\\nthe multi-modal fusion, the proposed method can learn the representation for\\nthe DRL agent from the visuo-tactile sensory data. The experimental result\\nshows that cross-modal attention is effective to outperform other early and\\nlate data fusion methods across different environments including unseen robot\\nmotions and objects.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO\", \"published\": \"2025-04-22T05:22:31Z\"}"}

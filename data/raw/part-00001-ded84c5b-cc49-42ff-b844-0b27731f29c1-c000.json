{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03435v1\", \"title\": \"Robustness in AI-Generated Detection: Enhancing Resistance to\\n  Adversarial Attacks\", \"summary\": \"The rapid advancement of generative image technology has introduced\\nsignificant security concerns, particularly in the domain of face generation\\ndetection. This paper investigates the vulnerabilities of current AI-generated\\nface detection systems. Our study reveals that while existing detection methods\\noften achieve high accuracy under standard conditions, they exhibit limited\\nrobustness against adversarial attacks. To address these challenges, we propose\\nan approach that integrates adversarial training to mitigate the impact of\\nadversarial examples. Furthermore, we utilize diffusion inversion and\\nreconstruction to further enhance detection robustness. Experimental results\\ndemonstrate that minor adversarial perturbations can easily bypass existing\\ndetection systems, but our method significantly improves the robustness of\\nthese systems. Additionally, we provide an in-depth analysis of adversarial and\\nbenign examples, offering insights into the intrinsic characteristics of\\nAI-generated content. All associated code will be made publicly available in a\\ndedicated repository to facilitate further research and verification.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-06T11:19:01Z\"}"}

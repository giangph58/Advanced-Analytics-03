{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01450v1\", \"title\": \"CASCADE Your Datasets for Cross-Mode Knowledge Retrieval of Language\\n  Models\", \"summary\": \"Language models often struggle with cross-mode knowledge retrieval -- the\\nability to access knowledge learned in one format (mode) when queried in\\nanother. We demonstrate that models trained on multiple data sources (e.g.,\\nWikipedia and TinyStories) exhibit significantly reduced accuracy when\\nretrieving knowledge in a format different from its original training mode.\\nThis paper quantitatively investigates this phenomenon through a controlled\\nstudy of random token sequence memorization across different modes. We first\\nexplore dataset rewriting as a solution, revealing that effective cross-mode\\nretrieval requires prohibitively extensive rewriting efforts that follow a\\nsigmoid-like relationship. As an alternative, we propose CASCADE, a novel\\npretraining algorithm that uses cascading datasets with varying sequence\\nlengths to capture knowledge at different scales. Our experiments demonstrate\\nthat CASCADE outperforms dataset rewriting approaches, even when compressed\\ninto a single model with a unified loss function. This work provides both\\nqualitative evidence of cross-mode retrieval limitations and a practical\\nsolution to enhance language models' ability to access knowledge independently\\nof its presentational format.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL\", \"published\": \"2025-04-02T08:02:07Z\"}"}

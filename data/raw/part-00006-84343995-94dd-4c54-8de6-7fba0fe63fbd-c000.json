{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03155v1\", \"title\": \"Rethinking the Global Convergence of Softmax Policy Gradient with Linear\\n  Function Approximation\", \"summary\": \"Policy gradient (PG) methods have played an essential role in the empirical\\nsuccesses of reinforcement learning. In order to handle large state-action\\nspaces, PG methods are typically used with function approximation. In this\\nsetting, the approximation error in modeling problem-dependent quantities is a\\nkey notion for characterizing the global convergence of PG methods. We focus on\\nSoftmax PG with linear function approximation (referred to as\\n$\\\\texttt{Lin-SPG}$) and demonstrate that the approximation error is irrelevant\\nto the algorithm's global convergence even for the stochastic bandit setting.\\nConsequently, we first identify the necessary and sufficient conditions on the\\nfeature representation that can guarantee the asymptotic global convergence of\\n$\\\\texttt{Lin-SPG}$. Under these feature conditions, we prove that $T$\\niterations of $\\\\texttt{Lin-SPG}$ with a problem-specific learning rate result\\nin an $O(1/T)$ convergence to the optimal policy. Furthermore, we prove that\\n$\\\\texttt{Lin-SPG}$ with any arbitrary constant learning rate can ensure\\nasymptotic global convergence to the optimal policy.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-06T04:03:06Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01550v1\", \"title\": \"Representation Bending for Large Language Model Safety\", \"summary\": \"Large Language Models (LLMs) have emerged as powerful tools, but their\\ninherent safety risks - ranging from harmful content generation to broader\\nsocietal harms - pose significant challenges. These risks can be amplified by\\nthe recent adversarial attacks, fine-tuning vulnerabilities, and the increasing\\ndeployment of LLMs in high-stakes environments. Existing safety-enhancing\\ntechniques, such as fine-tuning with human feedback or adversarial training,\\nare still vulnerable as they address specific threats and often fail to\\ngeneralize across unseen attacks, or require manual system-level defenses. This\\npaper introduces RepBend, a novel approach that fundamentally disrupts the\\nrepresentations underlying harmful behaviors in LLMs, offering a scalable\\nsolution to enhance (potentially inherent) safety. RepBend brings the idea of\\nactivation steering - simple vector arithmetic for steering model's behavior\\nduring inference - to loss-based fine-tuning. Through extensive evaluation,\\nRepBend achieves state-of-the-art performance, outperforming prior methods such\\nas Circuit Breaker, RMU, and NPO, with up to 95% reduction in attack success\\nrates across diverse jailbreak benchmarks, all with negligible reduction in\\nmodel usability and general capabilities.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL,cs.CR\", \"published\": \"2025-04-02T09:47:01Z\"}"}

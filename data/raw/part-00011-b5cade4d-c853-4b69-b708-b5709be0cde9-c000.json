{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05086v1\", \"title\": \"Beyond Low-rank Decomposition: A Shortcut Approach for Efficient\\n  On-Device Learning\", \"summary\": \"On-device learning has emerged as a promising direction for AI development,\\nparticularly because of its potential to reduce latency issues and mitigate\\nprivacy risks associated with device-server communication, while improving\\nenergy efficiency. Despite these advantages, significant memory and\\ncomputational constraints still represent major challenges for its deployment.\\nDrawing on previous studies on low-rank decomposition methods that address\\nactivation memory bottlenecks in backpropagation, we propose a novel shortcut\\napproach as an alternative. Our analysis and experiments demonstrate that our\\nmethod can reduce activation memory usage, even up to $120.09\\\\times$ compared\\nto vanilla training, while also reducing overall training FLOPs up to\\n$1.86\\\\times$ when evaluated on traditional benchmarks.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-05-08T09:34:15Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21375v1\", \"title\": \"Synergy-CLIP: Extending CLIP with Multi-modal Integration for Robust\\n  Representation Learning\", \"summary\": \"Multi-modal representation learning has become a pivotal area in artificial\\nintelligence, enabling the integration of diverse modalities such as vision,\\ntext, and audio to solve complex problems. However, existing approaches\\npredominantly focus on bimodal interactions, such as image-text pairs, which\\nlimits their ability to fully exploit the richness of multi-modal data.\\nFurthermore, the integration of modalities in equal-scale environments remains\\nunderexplored due to the challenges of constructing large-scale, balanced\\ndatasets. In this study, we propose Synergy-CLIP, a novel framework that\\nextends the contrastive language-image pre-training (CLIP) architecture to\\nenhance multi-modal representation learning by integrating visual, textual, and\\naudio modalities. Unlike existing methods that focus on adapting individual\\nmodalities to vanilla-CLIP, Synergy-CLIP aligns and captures latent information\\nacross three modalities equally. To address the high cost of constructing\\nlarge-scale multi-modal datasets, we introduce VGG-sound+, a triple-modal\\ndataset designed to provide equal-scale representation of visual, textual, and\\naudio data. Synergy-CLIP is validated on various downstream tasks, including\\nzero-shot classification, where it outperforms existing baselines.\\nAdditionally, we introduce a missing modality reconstruction task,\\ndemonstrating Synergy-CLIP's ability to extract synergy among modalities in\\nrealistic application scenarios. These contributions provide a robust\\nfoundation for advancing multi-modal representation learning and exploring new\\nresearch directions.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-30T07:14:58Z\"}"}

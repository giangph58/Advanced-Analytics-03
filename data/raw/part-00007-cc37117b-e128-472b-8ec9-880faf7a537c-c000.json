{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19982v1\", \"title\": \"TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining\\n  Turn-Level Precision with Dialogue-Level Comparisons\", \"summary\": \"Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\\nLarge Language Models (LLMs), yet the evaluation methodologies for these\\nsystems remain insufficient for their growing sophistication. While traditional\\nautomatic metrics effectively assessed earlier modular systems, they focus\\nsolely on the dialogue level and cannot detect critical intermediate errors\\nthat can arise during user-agent interactions. In this paper, we introduce\\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\\ncomparisons. At turn level, we evaluate each response along three TOD-specific\\ndimensions: conversation cohesion, backend knowledge consistency, and policy\\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\\n2.4 and {\\\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\\nexhibits better alignment with human judgments than traditional and LLM-based\\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\\nTOD system evaluation, efficiently assessing both turn and system levels with a\\nplug-and-play framework for future research.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-28T16:57:17Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00649v1\", \"title\": \"Investigating Task Arithmetic for Zero-Shot Information Retrieval\", \"summary\": \"Large Language Models (LLMs) have shown impressive zero-shot performance\\nacross a variety of Natural Language Processing tasks, including document\\nre-ranking. However, their effectiveness degrades on unseen tasks and domains,\\nlargely due to shifts in vocabulary and word distributions. In this paper, we\\ninvestigate Task Arithmetic, a technique that combines the weights of LLMs\\npre-trained on different tasks or domains via simple mathematical operations,\\nsuch as addition or subtraction, to adapt retrieval models without requiring\\nadditional fine-tuning. Our method is able to synthesize diverse tasks and\\ndomain knowledge into a single model, enabling effective zero-shot adaptation\\nin different retrieval contexts. Extensive experiments on publicly available\\nscientific, biomedical, and multilingual datasets show that our method improves\\nstate-of-the-art re-ranking performance by up to 18% in NDCG@10 and 15% in\\nP@10. In addition to these empirical gains, our analysis provides insights into\\nthe strengths and limitations of Task Arithmetic as a practical strategy for\\nzero-shot learning and model adaptation. We make our code publicly available at\\nhttps://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR,cs.CL,cs.LG\", \"published\": \"2025-05-01T16:48:37Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10000v1\", \"title\": \"Do We Really Need Curated Malicious Data for Safety Alignment in\\n  Multi-modal Large Language Models?\", \"summary\": \"Multi-modal large language models (MLLMs) have made significant progress, yet\\ntheir safety alignment remains limited. Typically, current open-source MLLMs\\nrely on the alignment inherited from their language module to avoid harmful\\ngenerations. However, the lack of safety measures specifically designed for\\nmulti-modal inputs creates an alignment gap, leaving MLLMs vulnerable to\\nvision-domain attacks such as typographic manipulation. Current methods utilize\\na carefully designed safety dataset to enhance model defense capability, while\\nthe specific knowledge or patterns acquired from the high-quality dataset\\nremain unclear. Through comparison experiments, we find that the alignment gap\\nprimarily arises from data distribution biases, while image content, response\\nquality, or the contrastive behavior of the dataset makes little contribution\\nto boosting multi-modal safety. To further investigate this and identify the\\nkey factors in improving MLLM safety, we propose finetuning MLLMs on a small\\nset of benign instruct-following data with responses replaced by simple, clear\\nrejection sentences. Experiments show that, without the need for\\nlabor-intensive collection of high-quality malicious data, model safety can\\nstill be significantly improved, as long as a specific fraction of rejection\\ndata exists in the finetuning set, indicating the security alignment is not\\nlost but rather obscured during multi-modal pretraining or instruction\\nfinetuning. Simply correcting the underlying data bias could narrow the safety\\ngap in the vision domain.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.AI,cs.CL,cs.CV,cs.LG\", \"published\": \"2025-04-14T09:03:51Z\"}"}

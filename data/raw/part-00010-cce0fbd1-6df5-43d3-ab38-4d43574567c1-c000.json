{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01690v1\", \"title\": \"Token Pruning in Audio Transformers: Optimizing Performance and Decoding\\n  Patch Importance\", \"summary\": \"Vision Transformers (ViTs) have achieved state-of-the-art performance across\\nvarious computer vision tasks, but their high computational cost remains a\\nchallenge. Token pruning has been proposed to reduce this cost by selectively\\nremoving less important tokens. While effective in vision tasks by discarding\\nnon-object regions, applying this technique to audio tasks presents unique\\nchallenges, as distinguishing relevant from irrelevant regions in\\ntime-frequency representations is less straightforward. In this study, for the\\nfirst time, we applied token pruning to ViT-based audio classification models\\nusing Mel-spectrograms and analyzed the trade-offs between model performance\\nand computational cost: TopK token pruning can reduce MAC operations of\\nAudioMAE and AST by 30-40%, with less than a 1% drop in classification\\naccuracy. Our analysis reveals that while high-intensity tokens contribute\\nsignificantly to model accuracy, low-intensity tokens remain important. In\\nparticular, they play a more critical role in general audio classification\\ntasks than in speech-specific tasks.\", \"main_category\": \"cs.SD\", \"categories\": \"cs.SD,cs.AI,eess.AS\", \"published\": \"2025-04-02T12:44:38Z\"}"}

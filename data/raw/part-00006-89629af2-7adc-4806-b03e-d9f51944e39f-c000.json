{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00626v1\", \"title\": \"The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning\\n  (and How to Fix Them)\", \"summary\": \"Large language models (LLMs) that integrate multiple input roles (e.g.,\\nsystem instructions, user queries, external tool outputs) are increasingly\\nprevalent in practice. Ensuring that the model accurately distinguishes\\nmessages from each role -- a concept we call \\\\emph{role separation} -- is\\ncrucial for consistent multi-role behavior. Although recent work often targets\\nstate-of-the-art prompt injection defenses, it remains unclear whether such\\nmethods truly teach LLMs to differentiate roles or merely memorize known\\ntriggers. In this paper, we examine \\\\emph{role-separation learning}: the\\nprocess of teaching LLMs to robustly distinguish system and user tokens.\\nThrough a \\\\emph{simple, controlled experimental framework}, we find that\\nfine-tuned models often rely on two proxies for role identification: (1) task\\ntype exploitation, and (2) proximity to begin-of-text. Although data\\naugmentation can partially mitigate these shortcuts, it generally leads to\\niterative patching rather than a deeper fix. To address this, we propose\\nreinforcing \\\\emph{invariant signals} that mark role boundaries by adjusting\\ntoken-wise cues in the model's input encoding. In particular, manipulating\\nposition IDs helps the model learn clearer distinctions and reduces reliance on\\nsuperficial proxies. By focusing on this mechanism-centered perspective, our\\nwork illuminates how LLMs can more reliably maintain consistent multi-role\\nbehavior without merely memorizing known prompts or triggers.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,I.2\", \"published\": \"2025-05-01T16:06:16Z\"}"}

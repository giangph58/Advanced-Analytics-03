{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04435v1\", \"title\": \"FedBWO: Enhancing Communication Efficiency in Federated Learning\", \"summary\": \"Federated Learning (FL) is a distributed Machine Learning (ML) setup, where a\\nshared model is collaboratively trained by various clients using their local\\ndatasets while keeping the data private. Considering resource-constrained\\ndevices, FL clients often suffer from restricted transmission capacity. Aiming\\nto enhance the system performance, the communication between clients and server\\nneeds to be diminished. Current FL strategies transmit a tremendous amount of\\ndata (model weights) within the FL process, which needs a high communication\\nbandwidth. Considering resource constraints, increasing the number of clients\\nand, consequently, the amount of data (model weights) can lead to a bottleneck.\\nIn this paper, we introduce the Federated Black Widow Optimization (FedBWO)\\ntechnique to decrease the amount of transmitted data by transmitting only a\\nperformance score rather than the local model weights from clients. FedBWO\\nemploys the BWO algorithm to improve local model updates. The conducted\\nexperiments prove that FedBWO remarkably improves the performance of the global\\nmodel and the communication efficiency of the overall system. According to the\\nexperimental outcomes, FedBWO enhances the global model accuracy by an average\\nof 21% over FedAvg, and 12% over FedGWO. Furthermore, FedBWO dramatically\\ndecreases the communication cost compared to other methods.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-05-07T14:02:35Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19499v1\", \"title\": \"Graph Reinforcement Learning for QoS-Aware Load Balancing in Open Radio\\n  Access Networks\", \"summary\": \"Next-generation wireless cellular networks are expected to provide\\nunparalleled Quality-of-Service (QoS) for emerging wireless applications,\\nnecessitating strict performance guarantees, e.g., in terms of link-level data\\nrates. A critical challenge in meeting these QoS requirements is the prevention\\nof cell congestion, which involves balancing the load to ensure sufficient\\nradio resources are available for each cell to serve its designated User\\nEquipments (UEs). In this work, a novel QoS-aware Load Balancing (LB) approach\\nis developed to optimize the performance of Guaranteed Bit Rate (GBR) and Best\\nEffort (BE) traffic in a multi-band Open Radio Access Network (O-RAN) under QoS\\nand resource constraints. The proposed solution builds on Graph Reinforcement\\nLearning (GRL), a powerful framework at the intersection of Graph Neural\\nNetwork (GNN) and RL. The QoS-aware LB is modeled as a Markov Decision Process,\\nwith states represented as graphs. QoS consideration are integrated into both\\nstate representations and reward signal design. The LB agent is then trained\\nusing an off-policy dueling Deep Q Network (DQN) that leverages a GNN-based\\narchitecture. This design ensures the LB policy is invariant to the ordering of\\nnodes (UE or cell), flexible in handling various network sizes, and capable of\\naccounting for spatial node dependencies in LB decisions. Performance of the\\nGRL-based solution is compared with two baseline methods. Results show\\nsubstantial performance gains, including a $53\\\\%$ reduction in QoS violations\\nand a fourfold increase in the 5th percentile rate for BE traffic.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.IT,cs.LG,cs.NI,eess.SP,math.IT\", \"published\": \"2025-04-28T05:41:31Z\"}"}

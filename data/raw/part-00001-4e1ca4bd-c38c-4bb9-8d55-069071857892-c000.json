{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06205v1\", \"title\": \"HRMedSeg: Unlocking High-resolution Medical Image segmentation via\\n  Memory-efficient Attention Modeling\", \"summary\": \"High-resolution segmentation is critical for precise disease diagnosis by\\nextracting micro-imaging information from medical images. Existing\\ntransformer-based encoder-decoder frameworks have demonstrated remarkable\\nversatility and zero-shot performance in medical segmentation. While\\nbeneficial, they usually require huge memory costs when handling large-size\\nsegmentation mask predictions, which are expensive to apply to real-world\\nscenarios. To address this limitation, we propose a memory-efficient framework\\nfor high-resolution medical image segmentation, called HRMedSeg. Specifically,\\nwe first devise a lightweight gated vision transformer (LGViT) as our image\\nencoder to model long-range dependencies with linear complexity. Then, we\\ndesign an efficient cross-multiscale decoder (ECM-Decoder) to generate\\nhigh-resolution segmentation masks. Moreover, we utilize feature distillation\\nduring pretraining to unleash the potential of our proposed model. Extensive\\nexperiments reveal that HRMedSeg outperforms state-of-the-arts in diverse\\nhigh-resolution medical image segmentation tasks. In particular, HRMedSeg uses\\nonly 0.59GB GPU memory per batch during fine-tuning, demonstrating low training\\ncosts. Besides, when HRMedSeg meets the Segment Anything Model (SAM), our\\nHRMedSegSAM takes 0.61% parameters of SAM-H. The code is available at\\nhttps://github.com/xq141839/HRMedSeg.\", \"main_category\": \"eess.IV\", \"categories\": \"eess.IV,cs.CV\", \"published\": \"2025-04-08T16:48:57Z\"}"}

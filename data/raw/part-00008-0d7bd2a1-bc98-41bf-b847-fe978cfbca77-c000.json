{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15707v1\", \"title\": \"RePOPE: Impact of Annotation Errors on the POPE Benchmark\", \"summary\": \"Since data annotation is costly, benchmark datasets often incorporate labels\\nfrom established image datasets. In this work, we assess the impact of label\\nerrors in MSCOCO on the frequently used object hallucination benchmark POPE. We\\nre-annotate the benchmark images and identify an imbalance in annotation errors\\nacross different subsets. Evaluating multiple models on the revised labels,\\nwhich we denote as RePOPE, we observe notable shifts in model rankings,\\nhighlighting the impact of label quality. Code and data are available at\\nhttps://github.com/YanNeu/RePOPE .\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.LG\", \"published\": \"2025-04-22T08:47:59Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.09929v1\", \"title\": \"Moderate Actor-Critic Methods: Controlling Overestimation Bias via\\n  Expectile Loss\", \"summary\": \"Overestimation is a fundamental characteristic of model-free reinforcement\\nlearning (MF-RL), arising from the principles of temporal difference learning\\nand the approximation of the Q-function. To address this challenge, we propose\\na novel moderate target in the Q-function update, formulated as a convex\\noptimization of an overestimated Q-function and its lower bound. Our primary\\ncontribution lies in the efficient estimation of this lower bound through the\\nlower expectile of the Q-value distribution conditioned on a state. Notably,\\nour moderate target integrates seamlessly into state-of-the-art (SOTA) MF-RL\\nalgorithms, including Deep Deterministic Policy Gradient (DDPG) and Soft Actor\\nCritic (SAC). Experimental results validate the effectiveness of our moderate\\ntarget in mitigating overestimation bias in DDPG, SAC, and distributional RL\\nalgorithms.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-14T06:41:15Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10481v1\", \"title\": \"xVerify: Efficient Answer Verifier for Reasoning Model Evaluations\", \"summary\": \"With the release of the o1 model by OpenAI, reasoning models adopting slow\\nthinking strategies have gradually emerged. As the responses generated by such\\nmodels often include complex reasoning, intermediate steps, and\\nself-reflection, existing evaluation methods are often inadequate. They\\nstruggle to determine whether the LLM output is truly equivalent to the\\nreference answer, and also have difficulty identifying and extracting the final\\nanswer from long, complex responses. To address this issue, we propose xVerify,\\nan efficient answer verifier for reasoning model evaluations. xVerify\\ndemonstrates strong capability in equivalence judgment, enabling it to\\neffectively determine whether the answers produced by reasoning models are\\nequivalent to reference answers across various types of objective questions. To\\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\\nquestion-answer pairs generated by multiple LLMs across various datasets,\\nleveraging multiple reasoning models and challenging evaluation sets designed\\nspecifically for reasoning model assessment. A multi-round annotation process\\nis employed to ensure label accuracy. Based on the VAR dataset, we train\\nmultiple xVerify models of different scales. In evaluation experiments\\nconducted on both the test set and generalization set, all xVerify models\\nachieve overall F1 scores and accuracy exceeding 95\\\\%. Notably, the smallest\\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\\nvalidate the effectiveness and generalizability of xVerify.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-14T17:59:36Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11889v1\", \"title\": \"Rethinking LLM-Based Recommendations: A Query Generation-Based,\\n  Training-Free Approach\", \"summary\": \"Existing large language model LLM-based recommendation methods face several\\nchallenges, including inefficiency in handling large candidate pools,\\nsensitivity to item order within prompts (\\\"lost in the middle\\\" phenomenon) poor\\nscalability, and unrealistic evaluation due to random negative sampling. To\\naddress these issues, we propose a Query-to-Recommendation approach that\\nleverages LLMs to generate personalized queries for retrieving relevant items\\nfrom the entire candidate pool, eliminating the need for candidate\\npre-selection. This method can be integrated into an ID-based recommendation\\nsystem without additional training, enhances recommendation performance and\\ndiversity through LLMs' world knowledge, and performs well even for less\\npopular item groups. Experiments on three datasets show up to 57 percent\\nimprovement, with an average gain of 31 percent, demonstrating strong zero-shot\\nperformance and further gains when ensembled with existing models.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR,cs.CL\", \"published\": \"2025-04-16T09:17:45Z\"}"}

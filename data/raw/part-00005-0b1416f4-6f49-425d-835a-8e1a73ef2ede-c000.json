{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03574v1\", \"title\": \"LlamaFirewall: An open source guardrail system for building secure AI\\n  agents\", \"summary\": \"Large language models (LLMs) have evolved from simple chatbots into\\nautonomous agents capable of performing complex tasks such as editing\\nproduction code, orchestrating workflows, and taking higher-stakes actions\\nbased on untrusted inputs like webpages and emails. These capabilities\\nintroduce new security risks that existing security measures, such as model\\nfine-tuning or chatbot-focused guardrails, do not fully address. Given the\\nhigher stakes and the absence of deterministic solutions to mitigate these\\nrisks, there is a critical need for a real-time guardrail monitor to serve as a\\nfinal layer of defense, and support system level, use case specific safety\\npolicy definition and enforcement. We introduce LlamaFirewall, an open-source\\nsecurity focused guardrail framework designed to serve as a final layer of\\ndefense against security risks associated with AI Agents. Our framework\\nmitigates risks such as prompt injection, agent misalignment, and insecure code\\nrisks through three powerful guardrails: PromptGuard 2, a universal jailbreak\\ndetector that demonstrates clear state of the art performance; Agent Alignment\\nChecks, a chain-of-thought auditor that inspects agent reasoning for prompt\\ninjection and goal misalignment, which, while still experimental, shows\\nstronger efficacy at preventing indirect injections in general scenarios than\\npreviously proposed approaches; and CodeShield, an online static analysis\\nengine that is both fast and extensible, aimed at preventing the generation of\\ninsecure or dangerous code by coding agents. Additionally, we include\\neasy-to-use customizable scanners that make it possible for any developer who\\ncan write a regular expression or an LLM prompt to quickly update an agent's\\nsecurity guardrails.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.AI\", \"published\": \"2025-05-06T14:34:21Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19596v1\", \"title\": \"Towards Robust Multimodal Physiological Foundation Models: Handling\\n  Arbitrary Missing Modalities\", \"summary\": \"Multimodal physiological signals, such as EEG, ECG, EOG, and EMG, are crucial\\nfor healthcare and brain-computer interfaces. While existing methods rely on\\nspecialized architectures and dataset-specific fusion strategies, they struggle\\nto learn universal representations that generalize across datasets and handle\\nmissing modalities at inference time. To address these issues, we propose\\nPhysioOmni, a foundation model for multimodal physiological signal analysis\\nthat models both homogeneous and heterogeneous features to decouple multimodal\\nsignals and extract generic representations while maintaining compatibility\\nwith arbitrary missing modalities. PhysioOmni trains a decoupled multimodal\\ntokenizer, enabling masked signal pre-training via modality-invariant and\\nmodality-specific objectives. To ensure adaptability to diverse and incomplete\\nmodality combinations, the pre-trained encoders undergo resilient fine-tuning\\nwith prototype alignment on downstream datasets. Extensive experiments on four\\ndownstream tasks, emotion recognition, sleep stage classification, motor\\nprediction, and mental workload detection, demonstrate that PhysioOmni achieves\\nstate-of-the-art performance while maintaining strong robustness to missing\\nmodalities. Our code and model weights will be released.\", \"main_category\": \"eess.SP\", \"categories\": \"eess.SP,cs.LG\", \"published\": \"2025-04-28T09:00:04Z\"}"}

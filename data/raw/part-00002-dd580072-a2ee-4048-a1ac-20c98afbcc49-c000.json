{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12667v1\", \"title\": \"Two Tasks, One Goal: Uniting Motion and Planning for Excellent End To\\n  End Autonomous Driving Performance\", \"summary\": \"End-to-end autonomous driving has made impressive progress in recent years.\\nFormer end-to-end autonomous driving approaches often decouple planning and\\nmotion tasks, treating them as separate modules. This separation overlooks the\\npotential benefits that planning can gain from learning out-of-distribution\\ndata encountered in motion tasks. However, unifying these tasks poses\\nsignificant challenges, such as constructing shared contextual representations\\nand handling the unobservability of other vehicles' states. To address these\\nchallenges, we propose TTOG, a novel two-stage trajectory generation framework.\\nIn the first stage, a diverse set of trajectory candidates is generated, while\\nthe second stage focuses on refining these candidates through vehicle state\\ninformation. To mitigate the issue of unavailable surrounding vehicle states,\\nTTOG employs a self-vehicle data-trained state estimator, subsequently extended\\nto other vehicles. Furthermore, we introduce ECSA (equivariant context-sharing\\nscene adapter) to enhance the generalization of scene representations across\\ndifferent agents. Experimental results demonstrate that TTOG achieves\\nstate-of-the-art performance across both planning and motion tasks. Notably, on\\nthe challenging open-loop nuScenes dataset, TTOG reduces the L2 distance by\\n36.06\\\\%. Furthermore, on the closed-loop Bench2Drive dataset, our approach\\nachieves a 22\\\\% improvement in the driving score (DS), significantly\\noutperforming existing baselines.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-17T05:52:35Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03281v1\", \"title\": \"Physics-inspired Energy Transition Neural Network for Sequence Learning\", \"summary\": \"Recently, the superior performance of Transformers has made them a more\\nrobust and scalable solution for sequence modeling than traditional recurrent\\nneural networks (RNNs). However, the effectiveness of Transformer in capturing\\nlong-term dependencies is primarily attributed to their comprehensive\\npair-modeling process rather than inherent inductive biases toward sequence\\nsemantics. In this study, we explore the capabilities of pure RNNs and reassess\\ntheir long-term learning mechanisms. Inspired by the physics energy transition\\nmodels that track energy changes over time, we propose a effective recurrent\\nstructure called the``Physics-inspired Energy Transition Neural Network\\\"\\n(PETNN). We demonstrate that PETNN's memory mechanism effectively stores\\ninformation over long-term dependencies. Experimental results indicate that\\nPETNN outperforms transformer-based methods across various sequence tasks.\\nFurthermore, owing to its recurrent nature, PETNN exhibits significantly lower\\ncomplexity. Our study presents an optimal foundational recurrent architecture\\nand highlights the potential for developing effective recurrent neural networks\\nin fields currently dominated by Transformer.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-05-06T08:07:15Z\"}"}

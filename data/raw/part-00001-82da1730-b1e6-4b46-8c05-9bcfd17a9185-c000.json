{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12732v1\", \"title\": \"Validating LLM-Generated Relevance Labels for Educational Resource\\n  Search\", \"summary\": \"Manual relevance judgements in Information Retrieval are costly and require\\nexpertise, driving interest in using Large Language Models (LLMs) for automatic\\nassessment. While LLMs have shown promise in general web search scenarios,\\ntheir effectiveness for evaluating domain-specific search results, such as\\neducational resources, remains unexplored. To investigate different ways of\\nincluding domain-specific criteria in LLM prompts for relevance judgement, we\\ncollected and released a dataset of 401 human relevance judgements from a user\\nstudy involving teaching professionals performing search tasks related to\\nlesson planning. We compared three approaches to structuring these prompts: a\\nsimple two-aspect evaluation baseline from prior work on using LLMs as\\nrelevance judges, a comprehensive 12-dimensional rubric derived from\\neducational literature, and criteria directly informed by the study\\nparticipants. Using domain-specific frameworks, LLMs achieved strong agreement\\nwith human judgements (Cohen's $\\\\kappa$ up to 0.650), significantly\\noutperforming the baseline approach. The participant-derived framework proved\\nparticularly robust, with GPT-3.5 achieving $\\\\kappa$ scores of 0.639 and 0.613\\nfor 10-dimension and 5-dimension versions respectively. System-level evaluation\\nshowed that LLM judgements reliably identified top-performing retrieval\\napproaches (RBO scores 0.71-0.76) while maintaining reasonable discrimination\\nbetween systems (RBO 0.52-0.56). These findings suggest that LLMs can\\neffectively evaluate educational resources when prompted with domain-specific\\ncriteria, though performance varies with framework complexity and input\\nstructure.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR\", \"published\": \"2025-04-17T08:14:45Z\"}"}

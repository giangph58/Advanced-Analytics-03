{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06651v1\", \"title\": \"Collision avoidance from monocular vision trained with novel view\\n  synthesis\", \"summary\": \"Collision avoidance can be checked in explicit environment models such as\\nelevation maps or occupancy grids, yet integrating such models with a\\nlocomotion policy requires accurate state estimation. In this work, we consider\\nthe question of collision avoidance from an implicit environment model. We use\\nmonocular RGB images as inputs and train a collisionavoidance policy from\\nphotorealistic images generated by 2D Gaussian splatting. We evaluate the\\nresulting pipeline in realworld experiments under velocity commands that bring\\nthe robot on an intercept course with obstacles. Our results suggest that RGB\\nimages can be enough to make collision-avoidance decisions, both in the room\\nwhere training data was collected and in out-of-distribution environments.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO\", \"published\": \"2025-04-09T07:39:12Z\"}"}

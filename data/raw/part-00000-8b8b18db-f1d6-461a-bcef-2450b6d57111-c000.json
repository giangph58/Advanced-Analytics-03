{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10074v1\", \"title\": \"MMKB-RAG: A Multi-Modal Knowledge-Based Retrieval-Augmented Generation\\n  Framework\", \"summary\": \"Recent advancements in large language models (LLMs) and multi-modal LLMs have\\nbeen remarkable. However, these models still rely solely on their parametric\\nknowledge, which limits their ability to generate up-to-date information and\\nincreases the risk of producing erroneous content. Retrieval-Augmented\\nGeneration (RAG) partially mitigates these challenges by incorporating external\\ndata sources, yet the reliance on databases and retrieval systems can introduce\\nirrelevant or inaccurate documents, ultimately undermining both performance and\\nreasoning quality. In this paper, we propose Multi-Modal Knowledge-Based\\nRetrieval-Augmented Generation (MMKB-RAG), a novel multi-modal RAG framework\\nthat leverages the inherent knowledge boundaries of models to dynamically\\ngenerate semantic tags for the retrieval process. This strategy enables the\\njoint filtering of retrieved documents, retaining only the most relevant and\\naccurate references. Extensive experiments on knowledge-based visual\\nquestion-answering tasks demonstrate the efficacy of our approach: on the E-VQA\\ndataset, our method improves performance by +4.2\\\\% on the Single-Hop subset and\\n+0.4\\\\% on the full dataset, while on the InfoSeek dataset, it achieves gains of\\n+7.8\\\\% on the Unseen-Q subset, +8.2\\\\% on the Unseen-E subset, and +8.1\\\\% on the\\nfull dataset. These results highlight significant enhancements in both accuracy\\nand robustness over the current state-of-the-art MLLM and RAG frameworks.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-14T10:19:47Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19797v1\", \"title\": \"Dynamic Tsetlin Machine Accelerators for On-Chip Training at the Edge\\n  using FPGAs\", \"summary\": \"The increased demand for data privacy and security in machine learning (ML)\\napplications has put impetus on effective edge training on Internet-of-Things\\n(IoT) nodes. Edge training aims to leverage speed, energy efficiency and\\nadaptability within the resource constraints of the nodes. Deploying and\\ntraining Deep Neural Networks (DNNs)-based models at the edge, although\\naccurate, posit significant challenges from the back-propagation algorithm's\\ncomplexity, bit precision trade-offs, and heterogeneity of DNN layers. This\\npaper presents a Dynamic Tsetlin Machine (DTM) training accelerator as an\\nalternative to DNN implementations. DTM utilizes logic-based on-chip inference\\nwith finite-state automata-driven learning within the same Field Programmable\\nGate Array (FPGA) package. Underpinned on the Vanilla and Coalesced Tsetlin\\nMachine algorithms, the dynamic aspect of the accelerator design allows for a\\nrun-time reconfiguration targeting different datasets, model architectures, and\\nmodel sizes without resynthesis. This makes the DTM suitable for targeting\\nmultivariate sensor-based edge tasks. Compared to DNNs, DTM trains with fewer\\nmultiply-accumulates, devoid of derivative computation. It is a data-centric ML\\nalgorithm that learns by aligning Tsetlin automata with input data to form\\nlogical propositions enabling efficient Look-up-Table (LUT) mapping and frugal\\nBlock RAM usage in FPGA training implementations. The proposed accelerator\\noffers 2.54x more Giga operations per second per Watt (GOP/s per W) and uses 6x\\nless power than the next-best comparable design.\", \"main_category\": \"cs.AR\", \"categories\": \"cs.AR,cs.LG\", \"published\": \"2025-04-28T13:38:53Z\"}"}

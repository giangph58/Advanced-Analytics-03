{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10071v1\", \"title\": \"Pay Attention to What and Where? Interpretable Feature Extractor in\\n  Vision-based Deep Reinforcement Learning\", \"summary\": \"Current approaches in Explainable Deep Reinforcement Learning have\\nlimitations in which the attention mask has a displacement with the objects in\\nvisual input. This work addresses a spatial problem within traditional\\nConvolutional Neural Networks (CNNs). We propose the Interpretable Feature\\nExtractor (IFE) architecture, aimed at generating an accurate attention mask to\\nillustrate both \\\"what\\\" and \\\"where\\\" the agent concentrates on in the spatial\\ndomain. Our design incorporates a Human-Understandable Encoding module to\\ngenerate a fully interpretable attention mask, followed by an Agent-Friendly\\nEncoding module to enhance the agent's learning efficiency. These two\\ncomponents together form the Interpretable Feature Extractor for vision-based\\ndeep reinforcement learning to enable the model's interpretability. The\\nresulting attention mask is consistent, highly understandable by humans,\\naccurate in spatial dimension, and effectively highlights important objects or\\nlocations in visual input. The Interpretable Feature Extractor is integrated\\ninto the Fast and Data-efficient Rainbow framework, and evaluated on 57 ATARI\\ngames to show the effectiveness of the proposed approach on Spatial\\nPreservation, Interpretability, and Data-efficiency. Finally, we showcase the\\nversatility of our approach by incorporating the IFE into the Asynchronous\\nAdvantage Actor-Critic Model.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-14T10:18:34Z\"}"}

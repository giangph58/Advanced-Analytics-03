{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11130v1\", \"title\": \"Divergence of Empirical Neural Tangent Kernel in Classification Problems\", \"summary\": \"This paper demonstrates that in classification problems, fully connected\\nneural networks (FCNs) and residual neural networks (ResNets) cannot be\\napproximated by kernel logistic regression based on the Neural Tangent Kernel\\n(NTK) under overtraining (i.e., when training time approaches infinity).\\nSpecifically, when using the cross-entropy loss, regardless of how large the\\nnetwork width is (as long as it is finite), the empirical NTK diverges from the\\nNTK on the training samples as training time increases. To establish this\\nresult, we first demonstrate the strictly positive definiteness of the NTKs for\\nmulti-layer FCNs and ResNets. Then, we prove that during training, % with the\\ncross-entropy loss, the neural network parameters diverge if the smallest\\neigenvalue of the empirical NTK matrix (Gram matrix) with respect to training\\nsamples is bounded below by a positive constant. This behavior contrasts\\nsharply with the lazy training regime commonly observed in regression problems.\\nConsequently, using a proof by contradiction, we show that the empirical NTK\\ndoes not uniformly converge to the NTK across all times on the training samples\\nas the network width increases. We validate our theoretical results through\\nexperiments on both synthetic data and the MNIST classification task. This\\nfinding implies that NTK theory is not applicable in this context, with\\nsignificant theoretical implications for understanding neural networks in\\nclassification problems.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,stat.ML\", \"published\": \"2025-04-15T12:30:21Z\"}"}

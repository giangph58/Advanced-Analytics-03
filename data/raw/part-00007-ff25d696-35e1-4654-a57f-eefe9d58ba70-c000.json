{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15933v1\", \"title\": \"Low-Rank Adaptation of Neural Fields\", \"summary\": \"Processing visual data often involves small adjustments or sequences of\\nchanges, such as in image filtering, surface smoothing, and video storage.\\nWhile established graphics techniques like normal mapping and video compression\\nexploit redundancy to encode such small changes efficiently, the problem of\\nencoding small changes to neural fields (NF) -- neural network\\nparameterizations of visual or physical functions -- has received less\\nattention.\\n  We propose a parameter-efficient strategy for updating neural fields using\\nlow-rank adaptations (LoRA). LoRA, a method from the parameter-efficient\\nfine-tuning LLM community, encodes small updates to pre-trained models with\\nminimal computational overhead. We adapt LoRA to instance-specific neural\\nfields, avoiding the need for large pre-trained models yielding a pipeline\\nsuitable for low-compute hardware.\\n  We validate our approach with experiments in image filtering, video\\ncompression, and geometry editing, demonstrating its effectiveness and\\nversatility for representing neural field updates.\", \"main_category\": \"cs.GR\", \"categories\": \"cs.GR,cs.LG\", \"published\": \"2025-04-22T14:21:34Z\"}"}

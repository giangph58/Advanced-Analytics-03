{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10443v1\", \"title\": \"Multimodal Long Video Modeling Based on Temporal Dynamic Context\", \"summary\": \"Recent advances in Large Language Models (LLMs) have led to significant\\nbreakthroughs in video understanding. However, existing models still struggle\\nwith long video processing due to the context length constraint of LLMs and the\\nvast amount of information within the video. Although some recent methods are\\ndesigned for long video understanding, they often lose crucial information\\nduring token compression and struggle with additional modality like audio. In\\nthis work, we propose a dynamic long video encoding method utilizing the\\ntemporal relationship between frames, named Temporal Dynamic Context (TDC).\\nFirstly, we segment the video into semantically consistent scenes based on\\ninter-frame similarities, then encode each frame into tokens using visual-audio\\nencoders. Secondly, we propose a novel temporal context compressor to reduce\\nthe number of tokens within each segment. Specifically, we employ a query-based\\nTransformer to aggregate video, audio, and instruction text tokens into a\\nlimited set of temporal context tokens. Finally, we feed the static frame\\ntokens and the temporal context tokens into the LLM for video understanding.\\nFurthermore, to handle extremely long videos, we propose a training-free\\nchain-of-thought strategy that progressively extracts answers from multiple\\nvideo segments. These intermediate answers serve as part of the reasoning\\nprocess and contribute to the final answer. We conduct extensive experiments on\\ngeneral video understanding and audio-video understanding benchmarks, where our\\nmethod demonstrates strong performance. The code and models are available at\\nhttps://github.com/Hoar012/TDC-Video.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.CL,cs.LG,cs.MM\", \"published\": \"2025-04-14T17:34:06Z\"}"}

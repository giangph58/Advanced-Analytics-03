{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12795v1\", \"title\": \"EarthGPT-X: Enabling MLLMs to Flexibly and Comprehensively Understand\\n  Multi-Source Remote Sensing Imagery\", \"summary\": \"Recent advances in the visual-language area have developed natural\\nmulti-modal large language models (MLLMs) for spatial reasoning through visual\\nprompting. However, due to remote sensing (RS) imagery containing abundant\\ngeospatial information that differs from natural images, it is challenging to\\neffectively adapt natural spatial models to the RS domain. Moreover, current RS\\nMLLMs are limited in overly narrow interpretation levels and interaction\\nmanner, hindering their applicability in real-world scenarios. To address those\\nchallenges, a spatial MLLM named EarthGPT-X is proposed, enabling a\\ncomprehensive understanding of multi-source RS imagery, such as optical,\\nsynthetic aperture radar (SAR), and infrared. EarthGPT-X offers zoom-in and\\nzoom-out insight, and possesses flexible multi-grained interactive abilities.\\nMoreover, EarthGPT-X unifies two types of critical spatial tasks (i.e.,\\nreferring and grounding) into a visual prompting framework. To achieve these\\nversatile capabilities, several key strategies are developed. The first is the\\nmulti-modal content integration method, which enhances the interplay between\\nimages, visual prompts, and text instructions. Subsequently, a cross-domain\\none-stage fusion training strategy is proposed, utilizing the large language\\nmodel (LLM) as a unified interface for multi-source multi-task learning.\\nFurthermore, by incorporating a pixel perception module, the referring and\\ngrounding tasks are seamlessly unified within a single framework. In addition,\\nthe experiments conducted demonstrate the superiority of the proposed\\nEarthGPT-X in multi-grained tasks and its impressive flexibility in multi-modal\\ninteraction, revealing significant advancements of MLLM in the RS field.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-17T09:56:35Z\"}"}

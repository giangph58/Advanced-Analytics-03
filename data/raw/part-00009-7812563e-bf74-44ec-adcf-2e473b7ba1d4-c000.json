{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19475v1\", \"title\": \"Prisma: An Open Source Toolkit for Mechanistic Interpretability in\\n  Vision and Video\", \"summary\": \"Robust tooling and publicly available pre-trained models have helped drive\\nrecent advances in mechanistic interpretability for language models. However,\\nsimilar progress in vision mechanistic interpretability has been hindered by\\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\\nopen-source framework designed to accelerate vision mechanistic\\ninterpretability research, providing a unified toolkit for accessing 75+ vision\\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\\ncaching, circuit analysis tools, and visualization tools; and educational\\nresources. Our analysis reveals surprising findings, including that effective\\nvision SAEs can exhibit substantially lower sparsity patterns than language\\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\\nPrisma enables new research directions for understanding vision model internals\\nwhile lowering barriers to entry in this emerging field.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.LG\", \"published\": \"2025-04-28T04:31:24Z\"}"}

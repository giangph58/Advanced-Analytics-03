{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07803v1\", \"title\": \"A System for Comprehensive Assessment of RAG Frameworks\", \"summary\": \"Retrieval Augmented Generation (RAG) has emerged as a standard paradigm for\\nenhancing the factual accuracy and contextual relevance of Large Language\\nModels (LLMs) by integrating retrieval mechanisms. However, existing evaluation\\nframeworks fail to provide a holistic black-box approach to assessing RAG\\nsystems, especially in real-world deployment scenarios. To address this gap, we\\nintroduce SCARF (System for Comprehensive Assessment of RAG Frameworks), a\\nmodular and flexible evaluation framework designed to benchmark deployed RAG\\napplications systematically. SCARF provides an end-to-end, black-box evaluation\\nmethodology, enabling a limited-effort comparison across diverse RAG\\nframeworks. Our framework supports multiple deployment configurations and\\nfacilitates automated testing across vector databases and LLM serving\\nstrategies, producing a detailed performance report. Moreover, SCARF integrates\\npractical considerations such as response coherence, providing a scalable and\\nadaptable solution for researchers and industry professionals evaluating RAG\\napplications. Using the REST APIs interface, we demonstrate how SCARF can be\\napplied to real-world scenarios, showcasing its flexibility in assessing\\ndifferent RAG frameworks and configurations. SCARF is available at GitHub\\nrepository.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-10T14:41:34Z\"}"}

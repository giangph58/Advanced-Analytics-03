{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20894v1\", \"title\": \"Does Feedback Help in Bandits with Arm Erasures?\", \"summary\": \"We study a distributed multi-armed bandit (MAB) problem over arm erasure\\nchannels, motivated by the increasing adoption of MAB algorithms over\\ncommunication-constrained networks. In this setup, the learner communicates the\\nchosen arm to play to an agent over an erasure channel with probability\\n$\\\\epsilon \\\\in [0,1)$; if an erasure occurs, the agent continues pulling the\\nlast successfully received arm; the learner always observes the reward of the\\narm pulled. In past work, we considered the case where the agent cannot convey\\nfeedback to the learner, and thus the learner does not know whether the arm\\nplayed is the requested or the last successfully received one. In this paper,\\nwe instead consider the case where the agent can send feedback to the learner\\non whether the arm request was received, and thus the learner exactly knows\\nwhich arm was played. Surprisingly, we prove that erasure feedback does not\\nimprove the worst-case regret upper bound order over the previously studied\\nno-feedback setting. In particular, we prove a regret lower bound of\\n$\\\\Omega(\\\\sqrt{KT} + K / (1 - \\\\epsilon))$, where $K$ is the number of arms and\\n$T$ the time horizon, that matches no-feedback upper bounds up to logarithmic\\nfactors. We note however that the availability of feedback enables simpler\\nalgorithm designs that may achieve better constants (albeit not better order)\\nregret bounds; we design one such algorithm and evaluate its performance\\nnumerically.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,stat.ML\", \"published\": \"2025-04-29T16:10:05Z\"}"}

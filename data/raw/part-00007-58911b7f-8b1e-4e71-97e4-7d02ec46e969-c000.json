{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11134v1\", \"title\": \"Visual Re-Ranking with Non-Visual Side Information\", \"summary\": \"The standard approach for visual place recognition is to use global image\\ndescriptors to retrieve the most similar database images for a given query\\nimage. The results can then be further improved with re-ranking methods that\\nre-order the top scoring images. However, existing methods focus on re-ranking\\nbased on the same image descriptors that were used for the initial retrieval,\\nwhich we argue provides limited additional signal.\\n  In this work we propose Generalized Contextual Similarity Aggregation (GCSA),\\nwhich is a graph neural network-based re-ranking method that, in addition to\\nthe visual descriptors, can leverage other types of available side information.\\nThis can for example be other sensor data (such as signal strength of nearby\\nWiFi or BlueTooth endpoints) or geometric properties such as camera poses for\\ndatabase images. In many applications this information is already present or\\ncan be acquired with low effort. Our architecture leverages the concept of\\naffinity vectors to allow for a shared encoding of the heterogeneous\\nmulti-modal input. Two large-scale datasets, covering both outdoor and indoor\\nlocalization scenarios, are utilized for training and evaluation. In\\nexperiments we show significant improvement not only on image retrieval\\nmetrics, but also for the downstream visual localization task.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-15T12:37:16Z\"}"}

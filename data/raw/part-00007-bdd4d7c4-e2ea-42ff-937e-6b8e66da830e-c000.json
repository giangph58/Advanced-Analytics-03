{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21318v1\", \"title\": \"Phi-4-reasoning Technical Report\", \"summary\": \"We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that\\nachieves strong performance on complex reasoning tasks. Trained via supervised\\nfine-tuning of Phi-4 on carefully curated set of \\\"teachable\\\" prompts-selected\\nfor the right level of complexity and diversity-and reasoning demonstrations\\ngenerated using o3-mini, Phi-4-reasoning generates detailed reasoning chains\\nthat effectively leverage inference-time compute. We further develop\\nPhi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based\\nreinforcement learning that offers higher performance by generating longer\\nreasoning traces. Across a wide range of reasoning tasks, both models\\noutperform significantly larger open-weight models such as\\nDeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full\\nDeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and\\nscientific reasoning, coding, algorithmic problem solving, planning, and\\nspatial understanding. Interestingly, we observe a non-trivial transfer of\\nimprovements to general-purpose benchmarks as well. In this report, we provide\\ninsights into our training data, our training methodologies, and our\\nevaluations. We show that the benefit of careful data curation for supervised\\nfine-tuning (SFT) extends to reasoning language models, and can be further\\namplified by reinforcement learning (RL). Finally, our evaluation points to\\nopportunities for improving how we assess the performance and robustness of\\nreasoning models.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CL\", \"published\": \"2025-04-30T05:05:09Z\"}"}

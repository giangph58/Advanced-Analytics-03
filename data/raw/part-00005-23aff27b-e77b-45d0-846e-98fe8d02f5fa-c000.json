{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16046v1\", \"title\": \"Certified Mitigation of Worst-Case LLM Copyright Infringement\", \"summary\": \"The exposure of large language models (LLMs) to copyrighted material during\\npre-training raises concerns about unintentional copyright infringement post\\ndeployment. This has driven the development of \\\"copyright takedown\\\" methods,\\npost-training approaches aimed at preventing models from generating content\\nsubstantially similar to copyrighted ones. While current mitigation approaches\\nare somewhat effective for average-case risks, we demonstrate that they\\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\\nhighly effective inference-time approach that provides certified copyright\\ntakedown. Our method repeatedly interleaves quote detection with rewriting\\ntechniques to transform potentially infringing segments. By leveraging\\nefficient data sketches (Bloom filters), our approach enables scalable\\ncopyright screening even for large-scale real-world corpora. When quotes beyond\\na length threshold cannot be removed, the system can abstain from responding,\\noffering certified risk reduction. Experimental results show that BloomScrub\\nreduces infringement risk, preserves utility, and accommodates different levels\\nof enforcement stringency with adaptive abstention. Our results suggest that\\nlightweight, inference-time methods can be surprisingly effective for copyright\\nprevention.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-22T17:16:53Z\"}"}

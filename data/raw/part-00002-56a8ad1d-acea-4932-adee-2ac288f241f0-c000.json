{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11829v1\", \"title\": \"D\\u00e9j\\u00e0 Vu: Multilingual LLM Evaluation through the Lens of Machine\\n  Translation Evaluation\", \"summary\": \"Generation capabilities and language coverage of multilingual large language\\nmodels (mLLMs) are advancing rapidly. However, evaluation practices for\\ngenerative abilities of mLLMs are still lacking comprehensiveness, scientific\\nrigor, and consistent adoption across research labs, which undermines their\\npotential to meaningfully guide mLLM development. We draw parallels with\\nmachine translation (MT) evaluation, a field that faced similar challenges and\\nhas, over decades, developed transparent reporting standards and reliable\\nevaluations for multilingual generative models. Through targeted experiments\\nacross key stages of the generative evaluation pipeline, we demonstrate how\\nbest practices from MT evaluation can deepen the understanding of quality\\ndifferences between models. Additionally, we identify essential components for\\nrobust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are\\nrigorously assessed. We distill these insights into a checklist of actionable\\nrecommendations for mLLM research and development.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-16T07:38:19Z\"}"}

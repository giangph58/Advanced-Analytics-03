{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05126v1\", \"title\": \"Taming OOD Actions for Offline Reinforcement Learning: An\\n  Advantage-Based Approach\", \"summary\": \"Offline reinforcement learning (RL) aims to learn decision-making policies\\nfrom fixed datasets without online interactions, providing a practical solution\\nwhere online data collection is expensive or risky. However, offline RL often\\nsuffers from distribution shift, resulting in inaccurate evaluation and\\nsubstantial overestimation on out-of-distribution (OOD) actions. To address\\nthis, existing approaches incorporate conservatism by indiscriminately\\ndiscouraging all OOD actions, thereby hindering the agent's ability to\\ngeneralize and exploit beneficial ones. In this paper, we propose\\nAdvantage-based Diffusion Actor-Critic (ADAC), a novel method that\\nsystematically evaluates OOD actions using the batch-optimal value function.\\nBased on this evaluation, ADAC defines an advantage function to modulate the\\nQ-function update, enabling more precise assessment of OOD action quality. We\\ndesign a custom PointMaze environment and collect datasets to visually reveal\\nthat advantage modulation can effectively identify and select superior OOD\\nactions. Extensive experiments show that ADAC achieves state-of-the-art\\nperformance on almost all tasks in the D4RL benchmark, with particularly clear\\nmargins on the more challenging tasks.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-08T10:57:28Z\"}"}

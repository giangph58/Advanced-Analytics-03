{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01472v1\", \"title\": \"ANNEXE: Unified Analyzing, Answering, and Pixel Grounding for Egocentric\\n  Interaction\", \"summary\": \"Egocentric interaction perception is one of the essential branches in\\ninvestigating human-environment interaction, which lays the basis for\\ndeveloping next-generation intelligent systems. However, existing egocentric\\ninteraction understanding methods cannot yield coherent textual and pixel-level\\nresponses simultaneously according to user queries, which lacks flexibility for\\nvarying downstream application requirements. To comprehend egocentric\\ninteractions exhaustively, this paper presents a novel task named Egocentric\\nInteraction Reasoning and pixel Grounding (Ego-IRG). Taking an egocentric image\\nwith the query as input, Ego-IRG is the first task that aims to resolve the\\ninteractions through three crucial steps: analyzing, answering, and pixel\\ngrounding, which results in fluent textual and fine-grained pixel-level\\nresponses. Another challenge is that existing datasets cannot meet the\\nconditions for the Ego-IRG task. To address this limitation, this paper creates\\nthe Ego-IRGBench dataset based on extensive manual efforts, which includes over\\n20k egocentric images with 1.6 million queries and corresponding multimodal\\nresponses about interactions. Moreover, we design a unified ANNEXE model to\\ngenerate text- and pixel-level outputs utilizing multimodal large language\\nmodels, which enables a comprehensive interpretation of egocentric\\ninteractions. The experiments on the Ego-IRGBench exhibit the effectiveness of\\nour ANNEXE model compared with other works.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-02T08:24:35Z\"}"}

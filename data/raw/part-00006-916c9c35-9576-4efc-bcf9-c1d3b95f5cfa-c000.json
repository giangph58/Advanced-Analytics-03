{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20020v1\", \"title\": \"Modular Machine Learning: An Indispensable Path towards New-Generation\\n  Large Language Models\", \"summary\": \"Large language models (LLMs) have dramatically advanced machine learning\\nresearch including natural language processing, computer vision, data mining,\\netc., yet they still exhibit critical limitations in reasoning, factual\\nconsistency, and interpretability. In this paper, we introduce a novel learning\\nparadigm -- Modular Machine Learning (MML) -- as an essential approach toward\\nnew-generation LLMs. MML decomposes the complex structure of LLMs into three\\ninterdependent components: modular representation, modular model, and modular\\nreasoning, aiming to enhance LLMs' capability of counterfactual reasoning,\\nmitigating hallucinations, as well as promoting fairness, safety, and\\ntransparency. Specifically, the proposed MML paradigm can: i) clarify the\\ninternal working mechanism of LLMs through the disentanglement of semantic\\ncomponents; ii) allow for flexible and task-adaptive model design; iii) enable\\ninterpretable and logic-driven decision-making process. We present a feasible\\nimplementation of MML-based LLMs via leveraging advanced techniques such as\\ndisentangled representation learning, neural architecture search and\\nneuro-symbolic learning. We critically identify key challenges, such as the\\nintegration of continuous neural and discrete symbolic processes, joint\\noptimization, and computational scalability, present promising future research\\ndirections that deserve further exploration. Ultimately, the integration of the\\nMML paradigm with LLMs has the potential to bridge the gap between statistical\\n(deep) learning and formal (logical) reasoning, thereby paving the way for\\nrobust, adaptable, and trustworthy AI systems across a wide range of real-world\\napplications.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-28T17:42:02Z\"}"}

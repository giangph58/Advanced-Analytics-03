{"value":"{\"aid\": \"http://arxiv.org/abs/2503.21699v1\", \"title\": \"MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX\", \"summary\": \"Frontier models have either been language-only or have primarily focused on\\nvision and language modalities. Although recent advancements in models with\\nvision and audio understanding capabilities have shown substantial progress,\\nthe field lacks a standardized evaluation framework for thoroughly assessing\\ntheir cross-modality perception performance. We introduce MAVERIX~(Multimodal\\nAudio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and\\n2,556 questions explicitly designed to evaluate multimodal models through tasks\\nthat necessitate close integration of video and audio information. MAVERIX\\nuniquely provides models with audiovisual tasks, closely mimicking the\\nmultimodal perceptual experiences available to humans during inference and\\ndecision-making processes. To our knowledge, MAVERIX is the first benchmark\\naimed explicitly at assessing comprehensive audiovisual integration.\\nExperiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show\\nperformance approaching human levels (around 70% accuracy), while human experts\\nreach near-ceiling performance (95.1%). With standardized evaluation protocols,\\na rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a\\nchallenging testbed for advancing audiovisual multimodal intelligence.\", \"main_category\": \"cs.SD\", \"categories\": \"cs.SD,cs.AI,cs.CV\", \"published\": \"2025-03-27T17:04:33Z\"}"}

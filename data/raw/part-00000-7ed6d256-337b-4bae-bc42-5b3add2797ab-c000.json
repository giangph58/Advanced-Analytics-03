{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10359v1\", \"title\": \"DICE: A Framework for Dimensional and Contextual Evaluation of Language\\n  Models\", \"summary\": \"Language models (LMs) are increasingly being integrated into a wide range of\\napplications, yet the modern evaluation paradigm does not sufficiently reflect\\nhow they are actually being used. Current evaluations rely on benchmarks that\\noften lack direct applicability to the real-world contexts in which LMs are\\nbeing deployed. To address this gap, we propose Dimensional and Contextual\\nEvaluation (DICE), an approach that evaluates LMs on granular,\\ncontext-dependent dimensions. In this position paper, we begin by examining the\\ninsufficiency of existing LM benchmarks, highlighting their limited\\napplicability to real-world use cases. Next, we propose a set of granular\\nevaluation parameters that capture dimensions of LM behavior that are more\\nmeaningful to stakeholders across a variety of application domains.\\nSpecifically, we introduce the concept of context-agnostic parameters - such as\\nrobustness, coherence, and epistemic honesty - and context-specific parameters\\nthat must be tailored to the specific contextual constraints and demands of\\nstakeholders choosing to deploy LMs into a particular setting. We then discuss\\npotential approaches to operationalize this evaluation framework, finishing\\nwith the opportunities and challenges DICE presents to the LM evaluation\\nlandscape. Ultimately, this work serves as a practical and approachable\\nstarting point for context-specific and stakeholder-relevant evaluation of LMs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.HC\", \"published\": \"2025-04-14T16:08:13Z\"}"}

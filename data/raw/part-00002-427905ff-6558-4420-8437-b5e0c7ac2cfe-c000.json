{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04535v1\", \"title\": \"Communication-Efficient Federated Fine-Tuning of Language Models via\\n  Dynamic Update Schedules\", \"summary\": \"Federated learning (FL) makes it possible to train models on data that would\\notherwise remain untapped and inaccessible. Simultaneously, pre-trained\\nlanguage models (LMs) have emerged as indispensable tools in modern workflows.\\nThese models exhibit extraordinary capabilities and are easily adapted to\\ndownstream tasks. This opens one of the most exciting frontiers in FL:\\nfine-tuning LMs. However, a persistent challenge in FL is the frequent, rigid\\ncommunication of parameters, a problem which is magnified by the sheer size of\\nthese modern models. Currently, the FedOpt family of algorithms is the\\nprevailing approach in FL, though it relies on fixed, heuristic intervals for\\nmodel synchronization. Recently, the FDA algorithm introduced a dynamic\\nalternative by monitoring training progress, but it came with its own\\ndrawbacks; namely, a hard-to-tune threshold parameter and a rigid\\nsynchronization scheme. In this work, we introduce the FDA-Opt family of\\nalgorithms -- a unified generalization that extends the principles behind both\\nFDA and FedOpt, while resolving their core limitations. We evaluate our\\napproach on fine-tuning LMs across a range of downstream NLP tasks, and\\ndemonstrate that it consistently outperforms FedOpt -- even when FDA-Opt\\noperates under hyper-parameter settings originally optimized for its\\ncompetitors. In other words, we show that FDA-Opt is a practical, drop-in\\nreplacement for FedOpt in modern FL libraries and systems: it requires no\\nadditional configuration and delivers superior performance out of the box.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-07T16:13:21Z\"}"}

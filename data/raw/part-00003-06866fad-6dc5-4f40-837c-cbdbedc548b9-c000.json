{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21625v1\", \"title\": \"Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn\\n  Instruction-Following Ability\", \"summary\": \"The ability to follow instructions accurately is fundamental for Large\\nLanguage Models (LLMs) to serve as reliable agents in real-world applications.\\nWhile existing instruction-following benchmarks are either single-turn or\\nintroduce new requirements in each turn without allowing self-correction,\\nMeeseeks simulates realistic human-LLM interactions through an iterative\\nfeedback process. This design enables models to self-correct based on specific\\nrequirement failures, better reflecting real-world user-end usage patterns. The\\nbenchmark implements a comprehensive evaluation system with 38 capability tags\\norganized across three dimensions: Intent Recognition, Granular Content\\nValidation, and Output Structure Validation. Through rigorous evaluation across\\nLLMs, Meeseeks provides valuable insights into LLMs' instruction-following\\ncapabilities in practical applications.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-30T13:28:19Z\"}"}

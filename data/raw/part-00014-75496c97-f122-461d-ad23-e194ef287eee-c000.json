{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03668v1\", \"title\": \"Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time\", \"summary\": \"This paper proposes an integration of temporal logical reasoning and\\nPartially Observable Markov Decision Processes (POMDPs) to achieve\\ninterpretable decision-making under uncertainty with macro-actions. Our method\\nleverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus\\n(EC) to generate \\\\emph{persistent} (i.e., constant) macro-actions, which guide\\nMonte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon,\\nsignificantly reducing inference time while ensuring robust performance. Such\\nmacro-actions are learnt via Inductive Logic Programming (ILP) from a few\\ntraces of execution (belief-action pairs), thus eliminating the need for\\nmanually designed heuristics and requiring only the specification of the POMDP\\ntransition model. In the Pocman and Rocksample benchmark scenarios, our learned\\nmacro-actions demonstrate increased expressiveness and generality when compared\\nto time-independent heuristics, indeed offering substantial computational\\nefficiency improvements.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-05-06T16:08:55Z\"}"}

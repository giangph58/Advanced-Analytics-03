{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15271v1\", \"title\": \"Eagle 2.5: Boosting Long-Context Post-Training for Frontier\\n  Vision-Language Models\", \"summary\": \"We introduce Eagle 2.5, a family of frontier vision-language models (VLMs)\\nfor long-context multimodal learning. Our work addresses the challenges in long\\nvideo comprehension and high-resolution image understanding, introducing a\\ngeneralist framework for both tasks. The proposed training framework\\nincorporates Automatic Degrade Sampling and Image Area Preservation, two\\ntechniques that preserve contextual integrity and visual details. The framework\\nalso includes numerous efficiency optimizations in the pipeline for\\nlong-context data training. Finally, we propose Eagle-Video-110K, a novel\\ndataset that integrates both story-level and clip-level annotations,\\nfacilitating long-video understanding. Eagle 2.5 demonstrates substantial\\nimprovements on long-context multimodal benchmarks, providing a robust solution\\nto the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B\\nachieves 72.4% on Video-MME with 512 input frames, matching the results of\\ntop-tier commercial model such as GPT-4o and large-scale open-source models\\nlike Qwen2.5-VL-72B and InternVL2.5-78B.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-21T17:57:28Z\"}"}

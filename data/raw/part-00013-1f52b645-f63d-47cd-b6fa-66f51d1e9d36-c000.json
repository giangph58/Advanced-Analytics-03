{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04421v1\", \"title\": \"LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders\", \"summary\": \"Modeling ultra-long user behavior sequences is critical for capturing both\\nlong- and short-term preferences in industrial recommender systems. Existing\\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\\nincuring upstream-downstream inconsistency and computational inefficiency. In\\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\\nfor stabilizing attention over long contexts, (ii) a token merge module with\\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\\ncomplexity, and (iii) a series of engineering optimizations, including training\\nwith mixed-precision and activation recomputation, KV cache serving, and the\\nfully synchronous model training and serving framework for unified GPU-based\\ndense and sparse parameter updates. LONGER consistently outperforms strong\\nbaselines in both offline metrics and online A/B testing in both advertising\\nand e-commerce services at ByteDance, validating its consistent effectiveness\\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\\nmore than 10 influential scenarios at ByteDance, serving billion users.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR\", \"published\": \"2025-05-07T13:54:26Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10045v1\", \"title\": \"CHARM: Calibrating Reward Models With Chatbot Arena Scores\", \"summary\": \"Reward models (RMs) play a crucial role in Reinforcement Learning from Human\\nFeedback by serving as proxies for human preferences in aligning large language\\nmodels. In this paper, we identify a model preference bias in RMs, where they\\nsystematically assign disproportionately high scores to responses from certain\\npolicy models. This bias distorts ranking evaluations and leads to unfair\\njudgments. To address this issue, we propose a calibration method named CHatbot\\nArena calibrated Reward Modeling (CHARM) that leverages Elo scores from the\\nChatbot Arena leaderboard to mitigate RM overvaluation. We also introduce a\\nMismatch Degree metric to measure this preference bias. Our approach is\\ncomputationally efficient, requiring only a small preference dataset for\\ncontinued training of the RM. We conduct extensive experiments on reward model\\nbenchmarks and human preference alignment. Results demonstrate that our\\ncalibrated RMs (1) achieve improved evaluation accuracy on RM-Bench and the\\nChat-Hard domain of RewardBench, and (2) exhibit a stronger correlation with\\nhuman preferences by producing scores more closely aligned with Elo rankings.\\nBy mitigating model preference bias, our method provides a generalizable and\\nefficient solution for building fairer and more reliable reward models.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.LG\", \"published\": \"2025-04-14T09:51:09Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10917v1\", \"title\": \"Towards A Universal Graph Structural Encoder\", \"summary\": \"Recent advancements in large-scale pre-training have shown the potential to\\nlearn generalizable representations for downstream tasks. In the graph domain,\\nhowever, capturing and transferring structural information across different\\ngraph domains remains challenging, primarily due to the inherent differences in\\ntopological patterns across various contexts. Additionally, most existing\\nmodels struggle to capture the complexity of rich graph structures, leading to\\ninadequate exploration of the embedding space. To address these challenges, we\\npropose GFSE, a universal graph structural encoder designed to capture\\ntransferable structural patterns across diverse domains such as molecular\\ngraphs, social networks, and citation networks. GFSE is the first cross-domain\\ngraph structural encoder pre-trained with multiple self-supervised learning\\nobjectives. Built on a Graph Transformer, GFSE incorporates attention\\nmechanisms informed by graph inductive bias, enabling it to encode intricate\\nmulti-level and fine-grained topological features. The pre-trained GFSE\\nproduces generic and theoretically expressive positional and structural\\nencoding for graphs, which can be seamlessly integrated with various downstream\\ngraph feature encoders, including graph neural networks for vectorized features\\nand Large Language Models for text-attributed graphs. Comprehensive experiments\\non synthetic and real-world datasets demonstrate GFSE's capability to\\nsignificantly enhance the model's performance while requiring substantially\\nless task-specific fine-tuning. Notably, GFSE achieves state-of-the-art\\nperformance in 81.6% evaluated cases, spanning diverse graph models and\\ndatasets, highlighting its potential as a powerful and versatile encoder for\\ngraph-structured data.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-15T06:57:26Z\"}"}

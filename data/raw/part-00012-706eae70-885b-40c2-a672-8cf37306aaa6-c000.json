{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15785v1\", \"title\": \"WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World\\n  Model-based LLM Agents\", \"summary\": \"Can we build accurate world models out of large language models (LLMs)? How\\ncan world models benefit LLM agents? The gap between the prior knowledge of\\nLLMs and the specified environment's dynamics usually bottlenecks LLMs'\\nperformance as world models. To bridge the gap, we propose a training-free\\n\\\"world alignment\\\" that learns an environment's symbolic knowledge complementary\\nto LLMs. The symbolic knowledge covers action rules, knowledge graphs, and\\nscene graphs, which are extracted by LLMs from exploration trajectories and\\nencoded into executable codes to regulate LLM agents' policies. We further\\npropose an RL-free, model-based agent \\\"WALL-E 2.0\\\" through the model-predictive\\ncontrol (MPC) framework. Unlike classical MPC requiring costly optimization on\\nthe fly, we adopt an LLM agent as an efficient look-ahead optimizer of future\\nsteps' actions by interacting with the neurosymbolic world model. While the LLM\\nagent's strong heuristics make it an efficient planner in MPC, the quality of\\nits planned actions is also secured by the accurate predictions of the aligned\\nworld model. They together considerably improve learning efficiency in a new\\nenvironment. On open-world challenges in Mars (Minecraft like) and ALFWorld\\n(embodied indoor environments), WALL-E 2.0 significantly outperforms existing\\nmethods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and\\nby at least 61.7% in score. In ALFWorld, it achieves a new record 98% success\\nrate after only 4 iterations.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-22T10:58:27Z\"}"}

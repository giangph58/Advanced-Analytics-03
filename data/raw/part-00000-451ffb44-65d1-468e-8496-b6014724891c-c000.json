{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06006v1\", \"title\": \"Optuna vs Code Llama: Are LLMs a New Paradigm for Hyperparameter Tuning?\", \"summary\": \"Optimal hyperparameter selection is critical for maximizing neural network\\nperformance, especially as models grow in complexity. This work investigates\\nthe viability of using large language models (LLMs) for hyperparameter\\noptimization by employing a fine-tuned version of Code Llama. Through\\nparameter-efficient fine-tuning using LoRA, we adapt the LLM to generate\\naccurate and efficient hyperparameter recommendations tailored to diverse\\nneural network architectures. Unlike traditional methods such as Optuna, which\\nrely on exhaustive trials, the proposed approach achieves competitive or\\nsuperior results in terms of Root Mean Square Error (RMSE) while significantly\\nreducing computational overhead. Our approach highlights that LLM-based\\noptimization not only matches state-of-the-art methods like Tree-structured\\nParzen Estimators but also accelerates the tuning process. This positions LLMs\\nas a promising alternative to conventional optimization techniques,\\nparticularly for rapid experimentation. Furthermore, the ability to generate\\nhyperparameters in a single inference step makes this method particularly\\nwell-suited for resource-constrained environments such as edge devices and\\nmobile applications, where computational efficiency is paramount. The results\\nconfirm that LLMs, beyond their efficiency, offer substantial time savings and\\ncomparable stability, underscoring their value in advancing machine learning\\nworkflows. All generated hyperparameters are included in the LEMUR Neural\\nNetwork (NN) Dataset, which is publicly available and serves as an open-source\\nbenchmark for hyperparameter optimization research.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.NE\", \"published\": \"2025-04-08T13:15:47Z\"}"}

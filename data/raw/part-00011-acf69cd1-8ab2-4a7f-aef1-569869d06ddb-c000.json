{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04371v1\", \"title\": \"Extending a Quantum Reinforcement Learning Exploration Policy with Flags\\n  to Connect Four\", \"summary\": \"Action selection based on flags is a Reinforcement Learning (RL) exploration\\npolicy that improves the exploration of the state space through the use of\\nflags, which can identify the most promising actions to take in each state. The\\nquantum counterpart of this exploration policy further improves upon this by\\ntaking advantage of a quadratic speedup for sampling flagged actions. This\\napproach has already been successfully employed for the game of Checkers. In\\nthis work, we describe the application of this method to the context of Connect\\nFour, in order to study its performance in a different setting, which can lead\\nto a better generalization of the technique. We also kept track of a metric\\nthat wasn't taken into account in previous work: the average number of\\niterations to obtain a flagged action. Since going second is a significant\\ndisadvantage in Connect Four, we also had the intent of exploring how this more\\ncomplex scenario would impact the performance of our approach. The experiments\\ninvolved training and testing classical and quantum RL agents that played\\neither going first or going second against a Randomized Negamax opponent. The\\nresults showed that both flagged exploration policies were clearly superior to\\na simple epsilon-greedy policy. Furthermore, the quantum agents did in fact\\nsample flagged actions in less iterations. Despite obtaining tagged actions\\nmore consistently, the win rates between the classical and quantum versions of\\nthe approach were identical, which could be due to the simplicity of the\\ntraining scenario chosen.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-07T12:44:59Z\"}"}

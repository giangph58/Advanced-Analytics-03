{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05441v1\", \"title\": \"GesPrompt: Leveraging Co-Speech Gestures to Augment LLM-Based\\n  Interaction in Virtual Reality\", \"summary\": \"Large Language Model (LLM)-based copilots have shown great potential in\\nExtended Reality (XR) applications. However, the user faces challenges when\\ndescribing the 3D environments to the copilots due to the complexity of\\nconveying spatial-temporal information through text or speech alone. To address\\nthis, we introduce GesPrompt, a multimodal XR interface that combines co-speech\\ngestures with speech, allowing end-users to communicate more naturally and\\naccurately with LLM-based copilots in XR environments. By incorporating\\ngestures, GesPrompt extracts spatial-temporal reference from co-speech\\ngestures, reducing the need for precise textual prompts and minimizing\\ncognitive load for end-users. Our contributions include (1) a workflow to\\nintegrate gesture and speech input in the XR environment, (2) a prototype VR\\nsystem that implements the workflow, and (3) a user study demonstrating its\\neffectiveness in improving user communication in VR environments.\", \"main_category\": \"cs.HC\", \"categories\": \"cs.HC\", \"published\": \"2025-05-08T17:31:28Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19720v1\", \"title\": \"Taming the Titans: A Survey of Efficient LLM Inference Serving\", \"summary\": \"Large Language Models (LLMs) for Generative AI have achieved remarkable\\nprogress, evolving into sophisticated and versatile tools widely adopted across\\nvarious domains and applications. However, the substantial memory overhead\\ncaused by their vast number of parameters, combined with the high computational\\ndemands of the attention mechanism, poses significant challenges in achieving\\nlow latency and high throughput for LLM inference services. Recent\\nadvancements, driven by groundbreaking research, have significantly accelerated\\nprogress in this field. This paper provides a comprehensive survey of these\\nmethods, covering fundamental instance-level approaches, in-depth cluster-level\\nstrategies, emerging scenario directions, and other miscellaneous but important\\nareas. At the instance level, we review model placement, request scheduling,\\ndecoding length prediction, storage management, and the disaggregation\\nparadigm. At the cluster level, we explore GPU cluster deployment,\\nmulti-instance load balancing, and cloud service solutions. For emerging\\nscenarios, we organize the discussion around specific tasks, modules, and\\nauxiliary methods. To ensure a holistic overview, we also highlight several\\nniche yet critical areas. Finally, we outline potential research directions to\\nfurther advance the field of LLM inference serving.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.DC,cs.LG\", \"published\": \"2025-04-28T12:14:02Z\"}"}

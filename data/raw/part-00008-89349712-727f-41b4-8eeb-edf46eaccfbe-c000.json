{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12951v1\", \"title\": \"Are Retrials All You Need? Enhancing Large Language Model Reasoning\\n  Without Verbalized Feedback\", \"summary\": \"Recent advancements in large language models (LLMs) have catalyzed the\\ndevelopment of general-purpose autonomous agents, demonstrating remarkable\\nperformance in complex reasoning tasks across various domains. This surge has\\nspurred the evolution of a plethora of prompt-based reasoning frameworks. A\\nrecent focus has been on iterative reasoning strategies that refine outputs\\nthrough self-evaluation and verbalized feedback. However, these strategies\\nrequire additional computational complexity to enable models to recognize and\\ncorrect their mistakes, leading to a significant increase in their cost. In\\nthis work, we introduce the concept of ``retrials without feedback'', an\\nembarrassingly simple yet powerful mechanism for enhancing reasoning frameworks\\nby allowing LLMs to retry problem-solving attempts upon identifying incorrect\\nanswers. Unlike conventional iterative refinement methods, our method does not\\nrequire explicit self-reflection or verbalized feedback, simplifying the\\nrefinement process. Our findings indicate that simpler retrial-based approaches\\noften outperform more sophisticated reasoning frameworks, suggesting that the\\nbenefits of complex methods may not always justify their computational costs.\\nBy challenging the prevailing assumption that more intricate reasoning\\nstrategies inherently lead to better performance, our work offers new insights\\ninto how simpler, more efficient approaches can achieve optimal results. So,\\nare retrials all you need?\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-17T13:52:48Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12285v1\", \"title\": \"BitNet b1.58 2B4T Technical Report\", \"summary\": \"We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\\ntrillion tokens, the model has been rigorously evaluated across benchmarks\\ncovering language understanding, mathematical reasoning, coding proficiency,\\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\\nachieves performance on par with leading open-weight, full-precision LLMs of\\nsimilar size, while offering significant advantages in computational\\nefficiency, including substantially reduced memory footprint, energy\\nconsumption, and decoding latency. To facilitate further research and adoption,\\nthe model weights are released via Hugging Face along with open-source\\ninference implementations for both GPU and CPU architectures.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-16T17:51:43Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19682v1\", \"title\": \"Explaining Vision GNNs: A Semantic and Visual Analysis of Graph-based\\n  Image Classification\", \"summary\": \"Graph Neural Networks (GNNs) have emerged as an efficient alternative to\\nconvolutional approaches for vision tasks such as image classification,\\nleveraging patch-based representations instead of raw pixels. These methods\\nconstruct graphs where image patches serve as nodes, and edges are established\\nbased on patch similarity or classification relevance. Despite their\\nefficiency, the explainability of GNN-based vision models remains\\nunderexplored, even though graphs are naturally interpretable. In this work, we\\nanalyze the semantic consistency of the graphs formed at different layers of\\nGNN-based image classifiers, focusing on how well they preserve object\\nstructures and meaningful relationships. A comprehensive analysis is presented\\nby quantifying the extent to which inter-layer graph connections reflect\\nsemantic similarity and spatial coherence. Explanations from standard and\\nadversarial settings are also compared to assess whether they reflect the\\nclassifiers' robustness. Additionally, we visualize the flow of information\\nacross layers through heatmap-based visualization techniques, thereby\\nhighlighting the models' explainability. Our findings demonstrate that the\\ndecision-making processes of these models can be effectively explained, while\\nalso revealing that their reasoning does not necessarily align with human\\nperception, especially in deeper layers.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG\", \"published\": \"2025-04-28T11:13:40Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01345v1\", \"title\": \"Breaking BERT: Gradient Attack on Twitter Sentiment Analysis for\\n  Targeted Misclassification\", \"summary\": \"Social media platforms like Twitter have increasingly relied on Natural\\nLanguage Processing NLP techniques to analyze and understand the sentiments\\nexpressed in the user generated content. One such state of the art NLP model is\\nBidirectional Encoder Representations from Transformers BERT which has been\\nwidely adapted in sentiment analysis. BERT is susceptible to adversarial\\nattacks. This paper aims to scrutinize the inherent vulnerabilities of such\\nmodels in Twitter sentiment analysis. It aims to formulate a framework for\\nconstructing targeted adversarial texts capable of deceiving these models,\\nwhile maintaining stealth. In contrast to conventional methodologies, such as\\nImportance Reweighting, this framework core idea resides in its reliance on\\ngradients to prioritize the importance of individual words within the text. It\\nuses a whitebox approach to attain fine grained sensitivity, pinpointing words\\nthat exert maximal influence on the classification outcome. This paper is\\norganized into three interdependent phases. It starts with fine-tuning a\\npre-trained BERT model on Twitter data. It then analyzes gradients of the model\\nto rank words on their importance, and iteratively replaces those with feasible\\ncandidates until an acceptable solution is found. Finally, it evaluates the\\neffectiveness of the adversarial text against the custom trained sentiment\\nclassification model. This assessment would help in gauging the capacity of the\\nadversarial text to successfully subvert classification without raising any\\nalarm.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-02T04:21:19Z\"}"}

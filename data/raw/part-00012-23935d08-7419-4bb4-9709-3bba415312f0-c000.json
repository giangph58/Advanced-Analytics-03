{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03323v1\", \"title\": \"Unraveling the Rainbow: can value-based methods schedule?\", \"summary\": \"Recently, deep reinforcement learning has emerged as a promising approach for\\nsolving complex combinatorial optimization problems. Broadly, deep\\nreinforcement learning methods fall into two categories: policy-based and\\nvalue-based. While value-based approaches have achieved notable success in\\ndomains such as the Arcade Learning Environment, the combinatorial optimization\\ncommunity has predominantly favored policy-based methods, often overlooking the\\npotential of value-based algorithms. In this work, we conduct a comprehensive\\nempirical evaluation of value-based algorithms, including the deep q-network\\nand several of its advanced extensions, within the context of two complex\\ncombinatorial problems: the job-shop and the flexible job-shop scheduling\\nproblems, two fundamental challenges with multiple industrial applications. Our\\nresults challenge the assumption that policy-based methods are inherently\\nsuperior for combinatorial optimization. We show that several value-based\\napproaches can match or even outperform the widely adopted proximal policy\\noptimization algorithm, suggesting that value-based strategies deserve greater\\nattention from the combinatorial optimization community. Our code is openly\\navailable at: https://github.com/AJ-Correa/Unraveling-the-Rainbow.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-06T08:51:17Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15266v1\", \"title\": \"Roll the dice & look before you leap: Going beyond the creative limits\\n  of next-token prediction\", \"summary\": \"We design a suite of minimal algorithmic tasks that are a loose abstraction\\nof open-ended real-world tasks. This allows us to cleanly and controllably\\nquantify the creative limits of the present-day language model. Much like\\nreal-world tasks that require a creative, far-sighted leap of thought, our\\ntasks require an implicit, open-ended stochastic planning step that either (a)\\ndiscovers new connections in an abstract knowledge graph (like in wordplay,\\ndrawing analogies, or research) or (b) constructs new patterns (like in\\ndesigning math problems or new proteins). In these tasks, we empirically and\\nconceptually argue how next-token learning is myopic and memorizes excessively;\\ncomparatively, multi-token approaches, namely teacherless training and\\ndiffusion models, excel in producing diverse and original output. Secondly, in\\nour tasks, we find that to elicit randomness from the Transformer without\\nhurting coherence, it is better to inject noise right at the input layer (via a\\nmethod we dub hash-conditioning) rather than defer to temperature sampling from\\nthe output layer. Thus, our work offers a principled, minimal test-bed for\\nanalyzing open-ended creative skills, and offers new arguments for going beyond\\nnext-token learning and softmax-based sampling. We make part of the code\\navailable under https://github.com/chenwu98/algorithmic-creativity\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL\", \"published\": \"2025-04-21T17:47:46Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15960v1\", \"title\": \"The Value Problem for Multiple-Environment MDPs with Parity Objective\", \"summary\": \"We consider multiple-environment Markov decision processes (MEMDP), which\\nconsist of a finite set of MDPs over the same state space, representing\\ndifferent scenarios of transition structure and probability. The value of a\\nstrategy is the probability to satisfy the objective, here a parity objective,\\nin the worst-case scenario, and the value of an MEMDP is the supremum of the\\nvalues achievable by a strategy.\\n  We show that deciding whether the value is 1 is a PSPACE-complete problem,\\nand even in P when the number of environments is fixed, along with new insights\\nto the almost-sure winning problem, which is to decide if there exists a\\nstrategy with value 1. Pure strategies are sufficient for theses problems,\\nwhereas randomization is necessary in general when the value is smaller than 1.\\nWe present an algorithm to approximate the value, running in double exponential\\nspace. Our results are in contrast to the related model of partially-observable\\nMDPs where all these problems are known to be undecidable.\", \"main_category\": \"cs.LO\", \"categories\": \"cs.LO\", \"published\": \"2025-04-22T14:58:09Z\"}"}

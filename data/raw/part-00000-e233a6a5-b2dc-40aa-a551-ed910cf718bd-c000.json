{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23905v1\", \"title\": \"Boosting MLLM Reasoning with Text-Debiased Hint-GRPO\", \"summary\": \"MLLM reasoning has drawn widespread research for its excellent\\nproblem-solving capability. Current reasoning methods fall into two types: PRM,\\nwhich supervises the intermediate reasoning steps, and ORM, which supervises\\nthe final results. Recently, DeepSeek-R1 has challenged the traditional view\\nthat PRM outperforms ORM, which demonstrates strong generalization performance\\nusing an ORM method (i.e., GRPO). However, current MLLM's GRPO algorithms still\\nstruggle to handle challenging and complex multimodal reasoning tasks (e.g.,\\nmathematical reasoning). In this work, we reveal two problems that impede the\\nperformance of GRPO on the MLLM: Low data utilization and Text-bias. Low data\\nutilization refers to that GRPO cannot acquire positive rewards to update the\\nMLLM on difficult samples, and text-bias is a phenomenon that the MLLM bypasses\\nimage condition and solely relies on text condition for generation after GRPO\\ntraining. To tackle these problems, this work proposes Hint-GRPO that improves\\ndata utilization by adaptively providing hints for samples of varying\\ndifficulty, and text-bias calibration that mitigates text-bias by calibrating\\nthe token prediction logits with image condition in test-time. Experiment\\nresults on three base MLLMs across eleven datasets demonstrate that our\\nproposed methods advance the reasoning capability of original MLLM by a large\\nmargin, exhibiting superior performance to existing MLLM reasoning methods. Our\\ncode is available at https://github.com/hqhQAQ/Hint-GRPO.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-31T09:54:55Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05225v1\", \"title\": \"Vision-Language Model Predictive Control for Manipulation Planning and\\n  Trajectory Generation\", \"summary\": \"Model Predictive Control (MPC) is a widely adopted control paradigm that\\nleverages predictive models to estimate future system states and optimize\\ncontrol inputs accordingly. However, while MPC excels in planning and control,\\nit lacks the capability for environmental perception, leading to failures in\\ncomplex and unstructured scenarios. To address this limitation, we introduce\\nVision-Language Model Predictive Control (VLMPC), a robotic manipulation\\nplanning framework that integrates the perception power of vision-language\\nmodels (VLMs) with MPC. VLMPC utilizes a conditional action sampling module\\nthat takes a goal image or language instruction as input and leverages VLM to\\ngenerate candidate action sequences. These candidates are fed into a video\\nprediction model that simulates future frames based on the actions. In\\naddition, we propose an enhanced variant, Traj-VLMPC, which replaces video\\nprediction with motion trajectory generation to reduce computational complexity\\nwhile maintaining accuracy. Traj-VLMPC estimates motion dynamics conditioned on\\nthe candidate actions, offering a more efficient alternative for long-horizon\\ntasks and real-time applications. Both VLMPC and Traj-VLMPC select the optimal\\naction sequence using a VLM-based hierarchical cost function that captures both\\npixel-level and knowledge-level consistency between the current observation and\\nthe task input. We demonstrate that both approaches outperform existing\\nstate-of-the-art methods on public benchmarks and achieve excellent performance\\nin various real-world robotic manipulation tasks. Code is available at\\nhttps://github.com/PPjmchen/VLMPC.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO\", \"published\": \"2025-04-07T16:13:09Z\"}"}

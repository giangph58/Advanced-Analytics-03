{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15917v1\", \"title\": \"Towards Test Generation from Task Description for Mobile Testing with\\n  Multi-modal Reasoning\", \"summary\": \"In Android GUI testing, generating an action sequence for a task that can be\\nreplayed as a test script is common. Generating sequences of actions and\\nrespective test scripts from task goals described in natural language can\\neliminate the need for manually writing test scripts. However, existing\\napproaches based on large language models (LLM) often struggle with identifying\\nthe final action, and either end prematurely or continue past the final screen.\\nIn this paper, we introduce VisiDroid, a multi-modal, LLM-based, multi-agent\\nframework that iteratively determines the next action and leverages visual\\nimages of screens to detect the task's completeness. The multi-modal approach\\nenhances our model in two significant ways. First, this approach enables it to\\navoid prematurely terminating a task when textual content alone provides\\nmisleading indications of task completion. Additionally, visual input helps the\\ntool avoid errors when changes in the GUI do not directly affect functionality\\ntoward task completion, such as adjustments to font sizes or colors. Second,\\nthe multi-modal approach also ensures the tool not progress beyond the final\\nscreen, which might lack explicit textual indicators of task completion but\\ncould display a visual element indicating task completion, which is common in\\nGUI apps. Our evaluation shows that VisiDroid achieves an accuracy of 87.3%,\\noutperforming the best baseline relatively by 23.5%. We also demonstrate that\\nour multi-modal framework with images and texts enables the LLM to better\\ndetermine when a task is completed.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE\", \"published\": \"2025-04-22T14:02:57Z\"}"}

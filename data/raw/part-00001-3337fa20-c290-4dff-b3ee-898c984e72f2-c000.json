{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04858v1\", \"title\": \"Don't Lag, RAG: Training-Free Adversarial Detection Using RAG\", \"summary\": \"Adversarial patch attacks pose a major threat to vision systems by embedding\\nlocalized perturbations that mislead deep models. Traditional defense methods\\noften require retraining or fine-tuning, making them impractical for real-world\\ndeployment. We propose a training-free Visual Retrieval-Augmented Generation\\n(VRAG) framework that integrates Vision-Language Models (VLMs) for adversarial\\npatch detection. By retrieving visually similar patches and images that\\nresemble stored attacks in a continuously expanding database, VRAG performs\\ngenerative reasoning to identify diverse attack types, all without additional\\ntraining or fine-tuning. We extensively evaluate open-source large-scale VLMs,\\nincluding Qwen-VL-Plus, Qwen2.5-VL-72B, and UI-TARS-72B-DPO, alongside\\nGemini-2.0, a closed-source model. Notably, the open-source UI-TARS-72B-DPO\\nmodel achieves up to 95 percent classification accuracy, setting a new\\nstate-of-the-art for open-source adversarial patch detection. Gemini-2.0\\nattains the highest overall accuracy, 98 percent, but remains closed-source.\\nExperimental results demonstrate VRAG's effectiveness in identifying a variety\\nof adversarial patches with minimal human annotation, paving the way for\\nrobust, practical defenses against evolving adversarial patch attacks.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.LG\", \"published\": \"2025-04-07T09:14:47Z\"}"}

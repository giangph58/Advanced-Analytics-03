{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10063v1\", \"title\": \"Hallucination Detection in LLMs via Topological Divergence on Attention\\n  Graphs\", \"summary\": \"Hallucination, i.e., generating factually incorrect content, remains a\\ncritical challenge for large language models (LLMs). We introduce TOHA, a\\nTOpology-based HAllucination detector in the RAG setting, which leverages a\\ntopological divergence metric to quantify the structural properties of graphs\\ninduced by attention matrices. Examining the topological divergence between\\nprompt and response subgraphs reveals consistent patterns: higher divergence\\nvalues in specific attention heads correlate with hallucinated outputs,\\nindependent of the dataset. Extensive experiments, including evaluation on\\nquestion answering and data-to-text tasks, show that our approach achieves\\nstate-of-the-art or competitive results on several benchmarks, two of which\\nwere annotated by us and are being publicly released to facilitate further\\nresearch. Beyond its strong in-domain performance, TOHA maintains remarkable\\ndomain transferability across multiple open-source LLMs. Our findings suggest\\nthat analyzing the topological structure of attention matrices can serve as an\\nefficient and robust indicator of factual reliability in LLMs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-14T10:06:27Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20667v1\", \"title\": \"Explanations Go Linear: Interpretable and Individual Latent Encoding for\\n  Post-hoc Explainability\", \"summary\": \"Post-hoc explainability is essential for understanding black-box machine\\nlearning models. Surrogate-based techniques are widely used for local and\\nglobal model-agnostic explanations but have significant limitations. Local\\nsurrogates capture non-linearities but are computationally expensive and\\nsensitive to parameters, while global surrogates are more efficient but\\nstruggle with complex local behaviors. In this paper, we present ILLUME, a\\nflexible and interpretable framework grounded in representation learning, that\\ncan be integrated with various surrogate models to provide explanations for any\\nblack-box classifier. Specifically, our approach combines a globally trained\\nsurrogate with instance-specific linear transformations learned with a\\nmeta-encoder to generate both local and global explanations. Through extensive\\nempirical evaluations, we demonstrate the effectiveness of ILLUME in producing\\nfeature attributions and decision rules that are not only accurate but also\\nrobust and faithful to the black-box, thus providing a unified explanation\\nframework that effectively addresses the limitations of traditional surrogate\\nmethods.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-29T11:46:48Z\"}"}

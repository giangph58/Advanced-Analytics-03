{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12712v1\", \"title\": \"Convergence and Implicit Bias of Gradient Descent on Continual Linear\\n  Classification\", \"summary\": \"We study continual learning on multiple linear classification tasks by\\nsequentially running gradient descent (GD) for a fixed budget of iterations per\\ntask. When all tasks are jointly linearly separable and are presented in a\\ncyclic/random order, we show the directional convergence of the trained linear\\nclassifier to the joint (offline) max-margin solution. This is surprising\\nbecause GD training on a single task is implicitly biased towards the\\nindividual max-margin solution for the task, and the direction of the joint\\nmax-margin solution can be largely different from these individual solutions.\\nAdditionally, when tasks are given in a cyclic order, we present a\\nnon-asymptotic analysis on cycle-averaged forgetting, revealing that (1)\\nalignment between tasks is indeed closely tied to catastrophic forgetting and\\nbackward knowledge transfer and (2) the amount of forgetting vanishes to zero\\nas the cycle repeats. Lastly, we analyze the case where the tasks are no longer\\njointly separable and show that the model trained in a cyclic order converges\\nto the unique minimum of the joint loss function.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,math.OC\", \"published\": \"2025-04-17T07:35:48Z\"}"}

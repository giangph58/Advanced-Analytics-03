{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19828v1\", \"title\": \"HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended\\n  Reality Exploiting Eye-Hand-Head Coordination\", \"summary\": \"We present HOIGaze - a novel learning-based approach for gaze estimation\\nduring hand-object interactions (HOI) in extended reality (XR). HOIGaze\\naddresses the challenging HOI setting by building on one key insight: The eye,\\nhand, and head movements are closely coordinated during HOIs and this\\ncoordination can be exploited to identify samples that are most useful for gaze\\nestimator training - as such, effectively denoising the training data. This\\ndenoising approach is in stark contrast to previous gaze estimation methods\\nthat treated all training samples as equal. Specifically, we propose: 1) a\\nnovel hierarchical framework that first recognises the hand currently visually\\nattended to and then estimates gaze direction based on the attended hand; 2) a\\nnew gaze estimator that uses cross-modal Transformers to fuse head and\\nhand-object features extracted using a convolutional neural network and a\\nspatio-temporal graph convolutional network; and 3) a novel eye-head\\ncoordination loss that upgrades training samples belonging to the coordinated\\neye-head movements. We evaluate HOIGaze on the HOT3D and Aria digital twin\\n(ADT) datasets and show that it significantly outperforms state-of-the-art\\nmethods, achieving an average improvement of 15.6% on HOT3D and 6.0% on ADT in\\nmean angular error. To demonstrate the potential of our method, we further\\nreport significant performance improvements for the sample downstream task of\\neye-based activity recognition on ADT. Taken together, our results underline\\nthe significant information content available in eye-hand-head coordination\\nand, as such, open up an exciting new direction for learning-based gaze\\nestimation.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-28T14:31:43Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12700v1\", \"title\": \"A Two-Phase Perspective on Deep Learning Dynamics\", \"summary\": \"We propose that learning in deep neural networks proceeds in two phases: a\\nrapid curve fitting phase followed by a slower compression or coarse graining\\nphase. This view is supported by the shared temporal structure of three\\nphenomena: grokking, double descent and the information bottleneck, all of\\nwhich exhibit a delayed onset of generalization well after training error\\nreaches zero. We empirically show that the associated timescales align in two\\nrather different settings. Mutual information between hidden layers and input\\ndata emerges as a natural progress measure, complementing circuit-based metrics\\nsuch as local complexity and the linear mapping number. We argue that the\\nsecond phase is not actively optimized by standard training algorithms and may\\nbe unnecessarily prolonged. Drawing on an analogy with the renormalization\\ngroup, we suggest that this compression phase reflects a principled form of\\nforgetting, critical for generalization.\", \"main_category\": \"hep-th\", \"categories\": \"hep-th,cond-mat.dis-nn,cs.LG\", \"published\": \"2025-04-17T06:57:37Z\"}"}

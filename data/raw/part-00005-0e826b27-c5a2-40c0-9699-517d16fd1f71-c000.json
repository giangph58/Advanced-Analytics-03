{"value":"{\"aid\": \"http://arxiv.org/abs/2504.13145v1\", \"title\": \"Exploring Expert Failures Improves LLM Agent Tuning\", \"summary\": \"Large Language Models (LLMs) have shown tremendous potential as agents,\\nexcelling at tasks that require multiple rounds of reasoning and interactions.\\nRejection Sampling Fine-Tuning (RFT) has emerged as an effective method for\\nfinetuning LLMs as agents: it first imitates expert-generated successful\\ntrajectories and further improves agentic skills through iterative fine-tuning\\non successful, self-generated trajectories. However, since the expert (e.g.,\\nGPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler\\nscenarios, many complex subtasks remain unsolved and persistently\\nout-of-distribution (OOD). Upon investigating these challenging subtasks, we\\ndiscovered that previously failed expert trajectories can often provide\\nvaluable guidance, e.g., plans and key actions, that can significantly improve\\nagent exploration efficiency and acquisition of critical skills. Motivated by\\nthese observations, we propose Exploring Expert Failures (EEF), which\\nidentifies beneficial actions from failed expert trajectories and integrates\\nthem into the training dataset. Potentially harmful actions are meticulously\\nexcluded to prevent contamination of the model learning process. By leveraging\\nthe beneficial actions in expert failures, EEF successfully solves some\\npreviously unsolvable subtasks and improves agent tuning performance.\\nRemarkably, our approach achieved a 62\\\\% win rate in WebShop, outperforming RFT\\n(53. 6\\\\%) and GPT-4 (35. 6\\\\%), and to the best of our knowledge, setting a new\\nstate-of-the-art as the first method to surpass a score of 0.81 in WebShop and\\nexceed 81 in SciWorld.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-17T17:53:54Z\"}"}

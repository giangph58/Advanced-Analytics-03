{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05925v1\", \"title\": \"SVLTA: Benchmarking Vision-Language Temporal Alignment via Synthetic\\n  Video Situation\", \"summary\": \"Vision-language temporal alignment is a crucial capability for human dynamic\\nrecognition and cognition in real-world scenarios. While existing research\\nfocuses on capturing vision-language relevance, it faces limitations due to\\nbiased temporal distributions, imprecise annotations, and insufficient\\ncompositionally. To achieve fair evaluation and comprehensive exploration, our\\nobjective is to investigate and evaluate the ability of models to achieve\\nalignment from a temporal perspective, specifically focusing on their capacity\\nto synchronize visual scenarios with linguistic context in a temporally\\ncoherent manner. As a preliminary step, we present the statistical analysis of\\nexisting benchmarks and reveal the existing challenges from a decomposed\\nperspective. To this end, we introduce SVLTA, the Synthetic Vision-Language\\nTemporal Alignment derived via a well-designed and feasible control generation\\nmethod within a simulation environment. The approach considers commonsense\\nknowledge, manipulable action, and constrained filtering, which generates\\nreasonable, diverse, and balanced data distributions for diagnostic\\nevaluations. Our experiments reveal diagnostic insights through the evaluations\\nin temporal question answering, distributional shift sensitiveness, and\\ntemporal alignment adaptation.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-08T11:31:37Z\"}"}

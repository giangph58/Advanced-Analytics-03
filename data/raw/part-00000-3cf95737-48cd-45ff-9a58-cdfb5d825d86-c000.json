{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07479v1\", \"title\": \"UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\\n  Pruning for Efficient Long-Context LLM Inference\", \"summary\": \"Transformer-based large language models (LLMs) have achieved impressive\\nperformance in various natural language processing (NLP) applications. However,\\nthe high memory and computation cost induced by the KV cache limits the\\ninference efficiency, especially for long input sequences. Compute-in-memory\\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\\npruning. However, as existing accelerators only support static pruning with a\\nfixed pattern or dynamic pruning with primitive implementations, they suffer\\nfrom either high accuracy degradation or low efficiency. In this paper, we\\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\\nCIM mode, static pruning can be supported based on accumulative similarity\\nscore, which is much more flexible compared to fixed patterns; 3) in the\\ncurrent-domain mode, exact attention computation can be conducted with a subset\\nof selected KV cache. We further propose a novel CAM/CIM cell design that\\nleverages the multi-level characteristics of FeFETs for signed multibit storage\\nof the KV cache and in-place attention computation. With extensive experimental\\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\\nlevel, along with high accuracy comparable with dense attention at the\\napplication level, showing its great potential for efficient long-context LLM\\ninference.\", \"main_category\": \"cs.AR\", \"categories\": \"cs.AR\", \"published\": \"2025-04-10T06:13:30Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20648v1\", \"title\": \"SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with\\n  Synthetic Data\", \"summary\": \"Vision-language models (VLMs) work well in tasks ranging from image\\ncaptioning to visual question answering (VQA), yet they struggle with spatial\\nreasoning, a key skill for understanding our physical world that humans excel\\nat. We find that spatial relations are generally rare in widely used VL\\ndatasets, with only a few being well represented, while most form a long tail\\nof underrepresented relations. This gap leaves VLMs ill-equipped to handle\\ndiverse spatial relationships. To bridge it, we construct a synthetic VQA\\ndataset focused on spatial reasoning generated from hyper-detailed image\\ndescriptions in Localized Narratives, DOCCI, and PixMo-Cap. Our dataset\\nconsists of 455k samples containing 3.4 million QA pairs. Trained on this\\ndataset, our Spatial-Reasoning Enhanced (SpaRE) VLMs show strong improvements\\non spatial reasoning benchmarks, achieving up to a 49% performance gain on the\\nWhat's Up benchmark, while maintaining strong results on general tasks. Our\\nwork narrows the gap between human and VLM spatial reasoning and makes VLMs\\nmore capable in real-world tasks such as robotics and navigation.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-29T11:18:38Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15619v1\", \"title\": \"AdaViP: Aligning Multi-modal LLMs via Adaptive Vision-enhanced\\n  Preference Optimization\", \"summary\": \"Preference alignment through Direct Preference Optimization (DPO) has\\ndemonstrated significant effectiveness in aligning multimodal large language\\nmodels (MLLMs) with human preferences. However, existing methods focus\\nprimarily on language preferences while neglecting the critical visual context.\\nIn this paper, we propose an Adaptive Vision-enhanced Preference optimization\\n(AdaViP) that addresses these limitations through two key innovations: (1)\\nvision-based preference pair construction, which integrates multiple visual\\nfoundation models to strategically remove key visual elements from the image,\\nenhancing MLLMs' sensitivity to visual details; and (2) adaptive preference\\noptimization that dynamically balances vision- and language-based preferences\\nfor more accurate alignment. Extensive evaluations across different benchmarks\\ndemonstrate our effectiveness. Notably, our AdaViP-7B achieves 93.7% and 96.4%\\nreductions in response-level and mentioned-level hallucination respectively on\\nthe Object HalBench, significantly outperforming current state-of-the-art\\nmethods.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-22T06:19:38Z\"}"}

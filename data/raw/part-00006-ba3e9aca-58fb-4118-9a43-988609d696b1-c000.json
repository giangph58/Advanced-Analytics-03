{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23951v1\", \"title\": \"JointTuner: Appearance-Motion Adaptive Joint Training for Customized\\n  Video Generation\", \"summary\": \"Recent text-to-video advancements have enabled coherent video synthesis from\\nprompts and expanded to fine-grained control over appearance and motion.\\nHowever, existing methods either suffer from concept interference due to\\nfeature domain mismatch caused by naive decoupled optimizations or exhibit\\nappearance contamination induced by spatial feature leakage resulting from the\\nentanglement of motion and appearance in reference video reconstructions. In\\nthis paper, we propose JointTuner, a novel adaptive joint training framework,\\nto alleviate these issues. Specifically, we develop Adaptive LoRA, which\\nincorporates a context-aware gating mechanism, and integrate the gated LoRA\\ncomponents into the spatial and temporal Transformers within the diffusion\\nmodel. These components enable simultaneous optimization of appearance and\\nmotion, eliminating concept interference. In addition, we introduce the\\nAppearance-independent Temporal Loss, which decouples motion patterns from\\nintrinsic appearance in reference video reconstructions through an\\nappearance-agnostic noise prediction task. The key innovation lies in adding\\nframe-wise offset noise to the ground-truth Gaussian noise, perturbing its\\ndistribution, thereby disrupting spatial attributes associated with frames\\nwhile preserving temporal coherence. Furthermore, we construct a benchmark\\ncomprising 90 appearance-motion customized combinations and 10 multi-type\\nautomatic metrics across four dimensions, facilitating a more comprehensive\\nevaluation for this customization task. Extensive experiments demonstrate the\\nsuperior performance of our method compared to current advanced approaches.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-31T11:04:07Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19583v1\", \"title\": \"Graph-Based Spectral Decomposition for Parameter Coordination in\\n  Language Model Fine-Tuning\", \"summary\": \"This paper proposes a parameter collaborative optimization algorithm for\\nlarge language models, enhanced with graph spectral analysis. The goal is to\\nimprove both fine-tuning efficiency and structural awareness during training.\\nIn the proposed method, the parameters of a pre-trained language model are\\ntreated as nodes in a graph. A weighted graph is constructed, and Laplacian\\nspectral decomposition is applied to enable frequency-domain modeling and\\nstructural representation of the parameter space. Based on this structure, a\\njoint loss function is designed. It combines the task loss with a spectral\\nregularization term to facilitate collaborative updates among parameters. In\\naddition, a spectral filtering mechanism is introduced during the optimization\\nphase. This mechanism adjusts gradients in a structure-aware manner, enhancing\\nthe model's training stability and convergence behavior. The method is\\nevaluated on multiple tasks, including traditional fine-tuning comparisons,\\nfew-shot generalization tests, and convergence speed analysis. In all settings,\\nthe proposed approach demonstrates superior performance. The experimental\\nresults confirm that the spectral collaborative optimization framework\\neffectively reduces parameter perturbations and improves fine-tuning quality\\nwhile preserving overall model performance. This work contributes significantly\\nto the field of artificial intelligence by advancing parameter-efficient\\ntraining methodologies for large-scale models, reinforcing the importance of\\nstructural signal processing in deep learning optimization, and offering a\\nrobust, generalizable framework for enhancing language model adaptability and\\nperformance.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL\", \"published\": \"2025-04-28T08:42:35Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11410v1\", \"title\": \"Randomized block proximal method with locally Lipschitz continuous\\n  gradient\", \"summary\": \"Block-coordinate algorithms are recognized to furnish efficient iterative\\nschemes for addressing large-scale problems, especially when the computation of\\nfull derivatives entails substantial memory requirements and computational\\nefforts. In this paper, we investigate a randomized block proximal gradient\\nalgorithm for minimizing the sum of a differentiable function and a separable\\nproper lower-semicontinuous function, both possibly nonconvex. In contrast to\\nprevious works, we only assume that the partial gradients of the differentiable\\nfunction are locally Lipschitz continuous. At each iteration, the method\\nadaptively selects a proximal stepsize to satisfy a sufficient decrease\\ncondition without prior knowledge of the local Lipschitz moduli of the partial\\ngradients of the differentiable function. In addition, we incorporate the\\npossibility of conducting an additional linesearch to enhance the performance\\nof the algorithm. Our main result establishes subsequential convergence to a\\nstationary point of the problem almost surely. Finally, we provide numerical\\nvalidation of the method in an experiment in image compression using a\\nnonnegative matrix factorization model.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC\", \"published\": \"2025-04-15T17:26:40Z\"}"}

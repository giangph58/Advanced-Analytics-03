{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21751v1\", \"title\": \"CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code\\n  Generation\", \"summary\": \"Real world development demands code that is readable, extensible, and\\ntestable by organizing the implementation into modular components and\\niteratively reuse pre-implemented code. We term this iterative, multi-turn\\nprocess codeflow and introduce CodeFlowBench, the first benchmark designed for\\ncomprehensively evaluating LLMs' ability to perform codeflow, namely to\\nimplement new functionality by reusing existing functions over multiple turns.\\nCodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously\\nupdated via an automated pipeline that decomposes each problem into a series of\\nfunction-level subproblems based on its dependency tree and each subproblem is\\npaired with unit tests. We further propose a novel evaluation framework with\\ntasks and metrics tailored to multi-turn code reuse to assess model\\nperformance. In experiments across various LLMs under both multi-turn and\\nsingle-turn patterns. We observe models' poor performance on CodeFlowBench,\\nwith a substantial performance drop in the iterative codeflow scenario. For\\ninstance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8%\\nin single-turn pattern. Further analysis shows that different models excel at\\ndifferent dependency depths, yet all struggle to correctly solve structurally\\ncomplex problems, highlighting challenges for current LLMs to serve as code\\ngeneration tools when performing codeflow. Overall, CodeFlowBench offers a\\ncomprehensive benchmark and new insights into LLM capabilities for multi-turn,\\niterative code generation, guiding future advances in code generation tasks.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE,cs.CL\", \"published\": \"2025-04-30T15:45:28Z\"}"}

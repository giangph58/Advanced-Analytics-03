{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06800v1\", \"title\": \"A Meaningful Perturbation Metric for Evaluating Explainability Methods\", \"summary\": \"Deep neural networks (DNNs) have demonstrated remarkable success, yet their\\nwide adoption is often hindered by their opaque decision-making. To address\\nthis, attribution methods have been proposed to assign relevance values to each\\npart of the input. However, different methods often produce entirely different\\nrelevance maps, necessitating the development of standardized metrics to\\nevaluate them. Typically, such evaluation is performed through perturbation,\\nwherein high- or low-relevance regions of the input image are manipulated to\\nexamine the change in prediction. In this work, we introduce a novel approach,\\nwhich harnesses image generation models to perform targeted perturbation.\\nSpecifically, we focus on inpainting only the high-relevance pixels of an input\\nimage to modify the model's predictions while preserving image fidelity. This\\nis in contrast to existing approaches, which often produce out-of-distribution\\nmodifications, leading to unreliable results. Through extensive experiments, we\\ndemonstrate the effectiveness of our approach in generating meaningful rankings\\nacross a wide range of models and attribution methods. Crucially, we establish\\nthat the ranking produced by our metric exhibits significantly higher\\ncorrelation with human preferences compared to existing approaches,\\nunderscoring its potential for enhancing interpretability in DNNs.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-09T11:46:41Z\"}"}

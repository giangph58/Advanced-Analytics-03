{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16450v1\", \"title\": \"An Effective Gram Matrix Characterizes Generalization in Deep Networks\", \"summary\": \"We derive a differential equation that governs the evolution of the\\ngeneralization gap when a deep network is trained by gradient descent. This\\ndifferential equation is controlled by two quantities, a contraction factor\\nthat brings together trajectories corresponding to slightly different datasets,\\nand a perturbation factor that accounts for them training on different\\ndatasets. We analyze this differential equation to compute an ``effective Gram\\nmatrix'' that characterizes the generalization gap after training in terms of\\nthe alignment between this Gram matrix and a certain initial ``residual''.\\nEmpirical evaluations on image classification datasets indicate that this\\nanalysis can predict the test loss accurately. Further, at any point during\\ntraining, the residual predominantly lies in the subspace of the effective Gram\\nmatrix with the smallest eigenvalues. This indicates that the training process\\nis benign, i.e., it does not lead to significant deterioration of the\\ngeneralization gap (which is zero at initialization). The alignment between the\\neffective Gram matrix and the residual is different for different datasets and\\narchitectures. The match/mismatch of the data and the architecture is primarily\\nresponsible for good/bad generalization.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,stat.ML\", \"published\": \"2025-04-23T06:24:42Z\"}"}

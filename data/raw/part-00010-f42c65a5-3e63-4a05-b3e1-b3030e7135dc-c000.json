{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03432v1\", \"title\": \"Wasserstein Convergence of Score-based Generative Models under\\n  Semiconvexity and Discontinuous Gradients\", \"summary\": \"Score-based Generative Models (SGMs) approximate a data distribution by\\nperturbing it with Gaussian noise and subsequently denoising it via a learned\\nreverse diffusion process. These models excel at modeling complex data\\ndistributions and generating diverse samples, achieving state-of-the-art\\nperformance across domains such as computer vision, audio generation,\\nreinforcement learning, and computational biology. Despite their empirical\\nsuccess, existing Wasserstein-2 convergence analysis typically assume strong\\nregularity conditions-such as smoothness or strict log-concavity of the data\\ndistribution-that are rarely satisfied in practice. In this work, we establish\\nthe first non-asymptotic Wasserstein-2 convergence guarantees for SGMs\\ntargeting semiconvex distributions with potentially discontinuous gradients.\\nOur upper bounds are explicit and sharp in key parameters, achieving optimal\\ndependence of $O(\\\\sqrt{d})$ on the data dimension $d$ and convergence rate of\\norder one. The framework accommodates a wide class of practically relevant\\ndistributions, including symmetric modified half-normal distributions, Gaussian\\nmixtures, double-well potentials, and elastic net potentials. By leveraging\\nsemiconvexity without requiring smoothness assumptions on the potential such as\\ndifferentiability, our results substantially broaden the theoretical\\nfoundations of SGMs, bridging the gap between empirical success and rigorous\\nguarantees in non-smooth, complex data regimes.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,math.OC,math.PR,stat.ML\", \"published\": \"2025-05-06T11:17:15Z\"}"}

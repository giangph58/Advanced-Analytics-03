{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23829v1\", \"title\": \"Expanding RL with Verifiable Rewards Across Diverse Domains\", \"summary\": \"Reinforcement learning (RL) with verifiable rewards (RLVR) has shown\\npromising results in mathematical reasoning and coding tasks where\\nwell-structured reference answers are available. However, its applicability to\\nbroader domains remains underexplored. In this work, we study the extension of\\nRLVR to more diverse domains such as medicine, chemistry, psychology, and\\neconomics. We observe high agreement in binary judgments across different large\\nlanguage models (LLMs) when objective reference answers exist, which challenges\\nthe necessity of large-scale annotation for training domain-specific reward\\nmodels. To address the limitations of binary rewards when handling unstructured\\nreference answers, we further incorporate model-based soft scoring into RLVR to\\nimprove its flexibility. Our experiments show that a distilled generative\\nreward model can serve as an effective cross-domain verifier, providing\\nreliable reward signals for RL without requiring domain-specific annotations.\\nBy fine-tuning a base 7B model using various RL algorithms against our reward\\nmodel, we obtain policies that outperform state-of-the-art open-source aligned\\nLLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large\\nmargin, across domains in free-form answer settings. This also strengthens\\nRLVR's robustness and scalability, highlighting its potential for real-world\\napplications with noisy or weak labels.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-03-31T08:22:49Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02767v1\", \"title\": \"How Deep Do Large Language Models Internalize Scientific Literature and\\n  Citation Practices?\", \"summary\": \"The spread of scientific knowledge depends on how researchers discover and\\ncite previous work. The adoption of large language models (LLMs) in the\\nscientific research process introduces a new layer to these citation practices.\\nHowever, it remains unclear to what extent LLMs align with human citation\\npractices, how they perform across domains, and may influence citation\\ndynamics. Here, we show that LLMs systematically reinforce the Matthew effect\\nin citations by consistently favoring highly cited papers when generating\\nreferences. This pattern persists across scientific domains despite significant\\nfield-specific variations in existence rates, which refer to the proportion of\\ngenerated references that match existing records in external bibliometric\\ndatabases. Analyzing 274,951 references generated by GPT-4o for 10,000 papers,\\nwe find that LLM recommendations diverge from traditional citation patterns by\\npreferring more recent references with shorter titles and fewer authors.\\nEmphasizing their content-level relevance, the generated references are\\nsemantically aligned with the content of each paper at levels comparable to the\\nground truth references and display similar network effects while reducing\\nauthor self-citations. These findings illustrate how LLMs may reshape citation\\npractices and influence the trajectory of scientific discovery by reflecting\\nand amplifying established trends. As LLMs become more integrated into the\\nscientific research process, it is important to understand their role in\\nshaping how scientific communities discover and build upon prior work.\", \"main_category\": \"cs.DL\", \"categories\": \"cs.DL,cs.AI,cs.LG,cs.SI\", \"published\": \"2025-04-03T17:04:56Z\"}"}

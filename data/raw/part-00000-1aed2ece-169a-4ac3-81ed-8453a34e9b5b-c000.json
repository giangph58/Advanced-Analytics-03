{"value":"{\"aid\": \"http://arxiv.org/abs/2503.21714v1\", \"title\": \"As easy as PIE: understanding when pruning causes language models to\\n  disagree\", \"summary\": \"Language Model (LM) pruning compresses the model by removing weights, nodes,\\nor other parts of its architecture. Typically, pruning focuses on the resulting\\nefficiency gains at the cost of effectiveness. However, when looking at how\\nindividual data points are affected by pruning, it turns out that a particular\\nsubset of data points always bears most of the brunt (in terms of reduced\\naccuracy) when pruning, but this effect goes unnoticed when reporting the mean\\naccuracy of all data points. These data points are called PIEs and have been\\nstudied in image processing, but not in NLP. In a study of various NLP\\ndatasets, pruning methods, and levels of compression, we find that PIEs impact\\ninference quality considerably, regardless of class frequency, and that BERT is\\nmore prone to this than BiLSTM. We also find that PIEs contain a high amount of\\ndata points that have the largest influence on how well the model generalises\\nto unseen data. This means that when pruning, with seemingly moderate loss to\\naccuracy across all data points, we in fact hurt tremendously those data points\\nthat matter the most. We trace what makes PIEs both hard and impactful to\\ninference to their overall longer and more semantically complex text. These\\nfindings are novel and contribute to understanding how LMs are affected by\\npruning. The code is available at: https://github.com/pietrotrope/AsEasyAsPIE\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-03-27T17:26:32Z\"}"}

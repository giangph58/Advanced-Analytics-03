{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21432v1\", \"title\": \"UAV-VLN: End-to-End Vision Language guided Navigation for UAVs\", \"summary\": \"A core challenge in AI-guided autonomy is enabling agents to navigate\\nrealistically and effectively in previously unseen environments based on\\nnatural language commands. We propose UAV-VLN, a novel end-to-end\\nVision-Language Navigation (VLN) framework for Unmanned Aerial Vehicles (UAVs)\\nthat seamlessly integrates Large Language Models (LLMs) with visual perception\\nto facilitate human-interactive navigation. Our system interprets free-form\\nnatural language instructions, grounds them into visual observations, and plans\\nfeasible aerial trajectories in diverse environments.\\n  UAV-VLN leverages the common-sense reasoning capabilities of LLMs to parse\\nhigh-level semantic goals, while a vision model detects and localizes\\nsemantically relevant objects in the environment. By fusing these modalities,\\nthe UAV can reason about spatial relationships, disambiguate references in\\nhuman instructions, and plan context-aware behaviors with minimal task-specific\\nsupervision. To ensure robust and interpretable decision-making, the framework\\nincludes a cross-modal grounding mechanism that aligns linguistic intent with\\nvisual context.\\n  We evaluate UAV-VLN across diverse indoor and outdoor navigation scenarios,\\ndemonstrating its ability to generalize to novel instructions and environments\\nwith minimal task-specific training. Our results show significant improvements\\nin instruction-following accuracy and trajectory efficiency, highlighting the\\npotential of LLM-driven vision-language interfaces for safe, intuitive, and\\ngeneralizable UAV autonomy.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.CV\", \"published\": \"2025-04-30T08:40:47Z\"}"}

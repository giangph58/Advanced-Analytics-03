{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20403v1\", \"title\": \"Creating Your Editable 3D Photorealistic Avatar with\\n  Tetrahedron-constrained Gaussian Splatting\", \"summary\": \"Personalized 3D avatar editing holds significant promise due to its\\nuser-friendliness and availability to applications such as AR/VR and virtual\\ntry-ons. Previous studies have explored the feasibility of 3D editing, but\\noften struggle to generate visually pleasing results, possibly due to the\\nunstable representation learning under mixed optimization of geometry and\\ntexture in complicated reconstructed scenarios. In this paper, we aim to\\nprovide an accessible solution for ordinary users to create their editable 3D\\navatars with precise region localization, geometric adaptability, and\\nphotorealistic renderings. To tackle this challenge, we introduce a\\nmeticulously designed framework that decouples the editing process into local\\nspatial adaptation and realistic appearance learning, utilizing a hybrid\\nTetrahedron-constrained Gaussian Splatting (TetGS) as the underlying\\nrepresentation. TetGS combines the controllable explicit structure of\\ntetrahedral grids with the high-precision rendering capabilities of 3D Gaussian\\nSplatting and is optimized in a progressive manner comprising three stages: 3D\\navatar instantiation from real-world monocular videos to provide accurate\\npriors for TetGS initialization; localized spatial adaptation with explicitly\\npartitioned tetrahedrons to guide the redistribution of Gaussian kernels; and\\ngeometry-based appearance generation with a coarse-to-fine activation strategy.\\nBoth qualitative and quantitative experiments demonstrate the effectiveness and\\nsuperiority of our approach in generating photorealistic 3D editable avatars.\", \"main_category\": \"cs.GR\", \"categories\": \"cs.GR,cs.CV\", \"published\": \"2025-04-29T03:56:36Z\"}"}

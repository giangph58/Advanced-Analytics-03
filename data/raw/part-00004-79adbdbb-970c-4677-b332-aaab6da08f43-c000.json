{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12721v1\", \"title\": \"TimeCapsule: Solving the Jigsaw Puzzle of Long-Term Time Series\\n  Forecasting with Compressed Predictive Representations\", \"summary\": \"Recent deep learning models for Long-term Time Series Forecasting (LTSF)\\noften emphasize complex, handcrafted designs, while simpler architectures like\\nlinear models or MLPs have often outperformed these intricate solutions. In\\nthis paper, we revisit and organize the core ideas behind several key\\ntechniques, such as redundancy reduction and multi-scale modeling, which are\\nfrequently employed in advanced LTSF models. Our goal is to streamline these\\nideas for more efficient deep learning utilization. To this end, we introduce\\nTimeCapsule, a model built around the principle of high-dimensional information\\ncompression that unifies these techniques in a generalized yet simplified\\nframework. Specifically, we model time series as a 3D tensor, incorporating\\ntemporal, variate, and level dimensions, and leverage mode production to\\ncapture multi-mode dependencies while achieving dimensionality compression. We\\npropose an internal forecast within the compressed representation domain,\\nsupported by the Joint-Embedding Predictive Architecture (JEPA), to monitor the\\nlearning of predictive representations. Extensive experiments on challenging\\nbenchmarks demonstrate the versatility of our method, showing that TimeCapsule\\ncan achieve state-of-the-art performance.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,eess.SP\", \"published\": \"2025-04-17T07:54:26Z\"}"}

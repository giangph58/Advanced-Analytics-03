{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23814v1\", \"title\": \"An extension of linear self-attention for in-context learning\", \"summary\": \"In-context learning is a remarkable property of transformers and has been the\\nfocus of recent research. An attention mechanism is a key component in\\ntransformers, in which an attention matrix encodes relationships between words\\nin a sentence and is used as weights for words in a sentence. This mechanism is\\neffective for capturing language representations. However, it is questionable\\nwhether naive self-attention is suitable for in-context learning in general\\ntasks, since the computation implemented by self-attention is somewhat\\nrestrictive in terms of matrix multiplication. In fact, we may need appropriate\\ninput form designs when considering heuristic implementations of computational\\nalgorithms. In this paper, in case of linear self-attention, we extend it by\\nintroducing a bias matrix in addition to a weight matrix for an input. Despite\\nthe simple extension, the extended linear self-attention can output any\\nconstant matrix, input matrix and multiplications of two or three matrices in\\nthe input. Note that the second property implies that it can be a skip\\nconnection. Therefore, flexible matrix manipulations can be implemented by\\nconnecting the extended linear self-attention components. As an example of\\nimplementation using the extended linear self-attention, we show a heuristic\\nconstruction of a batch-type gradient descent of ridge regression under a\\nreasonable input form.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-03-31T07:49:05Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03570v1\", \"title\": \"OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents\", \"summary\": \"In this paper, we introduce OSUniverse: a benchmark of complex, multimodal\\ndesktop-oriented tasks for advanced GUI-navigation AI agents that focuses on\\nease of use, extensibility, comprehensive coverage of test cases, and automated\\nvalidation. We divide the tasks in increasing levels of complexity, from basic\\nprecision clicking to multistep, multiapplication tests requiring dexterity,\\nprecision, and clear thinking from the agent. In version one of the benchmark,\\npresented here, we have calibrated the complexity of the benchmark test cases\\nto ensure that the SOTA (State of the Art) agents (at the time of publication)\\ndo not achieve results higher than 50%, while the average white collar worker\\ncan perform all these tasks with perfect accuracy. The benchmark can be scored\\nmanually, but we also introduce an automated validation mechanism that has an\\naverage error rate less than 2%. Therefore, this benchmark presents solid\\nground for fully automated measuring of progress, capabilities and the\\neffectiveness of GUI-navigation AI agents over the short and medium-term\\nhorizon. The source code of the benchmark is available at\\nhttps://github.com/agentsea/osuniverse.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-05-06T14:29:47Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10445v1\", \"title\": \"RealWebAssist: A Benchmark for Long-Horizon Web Assistance with\\n  Real-World Users\", \"summary\": \"To achieve successful assistance with long-horizon web-based tasks, AI agents\\nmust be able to sequentially follow real-world user instructions over a long\\nperiod. Unlike existing web-based agent benchmarks, sequential instruction\\nfollowing in the real world poses significant challenges beyond performing a\\nsingle, clearly defined task. For instance, real-world human instructions can\\nbe ambiguous, require different levels of AI assistance, and may evolve over\\ntime, reflecting changes in the user's mental state. To address this gap, we\\nintroduce RealWebAssist, a novel benchmark designed to evaluate sequential\\ninstruction-following in realistic scenarios involving long-horizon\\ninteractions with the web, visual GUI grounding, and understanding ambiguous\\nreal-world user instructions. RealWebAssist includes a dataset of sequential\\ninstructions collected from real-world human users. Each user instructs a\\nweb-based assistant to perform a series of tasks on multiple websites. A\\nsuccessful agent must reason about the true intent behind each instruction,\\nkeep track of the mental state of the user, understand user-specific routines,\\nand ground the intended tasks to actions on the correct GUI elements. Our\\nexperimental results show that state-of-the-art models struggle to understand\\nand ground user instructions, posing critical challenges in following\\nreal-world user instructions for long-horizon web assistance.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CL,cs.CV,cs.LG\", \"published\": \"2025-04-14T17:36:46Z\"}"}

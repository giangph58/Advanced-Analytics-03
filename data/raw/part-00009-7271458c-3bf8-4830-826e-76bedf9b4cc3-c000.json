{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17562v1\", \"title\": \"When Does Metadata Conditioning (NOT) Work for Language Model\\n  Pre-Training? A Study with Context-Free Grammars\", \"summary\": \"The ability to acquire latent semantics is one of the key properties that\\ndetermines the performance of language models. One convenient approach to\\ninvoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at\\nthe beginning of texts in the pre-training data, making it easier for the model\\nto access latent semantics before observing the entire text. Previous studies\\nhave reported that this technique actually improves the performance of trained\\nmodels in downstream tasks; however, this improvement has been observed only in\\nspecific downstream tasks, without consistent enhancement in average next-token\\nprediction loss. To understand this phenomenon, we closely investigate how\\nprepending metadata during pre-training affects model performance by examining\\nits behavior using artificial data. Interestingly, we found that this approach\\nproduces both positive and negative effects on the downstream tasks. We\\ndemonstrate that the effectiveness of the approach depends on whether latent\\nsemantics can be inferred from the downstream task's prompt. Specifically,\\nthrough investigations using data generated by probabilistic context-free\\ngrammars, we show that training with metadata helps improve model's performance\\nwhen the given context is long enough to infer the latent semantics. In\\ncontrast, the technique negatively impacts performance when the context lacks\\nthe necessary information to make an accurate posterior inference.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-24T13:56:43Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04196v1\", \"title\": \"A Large Language Model for Feasible and Diverse Population Synthesis\", \"summary\": \"Generating a synthetic population that is both feasible and diverse is\\ncrucial for ensuring the validity of downstream activity schedule simulation in\\nactivity-based models (ABMs). While deep generative models (DGMs), such as\\nvariational autoencoders and generative adversarial networks, have been applied\\nto this task, they often struggle to balance the inclusion of rare but\\nplausible combinations (i.e., sampling zeros) with the exclusion of implausible\\nones (i.e., structural zeros). To improve feasibility while maintaining\\ndiversity, we propose a fine-tuning method for large language models (LLMs)\\nthat explicitly controls the autoregressive generation process through\\ntopological orderings derived from a Bayesian Network (BN). Experimental\\nresults show that our hybrid LLM-BN approach outperforms both traditional DGMs\\nand proprietary LLMs (e.g., ChatGPT-4o) with few-shot learning. Specifically,\\nour approach achieves approximately 95% feasibility, significantly higher than\\nthe ~80% observed in DGMs, while maintaining comparable diversity, making it\\nwell-suited for practical applications. Importantly, the method is based on a\\nlightweight open-source LLM, enabling fine-tuning and inference on standard\\npersonal computing environments. This makes the approach cost-effective and\\nscalable for large-scale applications, such as synthesizing populations in\\nmegacities, without relying on expensive infrastructure. By initiating the ABM\\npipeline with high-quality synthetic populations, our method improves overall\\nsimulation reliability and reduces downstream error propagation. The source\\ncode for these methods is available for research and practical application.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.MA\", \"published\": \"2025-05-07T07:50:12Z\"}"}

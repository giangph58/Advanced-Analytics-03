{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12100v1\", \"title\": \"Generalized Visual Relation Detection with Diffusion Models\", \"summary\": \"Visual relation detection (VRD) aims to identify relationships (or\\ninteractions) between object pairs in an image. Although recent VRD models have\\nachieved impressive performance, they are all restricted to pre-defined\\nrelation categories, while failing to consider the semantic ambiguity\\ncharacteristic of visual relations. Unlike objects, the appearance of visual\\nrelations is always subtle and can be described by multiple predicate words\\nfrom different perspectives, e.g., ``ride'' can be depicted as ``race'' and\\n``sit on'', from the sports and spatial position views, respectively. To this\\nend, we propose to model visual relations as continuous embeddings, and design\\ndiffusion models to achieve generalized VRD in a conditional generative manner,\\ntermed Diff-VRD. We model the diffusion process in a latent space and generate\\nall possible relations in the image as an embedding sequence. During the\\ngeneration, the visual and text embeddings of subject-object pairs serve as\\nconditional signals and are injected via cross-attention. After the generation,\\nwe design a subsequent matching stage to assign the relation words to\\nsubject-object pairs by considering their semantic similarities. Benefiting\\nfrom the diffusion-based generative process, our Diff-VRD is able to generate\\nvisual relations beyond the pre-defined category labels of datasets. To\\nproperly evaluate this generalized VRD task, we introduce two evaluation\\nmetrics, i.e., text-to-image retrieval and SPICE PR Curve inspired by image\\ncaptioning. Extensive experiments in both human-object interaction (HOI)\\ndetection and scene graph generation (SGG) benchmarks attest to the superiority\\nand effectiveness of Diff-VRD.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-16T14:03:24Z\"}"}

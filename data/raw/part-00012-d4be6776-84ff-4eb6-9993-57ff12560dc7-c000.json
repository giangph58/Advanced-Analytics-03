{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00557v1\", \"title\": \"Triggering Hallucinations in LLMs: A Quantitative Study of\\n  Prompt-Induced Hallucination in Large Language Models\", \"summary\": \"Hallucinations in large language models (LLMs) present a growing challenge\\nacross real-world applications, from healthcare to law, where factual\\nreliability is essential. Despite advances in alignment and instruction tuning,\\nLLMs can still generate outputs that are fluent yet fundamentally untrue.\\nUnderstanding the cognitive dynamics that underlie these hallucinations remains\\nan open problem. In this study, we propose a prompt-based framework to\\nsystematically trigger and quantify hallucination: a Hallucination-Inducing\\nPrompt (HIP), which synthetically fuses semantically distant concepts (e.g.,\\nperiodic table of elements and tarot divination) in a misleading way, and a\\nHallucination Quantifying Prompt (HQP), which scores the plausibility,\\nconfidence, and coherence of the output. Controlled experiments across multiple\\nLLMs revealed that HIPs consistently produced less coherent and more\\nhallucinated responses than their null-fusion controls. These effects varied\\nacross models, with reasoning-oriented LLMs showing distinct profiles from\\ngeneral-purpose ones. Our framework provides a reproducible testbed for\\nstudying hallucination vulnerability, and opens the door to developing safer,\\nmore introspective LLMs that can detect and self-regulate the onset of\\nconceptual instability.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-05-01T14:33:47Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06207v1\", \"title\": \"An experimental survey and Perspective View on Meta-Learning for\\n  Automated Algorithms Selection and Parametrization\", \"summary\": \"Considerable progress has been made in the recent literature studies to\\ntackle the Algorithms Selection and Parametrization (ASP) problem, which is\\ndiversified in multiple meta-learning setups. Yet there is a lack of surveys\\nand comparative evaluations that critically analyze, summarize and assess the\\nperformance of existing methods. In this paper, we provide an overview of the\\nstate of the art in this continuously evolving field. The survey sheds light on\\nthe motivational reasons for pursuing classifiers selection through\\nmeta-learning. In this regard, Automated Machine Learning (AutoML) is usually\\ntreated as an ASP problem under the umbrella of the democratization of machine\\nlearning. Accordingly, AutoML makes machine learning techniques accessible to\\ndomain scientists who are interested in applying advanced analytics but lack\\nthe required expertise. It can ease the task of manually selecting ML\\nalgorithms and tuning related hyperparameters. We comprehensively discuss the\\ndifferent phases of classifiers selection based on a generic framework that is\\nformed as an outcome of reviewing prior works. Subsequently, we propose a\\nbenchmark knowledge base of 4 millions previously learned models and present\\nextensive comparative evaluations of the prominent methods for classifiers\\nselection based on 08 classification algorithms and 400 benchmark datasets. The\\ncomparative study quantitatively assesses the performance of algorithms\\nselection methods along while emphasizing the strengths and limitations of\\nexisting studies.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-08T16:51:22Z\"}"}

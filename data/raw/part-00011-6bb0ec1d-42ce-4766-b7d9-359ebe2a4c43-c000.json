{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04519v1\", \"title\": \"Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs\", \"summary\": \"Sparse large language models (LLMs) with Mixture of Experts (MoE) and close\\nto a trillion parameters are dominating the realm of most capable language\\nmodels. However, the massive model scale poses significant challenges for the\\nunderlying software and hardware systems. In this paper, we aim to uncover a\\nrecipe to harness such scale on Ascend NPUs. The key goals are better usage of\\nthe computing resources under the dynamic sparse model structures and\\nmaterializing the expected performance gain on the actual hardware. To select\\nmodel configurations suitable for Ascend NPUs without repeatedly running the\\nexpensive experiments, we leverage simulation to compare the trade-off of\\nvarious model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM\\nwith 718 billion parameters, and we conducted experiments on the model to\\nverify the simulation results. On the system side, we dig into Expert\\nParallelism to optimize the communication between NPU devices to reduce the\\nsynchronization overhead. We also optimize the memory efficiency within the\\ndevices to further reduce the parameter and activation management overhead. In\\nthe end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with\\nperformance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and\\ndemonstrate that the Ascend system is capable of harnessing all the training\\nstages of the state-of-the-art language models. Extensive experiments indicate\\nthat our recipe can lead to efficient training of large-scale sparse language\\nmodels with MoE. We also study the behaviors of such models for future\\nreference.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-05-07T15:46:36Z\"}"}

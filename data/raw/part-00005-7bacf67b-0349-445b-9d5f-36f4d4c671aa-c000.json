{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15279v1\", \"title\": \"VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal\\n  Large Language Models\", \"summary\": \"Visual reasoning is a core component of human intelligence and a critical\\ncapability for advanced multimodal models. Yet current reasoning evaluations of\\nmultimodal large language models (MLLMs) often rely on text descriptions and\\nallow language-based reasoning shortcuts, failing to measure genuine\\nvision-centric reasoning. To address this, we introduce VisuLogic: a benchmark\\nof 1,000 human-verified problems across six categories (e.g., quantitative\\nshifts, spatial relations, attribute comparisons). These various types of\\nquestions can be evaluated to assess the visual reasoning capabilities of MLLMs\\nfrom multiple perspectives. We evaluate leading MLLMs on this benchmark and\\nanalyze their results to identify common failure modes. Most models score below\\n30% accuracy-only slightly above the 25% random baseline and far below the\\n51.4% achieved by humans-revealing significant gaps in visual reasoning.\\nFurthermore, we provide a supplementary training dataset and a\\nreinforcement-learning baseline to support further progress.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-21T17:59:53Z\"}"}

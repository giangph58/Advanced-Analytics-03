{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10090v1\", \"title\": \"CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography\", \"summary\": \"Large language models (LLMs) and multimodal large language models (MLLMs)\\nhave significantly advanced artificial intelligence. However, visual reasoning,\\nreasoning involving both visual and textual inputs, remains underexplored.\\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\\n2.0 Flash Thinking, which incorporate image inputs, have opened this\\ncapability. In this ongoing work, we focus specifically on photography-related\\ntasks because a photo is a visual snapshot of the physical world where the\\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\\ncamera parameters. Successfully reasoning from the visual information of a\\nphoto to identify these numerical camera settings requires the MLLMs to have a\\ndeeper understanding of the underlying physics for precise visual\\ncomprehension, representing a challenging and intelligent capability essential\\nfor practical applications like photography assistant agents. We aim to\\nevaluate MLLMs on their ability to distinguish visual differences related to\\nnumerical camera settings, extending a methodology previously proposed for\\nvision-language models (VLMs). Our preliminary results demonstrate the\\nimportance of visual reasoning in photography-related tasks. Moreover, these\\nresults show that no single MLLM consistently dominates across all evaluation\\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\\nwith better visual reasoning.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.CL\", \"published\": \"2025-04-14T10:53:44Z\"}"}

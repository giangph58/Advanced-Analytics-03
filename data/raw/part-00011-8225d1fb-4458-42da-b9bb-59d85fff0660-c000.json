{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05271v1\", \"title\": \"T-T: Table Transformer for Tagging-based Aspect Sentiment Triplet\\n  Extraction\", \"summary\": \"Aspect sentiment triplet extraction (ASTE) aims to extract triplets composed\\nof aspect terms, opinion terms, and sentiment polarities from given sentences.\\nThe table tagging method is a popular approach to addressing this task, which\\nencodes a sentence into a 2-dimensional table, allowing for the tagging of\\nrelations between any two words. Previous efforts have focused on designing\\nvarious downstream relation learning modules to better capture interactions\\nbetween tokens in the table, revealing that a stronger capability to capture\\nrelations can lead to greater improvements in the model. Motivated by this, we\\nattempt to directly utilize transformer layers as downstream relation learning\\nmodules. Due to the powerful semantic modeling capability of transformers, it\\nis foreseeable that this will lead to excellent improvement. However, owing to\\nthe quadratic relation between the length of the table and the length of the\\ninput sentence sequence, using transformers directly faces two challenges:\\noverly long table sequences and unfair local attention interaction. To address\\nthese challenges, we propose a novel Table-Transformer (T-T) for the\\ntagging-based ASTE method. Specifically, we introduce a stripe attention\\nmechanism with a loop-shift strategy to tackle these challenges. The former\\nmodifies the global attention mechanism to only attend to a 2-dimensional local\\nattention window, while the latter facilitates interaction between different\\nattention windows. Extensive and comprehensive experiments demonstrate that the\\nT-T, as a downstream relation learning module, achieves state-of-the-art\\nperformance with lower computational costs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-05-08T14:17:27Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02367v1\", \"title\": \"CrystalFormer-RL: Reinforcement Fine-Tuning for Materials Design\", \"summary\": \"Reinforcement fine-tuning has instrumental enhanced the instruction-following\\nand reasoning abilities of large language models. In this work, we explore the\\napplications of reinforcement fine-tuning to the autoregressive\\ntransformer-based materials generative model CrystalFormer (arXiv:2403.15734)\\nusing discriminative machine learning models such as interatomic potentials and\\nproperty prediction models. By optimizing reward signals-such as energy above\\nthe convex hull and material property figures of merit-reinforcement\\nfine-tuning infuses knowledge from discriminative models into generative\\nmodels. The resulting model, CrystalFormer-RL, shows enhanced stability in\\ngenerated crystals and successfully discovers crystals with desirable yet\\nconflicting material properties, such as substantial dielectric constant and\\nband gap simultaneously. Notably, we observe that reinforcement fine-tuning\\nenables not only the property-guided novel material design ability of\\ngenerative pre-trained model but also unlocks property-driven material\\nretrieval from the unsupervised pre-training dataset. Leveraging rewards from\\ndiscriminative models to fine-tune materials generative models opens an\\nexciting gateway to the synergies of the machine learning ecosystem for\\nmaterials.\", \"main_category\": \"cond-mat.mtrl-sci\", \"categories\": \"cond-mat.mtrl-sci,cs.LG,physics.comp-ph\", \"published\": \"2025-04-03T07:59:30Z\"}"}

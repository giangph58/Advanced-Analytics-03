{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20782v1\", \"title\": \"Integrating Human Feedback into a Reinforcement Learning-Based Framework\\n  for Adaptive User Interfaces\", \"summary\": \"Adaptive User Interfaces (AUI) play a crucial role in modern software\\napplications by dynamically adjusting interface elements to accommodate users'\\ndiverse and evolving needs. However, existing adaptation strategies often lack\\nreal-time responsiveness. Reinforcement Learning (RL) has emerged as a\\npromising approach for addressing complex, sequential adaptation challenges,\\nenabling adaptive systems to learn optimal policies based on previous\\nadaptation experiences. Although RL has been applied to AUIs,integrating RL\\nagents effectively within user interactions remains a challenge.\\n  In this paper, we enhance a RL-based Adaptive User Interface adaption\\nframework by incorporating personalized human feedback directly into the\\nleaning process. Unlike prior approaches that rely on a single pre-trained RL\\nmodel, our approach trains a unique RL agent for each user, allowing\\nindividuals to actively shape their personal RL agent's policy, potentially\\nleading to more personalized and responsive UI adaptations. To evaluate this\\napproach, we conducted an empirical study to assess the impact of integrating\\nhuman feedback into the RL-based Adaptive User Interface adaption framework and\\nits effect on User Experience (UX). The study involved 33 participants\\ninteracting with AUIs incorporating human feedback and non-adaptive user\\ninterfaces in two domains: an e-learning platform and a trip-planning\\napplication. The results suggest that incorporating human feedback into\\nRL-driven adaptations significantly enhances UX, offering promising directions\\nfor advancing adaptive capabilities and user-centered design in AUIs.\", \"main_category\": \"cs.HC\", \"categories\": \"cs.HC,cs.SE\", \"published\": \"2025-04-29T14:00:22Z\"}"}

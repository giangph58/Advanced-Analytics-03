{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20858v1\", \"title\": \"Chaos Meets Attention: Transformers for Large-Scale Dynamical Prediction\", \"summary\": \"Generating long-term trajectories of dissipative chaotic systems\\nautoregressively is a highly challenging task. The inherent positive Lyapunov\\nexponents amplify prediction errors over time. Many chaotic systems possess a\\ncrucial property - ergodicity on their attractors, which makes long-term\\nprediction possible. State-of-the-art methods address ergodicity by preserving\\nstatistical properties using optimal transport techniques. However, these\\nmethods face scalability challenges due to the curse of dimensionality when\\nmatching distributions. To overcome this bottleneck, we propose a scalable\\ntransformer-based framework capable of stably generating long-term\\nhigh-dimensional and high-resolution chaotic dynamics while preserving\\nergodicity. Our method is grounded in a physical perspective, revisiting the\\nVon Neumann mean ergodic theorem to ensure the preservation of long-term\\nstatistics in the $\\\\mathcal{L}^2$ space. We introduce novel modifications to\\nthe attention mechanism, making the transformer architecture well-suited for\\nlearning large-scale chaotic systems. Compared to operator-based and\\ntransformer-based methods, our model achieves better performances across five\\nmetrics, from short-term prediction accuracy to long-term statistics. In\\naddition to our methodological contributions, we introduce a new chaotic system\\nbenchmark: a machine learning dataset of 140$k$ snapshots of turbulent channel\\nflow along with various evaluation metrics for both short- and long-term\\nperformances, which is well-suited for machine learning research on chaotic\\nsystems.\", \"main_category\": \"nlin.CD\", \"categories\": \"nlin.CD\", \"published\": \"2025-04-29T15:32:36Z\"}"}

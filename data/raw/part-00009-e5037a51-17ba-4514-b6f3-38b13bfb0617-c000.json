{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16505v1\", \"title\": \"TraveLLaMA: Facilitating Multi-modal Large Language Models to Understand\\n  Urban Scenes and Provide Travel Assistance\", \"summary\": \"Tourism and travel planning increasingly rely on digital assistance, yet\\nexisting multimodal AI systems often lack specialized knowledge and contextual\\nunderstanding of urban environments. We present TraveLLaMA, a specialized\\nmultimodal language model designed for urban scene understanding and travel\\nassistance. Our work addresses the fundamental challenge of developing\\npractical AI travel assistants through a novel large-scale dataset of 220k\\nquestion-answer pairs. This comprehensive dataset uniquely combines 130k text\\nQA pairs meticulously curated from authentic travel forums with GPT-enhanced\\nresponses, alongside 90k vision-language QA pairs specifically focused on map\\nunderstanding and scene comprehension. Through extensive fine-tuning\\nexperiments on state-of-the-art vision-language models (LLaVA, Qwen-VL,\\nShikra), we demonstrate significant performance improvements ranging from\\n6.5\\\\%-9.4\\\\% in both pure text travel understanding and visual question\\nanswering tasks. Our model exhibits exceptional capabilities in providing\\ncontextual travel recommendations, interpreting map locations, and\\nunderstanding place-specific imagery while offering practical information such\\nas operating hours and visitor reviews. Comparative evaluations show TraveLLaMA\\nsignificantly outperforms general-purpose models in travel-specific tasks,\\nestablishing a new benchmark for multi-modal travel assistance systems.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.MM\", \"published\": \"2025-04-23T08:32:25Z\"}"}

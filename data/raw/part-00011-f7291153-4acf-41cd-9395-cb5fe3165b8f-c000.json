{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15135v1\", \"title\": \"KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking\", \"summary\": \"Entity linking (EL) aligns textual mentions with their corresponding entities\\nin a knowledge base, facilitating various applications such as semantic search\\nand question answering. Recent advances in multimodal entity linking (MEL) have\\nshown that combining text and images can reduce ambiguity and improve alignment\\naccuracy. However, most existing MEL methods overlook the rich structural\\ninformation available in the form of knowledge-graph (KG) triples. In this\\npaper, we propose KGMEL, a novel framework that leverages KG triples to enhance\\nMEL. Specifically, it operates in three stages: (1) Generation: Produces\\nhigh-quality triples for each mention by employing vision-language models based\\non its text and images. (2) Retrieval: Learns joint mention-entity\\nrepresentations, via contrastive learning, that integrate text, images, and\\n(generated or KG) triples to retrieve candidate entities for each mention. (3)\\nReranking: Refines the KG triples of the candidate entities and employs large\\nlanguage models to identify the best-matching entity for the mention. Extensive\\nexperiments on benchmark datasets demonstrate that KGMEL outperforms existing\\nmethods. Our code and datasets are available at:\\nhttps://github.com/juyeonnn/KGMEL.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR,cs.AI,cs.CL\", \"published\": \"2025-04-21T14:38:44Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00347v1\", \"title\": \"Pushing the Limits of Low-Bit Optimizers: A Focus on EMA Dynamics\", \"summary\": \"The explosion in model sizes leads to continued growth in prohibitive\\ntraining/fine-tuning costs, particularly for stateful optimizers which maintain\\nauxiliary information of even 2x the model size to achieve optimal convergence.\\nWe therefore present in this work a novel type of optimizer that carries with\\nextremely lightweight state overloads, achieved through ultra-low-precision\\nquantization. While previous efforts have achieved certain success with 8-bit\\nor 4-bit quantization, our approach enables optimizers to operate at precision\\nas low as 3 bits, or even 2 bits per state element. This is accomplished by\\nidentifying and addressing two critical challenges: the signal swamping problem\\nin unsigned quantization that results in unchanged state dynamics, and the\\nrapidly increased gradient variance in signed quantization that leads to\\nincorrect descent directions. The theoretical analysis suggests a tailored\\nlogarithmic quantization for the former and a precision-specific momentum value\\nfor the latter. Consequently, the proposed SOLO achieves substantial memory\\nsavings (approximately 45 GB when training a 7B model) with minimal accuracy\\nloss. We hope that SOLO can contribute to overcoming the bottleneck in\\ncomputational resources, thereby promoting greater accessibility in fundamental\\nresearch.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-05-01T06:47:45Z\"}"}

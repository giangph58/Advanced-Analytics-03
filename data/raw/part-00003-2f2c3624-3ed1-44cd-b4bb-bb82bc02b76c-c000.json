{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20997v1\", \"title\": \"Toward Efficient Exploration by Large Language Model Agents\", \"summary\": \"A burgeoning area within reinforcement learning (RL) is the design of\\nsequential decision-making agents centered around large language models (LLMs).\\nWhile autonomous decision-making agents powered by modern LLMs could facilitate\\nnumerous real-world applications, such successes demand agents that are capable\\nof data-efficient RL. One key obstacle to achieving data efficiency in RL is\\nexploration, a challenge that we demonstrate many recent proposals for LLM\\nagent designs struggle to contend with. Meanwhile, classic algorithms from the\\nRL literature known to gracefully address exploration require technical\\nmachinery that can be challenging to operationalize in purely natural language\\nsettings. In this work, rather than relying on finetuning or in-context\\nlearning to coax LLMs into implicitly imitating a RL algorithm, we illustrate\\nhow LLMs can be used to explicitly implement an existing RL algorithm\\n(Posterior Sampling for Reinforcement Learning) whose capacity for\\nstatistically-efficient exploration is already well-studied. We offer empirical\\nresults demonstrating how our LLM-based implementation of a known,\\ndata-efficient RL algorithm can be considerably more effective in natural\\nlanguage tasks that demand prudent exploration.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-29T17:59:48Z\"}"}

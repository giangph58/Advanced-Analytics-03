{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20445v1\", \"title\": \"Head-Tail-Aware KL Divergence in Knowledge Distillation for Spiking\\n  Neural Networks\", \"summary\": \"Spiking Neural Networks (SNNs) have emerged as a promising approach for\\nenergy-efficient and biologically plausible computation. However, due to\\nlimitations in existing training methods and inherent model constraints, SNNs\\noften exhibit a performance gap when compared to Artificial Neural Networks\\n(ANNs). Knowledge distillation (KD) has been explored as a technique to\\ntransfer knowledge from ANN teacher models to SNN student models to mitigate\\nthis gap. Traditional KD methods typically use Kullback-Leibler (KL) divergence\\nto align output distributions. However, conventional KL-based approaches fail\\nto fully exploit the unique characteristics of SNNs, as they tend to\\noveremphasize high-probability predictions while neglecting low-probability\\nones, leading to suboptimal generalization. To address this, we propose\\nHead-Tail Aware Kullback-Leibler (HTA-KL) divergence, a novel KD method for\\nSNNs. HTA-KL introduces a cumulative probability-based mask to dynamically\\ndistinguish between high- and low-probability regions. It assigns adaptive\\nweights to ensure balanced knowledge transfer, enhancing the overall\\nperformance. By integrating forward KL (FKL) and reverse KL (RKL) divergence,\\nour method effectively align both head and tail regions of the distribution. We\\nevaluate our methods on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets. Our\\nmethod outperforms existing methods on most datasets with fewer timesteps.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-29T05:36:32Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20771v1\", \"title\": \"Turing Machine Evaluation for Large Language Model\", \"summary\": \"With the rapid development and widespread application of Large Language\\nModels (LLMs), rigorous evaluation has become particularly crucial. This\\nresearch adopts a novel perspective, focusing on evaluating the core\\ncomputational reasoning ability of LLMs, defined as the capacity of model to\\naccurately understand rules, and execute logically computing operations. This\\ncapability assesses the reliability of LLMs as precise executors, and is\\ncritical to advanced tasks such as complex code generation and multi-step\\nproblem-solving. We propose an evaluation framework based on Universal Turing\\nMachine (UTM) simulation. This framework requires LLMs to strictly follow\\ninstructions and track dynamic states, such as tape content and read/write head\\nposition, during multi-step computations. To enable standardized evaluation, we\\ndeveloped TMBench, a benchmark for systematically studying the computational\\nreasoning capabilities of LLMs. TMBench provides several key advantages,\\nincluding knowledge-agnostic evaluation, adjustable difficulty, foundational\\ncoverage through Turing machine encoding, and unlimited capacity for instance\\ngeneration, ensuring scalability as models continue to evolve. We find that\\nmodel performance on TMBench correlates strongly with performance on other\\nrecognized reasoning benchmarks (Pearson correlation coefficient is 0.73),\\nclearly demonstrating that computational reasoning is a significant dimension\\nfor measuring the deep capabilities of LLMs. Code and data are available at\\nhttps://github.com/HaitaoWuTJU/Turing-Machine-Bench.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-29T13:52:47Z\"}"}

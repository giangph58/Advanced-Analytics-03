{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02810v1\", \"title\": \"Generative Evaluation of Complex Reasoning in Large Language Models\", \"summary\": \"With powerful large language models (LLMs) demonstrating superhuman reasoning\\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\\nmerely recall answers from their extensive, web-scraped training datasets?\\nPublicly released benchmarks inevitably become contaminated once incorporated\\ninto subsequent LLM training sets, undermining their reliability as faithful\\nassessments. To address this, we introduce KUMO, a generative evaluation\\nframework designed specifically for assessing reasoning in LLMs. KUMO\\nsynergistically combines LLMs with symbolic engines to dynamically produce\\ndiverse, multi-turn reasoning tasks that are partially observable and\\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\\ngenerates novel tasks across open-ended domains, compelling models to\\ndemonstrate genuine generalization rather than memorization. We evaluated 23\\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\\nbenchmarking their reasoning abilities against university students. Our\\nfindings reveal that many LLMs have outperformed university-level performance\\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\\ntasks correlates strongly with results on newly released real-world reasoning\\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\\ngenuine LLM reasoning capabilities.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-03T17:54:18Z\"}"}

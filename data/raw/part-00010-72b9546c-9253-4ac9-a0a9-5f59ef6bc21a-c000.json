{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20879v1\", \"title\": \"The Leaderboard Illusion\", \"summary\": \"Measuring progress is fundamental to the advancement of any scientific field.\\nAs benchmarks play an increasingly central role, they also grow more\\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\\nfor ranking the most capable AI systems. Yet, in this work we identify\\nsystematic issues that have resulted in a distorted playing field. We find that\\nundisclosed private testing practices benefit a handful of providers who are\\nable to test multiple variants before public release and retract scores if\\ndesired. We establish that the ability of these providers to choose the best\\nscore leads to biased Arena scores due to selective disclosure of performance\\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\\nmodels are sampled at higher rates (number of battles) and have fewer models\\nremoved from the arena than open-weight and open-source alternatives. Both\\nthese policies lead to large data access asymmetries over time. Providers like\\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\\narena, respectively. In contrast, a combined 83 open-weight models have only\\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\\nArena data yields substantial benefits; even limited additional data can result\\nin relative performance gains of up to 112% on the arena distribution, based on\\nour conservative estimates. Together, these dynamics result in overfitting to\\nArena-specific dynamics rather than general model quality. The Arena builds on\\nthe substantial efforts of both the organizers and an open community that\\nmaintains this valuable evaluation platform. We offer actionable\\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\\nfairer, more transparent benchmarking for the field\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CL,cs.LG,stat.ME\", \"published\": \"2025-04-29T15:48:49Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.21730v1\", \"title\": \"Effective Skill Unlearning through Intervention and Abstention\", \"summary\": \"Large language Models (LLMs) have demonstrated remarkable skills across\\nvarious domains. Understanding the mechanisms behind their abilities and\\nimplementing controls over them is becoming increasingly important for\\ndeveloping better models. In this paper, we focus on skill unlearning in LLMs,\\nspecifically unlearning a particular skill while retaining their overall\\ncapabilities. We introduce two lightweight, training-free machine skill\\nunlearning techniques for LLMs. First, we observe that the pre-activation\\ndistribution of neurons in each Feed-Forward Layer (FFL) differs when the model\\ndemonstrates different skills. Additionally, we find that queries triggering\\nthe same skill cluster within the FFL key space and can be separated from other\\nqueries using a hypercube. Based on these observations, we propose two\\nlightweight, training-free skill unlearning methods via \\\\textit{intervention}\\nand \\\\textit{abstention} respectively: \\\\texttt{Neuron Adjust} and \\\\texttt{Key\\nSpace Detection}. We evaluate our methods on unlearning math-solving,\\nPython-coding, and comprehension skills across seven different languages. The\\nresults demonstrate their strong unlearning capabilities for the designated\\nskills. Specifically, \\\\texttt{Key Space Detection} achieves over 80\\\\% relative\\nperformance drop on the forgetting skill and less than 10\\\\% relative\\nperformance drop on other skills and the model's general knowledge (MMLU) for\\nmost unlearning tasks. Our code is available at\\nhttps://github.com/Trustworthy-ML-Lab/effective_skill_unlearning\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-03-27T17:45:06Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10995v1\", \"title\": \"TMCIR: Token Merge Benefits Composed Image Retrieval\", \"summary\": \"Composed Image Retrieval (CIR) retrieves target images using a multi-modal\\nquery that combines a reference image with text describing desired\\nmodifications. The primary challenge is effectively fusing this visual and\\ntextual information. Current cross-modal feature fusion approaches for CIR\\nexhibit an inherent bias in intention interpretation. These methods tend to\\ndisproportionately emphasize either the reference image features\\n(visual-dominant fusion) or the textual modification intent (text-dominant\\nfusion through image-to-text conversion). Such an imbalanced representation\\noften fails to accurately capture and reflect the actual search intent of the\\nuser in the retrieval results. To address this challenge, we propose TMCIR, a\\nnovel framework that advances composed image retrieval through two key\\ninnovations: 1) Intent-Aware Cross-Modal Alignment. We first fine-tune CLIP\\nencoders contrastively using intent-reflecting pseudo-target images,\\nsynthesized from reference images and textual descriptions via a diffusion\\nmodel. This step enhances the encoder ability of text to capture nuanced\\nintents in textual descriptions. 2) Adaptive Token Fusion. We further fine-tune\\nall encoders contrastively by comparing adaptive token-fusion features with the\\ntarget image. This mechanism dynamically balances visual and textual\\nrepresentations within the contrastive learning pipeline, optimizing the\\ncomposed feature for retrieval. Extensive experiments on Fashion-IQ and CIRR\\ndatasets demonstrate that TMCIR significantly outperforms state-of-the-art\\nmethods, particularly in capturing nuanced user intent.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-15T09:14:04Z\"}"}

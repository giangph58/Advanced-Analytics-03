{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12194v1\", \"title\": \"The Optimal Condition Number for ReLU Function\", \"summary\": \"ReLU is a widely used activation function in deep neural networks. This paper\\nexplores the stability properties of the ReLU map. For any weight matrix\\n$\\\\boldsymbol{A} \\\\in \\\\mathbb{R}^{m \\\\times n}$ and bias vector $\\\\boldsymbol{b}\\n\\\\in \\\\mathbb{R}^{m}$ at a given layer, we define the condition number\\n$\\\\beta_{\\\\boldsymbol{A},\\\\boldsymbol{b}}$ as\\n$\\\\beta_{\\\\boldsymbol{A},\\\\boldsymbol{b}} =\\n\\\\frac{\\\\mathcal{U}_{\\\\boldsymbol{A},\\\\boldsymbol{b}}}{\\\\mathcal{L}_{\\\\boldsymbol{A},\\\\boldsymbol{b}}}$,\\nwhere $\\\\mathcal{U}_{\\\\boldsymbol{A},\\\\boldsymbol{b}}$\\n  and $\\\\mathcal{L}_{\\\\boldsymbol{A},\\\\boldsymbol{b}}$ are the upper and lower\\nLipschitz constants, respectively. We first demonstrate that for any given\\n$\\\\boldsymbol{A}$ and $\\\\boldsymbol{b}$, the condition number satisfies\\n$\\\\beta_{\\\\boldsymbol{A},\\\\boldsymbol{b}} \\\\geq \\\\sqrt{2}$. Moreover, when the\\nweights of the network at a given layer are initialized as random i.i.d.\\nGaussian variables and the bias term is set to zero, the condition number\\nasymptotically approaches this lower bound. This theoretical finding suggests\\nthat Gaussian weight initialization is optimal for preserving distances in the\\ncontext of random deep neural network weights.\", \"main_category\": \"cs.IT\", \"categories\": \"cs.IT,math.IT\", \"published\": \"2025-04-16T15:47:38Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12742v1\", \"title\": \"Decentralized Nonconvex Composite Federated Learning with Gradient\\n  Tracking and Momentum\", \"summary\": \"Decentralized Federated Learning (DFL) eliminates the reliance on the\\nserver-client architecture inherent in traditional federated learning,\\nattracting significant research interest in recent years. Simultaneously, the\\nobjective functions in machine learning tasks are often nonconvex and\\nfrequently incorporate additional, potentially nonsmooth regularization terms\\nto satisfy practical requirements, thereby forming nonconvex composite\\noptimization problems. Employing DFL methods to solve such general optimization\\nproblems leads to the formulation of Decentralized Nonconvex Composite\\nFederated Learning (DNCFL), a topic that remains largely underexplored. In this\\npaper, we propose a novel DNCFL algorithm, termed \\\\bf{DEPOSITUM}. Built upon\\nproximal stochastic gradient tracking, DEPOSITUM mitigates the impact of data\\nheterogeneity by enabling clients to approximate the global gradient. The\\nintroduction of momentums in the proximal gradient descent step, replacing\\ntracking variables, reduces the variance introduced by stochastic gradients.\\nAdditionally, DEPOSITUM supports local updates of client variables,\\nsignificantly reducing communication costs. Theoretical analysis demonstrates\\nthat DEPOSITUM achieves an expected $\\\\epsilon$-stationary point with an\\niteration complexity of $\\\\mathcal{O}(1/\\\\epsilon^2)$. The proximal gradient,\\nconsensus errors, and gradient estimation errors decrease at a sublinear rate\\nof $\\\\mathcal{O}(1/T)$. With appropriate parameter selection, the algorithm\\nachieves network-independent linear speedup without requiring mega-batch\\nsampling. Finally, we apply DEPOSITUM to the training of neural networks on\\nreal-world datasets, systematically examining the influence of various\\nhyperparameters on its performance. Comparisons with other federated composite\\noptimization algorithms validate the effectiveness of the proposed method.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.DC,math.OC\", \"published\": \"2025-04-17T08:32:25Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21323v1\", \"title\": \"How to Backdoor the Knowledge Distillation\", \"summary\": \"Knowledge distillation has become a cornerstone in modern machine learning\\nsystems, celebrated for its ability to transfer knowledge from a large, complex\\nteacher model to a more efficient student model. Traditionally, this process is\\nregarded as secure, assuming the teacher model is clean. This belief stems from\\nconventional backdoor attacks relying on poisoned training data with backdoor\\ntriggers and attacker-chosen labels, which are not involved in the distillation\\nprocess. Instead, knowledge distillation uses the outputs of a clean teacher\\nmodel to guide the student model, inherently preventing recognition or response\\nto backdoor triggers as intended by an attacker. In this paper, we challenge\\nthis assumption by introducing a novel attack methodology that strategically\\npoisons the distillation dataset with adversarial examples embedded with\\nbackdoor triggers. This technique allows for the stealthy compromise of the\\nstudent model while maintaining the integrity of the teacher model. Our\\ninnovative approach represents the first successful exploitation of\\nvulnerabilities within the knowledge distillation process using clean teacher\\nmodels. Through extensive experiments conducted across various datasets and\\nattack settings, we demonstrate the robustness, stealthiness, and effectiveness\\nof our method. Our findings reveal previously unrecognized vulnerabilities and\\npave the way for future research aimed at securing knowledge distillation\\nprocesses against backdoor attacks.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.AI,cs.LG\", \"published\": \"2025-04-30T05:19:23Z\"}"}

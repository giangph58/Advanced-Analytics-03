{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12608v1\", \"title\": \"Code Copycat Conundrum: Demystifying Repetition in LLM-based Code\\n  Generation\", \"summary\": \"Despite recent advances in Large Language Models (LLMs) for code generation,\\nthe quality of LLM-generated code still faces significant challenges. One\\nsignificant issue is code repetition, which refers to the model's tendency to\\ngenerate structurally redundant code, resulting in inefficiencies and reduced\\nreadability. To address this, we conduct the first empirical study to\\ninvestigate the prevalence and nature of repetition across 19 state-of-the-art\\ncode LLMs using three widely-used benchmarks. Our study includes both\\nquantitative and qualitative analyses, revealing that repetition is pervasive\\nand manifests at various granularities and extents, including character,\\nstatement, and block levels. We further summarize a taxonomy of 20 repetition\\npatterns. Building on our findings, we propose DeRep, a rule-based technique\\ndesigned to detect and mitigate repetition in generated code. We evaluate DeRep\\nusing both open-source benchmarks and in an industrial setting. Our results\\ndemonstrate that DeRep significantly outperforms baselines in reducing\\nrepetition (with an average improvements of 91.3%, 93.5%, and 79.9% in rep-3,\\nrep-line, and sim-line metrics) and enhancing code quality (with a Pass@1\\nincrease of 208.3% over greedy search). Furthermore, integrating DeRep improves\\nthe performance of existing repetition mitigation methods, with Pass@1\\nimprovements ranging from 53.7% to 215.7%.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE,cs.AI\", \"published\": \"2025-04-17T03:13:39Z\"}"}

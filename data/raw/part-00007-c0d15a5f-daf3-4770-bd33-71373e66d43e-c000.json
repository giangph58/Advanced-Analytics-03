{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21478v1\", \"title\": \"CAE-DFKD: Bridging the Transferability Gap in Data-Free Knowledge\\n  Distillation\", \"summary\": \"Data-Free Knowledge Distillation (DFKD) enables the knowledge transfer from\\nthe given pre-trained teacher network to the target student model without\\naccess to the real training data. Existing DFKD methods focus primarily on\\nimproving image recognition performance on associated datasets, often\\nneglecting the crucial aspect of the transferability of learned\\nrepresentations. In this paper, we propose Category-Aware Embedding Data-Free\\nKnowledge Distillation (CAE-DFKD), which addresses at the embedding level the\\nlimitations of previous rely on image-level methods to improve model\\ngeneralization but fail when directly applied to DFKD. The superiority and\\nflexibility of CAE-DFKD are extensively evaluated, including:\\n\\\\textit{\\\\textbf{i.)}} Significant efficiency advantages resulting from altering\\nthe generator training paradigm; \\\\textit{\\\\textbf{ii.)}} Competitive performance\\nwith existing DFKD state-of-the-art methods on image recognition tasks;\\n\\\\textit{\\\\textbf{iii.)}} Remarkable transferability of data-free learned\\nrepresentations demonstrated in downstream tasks.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.NE\", \"published\": \"2025-04-30T09:58:02Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03729v1\", \"title\": \"Visual Imitation Enables Contextual Humanoid Control\", \"summary\": \"How can we teach humanoids to climb staircases and sit on chairs using the\\nsurrounding environment context? Arguably, the simplest way is to just show\\nthem-casually capture a human motion video and feed it to humanoids. We\\nintroduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday\\nvideos, jointly reconstructs the humans and the environment, and produces\\nwhole-body control policies for humanoid robots that perform the corresponding\\nskills. We demonstrate the results of our pipeline on real humanoid robots,\\nshowing robust, repeatable contextual control such as staircase ascents and\\ndescents, sitting and standing from chairs and benches, as well as other\\ndynamic whole-body skills-all from a single policy, conditioned on the\\nenvironment and global root commands. VIDEOMIMIC offers a scalable path towards\\nteaching humanoids to operate in diverse real-world environments.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.CV\", \"published\": \"2025-05-06T17:57:12Z\"}"}

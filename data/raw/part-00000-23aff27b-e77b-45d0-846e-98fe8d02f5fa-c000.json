{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16041v1\", \"title\": \"Muon Optimizer Accelerates Grokking\", \"summary\": \"This paper investigates the impact of different optimizers on the grokking\\nphenomenon, where models exhibit delayed generalization. We conducted\\nexperiments across seven numerical tasks (primarily modular arithmetic) using a\\nmodern Transformer architecture. The experimental configuration systematically\\nvaried the optimizer (Muon vs. AdamW) and the softmax activation function\\n(standard softmax, stablemax, and sparsemax) to assess their combined effect on\\nlearning dynamics. Our empirical evaluation reveals that the Muon optimizer,\\ncharacterized by its use of spectral norm constraints and second-order\\ninformation, significantly accelerates the onset of grokking compared to the\\nwidely used AdamW optimizer. Specifically, Muon reduced the mean grokking epoch\\nfrom 153.09 to 102.89 across all configurations, a statistically significant\\ndifference (t = 5.0175, p = 6.33e-08). This suggests that the optimizer choice\\nplays a crucial role in facilitating the transition from memorization to\\ngeneralization.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,I.2\", \"published\": \"2025-04-22T17:08:09Z\"}"}

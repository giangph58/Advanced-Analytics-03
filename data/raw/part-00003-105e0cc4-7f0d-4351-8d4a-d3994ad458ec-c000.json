{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05472v1\", \"title\": \"Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation\", \"summary\": \"Recent progress in unified models for image understanding and generation has\\nbeen impressive, yet most approaches remain limited to single-modal generation\\nconditioned on multiple modalities. In this paper, we present Mogao, a unified\\nframework that advances this paradigm by enabling interleaved multi-modal\\ngeneration through a causal approach. Mogao integrates a set of key technical\\nimprovements in architecture design, including a deep-fusion design, dual\\nvision encoders, interleaved rotary position embeddings, and multi-modal\\nclassifier-free guidance, which allow it to harness the strengths of both\\nautoregressive models for text generation and diffusion models for high-quality\\nimage synthesis. These practical improvements also make Mogao particularly\\neffective to process interleaved sequences of text and images arbitrarily. To\\nfurther unlock the potential of unified models, we introduce an efficient\\ntraining strategy on a large-scale, in-house dataset specifically curated for\\njoint text and image generation. Extensive experiments show that Mogao not only\\nachieves state-of-the-art performance in multi-modal understanding and\\ntext-to-image generation, but also excels in producing high-quality, coherent\\ninterleaved outputs. Its emergent capabilities in zero-shot image editing and\\ncompositional generation highlight Mogao as a practical omni-modal foundation\\nmodel, paving the way for future development and scaling the unified\\nmulti-modal systems.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-05-08T17:58:57Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03530v1\", \"title\": \"Causal Intervention Framework for Variational Auto Encoder Mechanistic\\n  Interpretability\", \"summary\": \"Mechanistic interpretability of deep learning models has emerged as a crucial\\nresearch direction for understanding the functioning of neural networks. While\\nsignificant progress has been made in interpreting discriminative models like\\ntransformers, understanding generative models such as Variational Autoencoders\\n(VAEs) remains challenging. This paper introduces a comprehensive causal\\nintervention framework for mechanistic interpretability of VAEs. We develop\\ntechniques to identify and analyze \\\"circuit motifs\\\" in VAEs, examining how\\nsemantic factors are encoded, processed, and disentangled through the network\\nlayers. Our approach uses targeted interventions at different levels: input\\nmanipulations, latent space perturbations, activation patching, and causal\\nmediation analysis. We apply our framework to both synthetic datasets with\\nknown causal relationships and standard disentanglement benchmarks. Results\\nshow that our interventions can successfully isolate functional circuits, map\\ncomputational graphs to causal graphs of semantic factors, and distinguish\\nbetween polysemantic and monosemantic units. Furthermore, we introduce metrics\\nfor causal effect strength, intervention specificity, and circuit modularity\\nthat quantify the interpretability of VAE components. Experimental results\\ndemonstrate clear differences between VAE variants, with FactorVAE achieving\\nhigher disentanglement scores (0.084) and effect strengths (mean 4.59) compared\\nto standard VAE (0.064, 3.99) and Beta-VAE (0.051, 3.43). Our framework\\nadvances the mechanistic understanding of generative models and provides tools\\nfor more transparent and controllable VAE architectures.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-06T13:40:59Z\"}"}

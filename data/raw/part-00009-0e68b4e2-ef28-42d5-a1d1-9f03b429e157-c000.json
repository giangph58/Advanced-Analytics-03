{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16754v1\", \"title\": \"HEMA : A Hippocampus-Inspired Extended Memory Architecture for\\n  Long-Context AI Conversations\", \"summary\": \"Large language models (LLMs) struggle with maintaining coherence in extended\\nconversations spanning hundreds of turns, despite performing well within their\\ncontext windows. This paper introduces HEMA (Hippocampus-Inspired Extended\\nMemory Architecture), a dual-memory system inspired by human cognitive\\nprocesses. HEMA combines Compact Memory - a continuously updated one-sentence\\nsummary preserving global narrative coherence, and Vector Memory - an episodic\\nstore of chunk embeddings queried via cosine similarity. When integrated with a\\n6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns\\nwhile keeping prompt length under 3,500 tokens. Experimental results show\\nsubstantial improvements: factual recall accuracy increases from 41% to 87%,\\nand human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K\\nindexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling\\nthe area under the precision-recall curve compared to summarization-only\\napproaches. Ablation studies reveal two key insights: semantic forgetting\\nthrough age-weighted pruning reduces retrieval latency by 34% with minimal\\nrecall loss, and a two-level summary hierarchy prevents cascade errors in\\nultra-long conversations exceeding 1,000 turns. HEMA demonstrates that\\ncombining verbatim recall with semantic continuity provides a practical\\nsolution for privacy-aware conversational AI capable of month-long dialogues\\nwithout model retraining.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-23T14:27:12Z\"}"}

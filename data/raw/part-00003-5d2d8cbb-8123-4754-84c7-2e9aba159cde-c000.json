{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17582v1\", \"title\": \"Occlusion-Aware Self-Supervised Monocular Depth Estimation for\\n  Weak-Texture Endoscopic Images\", \"summary\": \"We propose a self-supervised monocular depth estimation network tailored for\\nendoscopic scenes, aiming to infer depth within the gastrointestinal tract from\\nmonocular images. Existing methods, though accurate, typically assume\\nconsistent illumination, which is often violated due to dynamic lighting and\\nocclusions caused by GI motility. These variations lead to incorrect geometric\\ninterpretations and unreliable self-supervised signals, degrading depth\\nreconstruction quality. To address this, we introduce an occlusion-aware\\nself-supervised framework. First, we incorporate an occlusion mask for data\\naugmentation, generating pseudo-labels by simulating viewpoint-dependent\\nocclusion scenarios. This enhances the model's ability to learn robust depth\\nfeatures under partial visibility. Second, we leverage semantic segmentation\\nguided by non-negative matrix factorization, clustering convolutional\\nactivations to generate pseudo-labels in texture-deprived regions, thereby\\nimproving segmentation accuracy and mitigating information loss from lighting\\nchanges. Experimental results on the SCARED dataset show that our method\\nachieves state-of-the-art performance in self-supervised depth estimation.\\nAdditionally, evaluations on the Endo-SLAM and SERV-CT datasets demonstrate\\nstrong generalization across diverse endoscopic environments.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-24T14:12:57Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05279v1\", \"title\": \"MTL-UE: Learning to Learn Nothing for Multi-Task Learning\", \"summary\": \"Most existing unlearnable strategies focus on preventing unauthorized users\\nfrom training single-task learning (STL) models with personal data.\\nNevertheless, the paradigm has recently shifted towards multi-task data and\\nmulti-task learning (MTL), targeting generalist and foundation models that can\\nhandle multiple tasks simultaneously. Despite their growing importance, MTL\\ndata and models have been largely neglected while pursuing unlearnable\\nstrategies. This paper presents MTL-UE, the first unified framework for\\ngenerating unlearnable examples for multi-task data and MTL models. Instead of\\noptimizing perturbations for each sample, we design a generator-based structure\\nthat introduces label priors and class-wise feature embeddings which leads to\\nmuch better attacking performance. In addition, MTL-UE incorporates intra-task\\nand inter-task embedding regularization to increase inter-class separation and\\nsuppress intra-class variance which enhances the attack robustness greatly.\\nFurthermore, MTL-UE is versatile with good supports for dense prediction tasks\\nin MTL. It is also plug-and-play allowing integrating existing\\nsurrogate-dependent unlearnable methods with little adaptation. Extensive\\nexperiments show that MTL-UE achieves superior attacking performance\\nconsistently across 4 MTL datasets, 3 base UE methods, 5 model backbones, and 5\\nMTL task-weighting strategies.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CR,cs.CV\", \"published\": \"2025-05-08T14:26:00Z\"}"}

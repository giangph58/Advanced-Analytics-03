{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20593v1\", \"title\": \"Independent Learning in Performative Markov Potential Games\", \"summary\": \"Performative Reinforcement Learning (PRL) refers to a scenario in which the\\ndeployed policy changes the reward and transition dynamics of the underlying\\nenvironment. In this work, we study multi-agent PRL by incorporating\\nperformative effects into Markov Potential Games (MPGs). We introduce the\\nnotion of a performatively stable equilibrium (PSE) and show that it always\\nexists under a reasonable sensitivity assumption. We then provide convergence\\nresults for state-of-the-art algorithms used to solve MPGs. Specifically, we\\nshow that independent policy gradient ascent (IPGA) and independent natural\\npolicy gradient (INPG) converge to an approximate PSE in the best-iterate\\nsense, with an additional term that accounts for the performative effects.\\nFurthermore, we show that INPG asymptotically converges to a PSE in the\\nlast-iterate sense. As the performative effects vanish, we recover the\\nconvergence rates from prior work. For a special case of our game, we provide\\nfinite-time last-iterate convergence results for a repeated retraining\\napproach, in which agents independently optimize a surrogate objective. We\\nconduct extensive experiments to validate our theoretical findings.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-29T09:46:16Z\"}"}

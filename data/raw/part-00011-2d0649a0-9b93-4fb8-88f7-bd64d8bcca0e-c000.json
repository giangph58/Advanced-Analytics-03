{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12011v1\", \"title\": \"Balancing Graph Embedding Smoothness in Self-Supervised Learning via\\n  Information-Theoretic Decomposition\", \"summary\": \"Self-supervised learning (SSL) in graphs has garnered significant attention,\\nparticularly in employing Graph Neural Networks (GNNs) with pretext tasks\\ninitially designed for other domains, such as contrastive learning and feature\\nreconstruction. However, it remains uncertain whether these methods effectively\\nreflect essential graph properties, precisely representation similarity with\\nits neighbors. We observe that existing methods position opposite ends of a\\nspectrum driven by the graph embedding smoothness, with each end corresponding\\nto outperformance on specific downstream tasks. Decomposing the SSL objective\\ninto three terms via an information-theoretic framework with a neighbor\\nrepresentation variable reveals that this polarization stems from an imbalance\\namong the terms, which existing methods may not effectively maintain. Further\\ninsights suggest that balancing between the extremes can lead to improved\\nperformance across a wider range of downstream tasks. A framework, BSG\\n(Balancing Smoothness in Graph SSL), introduces novel loss functions designed\\nto supplement the representation quality in graph-based SSL by balancing the\\nderived three terms: neighbor loss, minimal loss, and divergence loss. We\\npresent a theoretical analysis of the effects of these loss functions,\\nhighlighting their significance from both the SSL and graph smoothness\\nperspectives. Extensive experiments on multiple real-world datasets across node\\nclassification and link prediction consistently demonstrate that BSG achieves\\nstate-of-the-art performance, outperforming existing methods. Our\\nimplementation code is available at https://github.com/steve30572/BSG.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-16T12:09:56Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05759v1\", \"title\": \"RETROcode: Leveraging a Code Database for Improved Natural Language to\\n  Code Generation\", \"summary\": \"As text and code resources have expanded, large-scale pre-trained models have\\nshown promising capabilities in code generation tasks, typically employing\\nsupervised fine-tuning with problem statement-program pairs. However,\\nincreasing model size and data volume for performance gains also raises\\ncomputational demands and risks of overfitting. Addressing these challenges, we\\npresent RETROcode, a novel adaptation of the RETRO architecture \\\\cite{RETRO}\\nfor sequence-to-sequence models, utilizing a large code database as an\\nauxiliary scaling method. This approach, diverging from simply enlarging model\\nand dataset sizes, allows RETROcode to leverage a vast code database for\\nprediction, enhancing the model's efficiency by integrating extensive memory.\\nOur findings indicate that RETROcode not only outperforms similar-sized\\ntraditional architectures on test sets but also approaches the effectiveness of\\nthe much larger Codex model, despite being trained from scratch on a\\nsubstantially smaller dataset.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-08T07:41:13Z\"}"}

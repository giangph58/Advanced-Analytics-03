{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24371v1\", \"title\": \"Policy Gradient for LQR with Domain Randomization\", \"summary\": \"Domain randomization (DR) enables sim-to-real transfer by training\\ncontrollers on a distribution of simulated environments, with the goal of\\nachieving robust performance in the real world. Although DR is widely used in\\npractice and is often solved using simple policy gradient (PG) methods,\\nunderstanding of its theoretical guarantees remains limited. Toward addressing\\nthis gap, we provide the first convergence analysis of PG methods for\\ndomain-randomized linear quadratic regulation (LQR). We show that PG converges\\nglobally to the minimizer of a finite-sample approximation of the DR objective\\nunder suitable bounds on the heterogeneity of the sampled systems. We also\\nquantify the sample-complexity associated with achieving a small performance\\ngap between the sample-average and population-level objectives. Additionally,\\nwe propose and analyze a discount-factor annealing algorithm that obviates the\\nneed for an initial jointly stabilizing controller, which may be challenging to\\nfind. Empirical results support our theoretical findings and highlight\\npromising directions for future work, including risk-sensitive DR formulations\\nand stochastic PG algorithms.\", \"main_category\": \"eess.SY\", \"categories\": \"eess.SY,cs.LG,cs.SY\", \"published\": \"2025-03-31T17:51:00Z\"}"}

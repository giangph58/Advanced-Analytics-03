{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00467v1\", \"title\": \"Red Teaming Large Language Models for Healthcare\", \"summary\": \"We present the design process and findings of the pre-conference workshop at\\nthe Machine Learning for Healthcare Conference (2024) entitled Red Teaming\\nLarge Language Models for Healthcare, which took place on August 15, 2024.\\nConference participants, comprising a mix of computational and clinical\\nexpertise, attempted to discover vulnerabilities -- realistic clinical prompts\\nfor which a large language model (LLM) outputs a response that could cause\\nclinical harm. Red-teaming with clinicians enables the identification of LLM\\nvulnerabilities that may not be recognised by LLM developers lacking clinical\\nexpertise. We report the vulnerabilities found, categorise them, and present\\nthe results of a replication study assessing the vulnerabilities across all\\nLLMs provided.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-05-01T11:43:27Z\"}"}

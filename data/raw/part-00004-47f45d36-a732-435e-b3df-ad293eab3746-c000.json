{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02404v1\", \"title\": \"AnesBench: Multi-Dimensional Evaluation of LLM Reasoning in\\n  Anesthesiology\", \"summary\": \"The application of large language models (LLMs) in the medical field has\\ngained significant attention, yet their reasoning capabilities in more\\nspecialized domains like anesthesiology remain underexplored. In this paper, we\\nsystematically evaluate the reasoning capabilities of LLMs in anesthesiology\\nand analyze key factors influencing their performance. To this end, we\\nintroduce AnesBench, a cross-lingual benchmark designed to assess\\nanesthesiology-related reasoning across three levels: factual retrieval (System\\n1), hybrid reasoning (System 1.x), and complex decision-making (System 2).\\nThrough extensive experiments, we first explore how model characteristics,\\nincluding model scale, Chain of Thought (CoT) length, and language\\ntransferability, affect reasoning performance. Then, we further evaluate the\\neffectiveness of different training strategies, leveraging our curated\\nanesthesiology-related dataset, including continuous pre-training (CPT) and\\nsupervised fine-tuning (SFT). Additionally, we also investigate how the\\ntest-time reasoning techniques, such as Best-of-N sampling and beam search,\\ninfluence reasoning performance, and assess the impact of reasoning-enhanced\\nmodel distillation, specifically DeepSeek-R1. We will publicly release\\nAnesBench, along with our CPT and SFT training datasets and evaluation code at\\nhttps://github.com/MiliLab/AnesBench.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-03T08:54:23Z\"}"}

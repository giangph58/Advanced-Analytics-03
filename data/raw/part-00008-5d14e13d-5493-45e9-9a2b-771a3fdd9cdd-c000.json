{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05150v1\", \"title\": \"A Reinforcement Learning Method for Environments with Stochastic\\n  Variables: Post-Decision Proximal Policy Optimization with Dual Critic\\n  Networks\", \"summary\": \"This paper presents Post-Decision Proximal Policy Optimization (PDPPO), a\\nnovel variation of the leading deep reinforcement learning method, Proximal\\nPolicy Optimization (PPO). The PDPPO state transition process is divided into\\ntwo steps: a deterministic step resulting in the post-decision state and a\\nstochastic step leading to the next state. Our approach incorporates\\npost-decision states and dual critics to reduce the problem's dimensionality\\nand enhance the accuracy of value function estimation. Lot-sizing is a mixed\\ninteger programming problem for which we exemplify such dynamics. The objective\\nof lot-sizing is to optimize production, delivery fulfillment, and inventory\\nlevels in uncertain demand and cost parameters. This paper evaluates the\\nperformance of PDPPO across various environments and configurations. Notably,\\nPDPPO with a dual critic architecture achieves nearly double the maximum reward\\nof vanilla PPO in specific scenarios, requiring fewer episode iterations and\\ndemonstrating faster and more consistent learning across different\\ninitializations. On average, PDPPO outperforms PPO in environments with a\\nstochastic component in the state transition. These results support the\\nbenefits of using a post-decision state. Integrating this post-decision state\\nin the value function approximation leads to more informed and efficient\\nlearning in high-dimensional and stochastic environments.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-07T14:56:43Z\"}"}

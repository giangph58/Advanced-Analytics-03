{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01848v1\", \"title\": \"PaperBench: Evaluating AI's Ability to Replicate AI Research\", \"summary\": \"We introduce PaperBench, a benchmark evaluating the ability of AI agents to\\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\\nSpotlight and Oral papers from scratch, including understanding paper\\ncontributions, developing a codebase, and successfully executing experiments.\\nFor objective evaluation, we develop rubrics that hierarchically decompose each\\nreplication task into smaller sub-tasks with clear grading criteria. In total,\\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\\nwith the author(s) of each ICML paper for accuracy and realism. To enable\\nscalable evaluation, we also develop an LLM-based judge to automatically grade\\nreplication attempts against rubrics, and assess our judge's performance by\\ncreating a separate benchmark for judges. We evaluate several frontier models\\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\\n(New) with open-source scaffolding, achieves an average replication score of\\n21.0\\\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\\nfinding that models do not yet outperform the human baseline. We\\n\\\\href{https://github.com/openai/preparedness}{open-source our code} to\\nfacilitate future research in understanding the AI engineering capabilities of\\nAI agents.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CL\", \"published\": \"2025-04-02T15:55:24Z\"}"}

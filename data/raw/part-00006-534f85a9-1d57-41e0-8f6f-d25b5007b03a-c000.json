{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10906v1\", \"title\": \"Understanding LLMs' Cross-Lingual Context Retrieval: How Good It Is And\\n  Where It Comes From\", \"summary\": \"The ability of cross-lingual context retrieval is a fundamental aspect of\\ncross-lingual alignment of large language models (LLMs), where the model\\nextracts context information in one language based on requests in another\\nlanguage. Despite its importance in real-life applications, this ability has\\nnot been adequately investigated for state-of-the-art models. In this paper, we\\nevaluate the cross-lingual context retrieval ability of over 40 LLMs across 12\\nlanguages to understand the source of this ability, using cross-lingual machine\\nreading comprehension (xMRC) as a representative scenario. Our results show\\nthat several small, post-trained open LLMs show strong cross-lingual context\\nretrieval ability, comparable to closed-source LLMs such as GPT-4o, and their\\nestimated oracle performances greatly improve after post-training. Our\\ninterpretability analysis shows that the cross-lingual context retrieval\\nprocess can be divided into two main phases: question encoding and answer\\nretrieval, which are formed in pre-training and post-training, respectively.\\nThe phasing stability correlates with xMRC performance, and the xMRC bottleneck\\nlies at the last model layers in the second phase, where the effect of\\npost-training can be evidently observed. Our results also indicate that\\nlarger-scale pretraining cannot improve the xMRC performance. Instead, larger\\nLLMs need further multilingual post-training to fully unlock their\\ncross-lingual context retrieval potential. Our code and is available at\\nhttps://github.com/NJUNLP/Cross-Lingual-Context-Retrieval\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-15T06:35:27Z\"}"}

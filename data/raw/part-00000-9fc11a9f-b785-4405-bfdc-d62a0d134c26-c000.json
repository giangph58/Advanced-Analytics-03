{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01952v1\", \"title\": \"Image Difference Grounding with Natural Language\", \"summary\": \"Visual grounding (VG) typically focuses on locating regions of interest\\nwithin an image using natural language, and most existing VG methods are\\nlimited to single-image interpretations. This limits their applicability in\\nreal-world scenarios like automatic surveillance, where detecting subtle but\\nmeaningful visual differences across multiple images is crucial. Besides,\\nprevious work on image difference understanding (IDU) has either focused on\\ndetecting all change regions without cross-modal text guidance, or on providing\\ncoarse-grained descriptions of differences. Therefore, to push towards\\nfiner-grained vision-language perception, we propose Image Difference Grounding\\n(IDG), a task designed to precisely localize visual differences based on user\\ninstructions. We introduce DiffGround, a large-scale and high-quality dataset\\nfor IDG, containing image pairs with diverse visual variations along with\\ninstructions querying fine-grained differences. Besides, we present a baseline\\nmodel for IDG, DiffTracker, which effectively integrates feature differential\\nenhancement and common suppression to precisely locate differences. Experiments\\non the DiffGround dataset highlight the importance of our IDG dataset in\\nenabling finer-grained IDU. To foster future research, both DiffGround data and\\nDiffTracker model will be publicly released.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-02T17:56:42Z\"}"}

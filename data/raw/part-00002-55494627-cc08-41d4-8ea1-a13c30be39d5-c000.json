{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00560v1\", \"title\": \"Efficient Recommendation with Millions of Items by Dynamic Pruning of\\n  Sub-Item Embeddings\", \"summary\": \"A large item catalogue is a major challenge for deploying modern sequential\\nrecommender models, since it makes the memory footprint of the model large and\\nincreases inference latency. One promising approach to address this is RecJPQ,\\nwhich replaces item embeddings with sub-item embeddings. However, slow\\ninference remains problematic because finding the top highest-scored items\\nusually requires scoring all items in the catalogue, which may not be feasible\\nfor large catalogues. By adapting dynamic pruning concepts from document\\nretrieval, we propose the RecJPQPrune dynamic pruning algorithm to efficiently\\nfind the top highest-scored items without computing the scores of all items in\\nthe catalogue. Our RecJPQPrune algorithm is safe-up-to-rank K since it\\ntheoretically guarantees that no potentially high-scored item is excluded from\\nthe final top K recommendation list, thereby ensuring no impact on\\neffectiveness. Our experiments on two large datasets and three recommendation\\nmodels demonstrate the efficiency achievable using RecJPQPrune: for instance,\\non the Tmall dataset with 2.2M items, we can reduce the median model scoring\\ntime by 64 times compared to the Transformer Default baseline, and 5.3 times\\ncompared to a recent scoring approach called PQTopK. Overall, this paper\\ndemonstrates the effective and efficient inference of Transformer-based\\nrecommendation models at catalogue scales not previously reported in the\\nliterature. Indeed, our RecJPQPrune algorithm can score 2 million items in\\nunder 10 milliseconds without GPUs, and without relying on Approximate Nearest\\nNeighbour (ANN) techniques.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR\", \"published\": \"2025-05-01T14:36:33Z\"}"}

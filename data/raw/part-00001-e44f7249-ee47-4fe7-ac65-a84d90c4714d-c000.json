{"value":"{\"aid\": \"http://arxiv.org/abs/2505.05063v1\", \"title\": \"CodeMixBench: Evaluating Large Language Models on Code Generation with\\n  Code-Mixed Prompts\", \"summary\": \"Large Language Models (LLMs) have achieved remarkable success in code\\ngeneration tasks, powering various applications like code completion,\\ndebugging, and programming assistance. However, existing benchmarks such as\\nHumanEval, MBPP, and BigCodeBench primarily evaluate LLMs on English-only\\nprompts, overlooking the real-world scenario where multilingual developers\\noften use code-mixed language while interacting with LLMs. To address this gap,\\nwe introduce CodeMixBench, a novel benchmark designed to evaluate the\\nrobustness of LLMs on code generation from code-mixed prompts. Built upon\\nBigCodeBench, CodeMixBench introduces controlled code-mixing (CMD) into the\\nnatural language parts of prompts across three language pairs: Hinglish\\n(Hindi-English), Spanish-English, and Chinese Pinyin-English. We\\ncomprehensively evaluate a diverse set of open-source code generation models\\nranging from 1.5B to 15B parameters. Our results show that code-mixed prompts\\nconsistently degrade Pass@1 performance compared to their English-only\\ncounterparts, with performance drops increasing under higher CMD levels for\\nsmaller models. CodeMixBench provides a realistic evaluation framework for\\nstudying multilingual code generation and highlights new challenges and\\ndirections for building robust code generation models that generalize well\\nacross diverse linguistic settings.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL\", \"published\": \"2025-05-08T08:55:32Z\"}"}

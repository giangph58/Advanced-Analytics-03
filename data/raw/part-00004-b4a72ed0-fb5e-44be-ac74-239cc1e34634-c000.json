{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11257v1\", \"title\": \"UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction\\n  Synthesis\", \"summary\": \"Recent advancements in Large Vision-Language Models are accelerating the\\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\\nvision perception capabilities to enhance productivity on digital devices.\\nCompared to approaches predicated on GUI metadata, which are platform-dependent\\nand vulnerable to implementation variations, vision-based approaches offer\\nbroader applicability. In this vision-based paradigm, the GUI instruction\\ngrounding, which maps user instruction to the location of corresponding element\\non the given screenshot, remains a critical challenge, particularly due to\\nlimited public training dataset and resource-intensive manual instruction data\\nannotation.In this paper, we delve into unexplored challenges in this task\\nincluding element-to-screen ratio, unbalanced element type, and implicit\\ninstruction. To address these challenges, we introduce a large-scale data\\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\\naddress the limitations of existing benchmarks by incorporating diverse\\nannotation aspects. Our model, trained on the synthesized data, achieves\\nsuperior performance in GUI instruction grounding, demonstrating the\\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\\naccompanied by extensive analyses, provides practical insights for future\\nresearch in GUI grounding. We will release corresponding artifacts at\\nhttps://colmon46.github.io/i2e-bench-leaderboard/\", \"main_category\": \"cs.HC\", \"categories\": \"cs.HC,cs.CL,cs.CV\", \"published\": \"2025-04-15T14:56:21Z\"}"}

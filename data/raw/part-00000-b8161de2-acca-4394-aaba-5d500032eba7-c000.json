{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24376v1\", \"title\": \"Exploring the Effect of Reinforcement Learning on Video Understanding:\\n  Insights from SEED-Bench-R1\", \"summary\": \"Recent advancements in Chain of Thought (COT) generation have significantly\\nimproved the reasoning capabilities of Large Language Models (LLMs), with\\nreinforcement learning (RL) emerging as an effective post-training approach.\\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\\nremain underexplored in tasks requiring both perception and logical reasoning.\\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\\nsystematically evaluate post-training methods for MLLMs in video understanding.\\nIt includes intricate real-world videos and complex everyday planning tasks in\\nthe format of multiple-choice questions, requiring sophisticated perception and\\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\\nhierarchy: in-distribution, cross-environment, and cross-environment-task\\nscenarios, equipped with a large-scale training dataset with easily verifiable\\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\\nsuperior performance on both in-distribution and out-of-distribution tasks,\\neven outperforming SFT on general video understanding benchmarks like\\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\\nperception but often produces less logically coherent reasoning chains. We\\nidentify key limitations such as inconsistent reasoning and overlooked visual\\ncues, and suggest future improvements in base model reasoning, reward modeling,\\nand RL robustness against noisy signals.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI,cs.CL,cs.LG\", \"published\": \"2025-03-31T17:55:23Z\"}"}

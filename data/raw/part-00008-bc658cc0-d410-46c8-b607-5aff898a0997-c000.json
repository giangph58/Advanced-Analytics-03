{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06738v1\", \"title\": \"EDIT: Enhancing Vision Transformers by Mitigating Attention Sink through\\n  an Encoder-Decoder Architecture\", \"summary\": \"In this paper, we propose EDIT (Encoder-Decoder Image Transformer), a novel\\narchitecture designed to mitigate the attention sink phenomenon observed in\\nVision Transformer models. Attention sink occurs when an excessive amount of\\nattention is allocated to the [CLS] token, distorting the model's ability to\\neffectively process image patches. To address this, we introduce a\\nlayer-aligned encoder-decoder architecture, where the encoder utilizes\\nself-attention to process image patches, while the decoder uses cross-attention\\nto focus on the [CLS] token. Unlike traditional encoder-decoder framework,\\nwhere the decoder depends solely on high-level encoder representations, EDIT\\nallows the decoder to extract information starting from low-level features,\\nprogressively refining the representation layer by layer. EDIT is naturally\\ninterpretable demonstrated through sequential attention maps, illustrating the\\nrefined, layer-by-layer focus on key image features. Experiments on ImageNet-1k\\nand ImageNet-21k, along with transfer learning tasks, show that EDIT achieves\\nconsistent performance improvements over DeiT3 models. These results highlight\\nthe effectiveness of EDIT's design in addressing attention sink and improving\\nvisual feature extraction.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-09T09:51:41Z\"}"}

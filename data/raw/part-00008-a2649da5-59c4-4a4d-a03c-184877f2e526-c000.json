{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07964v1\", \"title\": \"C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization\\n  for Test-Time Expert Re-Mixing\", \"summary\": \"Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely\\nsub-optimal expert pathways-our study reveals that naive expert selection\\nlearned from pretraining leaves a surprising 10-20% accuracy gap for\\nimprovement. Motivated by this observation, we develop a novel class of\\ntest-time optimization methods to re-weight or \\\"re-mixing\\\" the experts in\\ndifferent layers jointly for each test sample. Since the test sample's ground\\ntruth is unknown, we propose to optimize a surrogate objective defined by the\\nsample's \\\"successful neighbors\\\" from a reference set of samples. We introduce\\nthree surrogates and algorithms based on mode-finding, kernel regression, and\\nthe average loss of similar reference samples/tasks. To reduce the cost of\\noptimizing whole pathways, we apply our algorithms merely to the core experts'\\nmixing weights in critical layers, which enjoy similar performance but save\\nsignificant computation. This leads to \\\"Critical-Layer, Core-Expert,\\nCollaborative Pathway Optimization (C3PO)\\\". We apply C3PO to two recent MoE\\nLLMs and examine it on six widely-used benchmarks. It consistently improves the\\nbase model by 7-15% in accuracy and outperforms widely used test-time learning\\nbaselines, e.g., in-context learning and prompt/prefix tuning, by a large\\nmargin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to\\noutperform LLMs of 7-9B parameters, hence improving MoE's advantages on\\nefficiency. Our thorough ablation study further sheds novel insights on\\nachieving test-time improvement on MoE.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-10T17:59:56Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21308v1\", \"title\": \"AGHI-QA: A Subjective-Aligned Dataset and Metric for AI-Generated Human\\n  Images\", \"summary\": \"The rapid development of text-to-image (T2I) generation approaches has\\nattracted extensive interest in evaluating the quality of generated images,\\nleading to the development of various quality assessment methods for\\ngeneral-purpose T2I outputs. However, existing image quality assessment (IQA)\\nmethods are limited to providing global quality scores, failing to deliver\\nfine-grained perceptual evaluations for structurally complex subjects like\\nhumans, which is a critical challenge considering the frequent anatomical and\\ntextural distortions in AI-generated human images (AGHIs). To address this gap,\\nwe introduce AGHI-QA, the first large-scale benchmark specifically designed for\\nquality assessment of AGHIs. The dataset comprises 4,000 images generated from\\n400 carefully crafted text prompts using 10 state of-the-art T2I models. We\\nconduct a systematic subjective study to collect multidimensional annotations,\\nincluding perceptual quality scores, text-image correspondence scores, visible\\nand distorted body part labels. Based on AGHI-QA, we evaluate the strengths and\\nweaknesses of current T2I methods in generating human images from multiple\\ndimensions. Furthermore, we propose AGHI-Assessor, a novel quality metric that\\nintegrates the large multimodal model (LMM) with domain-specific human features\\nfor precise quality prediction and identification of visible and distorted body\\nparts in AGHIs. Extensive experimental results demonstrate that AGHI-Assessor\\nshowcases state-of-the-art performance, significantly outperforming existing\\nIQA methods in multidimensional quality assessment and surpassing leading LMMs\\nin detecting structural distortions in AGHIs.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-30T04:36:56Z\"}"}

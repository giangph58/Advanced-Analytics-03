{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21634v1\", \"title\": \"Quantitative Auditing of AI Fairness with Differentially Private\\n  Synthetic Data\", \"summary\": \"Fairness auditing of AI systems can identify and quantify biases. However,\\ntraditional auditing using real-world data raises security and privacy\\nconcerns. It exposes auditors to security risks as they become custodians of\\nsensitive information and targets for cyberattacks. Privacy risks arise even\\nwithout direct breaches, as data analyses can inadvertently expose confidential\\ninformation. To address these, we propose a framework that leverages\\ndifferentially private synthetic data to audit the fairness of AI systems. By\\napplying privacy-preserving mechanisms, it generates synthetic data that\\nmirrors the statistical properties of the original dataset while ensuring\\nprivacy. This method balances the goal of rigorous fairness auditing and the\\nneed for strong privacy protections. Through experiments on real datasets like\\nAdult, COMPAS, and Diabetes, we compare fairness metrics of synthetic and real\\ndata. By analyzing the alignment and discrepancies between these metrics, we\\nassess the capacity of synthetic data to preserve the fairness properties of\\nreal data. Our results demonstrate the framework's ability to enable meaningful\\nfairness evaluations while safeguarding sensitive information, proving its\\napplicability across critical and sensitive domains.\", \"main_category\": \"cs.CY\", \"categories\": \"cs.CY,cs.AI,cs.LG\", \"published\": \"2025-04-30T13:36:27Z\"}"}

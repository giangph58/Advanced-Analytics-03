{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10850v1\", \"title\": \"How to Enhance Downstream Adversarial Robustness (almost) without\\n  Touching the Pre-Trained Foundation Model?\", \"summary\": \"With the rise of powerful foundation models, a pre-training-fine-tuning\\nparadigm becomes increasingly popular these days: A foundation model is\\npre-trained using a huge amount of data from various sources, and then the\\ndownstream users only need to fine-tune and adapt it to specific downstream\\ntasks. However, due to the high computation complexity of adversarial training,\\nit is not feasible to fine-tune the foundation model to improve its robustness\\non the downstream task. Observing the above challenge, we want to improve the\\ndownstream robustness without updating/accessing the weights in the foundation\\nmodel. Inspired from existing literature in robustness inheritance (Kim et al.,\\n2020), through theoretical investigation, we identify a close relationship\\nbetween robust contrastive learning with the adversarial robustness of\\nsupervised learning. To further validate and utilize this theoretical insight,\\nwe design a simple-yet-effective robust auto-encoder as a data pre-processing\\nmethod before feeding the data into the foundation model. The proposed approach\\nhas zero access to the foundation model when training the robust auto-encoder.\\nExtensive experiments demonstrate the effectiveness of the proposed method in\\nimproving the robustness of downstream tasks, verifying the connection between\\nthe feature robustness (implied by small adversarial contrastive loss) and the\\nrobustness of the downstream task.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CR\", \"published\": \"2025-04-15T04:17:37Z\"}"}

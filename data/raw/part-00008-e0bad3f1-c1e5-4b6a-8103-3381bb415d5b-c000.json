{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10908v1\", \"title\": \"Generalizability of local neural operator: example for elastodynamic\\n  problems\", \"summary\": \"Local neural operator (LNO) conception has provided a feasible way for\\nscientific computations. The LNO learns transient partial differential\\nequations from random field samples, and then the pre-trained LNO solves\\npractical problems on specific computational domains. For applications, we may\\nask: Are the training samples rich enough? To what extent can we trust the\\nsolutions obtained from pre-trained LNO models for unknown cases? The\\ngeneralizability of LNO could answer these questions. Here, we propose to use\\ntwo plain scalar features, the amplitude and wavenumber of the input functions,\\nto indicate the richness of training samples and to evaluate the generalization\\nerror of pre-trained LNO. In elastodynamic practices, we find that isolated\\nevolving wavenumber modes for Lam\\\\'e-Navier equation caused the training\\ndataset to lack mode diversity. By data supplementation and model fine-tuning\\ntargeting to the discovered lack modes, the pre-trained and fine-tuned LNO\\nmodel solves Lamb problem correctly and efficiently. These results and the\\nproposed generalization criteria provide a paradigm for LNO applications.\", \"main_category\": \"physics.comp-ph\", \"categories\": \"physics.comp-ph\", \"published\": \"2025-04-15T06:41:05Z\"}"}

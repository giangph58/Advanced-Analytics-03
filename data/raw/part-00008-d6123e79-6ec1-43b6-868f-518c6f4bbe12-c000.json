{"value":"{\"aid\": \"http://arxiv.org/abs/2504.15897v1\", \"title\": \"SUPRA: Subspace Parameterized Attention for Neural Operator on General\\n  Domains\", \"summary\": \"Neural operators are efficient surrogate models for solving partial\\ndifferential equations (PDEs), but their key components face challenges: (1) in\\norder to improve accuracy, attention mechanisms suffer from computational\\ninefficiency on large-scale meshes, and (2) spectral convolutions rely on the\\nFast Fourier Transform (FFT) on regular grids and assume a flat geometry, which\\ncauses accuracy degradation on irregular domains. To tackle these problems, we\\nregard the matrix-vector operations in the standard attention mechanism on\\nvectors in Euclidean space as bilinear forms and linear operators in vector\\nspaces and generalize the attention mechanism to function spaces. This new\\nattention mechanism is fully equivalent to the standard attention but\\nimpossible to compute due to the infinite dimensionality of function spaces. To\\naddress this, inspired by model reduction techniques, we propose a Subspace\\nParameterized Attention (SUPRA) neural operator, which approximates the\\nattention mechanism within a finite-dimensional subspace. To construct a\\nsubspace on irregular domains for SUPRA, we propose using the Laplacian\\neigenfunctions, which naturally adapt to domains' geometry and guarantee the\\noptimal approximation for smooth functions. Experiments show that the SUPRA\\nneural operator reduces error rates by up to 33% on various PDE datasets while\\nmaintaining state-of-the-art computational efficiency.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-22T13:40:04Z\"}"}

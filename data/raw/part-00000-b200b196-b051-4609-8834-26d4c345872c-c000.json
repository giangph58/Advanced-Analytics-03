{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19740v1\", \"title\": \"Graph Fourier Transformer with Structure-Frequency Information\", \"summary\": \"Graph Transformers (GTs) have shown advantages in numerous graph structure\\ntasks but their self-attention mechanism ignores the generalization bias of\\ngraphs, with existing methods mainly compensating for this bias from aspects\\nlike position encoding, attention bias and relative distance yet still having\\nsub-optimal performance and being insufficient by only considering the\\nstructural perspective of generalization bias. To address this, this paper\\nproposes Grafourierformer, which innovatively combines GT with inductive bias\\ncontaining Frequency-Structure information by applying Graph Fourier Transform\\nto the Attention Matrix: specifically, eigenvalues from the Graph Laplacian\\nmatrix are used to construct an Eigenvalue matrix mask (reflecting node\\npositions and structural relationships with neighboring nodes to enable\\nconsideration of node range structural characteristics and focus on local graph\\ndetails), and inverse Fourier transform is employed to extract node\\nhigh-frequency and low-frequency features, calculate low-frequency and\\nhigh-frequency energy, and construct a node frequency-energy matrix to filter\\nthe eigenvalue matrix mask, allowing attention heads to incorporate both graph\\nstructural information and node frequency information optimization, adaptively\\ndistinguish global trends from local details, and effectively suppress\\nredundant information interference. Extensive experiments on various benchmarks\\nshow Grafourierformer consistently outperforms GNN and GT-based models in graph\\nclassification and node classification tasks, with ablation experiments further\\nvalidating the effectiveness and necessity of the method. Codes are available\\nat https://github.com/Arichibald/Grafourierformer.git\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.GR\", \"published\": \"2025-04-28T12:38:02Z\"}"}

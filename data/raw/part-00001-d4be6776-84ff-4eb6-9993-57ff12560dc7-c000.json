{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00546v1\", \"title\": \"Directly Forecasting Belief for Reinforcement Learning with Delays\", \"summary\": \"Reinforcement learning (RL) with delays is challenging as sensory perceptions\\nlag behind the actual events: the RL agent needs to estimate the real state of\\nits environment based on past observations. State-of-the-art (SOTA) methods\\ntypically employ recursive, step-by-step forecasting of states. This can cause\\nthe accumulation of compounding errors. To tackle this problem, our novel\\nbelief estimation method, named Directly Forecasting Belief Transformer (DFBT),\\ndirectly forecasts states from observations without incrementally estimating\\nintermediate states step-by-step. We theoretically demonstrate that DFBT\\ngreatly reduces compounding errors of existing recursively forecasting methods,\\nyielding stronger performance guarantees. In experiments with D4RL offline\\ndatasets, DFBT reduces compounding errors with remarkable prediction accuracy.\\nDFBT's capability to forecast state sequences also facilitates multi-step\\nbootstrapping, thus greatly improving learning efficiency. On the MuJoCo\\nbenchmark, our DFBT-based method substantially outperforms SOTA baselines.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-05-01T14:20:48Z\"}"}

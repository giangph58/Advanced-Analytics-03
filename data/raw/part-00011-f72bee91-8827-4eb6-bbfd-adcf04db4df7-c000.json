{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02263v1\", \"title\": \"MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated\\n  Expert Parallelism\", \"summary\": \"Mixture-of-Experts (MoE) showcases tremendous potential to scale large\\nlanguage models (LLMs) with enhanced performance and reduced computational\\ncomplexity. However, its sparsely activated architecture shifts feed-forward\\nnetworks (FFNs) from being compute-intensive to memory-intensive during\\ninference, leading to substantially lower GPU utilization and increased\\noperational costs. We present MegaScale-Infer, an efficient and cost-effective\\nsystem for serving large-scale MoE models. MegaScale-Infer disaggregates\\nattention and FFN modules within each model layer, enabling independent\\nscaling, tailored parallelism strategies, and heterogeneous deployment for both\\nmodules. To fully exploit disaggregation in the presence of MoE's sparsity,\\nMegaScale-Infer introduces ping-pong pipeline parallelism, which partitions a\\nrequest batch into micro-batches and shuttles them between attention and FFNs\\nfor inference. Combined with distinct model parallelism for each module,\\nMegaScale-Infer effectively hides communication overhead and maximizes GPU\\nutilization. To adapt to disaggregated attention and FFN modules and minimize\\ndata transmission overhead (e.g., token dispatch), MegaScale-Infer provides a\\nhigh-performance M2N communication library that eliminates unnecessary\\nGPU-to-CPU data copies, group initialization overhead, and GPU synchronization.\\nExperimental results indicate that MegaScale-Infer achieves up to 1.90x higher\\nper-GPU throughput than state-of-the-art solutions.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC,cs.LG\", \"published\": \"2025-04-03T04:20:44Z\"}"}

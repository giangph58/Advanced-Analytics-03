{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12637v1\", \"title\": \"Scaling Instruction-Tuned LLMs to Million-Token Contexts via\\n  Hierarchical Synthetic Data Generation\", \"summary\": \"Large Language Models (LLMs) struggle with long-context reasoning, not only\\ndue to the quadratic scaling of computational complexity with sequence length\\nbut also because of the scarcity and expense of annotating long-context data.\\nThere has been barely any open-source work that systematically ablates\\nlong-context data, nor is there any openly available instruction tuning dataset\\nwith contexts surpassing 100K tokens. To bridge this gap, we introduce a novel\\npost-training synthetic data generation strategy designed to efficiently extend\\nthe context window of LLMs while preserving their general task performance. Our\\napproach scalably extends to arbitrarily long context lengths, unconstrained by\\nthe length of available real-world data, which effectively addresses the\\nscarcity of raw long-context data. Through a step-by-step rotary position\\nembedding (RoPE) scaling training strategy, we demonstrate that our model, with\\na context length of up to 1M tokens, performs well on the RULER benchmark and\\nInfiniteBench and maintains robust performance on general language tasks.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-17T04:46:57Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11020v1\", \"title\": \"\\\"Even explanations will not help in trusting [this] fundamentally biased\\n  system\\\": A Predictive Policing Case-Study\", \"summary\": \"In today's society, where Artificial Intelligence (AI) has gained a vital\\nrole, concerns regarding user's trust have garnered significant attention. The\\nuse of AI systems in high-risk domains have often led users to either\\nunder-trust it, potentially causing inadequate reliance or over-trust it,\\nresulting in over-compliance. Therefore, users must maintain an appropriate\\nlevel of trust. Past research has indicated that explanations provided by AI\\nsystems can enhance user understanding of when to trust or not trust the\\nsystem. However, the utility of presentation of different explanations forms\\nstill remains to be explored especially in high-risk domains. Therefore, this\\nstudy explores the impact of different explanation types (text, visual, and\\nhybrid) and user expertise (retired police officers and lay users) on\\nestablishing appropriate trust in AI-based predictive policing. While we\\nobserved that the hybrid form of explanations increased the subjective trust in\\nAI for expert users, it did not led to better decision-making. Furthermore, no\\nform of explanations helped build appropriate trust. The findings of our study\\nemphasize the importance of re-evaluating the use of explanations to build\\n[appropriate] trust in AI based systems especially when the system's use is\\nquestionable. Finally, we synthesize potential challenges and policy\\nrecommendations based on our results to design for appropriate trust in\\nhigh-risk based AI-based systems.\", \"main_category\": \"cs.HC\", \"categories\": \"cs.HC,cs.AI\", \"published\": \"2025-04-15T09:43:48Z\"}"}

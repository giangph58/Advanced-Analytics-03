{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19824v1\", \"title\": \"Taming the Randomness: Towards Label-Preserving Cropping in Contrastive\\n  Learning\", \"summary\": \"Contrastive learning (CL) approaches have gained great recognition as a very\\nsuccessful subset of self-supervised learning (SSL) methods. SSL enables\\nlearning from unlabeled data, a crucial step in the advancement of deep\\nlearning, particularly in computer vision (CV), given the plethora of unlabeled\\nimage data. CL works by comparing different random augmentations (e.g.,\\ndifferent crops) of the same image, thus achieving self-labeling. Nevertheless,\\nrandomly augmenting images and especially random cropping can result in an\\nimage that is semantically very distant from the original and therefore leads\\nto false labeling, hence undermining the efficacy of the methods. In this\\nresearch, two novel parameterized cropping methods are introduced that increase\\nthe robustness of self-labeling and consequently increase the efficacy. The\\nresults show that the use of these methods significantly improves the accuracy\\nof the model by between 2.7\\\\% and 12.4\\\\% on the downstream task of classifying\\nCIFAR-10, depending on the crop size compared to that of the non-parameterized\\nrandom cropping method.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-28T14:24:25Z\"}"}

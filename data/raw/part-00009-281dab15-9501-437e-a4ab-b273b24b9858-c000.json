{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03258v1\", \"title\": \"IAFormer: Interaction-Aware Transformer network for collider data\\n  analysis\", \"summary\": \"In this paper, we introduce IAFormer, a novel Transformer-based architecture\\nthat efficiently integrates pairwise particle interactions through a dynamic\\nsparse attention mechanism. The IAformer has two new mechanisms within the\\nmodel. First, the attention matrix depends on predefined boost invariant\\npairwise quantities, reducing the network parameter significantly from the\\noriginal particle transformer models. Second, IAformer incorporate the sparse\\nattention mechanism by utilizing the ``differential attention'', so that it can\\ndynamically prioritizes relevant particle tokens while reducing computational\\noverhead associated with less informative ones. This approach significantly\\nlowers the model complexity without compromising performance. Despite being\\ncomputationally efficient by more than an order of magnitude than the Particle\\nTransformer network, IAFormer achieves state-of-the-art performance in\\nclassification tasks on the Top and quark-gluon datasets. Furthermore, we\\nemploy AI interpretability techniques, verifying that the model effectively\\ncaptures physically meaningful information layer by layer through its sparse\\nattention mechanism, building an efficient network output that is resistant to\\nstatistical fluctuations. IAformer highlights the need to sparse attention in\\nany Transformer analysis to reduce the network size while improving its\\nperformance.\", \"main_category\": \"hep-ph\", \"categories\": \"hep-ph,hep-ex\", \"published\": \"2025-05-06T07:37:34Z\"}"}

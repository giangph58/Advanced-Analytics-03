{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24230v1\", \"title\": \"GPU-centric Communication Schemes for HPC and ML Applications\", \"summary\": \"Compute nodes on modern heterogeneous supercomputing systems comprise CPUs,\\nGPUs, and high-speed network interconnects (NICs). Parallelization is\\nidentified as a technique for effectively utilizing these systems to execute\\nscalable simulation and deep learning workloads. The resulting inter-process\\ncommunication from the distributed execution of these parallel workloads is one\\nof the key factors contributing to its performance bottleneck. Most programming\\nmodels and runtime systems enabling the communication requirements on these\\nsystems support GPU-aware communication schemes that move the GPU-attached\\ncommunication buffers in the application directly from the GPU to the NIC\\nwithout staging through the host memory. A CPU thread is required to\\norchestrate the communication operations even with support for such\\nGPU-awareness. This survey discusses various available GPU-centric\\ncommunication schemes that move the control path of the communication\\noperations from the CPU to the GPU. This work presents the need for the new\\ncommunication schemes, various GPU and NIC capabilities required to implement\\nthe schemes, and the potential use-cases addressed. Based on these discussions,\\nchallenges involved in supporting the exhibited GPU-centric communication\\nschemes are discussed.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC,cs.LG\", \"published\": \"2025-03-31T15:43:18Z\"}"}

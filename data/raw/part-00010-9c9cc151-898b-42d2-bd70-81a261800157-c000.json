{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00693v1\", \"title\": \"Robotic Visual Instruction\", \"summary\": \"Recently, natural language has been the primary medium for human-robot\\ninteraction. However, its inherent lack of spatial precision for robotic\\ncontrol introduces challenges such as ambiguity and verbosity. To address these\\nlimitations, we introduce the Robotic Visual Instruction (RoVI), a novel\\nparadigm to guide robotic tasks through an object-centric, hand-drawn symbolic\\nrepresentation. RoVI effectively encodes spatial-temporal information into\\nhuman-interpretable visual instructions through 2D sketches, utilizing arrows,\\ncircles, colors, and numbers to direct 3D robotic manipulation. To enable\\nrobots to understand RoVI better and generate precise actions based on RoVI, we\\npresent Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for\\nRoVI-conditioned policies. This approach leverages Vision-Language Models\\n(VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from\\n2D pixel space via keypoint extraction, and then transform them into executable\\n3D action sequences. We additionally curate a specialized dataset of 15K\\ninstances to fine-tune small VLMs for edge deployment, enabling them to\\neffectively learn RoVI capabilities. Our approach is rigorously validated\\nacross 11 novel tasks in both real and simulated environments, demonstrating\\nsignificant generalization capability. Notably, VIEW achieves an 87.5% success\\nrate in real-world scenarios involving unseen tasks that feature multi-step\\nactions, with disturbances, and trajectory-following requirements. Code and\\nDatasets in this paper will be released soon.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.AI,cs.CV\", \"published\": \"2025-05-01T17:55:05Z\"}"}

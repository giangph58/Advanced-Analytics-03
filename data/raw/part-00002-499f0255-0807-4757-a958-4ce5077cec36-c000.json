{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21423v1\", \"title\": \"Diff-Prompt: Diffusion-Driven Prompt Generator with Mask Supervision\", \"summary\": \"Prompt learning has demonstrated promising results in fine-tuning pre-trained\\nmultimodal models. However, the performance improvement is limited when applied\\nto more complex and fine-grained tasks. The reason is that most existing\\nmethods directly optimize the parameters involved in the prompt generation\\nprocess through loss backpropagation, which constrains the richness and\\nspecificity of the prompt representations. In this paper, we propose\\nDiffusion-Driven Prompt Generator (Diff-Prompt), aiming to use the diffusion\\nmodel to generate rich and fine-grained prompt information for complex\\ndownstream tasks. Specifically, our approach consists of three stages. In the\\nfirst stage, we train a Mask-VAE to compress the masks into latent space. In\\nthe second stage, we leverage an improved Diffusion Transformer (DiT) to train\\na prompt generator in the latent space, using the masks for supervision. In the\\nthird stage, we align the denoising process of the prompt generator with the\\npre-trained model in the semantic space, and use the generated prompts to\\nfine-tune the model. We conduct experiments on a complex pixel-level downstream\\ntask, referring expression comprehension, and compare our method with various\\nparameter-efficient fine-tuning approaches. Diff-Prompt achieves a maximum\\nimprovement of 8.87 in R@1 and 14.05 in R@5 compared to the foundation model\\nand also outperforms other state-of-the-art methods across multiple metrics.\\nThe experimental results validate the effectiveness of our approach and\\nhighlight the potential of using generative models for prompt generation. Code\\nis available at https://github.com/Kelvin-ywc/diff-prompt.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-30T08:28:38Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.11834v1\", \"title\": \"Estimation and inference in error-in-operator model\", \"summary\": \"Many statistical problems can be reduced to a linear inverse problem in which\\nonly a noisy version of the operator is available. Particular examples include\\nrandom design regression, deconvolution problem, instrumental variable\\nregression, functional data analysis, error-in-variable regression, drift\\nestimation in stochastic diffusion, and many others. The pragmatic plug-in\\napproach can be well justified in the classical asymptotic setup with a growing\\nsample size. However, recent developments in high dimensional inference reveal\\nsome new features of this problem. In high dimensional linear regression with a\\nrandom design, the plug-in approach is questionable but the use of a simple\\nridge penalization yields a benign overfitting phenomenon; see\\n\\\\cite{baLoLu2020}, \\\\cite{ChMo2022}, \\\\cite{NoPuSp2024}. This paper revisits the\\ngeneral Error-in-Operator problem for finite samples and high dimension of the\\nsource and image spaces. A particular focus is on the choice of a proper\\nregularization. We show that a simple ridge penalty (Tikhonov regularization)\\nworks properly in the case when the operator is more regular than the signal.\\nIn the opposite case, some model reduction technique like spectral truncation\\nshould be applied.\", \"main_category\": \"math.ST\", \"categories\": \"math.ST,stat.TH\", \"published\": \"2025-04-16T07:45:44Z\"}"}

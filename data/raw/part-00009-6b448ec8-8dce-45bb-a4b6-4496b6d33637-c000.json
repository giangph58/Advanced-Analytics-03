{"value":"{\"aid\": \"http://arxiv.org/abs/2504.17502v1\", \"title\": \"RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image\\n  Generation\", \"summary\": \"Subject-driven text-to-image (T2I) generation aims to produce images that\\nalign with a given textual description, while preserving the visual identity\\nfrom a referenced subject image. Despite its broad downstream applicability --\\nranging from enhanced personalization in image generation to consistent\\ncharacter representation in video rendering -- progress in this field is\\nlimited by the lack of reliable automatic evaluation. Existing methods either\\nassess only one aspect of the task (i.e., textual alignment or subject\\npreservation), misalign with human judgments, or rely on costly API-based\\nevaluation. To address this, we introduce RefVNLI, a cost-effective metric that\\nevaluates both textual alignment and subject preservation in a single\\nprediction. Trained on a large-scale dataset derived from video-reasoning\\nbenchmarks and image perturbations, RefVNLI outperforms or matches existing\\nbaselines across multiple benchmarks and subject categories (e.g.,\\n\\\\emph{Animal}, \\\\emph{Object}), achieving up to 6.4-point gains in textual\\nalignment and 8.5-point gains in subject consistency. It also excels with\\nlesser-known concepts, aligning with human preferences at over 87\\\\% accuracy.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-24T12:44:51Z\"}"}

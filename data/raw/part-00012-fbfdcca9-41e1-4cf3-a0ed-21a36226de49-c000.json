{"value":"{\"aid\": \"http://arxiv.org/abs/2504.21545v1\", \"title\": \"Meta knowledge assisted Evolutionary Neural Architecture Search\", \"summary\": \"Evolutionary computation (EC)-based neural architecture search (NAS) has\\nachieved remarkable performance in the automatic design of neural\\narchitectures. However, the high computational cost associated with evaluating\\nsearched architectures poses a challenge for these methods, and a fixed form of\\nlearning rate (LR) schedule means greater information loss on diverse searched\\narchitectures. This paper introduces an efficient EC-based NAS method to solve\\nthese problems via an innovative meta-learning framework. Specifically, a\\nmeta-learning-rate (Meta-LR) scheme is used through pretraining to obtain a\\nsuitable LR schedule, which guides the training process with lower information\\nloss when evaluating each individual. An adaptive surrogate model is designed\\nthrough an adaptive threshold to select the potential architectures in a few\\nepochs and then evaluate the potential architectures with complete epochs.\\nAdditionally, a periodic mutation operator is proposed to increase the\\ndiversity of the population, which enhances the generalizability and\\nrobustness. Experiments on CIFAR-10, CIFAR-100, and ImageNet1K datasets\\ndemonstrate that the proposed method achieves high performance comparable to\\nthat of many state-of-the-art peer methods, with lower computational cost and\\ngreater robustness.\", \"main_category\": \"cs.NE\", \"categories\": \"cs.NE,cs.AI\", \"published\": \"2025-04-30T11:43:07Z\"}"}

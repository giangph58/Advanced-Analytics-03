{"value":"{\"aid\": \"http://arxiv.org/abs/2504.20673v1\", \"title\": \"CoCo-Bench: A Comprehensive Code Benchmark For Multi-task Large Language\\n  Model Evaluation\", \"summary\": \"Large language models (LLMs) play a crucial role in software engineering,\\nexcelling in tasks like code generation and maintenance. However, existing\\nbenchmarks are often narrow in scope, focusing on a specific task and lack a\\ncomprehensive evaluation framework that reflects real-world applications. To\\naddress these gaps, we introduce CoCo-Bench (Comprehensive Code Benchmark),\\ndesigned to evaluate LLMs across four critical dimensions: code understanding,\\ncode generation, code modification, and code review. These dimensions capture\\nessential developer needs, ensuring a more systematic and representative\\nevaluation. CoCo-Bench includes multiple programming languages and varying task\\ndifficulties, with rigorous manual review to ensure data quality and accuracy.\\nEmpirical results show that CoCo-Bench aligns with existing benchmarks while\\nuncovering significant variations in model performance, effectively\\nhighlighting strengths and weaknesses. By offering a holistic and objective\\nevaluation, CoCo-Bench provides valuable insights to guide future research and\\ntechnological advancements in code-oriented LLMs, establishing a reliable\\nbenchmark for the field.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE,cs.AI\", \"published\": \"2025-04-29T11:57:23Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01954v1\", \"title\": \"Towards Unified Referring Expression Segmentation Across Omni-Level\\n  Visual Target Granularities\", \"summary\": \"Referring expression segmentation (RES) aims at segmenting the entities'\\nmasks that match the descriptive language expression. While traditional RES\\nmethods primarily address object-level grounding, real-world scenarios demand a\\nmore versatile framework that can handle multiple levels of target granularity,\\nsuch as multi-object, single object or part-level references. This introduces\\ngreat challenges due to the diverse and nuanced ways users describe targets.\\nHowever, existing datasets and models mainly focus on designing grounding\\nspecialists for object-level target localization, lacking the necessary data\\nresources and unified frameworks for the more practical multi-grained RES. In\\nthis paper, we take a step further towards visual granularity unified RES task.\\nTo overcome the limitation of data scarcity, we introduce a new\\nmulti-granularity referring expression segmentation (MRES) task, alongside the\\nRefCOCOm benchmark, which includes part-level annotations for advancing\\nfiner-grained visual understanding. In addition, we create MRES-32M, the\\nlargest visual grounding dataset, comprising over 32.2M masks and captions\\nacross 1M images, specifically designed for part-level vision-language\\ngrounding. To tackle the challenges of multi-granularity RES, we propose\\nUniRES++, a unified multimodal large language model that integrates\\nobject-level and part-level RES tasks. UniRES++ incorporates targeted designs\\nfor fine-grained visual feature exploration. With the joint model architecture\\nand parameters, UniRES++ achieves state-of-the-art performance across multiple\\nbenchmarks, including RefCOCOm for MRES, gRefCOCO for generalized RES, and\\nRefCOCO, RefCOCO+, RefCOCOg for classic RES. To foster future research into\\nmulti-grained visual grounding, our RefCOCOm benchmark, MRES-32M dataset and\\nmodel UniRES++ will be publicly available at\\nhttps://github.com/Rubics-Xuan/MRES.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-02T17:58:05Z\"}"}

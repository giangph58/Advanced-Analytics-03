{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16061v1\", \"title\": \"Vision language models are unreliable at trivial spatial cognition\", \"summary\": \"Vision language models (VLMs) are designed to extract relevant visuospatial\\ninformation from images. Some research suggests that VLMs can exhibit humanlike\\nscene understanding, while other investigations reveal difficulties in their\\nability to process relational information. To achieve widespread applicability,\\nVLMs must perform reliably, yielding comparable competence across a wide\\nvariety of related tasks. We sought to test how reliable these architectures\\nare at engaging in trivial spatial cognition, e.g., recognizing whether one\\nobject is left of another in an uncluttered scene. We developed a benchmark\\ndataset -- TableTest -- whose images depict 3D scenes of objects arranged on a\\ntable, and used it to evaluate state-of-the-art VLMs. Results show that\\nperformance could be degraded by minor variations of prompts that use logically\\nequivalent descriptions. These analyses suggest limitations in how VLMs may\\nreason about spatial relations in real-world applications. They also reveal\\nnovel opportunities for bolstering image caption corpora for more efficient\\ntraining and testing.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-22T17:38:01Z\"}"}

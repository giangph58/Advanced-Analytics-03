{"value":"{\"aid\": \"http://arxiv.org/abs/2505.04434v1\", \"title\": \"Theoretical Guarantees for LT-TTD: A Unified Transformer-based\\n  Architecture for Two-Level Ranking Systems\", \"summary\": \"Modern recommendation and search systems typically employ multi-stage ranking\\narchitectures to efficiently handle billions of candidates. The conventional\\napproach uses distinct L1 (candidate retrieval) and L2 (re-ranking) models with\\ndifferent optimization objectives, introducing critical limitations including\\nirreversible error propagation and suboptimal ranking. This paper identifies\\nand analyzes the fundamental limitations of this decoupled paradigm and\\nproposes LT-TTD (Listwise Transformer with Two-Tower Distillation), a novel\\nunified architecture that bridges retrieval and ranking phases. Our approach\\ncombines the computational efficiency of two-tower models with the expressivity\\nof transformers in a unified listwise learning framework. We provide a\\ncomprehensive theoretical analysis of our architecture and establish formal\\nguarantees regarding error propagation mitigation, ranking quality\\nimprovements, and optimization convergence. We derive theoretical bounds\\nshowing that LT-TTD reduces the upper limit on irretrievable relevant items by\\na factor that depends on the knowledge distillation strength, and prove that\\nour multi-objective optimization framework achieves a provably better global\\noptimum than disjoint training. Additionally, we analyze the computational\\ncomplexity of our approach, demonstrating that the asymptotic complexity\\nremains within practical bounds for real-world applications. We also introduce\\nUPQE, a novel evaluation metric specifically designed for unified ranking\\narchitectures that holistically captures retrieval quality, ranking\\nperformance, and computational efficiency.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR,stat.ML\", \"published\": \"2025-05-07T14:01:22Z\"}"}

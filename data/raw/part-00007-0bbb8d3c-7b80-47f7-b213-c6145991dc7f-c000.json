{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10006v1\", \"title\": \"Improving Controller Generalization with Dimensionless Markov Decision\\n  Processes\", \"summary\": \"Controllers trained with Reinforcement Learning tend to be very specialized\\nand thus generalize poorly when their testing environment differs from their\\ntraining one. We propose a Model-Based approach to increase generalization\\nwhere both world model and policy are trained in a dimensionless state-action\\nspace. To do so, we introduce the Dimensionless Markov Decision Process\\n($\\\\Pi$-MDP): an extension of Contextual-MDPs in which state and action spaces\\nare non-dimensionalized with the Buckingham-$\\\\Pi$ theorem. This procedure\\ninduces policies that are equivariant with respect to changes in the context of\\nthe underlying dynamics. We provide a generic framework for this approach and\\napply it to a model-based policy search algorithm using Gaussian Process\\nmodels. We demonstrate the applicability of our method on simulated actuated\\npendulum and cartpole systems, where policies trained on a single environment\\nare robust to shifts in the distribution of the context.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-14T09:08:53Z\"}"}

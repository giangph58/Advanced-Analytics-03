{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00259v1\", \"title\": \"Pack-PTQ: Advancing Post-training Quantization of Neural Networks by\\n  Pack-wise Reconstruction\", \"summary\": \"Post-training quantization (PTQ) has evolved as a prominent solution for\\ncompressing complex models, which advocates a small calibration dataset and\\navoids end-to-end retraining. However, most existing PTQ methods employ\\nblock-wise reconstruction, which neglects cross-block dependency and exhibits a\\nnotable accuracy drop in low-bit cases. To address these limitations, this\\npaper presents a novel PTQ method, dubbed Pack-PTQ. First, we design a\\nHessian-guided adaptive packing mechanism to partition blocks into\\nnon-overlapping packs, which serve as the base unit for reconstruction, thereby\\npreserving the cross-block dependency and enabling accurate quantization\\nparameters estimation. Second, based on the pack configuration, we propose a\\nmixed-precision quantization approach to assign varied bit-widths to packs\\naccording to their distinct sensitivities, thereby further enhancing\\nperformance. Extensive experiments on 2D image and 3D point cloud\\nclassification tasks, using various network architectures, demonstrate the\\nsuperiority of our method over the state-of-the-art PTQ methods.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-05-01T02:53:46Z\"}"}

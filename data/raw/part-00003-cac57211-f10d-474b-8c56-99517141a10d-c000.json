{"value":"{\"aid\": \"http://arxiv.org/abs/2504.19557v1\", \"title\": \"CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel\\n  View Synthesis in Autonomous Driving Scenes\", \"summary\": \"Current point-based approaches encounter limitations in scalability and\\nrendering quality when using large 3D point cloud maps because using them\\ndirectly for novel view synthesis (NVS) leads to degraded visualizations. We\\nidentify the primary issue behind these low-quality renderings as a visibility\\nmismatch between geometry and appearance, stemming from using these two\\nmodalities together. To address this problem, we present CE-NPBG, a new\\napproach for novel view synthesis (NVS) in large-scale autonomous driving\\nscenes. Our method is a neural point-based technique that leverages two\\nmodalities: posed images (cameras) and synchronized raw 3D point clouds\\n(LiDAR). We first employ a connectivity relationship graph between appearance\\nand geometry, which retrieves points from a large 3D point cloud map observed\\nfrom the current camera perspective and uses them for rendering. By leveraging\\nthis connectivity, our method significantly improves rendering quality and\\nenhances run-time and scalability by using only a small subset of points from\\nthe large 3D point cloud map. Our approach associates neural descriptors with\\nthe points and uses them to synthesize views. To enhance the encoding of these\\ndescriptors and elevate rendering quality, we propose a joint adversarial and\\npoint rasterization training. During training, we pair an image-synthesizer\\nnetwork with a multi-resolution discriminator. At inference, we decouple them\\nand use the image-synthesizer to generate novel views. We also integrate our\\nproposal into the recent 3D Gaussian Splatting work to highlight its benefits\\nfor improved rendering and scalability.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-28T08:02:02Z\"}"}

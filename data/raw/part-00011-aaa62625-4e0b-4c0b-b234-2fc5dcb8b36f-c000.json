{"value":"{\"aid\": \"http://arxiv.org/abs/2504.09963v1\", \"title\": \"Towards Unbiased Federated Graph Learning: Label and Topology\\n  Perspectives\", \"summary\": \"Federated Graph Learning (FGL) enables privacy-preserving, distributed\\ntraining of graph neural networks without sharing raw data. Among its\\napproaches, subgraph-FL has become the dominant paradigm, with most work\\nfocused on improving overall node classification accuracy. However, these\\nmethods often overlook fairness due to the complexity of node features, labels,\\nand graph structures. In particular, they perform poorly on nodes with\\ndisadvantaged properties, such as being in the minority class within subgraphs\\nor having heterophilous connections (neighbors with dissimilar labels or\\nmisleading features). This reveals a critical issue: high accuracy can mask\\ndegraded performance on structurally or semantically marginalized nodes. To\\naddress this, we advocate for two fairness goals: (1) improving representation\\nof minority class nodes for class-wise fairness and (2) mitigating topological\\nbias from heterophilous connections for topology-aware fairness. We propose\\nFairFGL, a novel framework that enhances fairness through fine-grained graph\\nmining and collaborative learning. On the client side, the History-Preserving\\nModule prevents overfitting to dominant local classes, while the Majority\\nAlignment Module refines representations of heterophilous majority-class nodes.\\nThe Gradient Modification Module transfers minority-class knowledge from\\nstructurally favorable clients to improve fairness. On the server side, FairFGL\\nuploads only the most influenced subset of parameters to reduce communication\\ncosts and better reflect local distributions. A cluster-based aggregation\\nstrategy reconciles conflicting updates and curbs global majority dominance .\\nExtensive evaluations on eight benchmarks show FairFGL significantly improves\\nminority-group performance , achieving up to a 22.62 percent Macro-F1 gain\\nwhile enhancing convergence over state-of-the-art baselines.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.DB,cs.SI\", \"published\": \"2025-04-14T08:00:20Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2505.03577v1\", \"title\": \"Information-theoretic reduction of deep neural networks to linear models\\n  in the overparametrized proportional regime\", \"summary\": \"We rigorously analyse fully-trained neural networks of arbitrary depth in the\\nBayesian optimal setting in the so-called proportional scaling regime where the\\nnumber of training samples and width of the input and all inner layers diverge\\nproportionally. We prove an information-theoretic equivalence between the\\nBayesian deep neural network model trained from data generated by a teacher\\nwith matching architecture, and a simpler model of optimal inference in a\\ngeneralized linear model. This equivalence enables us to compute the optimal\\ngeneralization error for deep neural networks in this regime. We thus prove the\\n\\\"deep Gaussian equivalence principle\\\" conjectured in Cui et al. (2023)\\n(arXiv:2302.00375). Our result highlights that in order to escape this\\n\\\"trivialisation\\\" of deep neural networks (in the sense of reduction to a linear\\nmodel) happening in the strongly overparametrized proportional regime, models\\ntrained from much more data have to be considered.\", \"main_category\": \"math.ST\", \"categories\": \"math.ST,cond-mat.dis-nn,cs.IT,math-ph,math.IT,math.MP,stat.ML,stat.TH\", \"published\": \"2025-05-06T14:36:07Z\"}"}

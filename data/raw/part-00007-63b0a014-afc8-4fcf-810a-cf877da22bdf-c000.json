{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01849v1\", \"title\": \"An Approach to Technical AGI Safety and Security\", \"summary\": \"Artificial General Intelligence (AGI) promises transformative benefits but\\nalso presents significant risks. We develop an approach to address the risk of\\nharms consequential enough to significantly harm humanity. We identify four\\nareas of risk: misuse, misalignment, mistakes, and structural risks. Of these,\\nwe focus on technical approaches to misuse and misalignment. For misuse, our\\nstrategy aims to prevent threat actors from accessing dangerous capabilities,\\nby proactively identifying dangerous capabilities, and implementing robust\\nsecurity, access restrictions, monitoring, and model safety mitigations. To\\naddress misalignment, we outline two lines of defense. First, model-level\\nmitigations such as amplified oversight and robust training can help to build\\nan aligned model. Second, system-level security measures such as monitoring and\\naccess control can mitigate harm even if the model is misaligned. Techniques\\nfrom interpretability, uncertainty estimation, and safer design patterns can\\nenhance the effectiveness of these mitigations. Finally, we briefly outline how\\nthese ingredients could be combined to produce safety cases for AGI systems.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CY,cs.LG\", \"published\": \"2025-04-02T15:59:31Z\"}"}

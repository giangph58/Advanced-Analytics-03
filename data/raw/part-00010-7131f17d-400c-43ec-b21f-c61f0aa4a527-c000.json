{"value":"{\"aid\": \"http://arxiv.org/abs/2505.00284v1\", \"title\": \"LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous\\n  Driving\", \"summary\": \"Vision-Language Models (VLMs) have demonstrated significant potential for\\nend-to-end autonomous driving. However, fully exploiting their capabilities for\\nsafe and reliable vehicle control remains an open research challenge. To\\nsystematically examine advances and limitations of VLMs in driving tasks, we\\nintroduce LightEMMA, a Lightweight End-to-End Multimodal Model for Autonomous\\ndriving. LightEMMA provides a unified, VLM-based autonomous driving framework\\nwithout ad hoc customizations, enabling easy integration and evaluation of\\nevolving state-of-the-art commercial and open-source models. We construct\\ntwelve autonomous driving agents using various VLMs and evaluate their\\nperformance on the nuScenes prediction task, comprehensively assessing metrics\\nsuch as inference time, computational cost, and predictive accuracy.\\nIllustrative examples highlight that, despite their strong scenario\\ninterpretation capabilities, VLMs' practical performance in autonomous driving\\ntasks remains concerning, emphasizing the need for further improvements. The\\ncode is available at https://github.com/michigan-traffic-lab/LightEMMA.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.AI\", \"published\": \"2025-05-01T04:12:41Z\"}"}

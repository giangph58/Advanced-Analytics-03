{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16078v1\", \"title\": \"LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making\\n  Abilities\", \"summary\": \"The success of Large Language Models (LLMs) has sparked interest in various\\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\\nsolve complex domains. However, LLM agents have been found to suffer from\\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\\nact on knowledge present in the model. In this work, we systematically study\\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\\nclosely examine three prevalent failure modes: greediness, frequency bias, and\\nthe knowing-doing gap. We propose mitigation of these shortcomings by\\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\\nOur experiments across multi-armed bandits, contextual bandits, and\\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\\ngap. Finally, we study both classic exploration mechanisms, such as\\n$\\\\epsilon$-greedy, and LLM-specific approaches, such as self-correction and\\nself-consistency, to enable more effective fine-tuning of LLMs for\\ndecision-making.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-22T17:57:14Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.14985v1\", \"title\": \"aiXamine: LLM Safety and Security Simplified\", \"summary\": \"Evaluating Large Language Models (LLMs) for safety and security remains a\\ncomplex task, often requiring users to navigate a fragmented landscape of ad\\nhoc benchmarks, datasets, metrics, and reporting formats. To address this\\nchallenge, we present aiXamine, a comprehensive black-box evaluation platform\\nfor LLM safety and security. aiXamine integrates over 40 tests (i.e.,\\nbenchmarks) organized into eight key services targeting specific dimensions of\\nsafety and security: adversarial robustness, code security, fairness and bias,\\nhallucination, model and data privacy, out-of-distribution (OOD) robustness,\\nover-refusal, and safety alignment. The platform aggregates the evaluation\\nresults into a single detailed report per model, providing a detailed breakdown\\nof model performance, test examples, and rich visualizations. We used aiXamine\\nto assess over 50 publicly available and proprietary LLMs, conducting over 2K\\nexaminations. Our findings reveal notable vulnerabilities in leading models,\\nincluding susceptibility to adversarial attacks in OpenAI's GPT-4o, biased\\noutputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.\\nAdditionally, we observe that open-source models can match or exceed\\nproprietary models in specific services such as safety alignment, fairness and\\nbias, and OOD robustness. Finally, we identify trade-offs between distillation\\nstrategies, model size, training methods, and architectural choices.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.AI\", \"published\": \"2025-04-21T09:26:05Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.12691v1\", \"title\": \"Why and How LLMs Hallucinate: Connecting the Dots with Subsequence\\n  Associations\", \"summary\": \"Large language models (LLMs) frequently generate hallucinations-content that\\ndeviates from factual accuracy or provided context-posing challenges for\\ndiagnosis due to the complex interplay of underlying causes. This paper\\nintroduces a subsequence association framework to systematically trace and\\nunderstand hallucinations. Our key insight is that hallucinations arise when\\ndominant hallucinatory associations outweigh faithful ones. Through theoretical\\nand empirical analyses, we demonstrate that decoder-only transformers\\neffectively function as subsequence embedding models, with linear layers\\nencoding input-output associations. We propose a tracing algorithm that\\nidentifies causal subsequences by analyzing hallucination probabilities across\\nrandomized input contexts. Experiments show our method outperforms standard\\nattribution techniques in identifying hallucination causes and aligns with\\nevidence from the model's training corpus. This work provides a unified\\nperspective on hallucinations and a robust framework for their tracing and\\nanalysis.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-17T06:34:45Z\"}"}

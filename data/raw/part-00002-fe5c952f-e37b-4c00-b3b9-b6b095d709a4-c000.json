{"value":"{\"aid\": \"http://arxiv.org/abs/2504.16408v1\", \"title\": \"Less is More: Enhancing Structured Multi-Agent Reasoning via\\n  Quality-Guided Distillation\", \"summary\": \"The XLLM@ACL2025 Shared Task-III formulates a low-resource structural\\nreasoning task that challenges LLMs to generate interpretable, step-by-step\\nrationales with minimal labeled data. We present Less is More, the third-place\\nwinning approach in the XLLM@ACL2025 Shared Task-III, which focuses on\\nstructured reasoning from only 24 labeled examples. Our approach leverages a\\nmulti-agent framework with reverse-prompt induction, retrieval-augmented\\nreasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to\\ndistill high-quality supervision across three subtasks: question parsing, CoT\\nparsing, and step-level verification. All modules are fine-tuned from\\nMeta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure\\nvalidation with reward filtering across few-shot and zero-shot prompts, our\\npipeline consistently improves structure reasoning quality. These results\\nunderscore the value of controllable data distillation in enhancing structured\\ninference under low-resource constraints. Our code is available at\\nhttps://github.com/Jiahao-Yuan/Less-is-More.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-23T04:19:52Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06261v1\", \"title\": \"Hogwild! Inference: Parallel LLM Generation via Concurrent Attention\", \"summary\": \"Large Language Models (LLMs) have demonstrated the ability to tackle\\nincreasingly complex tasks through advanced reasoning, long-form content\\ngeneration, and tool use. Solving these tasks often involves long\\ninference-time computations. In human problem solving, a common strategy to\\nexpedite work is collaboration: by dividing the problem into sub-tasks,\\nexploring different strategies concurrently, etc. Recent research has shown\\nthat LLMs can also operate in parallel by implementing explicit cooperation\\nframeworks, such as voting mechanisms or the explicit creation of independent\\nsub-tasks that can be executed in parallel. However, each of these frameworks\\nmay not be suitable for all types of tasks, which can hinder their\\napplicability. In this work, we propose a different design approach: we run LLM\\n\\\"workers\\\" in parallel , allowing them to synchronize via a concurrently-updated\\nattention cache and prompt these workers to decide how best to collaborate. Our\\napproach allows the instances to come up with their own collaboration strategy\\nfor the problem at hand, all the while \\\"seeing\\\" each other's partial progress\\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\\nparallel LLM inference engine where multiple instances of the same LLM run in\\nparallel with the same attention cache, with \\\"instant\\\" access to each other's\\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\\nutilization. We find that modern reasoning-capable LLMs can perform inference\\nwith shared Key-Value cache out of the box, without additional fine-tuning.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL\", \"published\": \"2025-04-08T17:59:41Z\"}"}

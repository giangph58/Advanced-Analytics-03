{"value":"{\"aid\": \"http://arxiv.org/abs/2504.10845v1\", \"title\": \"Moving Beyond Next-Token Prediction: Transformers are Context-Sensitive\\n  Language Generators\", \"summary\": \"Large Language Models (LLMs), powered by Transformers, have demonstrated\\nhuman-like intelligence capabilities, yet their underlying mechanisms remain\\npoorly understood. This paper presents a novel framework for interpreting LLMs\\nas probabilistic left context-sensitive languages (CSLs) generators. We\\nhypothesize that Transformers can be effectively decomposed into three\\nfundamental components: context windows, attention mechanisms, and\\nautoregressive generation frameworks. This decomposition allows for the\\ndevelopment of more flexible and interpretable computational models, moving\\nbeyond the traditional view of attention and autoregression as inseparable\\nprocesses. We argue that next-token predictions can be understood as\\nprobabilistic, dynamic approximations of left CSL production rules, providing\\nan intuitive explanation for how simple token predictions can yield human-like\\nintelligence outputs. Given that all CSLs are left context-sensitive\\n(Penttonen, 1974), we conclude that Transformers stochastically approximate\\nCSLs, which are widely recognized as models of human-like intelligence. This\\ninterpretation bridges the gap between Formal Language Theory and the observed\\ngenerative power of Transformers, laying a foundation for future advancements\\nin generative AI theory and applications. Our novel perspective on Transformer\\narchitectures will foster a deeper understanding of LLMs and their future\\npotentials.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-15T04:06:27Z\"}"}

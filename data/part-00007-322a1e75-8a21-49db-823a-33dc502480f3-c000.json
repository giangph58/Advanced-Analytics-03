{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05913v1\", \"title\": \"Balancing long- and short-term dynamics for the modeling of saliency in\\n  videos\", \"summary\": \"The role of long- and short-term dynamics towards salient object detection in\\nvideos is under-researched. We present a Transformer-based approach to learn a\\njoint representation of video frames and past saliency information. Our model\\nembeds long- and short-term information to detect dynamically shifting saliency\\nin video. We provide our model with a stream of video frames and past saliency\\nmaps, which acts as a prior for the next prediction, and extract spatiotemporal\\ntokens from both modalities. The decomposition of the frame sequence into\\ntokens lets the model incorporate short-term information from within the token,\\nwhile being able to make long-term connections between tokens throughout the\\nsequence. The core of the system consists of a dual-stream Transformer\\narchitecture to process the extracted sequences independently before fusing the\\ntwo modalities. Additionally, we apply a saliency-based masking scheme to the\\ninput frames to learn an embedding that facilitates the recognition of\\ndeviations from previous outputs. We observe that the additional prior\\ninformation aids in the first detection of the salient location. Our findings\\nindicate that the ratio of spatiotemporal long- and short-term features\\ndirectly impacts the model's performance. While increasing the short-term\\ncontext is beneficial up to a certain threshold, the model's performance\\ngreatly benefits from an expansion of the long-term context.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-08T11:09:37Z\"}"}

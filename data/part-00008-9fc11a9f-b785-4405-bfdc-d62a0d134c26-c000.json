{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01960v1\", \"title\": \"Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D\\n  Reconstruction and Novel View Synthesis\", \"summary\": \"Recent advancements in 3D Gaussian Splatting (3DGS) and Neural Radiance\\nFields (NeRF) have achieved impressive results in real-time 3D reconstruction\\nand novel view synthesis. However, these methods struggle in large-scale,\\nunconstrained environments where sparse and uneven input coverage, transient\\nocclusions, appearance variability, and inconsistent camera settings lead to\\ndegraded quality. We propose GS-Diff, a novel 3DGS framework guided by a\\nmulti-view diffusion model to address these limitations. By generating\\npseudo-observations conditioned on multi-view inputs, our method transforms\\nunder-constrained 3D reconstruction problems into well-posed ones, enabling\\nrobust optimization even with sparse data. GS-Diff further integrates several\\nenhancements, including appearance embedding, monocular depth priors, dynamic\\nobject modeling, anisotropy regularization, and advanced rasterization\\ntechniques, to tackle geometric and photometric challenges in real-world\\nsettings. Experiments on four benchmarks demonstrate that GS-Diff consistently\\noutperforms state-of-the-art baselines by significant margins.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG\", \"published\": \"2025-04-02T17:59:46Z\"}"}

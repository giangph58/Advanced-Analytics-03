{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06048v1\", \"title\": \"Trust-Region Twisted Policy Improvement\", \"summary\": \"Monte-Carlo tree search (MCTS) has driven many recent breakthroughs in deep\\nreinforcement learning (RL). However, scaling MCTS to parallel compute has\\nproven challenging in practice which has motivated alternative planners like\\nsequential Monte-Carlo (SMC). Many of these SMC methods adopt particle filters\\nfor smoothing through a reformulation of RL as a policy inference problem. Yet,\\npersisting design choices of these particle filters often conflict with the aim\\nof online planning in RL, which is to obtain a policy improvement at the start\\nof planning. Drawing inspiration from MCTS, we tailor SMC planners specifically\\nfor RL by improving data generation within the planner through constrained\\naction sampling and explicit terminal state handling, as well as improving\\npolicy and value target estimation. This leads to our Trust-Region Twisted SMC\\n(TRT-SMC), which shows improved runtime and sample-efficiency over baseline\\nMCTS and SMC methods in both discrete and continuous domains.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-08T13:47:07Z\"}"}

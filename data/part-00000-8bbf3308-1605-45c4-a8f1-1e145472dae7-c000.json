{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01667v1\", \"title\": \"Testing Low-Resource Language Support in LLMs Using Language Proficiency\\n  Exams: the Case of Luxembourgish\", \"summary\": \"Large Language Models (LLMs) have become an increasingly important tool in\\nresearch and society at large. While LLMs are regularly used all over the world\\nby experts and lay-people alike, they are predominantly developed with\\nEnglish-speaking users in mind, performing well in English and other\\nwide-spread languages while less-resourced languages such as Luxembourgish are\\nseen as a lower priority. This lack of attention is also reflected in the\\nsparsity of available evaluation tools and datasets. In this study, we\\ninvestigate the viability of language proficiency exams as such evaluation\\ntools for the Luxembourgish language. We find that large models such as\\nChatGPT, Claude and DeepSeek-R1 typically achieve high scores, while smaller\\nmodels show weak performances. We also find that the performances in such\\nlanguage exams can be used to predict performances in other NLP tasks.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-02T12:16:14Z\"}"}

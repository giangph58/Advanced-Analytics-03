{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01650v1\", \"title\": \"Sparse Gaussian Neural Processes\", \"summary\": \"Despite significant recent advances in probabilistic meta-learning, it is\\ncommon for practitioners to avoid using deep learning models due to a\\ncomparative lack of interpretability. Instead, many practitioners simply use\\nnon-meta-models such as Gaussian processes with interpretable priors, and\\nconduct the tedious procedure of training their model from scratch for each\\ntask they encounter. While this is justifiable for tasks with a limited number\\nof data points, the cubic computational cost of exact Gaussian process\\ninference renders this prohibitive when each task has many observations. To\\nremedy this, we introduce a family of models that meta-learn sparse Gaussian\\nprocess inference. Not only does this enable rapid prediction on new tasks with\\nsparse Gaussian processes, but since our models have clear interpretations as\\nmembers of the neural process family, it also allows manual elicitation of\\npriors in a neural process for the first time. In meta-learning regimes for\\nwhich the number of observed tasks is small or for which expert domain\\nknowledge is available, this offers a crucial advantage.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.LG\", \"published\": \"2025-04-02T12:00:09Z\"}"}

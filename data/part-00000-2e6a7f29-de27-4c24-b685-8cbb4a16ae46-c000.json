{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07891v1\", \"title\": \"SpecReason: Fast and Accurate Inference-Time Compute via Speculative\\n  Reasoning\", \"summary\": \"Recent advances in inference-time compute have significantly improved\\nperformance on complex tasks by generating long chains of thought (CoTs) using\\nLarge Reasoning Models (LRMs). However, this improved accuracy comes at the\\ncost of high inference latency due to the length of generated reasoning\\nsequences and the autoregressive nature of decoding. Our key insight in\\ntackling these overheads is that LRM inference, and the reasoning that it\\nembeds, is highly tolerant of approximations: complex tasks are typically\\nbroken down into simpler steps, each of which brings utility based on the\\nsemantic insight it provides for downstream steps rather than the exact tokens\\nit generates. Accordingly, we introduce SpecReason, a system that automatically\\naccelerates LRM inference by using a lightweight model to (speculatively) carry\\nout simpler intermediate reasoning steps and reserving the costly base model\\nonly to assess (and potentially correct) the speculated outputs. Importantly,\\nSpecReason's focus on exploiting the semantic flexibility of thinking tokens in\\npreserving final-answer accuracy is complementary to prior speculation\\ntechniques, most notably speculative decoding, which demands token-level\\nequivalence at each step. Across a variety of reasoning benchmarks, SpecReason\\nachieves 1.5-2.5$\\\\times$ speedup over vanilla LRM inference while improving\\naccuracy by 1.0-9.9\\\\%. Compared to speculative decoding without SpecReason,\\ntheir combination yields an additional 19.4-44.2\\\\% latency reduction. We\\nopen-source SpecReason at https://github.com/ruipeterpan/specreason.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-10T16:05:19Z\"}"}

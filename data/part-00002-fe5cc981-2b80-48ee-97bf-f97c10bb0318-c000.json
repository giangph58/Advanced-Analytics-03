{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02542v1\", \"title\": \"Audio-visual Controlled Video Diffusion with Masked Selective State\\n  Spaces Modeling for Natural Talking Head Generation\", \"summary\": \"Talking head synthesis is vital for virtual avatars and human-computer\\ninteraction. However, most existing methods are typically limited to accepting\\ncontrol from a single primary modality, restricting their practical utility. To\\nthis end, we introduce \\\\textbf{ACTalker}, an end-to-end video diffusion\\nframework that supports both multi-signals control and single-signal control\\nfor talking head video generation. For multiple control, we design a parallel\\nmamba structure with multiple branches, each utilizing a separate driving\\nsignal to control specific facial regions. A gate mechanism is applied across\\nall branches, providing flexible control over video generation. To ensure\\nnatural coordination of the controlled video both temporally and spatially, we\\nemploy the mamba structure, which enables driving signals to manipulate feature\\ntokens across both dimensions in each branch. Additionally, we introduce a\\nmask-drop strategy that allows each driving signal to independently control its\\ncorresponding facial region within the mamba structure, preventing control\\nconflicts. Experimental results demonstrate that our method produces\\nnatural-looking facial videos driven by diverse signals and that the mamba\\nlayer seamlessly integrates multiple driving modalities without conflict.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-03T12:44:41Z\"}"}

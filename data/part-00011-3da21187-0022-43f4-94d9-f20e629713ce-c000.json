{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07655v1\", \"title\": \"Synthesizing High-Quality Programming Tasks with LLM-based Expert and\\n  Student Agents\", \"summary\": \"Generative AI is transforming computing education by enabling the automatic\\ngeneration of personalized content and feedback. We investigate its\\ncapabilities in providing high-quality programming tasks to students. Despite\\npromising advancements in task generation, a quality gap remains between\\nAI-generated and expert-created tasks. The AI-generated tasks may not align\\nwith target programming concepts, could be incomprehensible for students to\\nsolve, or may contain critical issues such as incorrect tests. Existing works\\noften require interventions from human teachers for validation. We address\\nthese challenges by introducing PyTaskSyn, a novel synthesis technique that\\nfirst generates a programming task and then decides whether it meets certain\\nquality criteria to be given to students. The key idea is to break this process\\ninto multiple stages performed by expert and student agents simulated using\\nboth strong and weaker generative models. Through extensive evaluation, we show\\nthat PyTaskSyn significantly improves task quality compared to baseline\\ntechniques and showcases the importance of each specialized agent type in our\\nvalidation pipeline. Additionally, we conducted user studies using our publicly\\navailable web application and show that PyTaskSyn can deliver high-quality\\nprogramming tasks comparable to expert-designed ones while reducing workload\\nand costs, and being more engaging than programming tasks that are available in\\nonline resources.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CY\", \"published\": \"2025-04-10T11:08:39Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05822v1\", \"title\": \"Federated Unlearning Made Practical: Seamless Integration via Negated\\n  Pseudo-Gradients\", \"summary\": \"The right to be forgotten is a fundamental principle of privacy-preserving\\nregulations and extends to Machine Learning (ML) paradigms such as Federated\\nLearning (FL). While FL enhances privacy by enabling collaborative model\\ntraining without sharing private data, trained models still retain the\\ninfluence of training data. Federated Unlearning (FU) methods recently proposed\\noften rely on impractical assumptions for real-world FL deployments, such as\\nstoring client update histories or requiring access to a publicly available\\ndataset. To address these constraints, this paper introduces a novel method\\nthat leverages negated Pseudo-gradients Updates for Federated Unlearning (PUF).\\nOur approach only uses standard client model updates, anyway employed during\\nregular FL rounds, and interprets them as pseudo-gradients. When a client needs\\nto be forgotten, we apply the negated of their pseudo-gradients, appropriately\\nscaled, to the global model. Unlike state-of-the-art mechanisms, PUF seamlessly\\nintegrates with FL workflows, incurs no additional computational and\\ncommunication overhead beyond standard FL rounds, and supports concurrent\\nunlearning requests. We extensively evaluated the proposed method on two\\nwell-known benchmark image classification datasets (CIFAR-10 and CIFAR-100) and\\na real-world medical imaging dataset for segmentation (ProstateMRI), using\\nthree different neural architectures: two residual networks and a vision\\ntransformer. The experimental results across various settings demonstrate that\\nPUF achieves state-of-the-art forgetting effectiveness and recovery time,\\nwithout relying on any additional assumptions, thus underscoring its practical\\napplicability.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-08T09:05:33Z\"}"}

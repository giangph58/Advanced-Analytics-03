{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23972v1\", \"title\": \"Noise-based reward-modulated learning\", \"summary\": \"Recent advances in reinforcement learning (RL) have led to significant\\nimprovements in task performance. However, training neural networks in an RL\\nregime is typically achieved in combination with backpropagation, limiting\\ntheir applicability in resource-constrained environments or when using\\nnon-differentiable neural networks. While noise-based alternatives like\\nreward-modulated Hebbian learning (RMHL) have been proposed, their performance\\nhas remained limited, especially in scenarios with delayed rewards, which\\nrequire retrospective credit assignment over time. Here, we derive a novel\\nnoise-based learning rule that addresses these challenges. Our approach\\ncombines directional derivative theory with Hebbian-like updates to enable\\nefficient, gradient-free learning in RL. It features stochastic noisy neurons\\nwhich can approximate gradients, and produces local synaptic updates modulated\\nby a global reward signal. Drawing on concepts from neuroscience, our method\\nuses reward prediction error as its optimization target to generate\\nincreasingly advantageous behavior, and incorporates an eligibility trace to\\nfacilitate temporal credit assignment in environments with delayed rewards. Its\\nformulation relies on local information alone, making it compatible with\\nimplementations in neuromorphic hardware. Experimental validation shows that\\nour approach significantly outperforms RMHL and is competitive with BP-based\\nbaselines, highlighting the promise of noise-based, biologically inspired\\nlearning for low-power and real-time applications.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-03-31T11:35:23Z\"}"}

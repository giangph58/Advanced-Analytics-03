{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01798v1\", \"title\": \"A Novel Approach To Implementing Knowledge Distillation In Tsetlin\\n  Machines\", \"summary\": \"The Tsetlin Machine (TM) is a propositional logic based model that uses\\nconjunctive clauses to learn patterns from data. As with typical neural\\nnetworks, the performance of a Tsetlin Machine is largely dependent on its\\nparameter count, with a larger number of parameters producing higher accuracy\\nbut slower execution. Knowledge distillation in neural networks transfers\\ninformation from an already-trained teacher model to a smaller student model to\\nincrease accuracy in the student without increasing execution time. We propose\\na novel approach to implementing knowledge distillation in Tsetlin Machines by\\nutilizing the probability distributions of each output sample in the teacher to\\nprovide additional context to the student. Additionally, we propose a novel\\nclause-transfer algorithm that weighs the importance of each clause in the\\nteacher and initializes the student with only the most essential data. We find\\nthat our algorithm can significantly improve performance in the student model\\nwithout negatively impacting latency in the tested domains of image recognition\\nand text classification.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.LG,cs.LO\", \"published\": \"2025-04-02T15:06:27Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.21717v1\", \"title\": \"CLAIMCHECK: How Grounded are LLM Critiques of Scientific Papers?\", \"summary\": \"A core part of scientific peer review involves providing expert critiques\\nthat directly assess the scientific claims a paper makes. While it is now\\npossible to automatically generate plausible (if generic) reviews, ensuring\\nthat these reviews are sound and grounded in the papers' claims remains\\nchallenging. To facilitate LLM benchmarking on these challenges, we introduce\\nCLAIMCHECK, an annotated dataset of NeurIPS 2023 and 2024 submissions and\\nreviews mined from OpenReview. CLAIMCHECK is richly annotated by ML experts for\\nweakness statements in the reviews and the paper claims that they dispute, as\\nwell as fine-grained labels of the validity, objectivity, and type of the\\nidentified weaknesses. We benchmark several LLMs on three claim-centric tasks\\nsupported by CLAIMCHECK, requiring models to (1) associate weaknesses with the\\nclaims they dispute, (2) predict fine-grained labels for weaknesses and rewrite\\nthe weaknesses to enhance their specificity, and (3) verify a paper's claims\\nwith grounded reasoning. Our experiments reveal that cutting-edge LLMs, while\\ncapable of predicting weakness labels in (2), continue to underperform relative\\nto human experts on all other tasks.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-03-27T17:29:45Z\"}"}

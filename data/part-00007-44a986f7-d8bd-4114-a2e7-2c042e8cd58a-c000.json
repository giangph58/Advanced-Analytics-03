{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07809v1\", \"title\": \"A Riemannian Gradient Descent Method for the Least Squares Inverse\\n  Eigenvalue Problem\", \"summary\": \"We address an algorithm for the least squares fitting of a subset of the\\neigenvalues of an unknown Hermitian matrix lying an an affine subspace, called\\nthe Lift and Projection (LP) method, due to Chen and Chu (SIAM Journal on\\nNumerical Analysis, 33 (1996), pp.2417-2430). The LP method iteratively `lifts'\\nthe current iterate onto the spectral constraint manifold then 'projects' onto\\nthe solution's affine subspace. We prove that this is equivalent to a\\nRiemannian Gradient Descent with respect to a natural Riemannian metric. This\\ninsight allows us to derive a more efficient implementation, analyse more\\nprecisely its global convergence properties, and naturally append additional\\nconstraints to the problem. We provide several numerical experiments to\\ndemonstrate the improvement in computation time, which can be more than an\\norder of magnitude if the eigenvalue constraints are on the smallest\\neigenvalues, the largest eigenvalues, or the eigenvalues closest to a given\\nnumber. These experiments include an inverse eigenvalue problem arising in\\nInelastic Neutron Scattering of Manganese-6, which requires the least squares\\nfitting of 16 experimentally observed eigenvalues of a $32400\\\\times32400$\\nsparse matrix from a 5-dimensional subspace of spin Hamiltonian matrices.\", \"main_category\": \"math.NA\", \"categories\": \"math.NA,cs.NA\", \"published\": \"2025-04-10T14:47:16Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23777v1\", \"title\": \"CONGRAD:Conflicting Gradient Filtering for Multilingual Preference\\n  Alignment\", \"summary\": \"Naive joint training of large language models (LLMs) for multilingual\\npreference alignment can suffer from negative interference. This is a known\\nissue in multilingual training, where conflicting objectives degrade overall\\nperformance. However, the impact of this phenomenon in the context of\\nmultilingual preference alignment remains largely underexplored. To address\\nthis issue, we propose CONGRAD, a scalable and effective filtering method that\\nselects high-quality preference samples with minimal gradient conflicts across\\nlanguages. Our method leverages gradient surgery to retain samples aligned with\\nan aggregated multilingual update direction. Additionally, we incorporate a\\nsublinear gradient compression strategy that reduces memory overhead during\\ngradient accumulation. We integrate CONGRAD into self-rewarding framework and\\nevaluate on LLaMA3-8B and Gemma2-2B across 10 languages. Results show that\\nCONGRAD consistently outperforms strong baselines in both seen and unseen\\nlanguages, with minimal alignment tax.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-03-31T06:52:56Z\"}"}

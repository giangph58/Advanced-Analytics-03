{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02646v1\", \"title\": \"Prompt Optimization with Logged Bandit Data\", \"summary\": \"We study how to use naturally available user feedback, such as clicks, to\\noptimize large language model (LLM) pipelines for generating personalized\\nsentences using prompts. Naive approaches, which estimate the policy gradient\\nin the prompt space, suffer either from variance caused by the large action\\nspace of prompts or bias caused by inaccurate reward predictions. To circumvent\\nthese challenges, we propose a novel kernel-based off-policy gradient method,\\nwhich estimates the policy gradient by leveraging similarity among generated\\nsentences, substantially reducing variance while suppressing the bias.\\nEmpirical results on our newly established suite of benchmarks demonstrate the\\neffectiveness of the proposed approach in generating personalized descriptions\\nfor movie recommendations, particularly when the number of candidate prompts is\\nlarge.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.IR,stat.ML\", \"published\": \"2025-04-03T14:40:40Z\"}"}

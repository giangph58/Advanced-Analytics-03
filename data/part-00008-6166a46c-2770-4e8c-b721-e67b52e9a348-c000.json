{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24182v1\", \"title\": \"CIBR: Cross-modal Information Bottleneck Regularization for Robust CLIP\\n  Generalization\", \"summary\": \"Contrastive Language-Image Pretraining (CLIP) has achieved remarkable success\\nin cross-modal tasks such as zero-shot image classification and text-image\\nretrieval by effectively aligning visual and textual representations. However,\\nthe theoretical foundations underlying CLIP's strong generalization remain\\nunclear. In this work, we address this gap by proposing the Cross-modal\\nInformation Bottleneck (CIB) framework. CIB offers a principled interpretation\\nof CLIP's contrastive learning objective as an implicit Information Bottleneck\\noptimization. Under this view, the model maximizes shared cross-modal\\ninformation while discarding modality-specific redundancies, thereby preserving\\nessential semantic alignment across modalities. Building on this insight, we\\nintroduce a Cross-modal Information Bottleneck Regularization (CIBR) method\\nthat explicitly enforces these IB principles during training. CIBR introduces a\\npenalty term to discourage modality-specific redundancy, thereby enhancing\\nsemantic alignment between image and text features. We validate CIBR on\\nextensive vision-language benchmarks, including zero-shot classification across\\nseven diverse image datasets and text-image retrieval on MSCOCO and Flickr30K.\\nThe results show consistent performance gains over standard CLIP. These\\nfindings provide the first theoretical understanding of CLIP's generalization\\nthrough the IB lens. They also demonstrate practical improvements, offering\\nguidance for future cross-modal representation learning.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-31T15:00:01Z\"}"}

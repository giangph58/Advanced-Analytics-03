{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23798v1\", \"title\": \"Adaptive Layer-skipping in Pre-trained LLMs\", \"summary\": \"Various layer-skipping methods have been proposed to accelerate token\\ngeneration in large language models (LLMs). However, they have overlooked a\\nfundamental question: How do computational demands vary across the generation\\nof different tokens? In this work, we introduce FlexiDepth, a method that\\ndynamically adjusts the number of Transformer layers used in text generation.\\nBy incorporating a plug-in router and adapter, FlexiDepth enables adaptive\\nlayer-skipping in LLMs without modifying their original parameters. Introducing\\nFlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32,\\nand meanwhile maintains the full 100\\\\% benchmark performance. Experimental\\nresults with FlexiDepth demonstrate that computational demands in LLMs\\nsignificantly vary based on token type. Specifically, generating repetitive\\ntokens or fixed phrases requires fewer layers, whereas producing tokens\\ninvolving computation or high uncertainty requires more layers. Interestingly,\\nthis adaptive allocation pattern aligns with human intuition. To advance\\nresearch in this area, we open sourced FlexiDepth and a dataset documenting\\nFlexiDepth's layer allocation patterns for future exploration.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-03-31T07:20:58Z\"}"}

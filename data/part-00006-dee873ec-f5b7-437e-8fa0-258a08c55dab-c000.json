{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05228v1\", \"title\": \"NoveltyBench: Evaluating Creativity and Diversity in Language Models\", \"summary\": \"Language models have demonstrated remarkable capabilities on standard\\nbenchmarks, yet they struggle increasingly from mode collapse, the inability to\\ngenerate diverse and novel outputs. Our work introduces NoveltyBench, a\\nbenchmark specifically designed to evaluate the ability of language models to\\nproduce multiple distinct and high-quality outputs. NoveltyBench utilizes\\nprompts curated to elicit diverse answers and filtered real-world user queries.\\nEvaluating 20 leading language models, we find that current state-of-the-art\\nsystems generate significantly less diversity than human writers. Notably,\\nlarger models within a family often exhibit less diversity than their smaller\\ncounterparts, challenging the notion that capability on standard benchmarks\\ntranslates directly to generative utility. While prompting strategies like\\nin-context regeneration can elicit diversity, our findings highlight a\\nfundamental lack of distributional diversity in current models, reducing their\\nutility for users seeking varied responses and suggesting the need for new\\ntraining and evaluation paradigms that prioritize creativity alongside quality.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-07T16:14:23Z\"}"}

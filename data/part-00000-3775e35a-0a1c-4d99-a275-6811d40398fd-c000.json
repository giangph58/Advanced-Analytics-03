{"value":"{\"aid\": \"http://arxiv.org/abs/2503.21745v1\", \"title\": \"3DGen-Bench: Comprehensive Benchmark Suite for 3D Generative Models\", \"summary\": \"3D generation is experiencing rapid advancements, while the development of 3D\\nevaluation has not kept pace. How to keep automatic evaluation equitably\\naligned with human perception has become a well-recognized challenge. Recent\\nadvances in the field of language and image generation have explored human\\npreferences and showcased respectable fitting ability. However, the 3D domain\\nstill lacks such a comprehensive preference dataset over generative models. To\\nmitigate this absence, we develop 3DGen-Arena, an integrated platform in a\\nbattle manner. Then, we carefully design diverse text and image prompts and\\nleverage the arena platform to gather human preferences from both public users\\nand expert annotators, resulting in a large-scale multi-dimension human\\npreference dataset 3DGen-Bench. Using this dataset, we further train a\\nCLIP-based scoring model, 3DGen-Score, and a MLLM-based automatic evaluator,\\n3DGen-Eval. These two models innovatively unify the quality evaluation of\\ntext-to-3D and image-to-3D generation, and jointly form our automated\\nevaluation system with their respective strengths. Extensive experiments\\ndemonstrate the efficacy of our scoring model in predicting human preferences,\\nexhibiting a superior correlation with human ranks compared to existing\\nmetrics. We believe that our 3DGen-Bench dataset and automated evaluation\\nsystem will foster a more equitable evaluation in the field of 3D generation,\\nfurther promoting the development of 3D generative models and their downstream\\napplications.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-27T17:53:00Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04784v1\", \"title\": \"Disentangling Instruction Influence in Diffusion Transformers for\\n  Parallel Multi-Instruction-Guided Image Editing\", \"summary\": \"Instruction-guided image editing enables users to specify modifications using\\nnatural language, offering more flexibility and control. Among existing\\nframeworks, Diffusion Transformers (DiTs) outperform U-Net-based diffusion\\nmodels in scalability and performance. However, while real-world scenarios\\noften require concurrent execution of multiple instructions, step-by-step\\nediting suffers from accumulated errors and degraded quality, and integrating\\nmultiple instructions with a single prompt usually results in incomplete edits\\ndue to instruction conflicts. We propose Instruction Influence Disentanglement\\n(IID), a novel framework enabling parallel execution of multiple instructions\\nin a single denoising process, designed for DiT-based models. By analyzing\\nself-attention mechanisms in DiTs, we identify distinctive attention patterns\\nin multi-instruction settings and derive instruction-specific attention masks\\nto disentangle each instruction's influence. These masks guide the editing\\nprocess to ensure localized modifications while preserving consistency in\\nnon-edited regions. Extensive experiments on open-source and custom datasets\\ndemonstrate that IID reduces diffusion steps while improving fidelity and\\ninstruction completion compared to existing baselines. The codes will be\\npublicly released upon the acceptance of the paper.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-07T07:26:25Z\"}"}

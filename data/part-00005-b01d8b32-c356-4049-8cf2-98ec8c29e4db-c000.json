{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23988v1\", \"title\": \"Deep Learning Model Deployment in Multiple Cloud Providers: an\\n  Exploratory Study Using Low Computing Power Environments\", \"summary\": \"The deployment of Machine Learning models at cloud have grown by tech\\ncompanies. Hardware requirements are higher when these models involve Deep\\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\\nexplore deploying DL models using for experiments the GECToR model, a DL\\nsolution for Grammatical Error Correction, across three of the major cloud\\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\\nusage and cost at each cloud provider by 7 execution environments with 10\\nexperiments reproduced. We found that while GPUs excel in performance, they had\\nan average cost 300% higher than solutions without GPU. Our analysis also\\nidentifies that processor cache size is crucial for cost-effective CPU\\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\\ndemonstrates the feasibility and affordability of cloud-based DL inference\\nsolutions without GPUs, benefiting resource-constrained users like startups.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC,cs.AI,cs.PF\", \"published\": \"2025-03-31T11:58:37Z\"}"}

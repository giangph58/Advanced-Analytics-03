{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06141v1\", \"title\": \"Adversarial Training of Reward Models\", \"summary\": \"Reward modeling has emerged as a promising approach for the scalable\\nalignment of language models. However, contemporary reward models (RMs) often\\nlack robustness, awarding high rewards to low-quality, out-of-distribution\\n(OOD) samples. This can lead to reward hacking, where policies exploit\\nunintended shortcuts to maximize rewards, undermining alignment. To address\\nthis challenge, we introduce Adv-RM, a novel adversarial training framework\\nthat automatically identifies adversarial examples -- responses that receive\\nhigh rewards from the target RM but are OOD and of low quality. By leveraging\\nreinforcement learning, Adv-RM trains a policy to generate adversarial examples\\nthat reliably expose vulnerabilities in large state-of-the-art reward models\\nsuch as Nemotron 340B RM. Incorporating these adversarial examples into the\\nreward training process improves the robustness of RMs, mitigating reward\\nhacking and enhancing downstream performance in RLHF. We demonstrate that\\nAdv-RM significantly outperforms conventional RM training, increasing stability\\nand enabling more effective RLHF training in both synthetic and real-data\\nsettings.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-08T15:38:25Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23782v1\", \"title\": \"Distributional regression with reject option\", \"summary\": \"Selective prediction, where a model has the option to abstain from making a\\ndecision, is crucial for machine learning applications in which mistakes are\\ncostly. In this work, we focus on distributional regression and introduce a\\nframework that enables the model to abstain from estimation in situations of\\nhigh uncertainty. We refer to this approach as distributional regression with\\nreject option, inspired by similar concepts in classification and regression\\nwith reject option. We study the scenario where the rejection rate is fixed. We\\nderive a closed-form expression for the optimal rule, which relies on\\nthresholding the entropy function of the Continuous Ranked Probability Score\\n(CRPS). We propose a semi-supervised estimation procedure for the optimal rule,\\nusing two datasets: the first, labeled, is used to estimate both the\\nconditional distribution function and the entropy function of the CRPS, while\\nthe second, unlabeled, is employed to calibrate the desired rejection rate.\\nNotably, the control of the rejection rate is distribution-free. Under mild\\nconditions, we show that our procedure is asymptotically as effective as the\\noptimal rule, both in terms of error rate and rejection rate. Additionally, we\\nestablish rates of convergence for our approach based on distributional\\nk-nearest neighbor. A numerical analysis on real-world datasets demonstrates\\nthe strong performance of our procedure\", \"main_category\": \"math.ST\", \"categories\": \"math.ST,stat.TH\", \"published\": \"2025-03-31T06:56:18Z\"}"}

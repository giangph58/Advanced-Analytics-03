{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01959v1\", \"title\": \"Slot-Level Robotic Placement via Visual Imitation from Single Human\\n  Video\", \"summary\": \"The majority of modern robot learning methods focus on learning a set of\\npre-defined tasks with limited or no generalization to new tasks. Extending the\\nrobot skillset to novel tasks involves gathering an extensive amount of\\ntraining data for additional tasks. In this paper, we address the problem of\\nteaching new tasks to robots using human demonstration videos for repetitive\\ntasks (e.g., packing). This task requires understanding the human video to\\nidentify which object is being manipulated (the pick object) and where it is\\nbeing placed (the placement slot). In addition, it needs to re-identify the\\npick object and the placement slots during inference along with the relative\\nposes to enable robot execution of the task. To tackle this, we propose SLeRP,\\na modular system that leverages several advanced visual foundation models and a\\nnovel slot-level placement detector Slot-Net, eliminating the need for\\nexpensive video demonstrations for training. We evaluate our system using a new\\nbenchmark of real-world videos. The evaluation results show that SLeRP\\noutperforms several baselines and can be deployed on a real robot.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.CV\", \"published\": \"2025-04-02T17:59:45Z\"}"}

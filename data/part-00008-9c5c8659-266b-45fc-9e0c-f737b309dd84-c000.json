{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01589v1\", \"title\": \"Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in\\n  Vision-Language Models\", \"summary\": \"Vision-language models (VLMs) have advanced rapidly in processing multimodal\\ninformation, but their ability to reconcile conflicting signals across\\nmodalities remains underexplored. This work investigates how VLMs process ASCII\\nart, a unique medium where textual elements collectively form visual patterns,\\npotentially creating semantic-visual conflicts. We introduce a novel evaluation\\nframework that systematically challenges five state-of-the-art models\\n(including GPT-4o, Claude, and Gemini) using adversarial ASCII art, where\\ncharacter-level semantics deliberately contradict global visual patterns. Our\\nexperiments reveal a strong text-priority bias: VLMs consistently prioritize\\ntextual information over visual patterns, with visual recognition ability\\ndeclining dramatically as semantic complexity increases. Various mitigation\\nattempts through visual parameter tuning and prompt engineering yielded only\\nmodest improvements, suggesting that this limitation requires\\narchitectural-level solutions. These findings uncover fundamental flaws in how\\ncurrent VLMs integrate multimodal information, providing important guidance for\\nfuture model development while highlighting significant implications for\\ncontent moderation systems vulnerable to adversarial examples.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-02T10:47:07Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01707v1\", \"title\": \"InfiniteICL: Breaking the Limit of Context Window Size via Long\\n  Short-term Memory Transformation\", \"summary\": \"In-context learning (ICL) is critical for large language models (LLMs), but\\nits effectiveness is constrained by finite context windows, particularly in\\nultra-long contexts. To overcome this, we introduce InfiniteICL, a framework\\nthat parallels context and parameters in LLMs with short- and long-term memory\\nin human cognitive systems, focusing on transforming temporary context\\nknowledge into permanent parameter updates. This approach significantly reduces\\nmemory usage, maintains robust performance across varying input lengths, and\\ntheoretically enables infinite context integration through the principles of\\ncontext knowledge elicitation, selection, and consolidation. Evaluations\\ndemonstrate that our method reduces context length by 90% while achieving 103%\\naverage performance of full-context prompting across fact recall, grounded\\nreasoning, and skill acquisition tasks. When conducting sequential multi-turn\\ntransformations on complex, real-world contexts (with length up to 2M tokens),\\nour approach surpasses full-context prompting while using only 0.4% of the\\noriginal contexts. These findings highlight InfiniteICL's potential to enhance\\nthe scalability and efficiency of LLMs by breaking the limitations of\\nconventional context window sizes.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-02T13:15:44Z\"}"}

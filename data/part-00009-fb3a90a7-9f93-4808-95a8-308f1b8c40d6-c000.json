{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06020v1\", \"title\": \"Information-Theoretic Reward Decomposition for Generalizable RLHF\", \"summary\": \"A generalizable reward model is crucial in Reinforcement Learning from Human\\nFeedback (RLHF) as it enables correctly evaluating unseen prompt-response\\npairs. However, existing reward models lack this ability, as they are typically\\ntrained by increasing the reward gap between chosen and rejected responses,\\nwhile overlooking the prompts that the responses are conditioned on.\\nConsequently, when the trained reward model is evaluated on prompt-response\\npairs that lie outside the data distribution, neglecting the effect of prompts\\nmay result in poor generalization of the reward model. To address this issue,\\nwe decompose the reward value into two independent components: prompt-free\\nreward and prompt-related reward. Prompt-free reward represents the evaluation\\nthat is determined only by responses, while the prompt-related reward reflects\\nthe reward that derives from both the prompt and the response. We extract these\\ntwo components from an information-theoretic perspective, which requires no\\nextra models. Subsequently, we propose a new reward learning algorithm by\\nprioritizing data samples based on their prompt-free reward values. Through toy\\nexamples, we demonstrate that the extracted prompt-free and prompt-related\\nrewards effectively characterize two parts of the reward model. Further,\\nstandard evaluations show that our method improves both the alignment\\nperformance and the generalization capability of the reward model.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-08T13:26:07Z\"}"}

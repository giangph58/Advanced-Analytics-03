{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05118v1\", \"title\": \"VAPO: Efficient and Reliable Reinforcement Learning for Advanced\\n  Reasoning Tasks\", \"summary\": \"We present VAPO, Value-based Augmented Proximal Policy Optimization framework\\nfor reasoning models., a novel framework tailored for reasoning models within\\nthe value-based paradigm. Benchmarked the AIME 2024 dataset, VAPO, built on the\\nQwen 32B pre-trained model, attains a state-of-the-art score of\\n$\\\\mathbf{60.4}$. In direct comparison under identical experimental settings,\\nVAPO outperforms the previously reported results of DeepSeek-R1-Zero-Qwen-32B\\nand DAPO by more than 10 points. The training process of VAPO stands out for\\nits stability and efficiency. It reaches state-of-the-art performance within a\\nmere 5,000 steps. Moreover, across multiple independent runs, no training\\ncrashes occur, underscoring its reliability. This research delves into long\\nchain-of-thought (long-CoT) reasoning using a value-based reinforcement\\nlearning framework. We pinpoint three key challenges that plague value-based\\nmethods: value model bias, the presence of heterogeneous sequence lengths, and\\nthe sparsity of reward signals. Through systematic design, VAPO offers an\\nintegrated solution that effectively alleviates these challenges, enabling\\nenhanced performance in long-CoT reasoning tasks.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-07T14:21:11Z\"}"}

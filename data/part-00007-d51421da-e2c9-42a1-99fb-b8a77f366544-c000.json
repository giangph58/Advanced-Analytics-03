{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05753v1\", \"title\": \"Accelerated Natural Gradient Method for Parametric Manifold Optimization\", \"summary\": \"Parametric manifold optimization problems frequently arise in various machine\\nlearning tasks, where state functions are defined on infinite-dimensional\\nmanifolds. We propose a unified accelerated natural gradient descent (ANGD)\\nframework to address these problems. By incorporating a Hessian-driven damping\\nterm into the manifold update, we derive an accelerated Riemannian gradient\\n(ARG) flow that mitigates oscillations. An equivalent first-order system is\\nfurther presented for the ARG flow, enabling a unified discretization scheme\\nthat leads to the ANGD method. In our discrete update, our framework considers\\nvarious advanced techniques, including least squares approximation of the\\nupdate direction, projected momentum to accelerate convergence, and efficient\\napproximation methods through the Kronecker product. It accommodates various\\nmetrics, including $H^s$, Fisher-Rao, and Wasserstein-2 metrics, providing a\\ncomputationally efficient solution for large-scale parameter spaces. We\\nestablish a convergence rate for the ARG flow under geodesic convexity\\nassumptions. Numerical experiments demonstrate that ANGD outperforms standard\\nNGD, underscoring its effectiveness across diverse deep learning tasks.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC\", \"published\": \"2025-04-08T07:33:04Z\"}"}

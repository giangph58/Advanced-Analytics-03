{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05024v1\", \"title\": \"Concept Extraction for Time Series with ECLAD-ts\", \"summary\": \"Convolutional neural networks (CNNs) for time series classification (TSC) are\\nbeing increasingly used in applications ranging from quality prediction to\\nmedical diagnosis. The black box nature of these models makes understanding\\ntheir prediction process difficult. This issue is crucial because CNNs are\\nprone to learning shortcuts and biases, compromising their robustness and\\nalignment with human expectations. To assess whether such mechanisms are being\\nused and the associated risk, it is essential to provide model explanations\\nthat reflect the inner workings of the model. Concept Extraction (CE) methods\\noffer such explanations, but have mostly been developed for the image domain so\\nfar, leaving a gap in the time series domain. In this work, we present a CE and\\nlocalization method tailored to the time series domain, based on the ideas of\\nCE methods for images. We propose the novel method ECLAD-ts, which provides\\npost-hoc global explanations based on how the models encode subsets of the\\ninput at different levels of abstraction. For this, concepts are produced by\\nclustering timestep-wise aggregations of CNN activation maps, and their\\nimportance is computed based on their impact on the prediction process. We\\nevaluate our method on synthetic and natural datasets. Furthermore, we assess\\nthe advantages and limitations of CE in time series through empirical results.\\nOur results show that ECLAD-ts effectively explains models by leveraging their\\ninternal representations, providing useful insights about their prediction\\nprocess.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-07T12:49:20Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24289v1\", \"title\": \"Rec-R1: Bridging Generative Large Language Models and User-Centric\\n  Recommendation Systems via Reinforcement Learning\", \"summary\": \"We propose Rec-R1, a general reinforcement learning framework that bridges\\nlarge language models (LLMs) with recommendation systems through closed-loop\\noptimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1\\ndirectly optimizes LLM generation using feedback from a fixed black-box\\nrecommendation model, without relying on synthetic SFT data from proprietary\\nmodels such as GPT-4o. This avoids the substantial cost and effort required for\\ndata distillation. To verify the effectiveness of Rec-R1, we evaluate it on two\\nrepresentative tasks: product search and sequential recommendation.\\nExperimental results demonstrate that Rec-R1 not only consistently outperforms\\nprompting- and SFT-based methods, but also achieves significant gains over\\nstrong discriminative baselines, even when used with simple retrievers such as\\nBM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM,\\nunlike SFT, which often impairs instruction-following and reasoning. These\\nfindings suggest Rec-R1 as a promising foundation for continual task-specific\\nadaptation without catastrophic forgetting.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR,cs.CL\", \"published\": \"2025-03-31T16:36:00Z\"}"}

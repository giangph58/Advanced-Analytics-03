{"value":"{\"aid\": \"http://arxiv.org/abs/2503.21696v1\", \"title\": \"Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for\\n  Embodied Interactive Tasks\", \"summary\": \"Recent advances in deep thinking models have demonstrated remarkable\\nreasoning capabilities on mathematical and coding tasks. However, their\\neffectiveness in embodied domains which require continuous interaction with\\nenvironments through image action interleaved trajectories remains largely\\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\\nthat relies primarily on logical deduction, embodied scenarios demand spatial\\nunderstanding, temporal reasoning, and ongoing self-reflection based on\\ninteraction history. To address these challenges, we synthesize 9.3k coherent\\nObservation-Thought-Action trajectories containing 64k interactive images and\\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\\nplanning, and verification). We develop a three-stage training pipeline that\\nprogressively enhances the model's capabilities through imitation learning,\\nself-exploration via rejection sampling, and self-correction through reflection\\ntuning. The evaluation shows that our model significantly outperforms those\\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\\nClaude-3.7 by +9\\\\%, 24\\\\%, and +13\\\\%. Analysis reveals our model exhibits fewer\\nrepeated searches and logical inconsistencies, with particular advantages in\\ncomplex long-horizon tasks. Real-world environments also show our superiority\\nwhile exhibiting fewer repeated searches and logical inconsistency cases.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.CV\", \"published\": \"2025-03-27T17:00:51Z\"}"}

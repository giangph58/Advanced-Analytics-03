{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01766v1\", \"title\": \"Learning with Imperfect Models: When Multi-step Prediction Mitigates\\n  Compounding Error\", \"summary\": \"Compounding error, where small prediction mistakes accumulate over time,\\npresents a major challenge in learning-based control. For example, this issue\\noften limits the performance of model-based reinforcement learning and\\nimitation learning. One common approach to mitigate compounding error is to\\ntrain multi-step predictors directly, rather than relying on autoregressive\\nrollout of a single-step model. However, it is not well understood when the\\nbenefits of multi-step prediction outweigh the added complexity of learning a\\nmore complicated model. In this work, we provide a rigorous analysis of this\\ntrade-off in the context of linear dynamical systems. We show that when the\\nmodel class is well-specified and accurately captures the system dynamics,\\nsingle-step models achieve lower asymptotic prediction error. On the other\\nhand, when the model class is misspecified due to partial observability, direct\\nmulti-step predictors can significantly reduce bias and thus outperform\\nsingle-step approaches. These theoretical results are supported by numerical\\nexperiments, wherein we also (a) empirically evaluate an intermediate strategy\\nwhich trains a single-step model using a multi-step loss and (b) evaluate\\nperformance of single step and multi-step predictors in a closed loop control\\nsetting.\", \"main_category\": \"eess.SY\", \"categories\": \"eess.SY,cs.LG,cs.SY\", \"published\": \"2025-04-02T14:18:52Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06214v1\", \"title\": \"From 128K to 4M: Efficient Training of Ultra-Long Context Large Language\\n  Models\", \"summary\": \"Long-context capabilities are essential for a wide range of applications,\\nincluding document and video understanding, in-context learning, and\\ninference-time scaling, all of which require models to process and reason over\\nlong sequences of text and multimodal data. In this work, we introduce a\\nefficient training recipe for building ultra-long context LLMs from aligned\\ninstruct model, pushing the boundaries of context lengths from 128K to 1M, 2M,\\nand 4M tokens. Our approach leverages efficient continued pretraining\\nstrategies to extend the context window and employs effective instruction\\ntuning to maintain the instruction-following and reasoning abilities. Our\\nUltraLong-8B, built on Llama3.1-Instruct with our recipe, achieves\\nstate-of-the-art performance across a diverse set of long-context benchmarks.\\nImportantly, models trained with our approach maintain competitive performance\\non standard benchmarks, demonstrating balanced improvements for both long and\\nshort context tasks. We further provide an in-depth analysis of key design\\nchoices, highlighting the impacts of scaling strategies and data composition.\\nOur findings establish a robust framework for efficiently scaling context\\nlengths while preserving general model capabilities. We release all model\\nweights at: https://ultralong.github.io/.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-08T16:58:58Z\"}"}

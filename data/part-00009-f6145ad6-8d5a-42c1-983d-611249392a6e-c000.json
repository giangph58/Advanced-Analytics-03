{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02323v1\", \"title\": \"CoTAL: Human-in-the-Loop Prompt Engineering, Chain-of-Thought Reasoning,\\n  and Active Learning for Generalizable Formative Assessment Scoring\", \"summary\": \"Large language models (LLMs) have created new opportunities to assist\\nteachers and support student learning. Methods such as chain-of-thought (CoT)\\nprompting enable LLMs to grade formative assessments in science, providing\\nscores and relevant feedback to students. However, the extent to which these\\nmethods generalize across curricula in multiple domains (such as science,\\ncomputing, and engineering) remains largely untested. In this paper, we\\nintroduce Chain-of-Thought Prompting + Active Learning (CoTAL), an LLM-based\\napproach to formative assessment scoring that (1) leverages Evidence-Centered\\nDesign (ECD) principles to develop curriculum-aligned formative assessments and\\nrubrics, (2) applies human-in-the-loop prompt engineering to automate response\\nscoring, and (3) incorporates teacher and student feedback to iteratively\\nrefine assessment questions, grading rubrics, and LLM prompts for automated\\ngrading. Our findings demonstrate that CoTAL improves GPT-4's scoring\\nperformance, achieving gains of up to 24.5% over a non-prompt-engineered\\nbaseline. Both teachers and students view CoTAL as effective in scoring and\\nexplaining student responses, each providing valuable refinements to enhance\\ngrading accuracy and explanation quality.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-03T06:53:34Z\"}"}

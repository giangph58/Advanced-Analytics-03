{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01515v1\", \"title\": \"Training-free Dense-Aligned Diffusion Guidance for Modular Conditional\\n  Image Synthesis\", \"summary\": \"Conditional image synthesis is a crucial task with broad applications, such\\nas artistic creation and virtual reality. However, current generative methods\\nare often task-oriented with a narrow scope, handling a restricted condition\\nwith constrained applicability. In this paper, we propose a novel approach that\\ntreats conditional image synthesis as the modular combination of diverse\\nfundamental condition units. Specifically, we divide conditions into three\\nprimary units: text, layout, and drag. To enable effective control over these\\nconditions, we design a dedicated alignment module for each. For the text\\ncondition, we introduce a Dense Concept Alignment (DCA) module, which achieves\\ndense visual-text alignment by drawing on diverse textual concepts. For the\\nlayout condition, we propose a Dense Geometry Alignment (DGA) module to enforce\\ncomprehensive geometric constraints that preserve the spatial configuration.\\nFor the drag condition, we introduce a Dense Motion Alignment (DMA) module to\\napply multi-level motion regularization, ensuring that each pixel follows its\\ndesired trajectory without visual artifacts. By flexibly inserting and\\ncombining these alignment modules, our framework enhances the model's\\nadaptability to diverse conditional generation tasks and greatly expands its\\napplication range. Extensive experiments demonstrate the superior performance\\nof our framework across a variety of conditions, including textual description,\\nsegmentation mask (bounding box), drag manipulation, and their combinations.\\nCode is available at https://github.com/ZixuanWang0525/DADG.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-02T09:00:28Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01482v1\", \"title\": \"A Robust Model-Based Approach for Continuous-Time Policy Evaluation with\\n  Unknown L\\u00e9vy Process Dynamics\", \"summary\": \"This paper develops a model-based framework for continuous-time policy\\nevaluation (CTPE) in reinforcement learning, incorporating both Brownian and\\nL\\\\'evy noise to model stochastic dynamics influenced by rare and extreme\\nevents. Our approach formulates the policy evaluation problem as solving a\\npartial integro-differential equation (PIDE) for the value function with\\nunknown coefficients. A key challenge in this setting is accurately recovering\\nthe unknown coefficients in the stochastic dynamics, particularly when driven\\nby L\\\\'evy processes with heavy tail effects. To address this, we propose a\\nrobust numerical approach that effectively handles both unbiased and censored\\ntrajectory datasets. This method combines maximum likelihood estimation with an\\niterative tail correction mechanism, improving the stability and accuracy of\\ncoefficient recovery. Additionally, we establish a theoretical bound for the\\npolicy evaluation error based on coefficient recovery error. Through numerical\\nexperiments, we demonstrate the effectiveness and robustness of our method in\\nrecovering heavy-tailed L\\\\'evy dynamics and verify the theoretical error\\nanalysis in policy evaluation.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.NA,math.NA\", \"published\": \"2025-04-02T08:37:14Z\"}"}

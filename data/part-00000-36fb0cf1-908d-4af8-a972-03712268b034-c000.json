{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06664v1\", \"title\": \"SEE: Continual Fine-tuning with Sequential Ensemble of Experts\", \"summary\": \"Continual fine-tuning of large language models (LLMs) suffers from\\ncatastrophic forgetting. Rehearsal-based methods mitigate this problem by\\nretaining a small set of old data. Nevertheless, they still suffer inevitable\\nperformance loss. Although training separate experts for each task can help\\nprevent forgetting, effectively assembling them remains a challenge. Some\\napproaches use routers to assign tasks to experts, but in continual learning,\\nthey often require retraining for optimal performance. To address these\\nchallenges, we introduce the Sequential Ensemble of Experts (SEE) framework.\\nSEE removes the need for an additional router, allowing each expert to\\nindependently decide whether a query should be handled. The framework employs\\ndistributed routing, and during continual fine-tuning, SEE only requires the\\ntraining of new experts for incoming tasks rather than retraining the entire\\nsystem. Experiments reveal that the SEE outperforms prior approaches, including\\nmulti-task learning, in continual fine-tuning. It also demonstrates remarkable\\ngeneralization ability, as the expert can effectively identify\\nout-of-distribution queries, which can then be directed to a more generalized\\nmodel for resolution. This work highlights the promising potential of\\nintegrating routing and response mechanisms within each expert, paving the way\\nfor the future of distributed model ensembling.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-09T07:56:56Z\"}"}

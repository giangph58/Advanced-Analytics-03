{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02807v1\", \"title\": \"MegaMath: Pushing the Limits of Open Math Corpora\", \"summary\": \"Mathematical reasoning is a cornerstone of human intelligence and a key\\nbenchmark for advanced capabilities in large language models (LLMs). However,\\nthe research community still lacks an open, large-scale, high-quality corpus\\ntailored to the demands of math-centric LLM pre-training. We present MegaMath,\\nan open dataset curated from diverse, math-focused sources through following\\npractices: (1) Revisiting web data: We re-extracted mathematical documents from\\nCommon Crawl with math-oriented HTML optimizations, fasttext-based filtering\\nand deduplication, all for acquiring higher-quality data on the Internet. (2)\\nRecalling Math-related code data: We identified high quality math-related code\\nfrom large code training corpus, Stack-V2, further enhancing data diversity.\\n(3) Exploring Synthetic data: We synthesized QA-style text, math-related code,\\nand interleaved text-code blocks from web data or code data. By integrating\\nthese strategies and validating their effectiveness through extensive\\nablations, MegaMath delivers 371B tokens with the largest quantity and top\\nquality among existing open math pre-training datasets.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-03T17:52:07Z\"}"}

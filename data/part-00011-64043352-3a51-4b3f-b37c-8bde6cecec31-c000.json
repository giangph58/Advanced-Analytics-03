{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06562v1\", \"title\": \"FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion\", \"summary\": \"Heterogeneous model fusion enhances the performance of LLMs by integrating\\nthe knowledge and capabilities of multiple structurally diverse models.\\nHowever, existing approaches often rely solely on selecting the best output for\\neach prompt from source models, which underutilizes their full potential due to\\nlimited source knowledge and results in sparse optimization signals. To address\\nthis limitation, we propose FuseRL, a novel two-stage framework comprising\\nFuseSFT and FusePO to maximize the utilization of source LLMs. FuseSFT\\nestablishes a robust initialization by integrating the strengths of\\nheterogeneous source models through weighted supervised fine-tuning (SFT) on\\ndiverse outputs for each prompt. FusePO optimizes weighted preferences based on\\nthe outputs of multiple source models to enable superior alignment performance.\\nExtensive experiments demonstrate the effectiveness of our framework across\\nvarious preference alignment methods, including RLOO, DPO, and SimPO. Using\\nLlama-3.1-8B-Instruct as the target model, our approach achieves\\nstate-of-the-art performance among 8B LLMs on the AlpacaEval-2 and Arena-Hard\\nbenchmarks. Further analysis suggests that FuseSFT regularizes the training\\nprocess to reduce overfitting, while FusePO introduces dense and diverse\\nsignals for preference optimization.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-09T03:51:53Z\"}"}

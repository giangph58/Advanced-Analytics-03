{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07960v1\", \"title\": \"VisualCloze: A Universal Image Generation Framework via Visual\\n  In-Context Learning\", \"summary\": \"Recent progress in diffusion models significantly advances various image\\ngeneration tasks. However, the current mainstream approach remains focused on\\nbuilding task-specific models, which have limited efficiency when supporting a\\nwide range of different needs. While universal models attempt to address this\\nlimitation, they face critical challenges, including generalizable task\\ninstruction, appropriate task distributions, and unified architectural design.\\nTo tackle these challenges, we propose VisualCloze, a universal image\\ngeneration framework, which supports a wide range of in-domain tasks,\\ngeneralization to unseen ones, unseen unification of multiple tasks, and\\nreverse generation. Unlike existing methods that rely on language-based task\\ninstruction, leading to task ambiguity and weak generalization, we integrate\\nvisual in-context learning, allowing models to identify tasks from visual\\ndemonstrations. Meanwhile, the inherent sparsity of visual task distributions\\nhampers the learning of transferable knowledge across tasks. To this end, we\\nintroduce Graph200K, a graph-structured dataset that establishes various\\ninterrelated tasks, enhancing task density and transferable knowledge.\\nFurthermore, we uncover that our unified image generation formulation shared a\\nconsistent objective with image infilling, enabling us to leverage the strong\\ngenerative priors of pre-trained infilling models without modifying the\\narchitectures.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-10T17:59:42Z\"}"}

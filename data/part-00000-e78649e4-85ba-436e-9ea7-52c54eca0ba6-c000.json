{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23726v1\", \"title\": \"PDSL: Privacy-Preserved Decentralized Stochastic Learning with\\n  Heterogeneous Data Distribution\", \"summary\": \"In the paradigm of decentralized learning, a group of agents collaborates to\\nlearn a global model using distributed datasets without a central server.\\nHowever, due to the heterogeneity of the local data across the different\\nagents, learning a robust global model is rather challenging. Moreover, the\\ncollaboration of the agents relies on their gradient information exchange,\\nwhich poses a risk of privacy leakage. In this paper, to address these issues,\\nwe propose PDSL, a novel privacy-preserved decentralized stochastic learning\\nalgorithm with heterogeneous data distribution. On one hand, we innovate in\\nutilizing the notion of Shapley values such that each agent can precisely\\nmeasure the contributions of its heterogeneous neighbors to the global learning\\ngoal; on the other hand, we leverage the notion of differential privacy to\\nprevent each agent from suffering privacy leakage when it contributes gradient\\ninformation to its neighbors. We conduct both solid theoretical analysis and\\nextensive experiments to demonstrate the efficacy of our PDSL algorithm in\\nterms of privacy preservation and convergence.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-03-31T04:58:05Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04837v1\", \"title\": \"Uni4D: A Unified Self-Supervised Learning Framework for Point Cloud\\n  Videos\", \"summary\": \"Point cloud video representation learning is primarily built upon the masking\\nstrategy in a self-supervised manner. However, the progress is slow due to\\nseveral significant challenges: (1) existing methods learn the motion\\nparticularly with hand-crafted designs, leading to unsatisfactory motion\\npatterns during pre-training which are non-transferable on fine-tuning\\nscenarios. (2) previous Masked AutoEncoder (MAE) frameworks are limited in\\nresolving the huge representation gap inherent in 4D data. In this study, we\\nintroduce the first self-disentangled MAE for learning discriminative 4D\\nrepresentations in the pre-training stage. To address the first challenge, we\\npropose to model the motion representation in a latent space. The second issue\\nis resolved by introducing the latent tokens along with the typical geometry\\ntokens to disentangle high-level and low-level features during decoding.\\nExtensive experiments on MSR-Action3D, NTU-RGBD, HOI4D, NvGesture, and SHREC'17\\nverify this self-disentangled learning framework. We demonstrate that it can\\nboost the fine-tuning performance on all 4D tasks, which we term Uni4D. Our\\npre-trained model presents discriminative and meaningful 4D representations,\\nparticularly benefits processing long videos, as Uni4D gets $+3.8\\\\%$\\nsegmentation accuracy on HOI4D, significantly outperforming either\\nself-supervised or fully-supervised methods after end-to-end fine-tuning.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-07T08:47:36Z\"}"}

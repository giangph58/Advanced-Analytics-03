{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06160v1\", \"title\": \"Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack\\n  Narratives Targeting Mental Health Groups\", \"summary\": \"Large Language Models (LLMs) have been shown to demonstrate imbalanced biases\\nagainst certain groups. However, the study of unprovoked targeted attacks by\\nLLMs towards at-risk populations remains underexplored. Our paper presents\\nthree novel contributions: (1) the explicit evaluation of LLM-generated attacks\\non highly vulnerable mental health groups; (2) a network-based framework to\\nstudy the propagation of relative biases; and (3) an assessment of the relative\\ndegree of stigmatization that emerges from these attacks. Our analysis of a\\nrecently released large-scale bias audit dataset reveals that mental health\\nentities occupy central positions within attack narrative networks, as revealed\\nby a significantly higher mean centrality of closeness (p-value = 4.06e-10) and\\ndense clustering (Gini coefficient = 0.7). Drawing from sociological\\nfoundations of stigmatization theory, our stigmatization analysis indicates\\nincreased labeling components for mental health disorder-related targets\\nrelative to initial targets in generation chains. Taken together, these\\ninsights shed light on the structural predilections of large language models to\\nheighten harmful discourse and highlight the need for suitable approaches for\\nmitigation.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.CY,cs.LG,cs.SI\", \"published\": \"2025-04-08T15:56:57Z\"}"}

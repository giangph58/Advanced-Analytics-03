{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02605v1\", \"title\": \"Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving\", \"summary\": \"The task of issue resolving is to modify a codebase to generate a patch that\\naddresses a given issue. However, existing benchmarks, such as SWE-bench, focus\\nalmost exclusively on Python, making them insufficient for evaluating Large\\nLanguage Models (LLMs) across diverse software ecosystems. To address this, we\\nintroduce a multilingual issue-resolving benchmark, called Multi-SWE-bench,\\ncovering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a\\ntotal of 1,632 high-quality instances, which were carefully annotated from\\n2,456 candidates by 68 expert annotators, ensuring that the benchmark can\\nprovide an accurate and reliable evaluation. Based on Multi-SWE-bench, we\\nevaluate a series of state-of-the-art models using three representative methods\\n(Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with\\nkey empirical insights. In addition, we launch a Multi-SWE-RL open-source\\ncommunity, aimed at building large-scale reinforcement learning (RL) training\\ndatasets for issue-resolving tasks. As an initial contribution, we release a\\nset of 4,723 well-structured instances spanning seven programming languages,\\nlaying a solid foundation for RL research in this domain. More importantly, we\\nopen-source our entire data production pipeline, along with detailed tutorials,\\nencouraging the open-source community to continuously contribute and expand the\\ndataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL\\ncommunity as catalysts for advancing RL toward its full potential, bringing us\\none step closer to the dawn of AGI.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE,cs.AI,cs.CL\", \"published\": \"2025-04-03T14:06:17Z\"}"}

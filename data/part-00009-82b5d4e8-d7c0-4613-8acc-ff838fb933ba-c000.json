{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23934v1\", \"title\": \"Green MLOps to Green GenOps: An Empirical Study of Energy Consumption in\\n  Discriminative and Generative AI Operations\", \"summary\": \"This study presents an empirical investigation into the energy consumption of\\nDiscriminative and Generative AI models within real-world MLOps pipelines. For\\nDiscriminative models, we examine various architectures and hyperparameters\\nduring training and inference and identify energy-efficient practices. For\\nGenerative AI, Large Language Models (LLMs) are assessed, focusing primarily on\\nenergy consumption across different model sizes and varying service requests.\\nOur study employs software-based power measurements, ensuring ease of\\nreplication across diverse configurations, models, and datasets. We analyse\\nmultiple models and hardware setups to uncover correlations among various\\nmetrics, identifying key contributors to energy consumption. The results\\nindicate that for Discriminative models, optimising architectures,\\nhyperparameters, and hardware can significantly reduce energy consumption\\nwithout sacrificing performance. For LLMs, energy efficiency depends on\\nbalancing model size, reasoning complexity, and request-handling capacity, as\\nlarger models do not necessarily consume more energy when utilisation remains\\nlow. This analysis provides practical guidelines for designing green and\\nsustainable ML operations, emphasising energy consumption and carbon footprint\\nreductions while maintaining performance. This paper can serve as a benchmark\\nfor accurately estimating total energy use across different types of AI models.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-03-31T10:28:04Z\"}"}

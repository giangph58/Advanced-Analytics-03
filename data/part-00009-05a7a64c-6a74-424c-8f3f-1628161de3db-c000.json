{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01666v1\", \"title\": \"CLIP-SLA: Parameter-Efficient CLIP Adaptation for Continuous Sign\\n  Language Recognition\", \"summary\": \"Continuous sign language recognition (CSLR) focuses on interpreting and\\ntranscribing sequences of sign language gestures in videos. In this work, we\\npropose CLIP sign language adaptation (CLIP-SLA), a novel CSLR framework that\\nleverages the powerful pre-trained visual encoder from the CLIP model to sign\\nlanguage tasks through parameter-efficient fine-tuning (PEFT). We introduce two\\nvariants, SLA-Adapter and SLA-LoRA, which integrate PEFT modules into the CLIP\\nvisual encoder, enabling fine-tuning with minimal trainable parameters. The\\neffectiveness of the proposed frameworks is validated on four datasets:\\nPhoenix2014, Phoenix2014-T, CSL-Daily, and Isharah-500, where both CLIP-SLA\\nvariants outperformed several SOTA models with fewer trainable parameters.\\nExtensive ablation studies emphasize the effectiveness and flexibility of the\\nproposed methods with different vision-language models for CSLR. These findings\\nshowcase the potential of adapting large-scale pre-trained models for scalable\\nand efficient CSLR, which pave the way for future advancements in sign language\\nunderstanding.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-02T12:15:33Z\"}"}

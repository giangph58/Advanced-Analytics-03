{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02349v1\", \"title\": \"Large (Vision) Language Models are Unsupervised In-Context Learners\", \"summary\": \"Recent advances in large language and vision-language models have enabled\\nzero-shot inference, allowing models to solve new tasks without task-specific\\ntraining. Various adaptation techniques such as prompt engineering, In-Context\\nLearning (ICL), and supervised fine-tuning can further enhance the model's\\nperformance on a downstream task, but they require substantial manual effort to\\nconstruct effective prompts or labeled examples. In this work, we introduce a\\njoint inference framework for fully unsupervised adaptation, eliminating the\\nneed for manual prompt engineering and labeled examples. Unlike zero-shot\\ninference, which makes independent predictions, the joint inference makes\\npredictions simultaneously for all inputs in a given task. Since direct joint\\ninference involves computationally expensive optimization, we develop efficient\\napproximation techniques, leading to two unsupervised adaptation methods:\\nunsupervised fine-tuning and unsupervised ICL. We demonstrate the effectiveness\\nof our methods across diverse tasks and models, including language-only\\nLlama-3.1 on natural language processing tasks, reasoning-oriented Qwen2.5-Math\\non grade school math problems, vision-language OpenFlamingo on vision tasks,\\nand the API-only access GPT-4o model on massive multi-discipline tasks. Our\\nexperiments demonstrate substantial improvements over the standard zero-shot\\napproach, including 39% absolute improvement on the challenging GSM8K math\\nreasoning dataset. Remarkably, despite being fully unsupervised, our framework\\noften performs on par with supervised approaches that rely on ground truth\\nlabels.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-03T07:33:02Z\"}"}

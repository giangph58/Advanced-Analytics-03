{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05004v1\", \"title\": \"Stacking Variational Bayesian Monte Carlo\", \"summary\": \"Variational Bayesian Monte Carlo (VBMC) is a sample-efficient method for\\napproximate Bayesian inference with computationally expensive likelihoods.\\nWhile VBMC's local surrogate approach provides stable approximations, its\\nconservative exploration strategy and limited evaluation budget can cause it to\\nmiss regions of complex posteriors. In this work, we introduce Stacking\\nVariational Bayesian Monte Carlo (S-VBMC), a method that constructs global\\nposterior approximations by merging independent VBMC runs through a principled\\nand inexpensive post-processing step. Our approach leverages VBMC's mixture\\nposterior representation and per-component evidence estimates, requiring no\\nadditional likelihood evaluations while being naturally parallelizable. We\\ndemonstrate S-VBMC's effectiveness on two synthetic problems designed to\\nchallenge VBMC's exploration capabilities and two real-world applications from\\ncomputational neuroscience, showing substantial improvements in posterior\\napproximation quality across all cases.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.LG\", \"published\": \"2025-04-07T12:30:59Z\"}"}

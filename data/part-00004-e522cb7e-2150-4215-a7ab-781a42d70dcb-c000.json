{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01961v1\", \"title\": \"Learning from Streaming Video with Orthogonal Gradients\", \"summary\": \"We address the challenge of representation learning from a continuous stream\\nof video as input, in a self-supervised manner. This differs from the standard\\napproaches to video learning where videos are chopped and shuffled during\\ntraining in order to create a non-redundant batch that satisfies the\\nindependently and identically distributed (IID) sample assumption expected by\\nconventional training paradigms. When videos are only available as a continuous\\nstream of input, the IID assumption is evidently broken, leading to poor\\nperformance. We demonstrate the drop in performance when moving from shuffled\\nto sequential learning on three tasks: the one-video representation learning\\nmethod DoRA, standard VideoMAE on multi-video datasets, and the task of future\\nvideo prediction. To address this drop, we propose a geometric modification to\\nstandard optimizers, to decorrelate batches by utilising orthogonal gradients\\nduring training. The proposed modification can be applied to any optimizer --\\nwe demonstrate it with Stochastic Gradient Descent (SGD) and AdamW. Our\\nproposed orthogonal optimizer allows models trained from streaming videos to\\nalleviate the drop in representation learning performance, as evaluated on\\ndownstream tasks. On three scenarios (DoRA, VideoMAE, future prediction), we\\nshow our orthogonal optimizer outperforms the strong AdamW in all three\\nscenarios.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-02T17:59:57Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06022v1\", \"title\": \"CamContextI2V: Context-aware Controllable Video Generation\", \"summary\": \"Recently, image-to-video (I2V) diffusion models have demonstrated impressive\\nscene understanding and generative quality, incorporating image conditions to\\nguide generation. However, these models primarily animate static images without\\nextending beyond their provided context. Introducing additional constraints,\\nsuch as camera trajectories, can enhance diversity but often degrades visual\\nquality, limiting their applicability for tasks requiring faithful scene\\nrepresentation. We propose CamContextI2V, an I2V model that integrates multiple\\nimage conditions with 3D constraints alongside camera control to enrich both\\nglobal semantics and fine-grained visual details. This enables more coherent\\nand context-aware video generation. Moreover, we motivate the necessity of\\ntemporal awareness for an effective context representation. Our comprehensive\\nstudy on the RealEstate10K dataset demonstrates improvements in visual quality\\nand camera controllability. We make our code and models publicly available at:\\nhttps://github.com/LDenninger/CamContextI2V.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-08T13:26:59Z\"}"}

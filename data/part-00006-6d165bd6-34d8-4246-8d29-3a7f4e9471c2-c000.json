{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07745v1\", \"title\": \"SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained\\n  Understanding\", \"summary\": \"Video-based Large Language Models (Video-LLMs) have witnessed substantial\\nadvancements in recent years, propelled by the advancement in multi-modal LLMs.\\nAlthough these models have demonstrated proficiency in providing the overall\\ndescription of videos, they struggle with fine-grained understanding,\\nparticularly in aspects such as visual dynamics and video details inquiries. To\\ntackle these shortcomings, we find that fine-tuning Video-LLMs on\\nself-supervised fragment tasks, greatly improve their fine-grained video\\nunderstanding abilities. Hence we propose two key contributions:(1)\\nSelf-Supervised Fragment Fine-Tuning (SF$^2$T), a novel effortless fine-tuning\\nmethod, employs the rich inherent characteristics of videos for training, while\\nunlocking more fine-grained understanding ability of Video-LLMs. Moreover, it\\nrelieves researchers from labor-intensive annotations and smartly circumvents\\nthe limitations of natural language, which often fails to capture the complex\\nspatiotemporal variations in videos; (2) A novel benchmark dataset, namely\\nFineVidBench, for rigorously assessing Video-LLMs' performance at both the\\nscene and fragment levels, offering a comprehensive evaluation of their\\ncapabilities. We assessed multiple models and validated the effectiveness of\\nSF$^2$T on them. Experimental results reveal that our approach improves their\\nability to capture and interpret spatiotemporal details.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-10T13:40:34Z\"}"}

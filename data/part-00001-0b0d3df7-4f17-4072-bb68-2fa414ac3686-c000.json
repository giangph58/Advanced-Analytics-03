{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02451v1\", \"title\": \"ConMo: Controllable Motion Disentanglement and Recomposition for\\n  Zero-Shot Motion Transfer\", \"summary\": \"The development of Text-to-Video (T2V) generation has made motion transfer\\npossible, enabling the control of video motion based on existing footage.\\nHowever, current methods have two limitations: 1) struggle to handle\\nmulti-subjects videos, failing to transfer specific subject motion; 2) struggle\\nto preserve the diversity and accuracy of motion as transferring to subjects\\nwith varying shapes. To overcome these, we introduce \\\\textbf{ConMo}, a\\nzero-shot framework that disentangle and recompose the motions of subjects and\\ncamera movements. ConMo isolates individual subject and background motion cues\\nfrom complex trajectories in source videos using only subject masks, and\\nreassembles them for target video generation. This approach enables more\\naccurate motion control across diverse subjects and improves performance in\\nmulti-subject scenarios. Additionally, we propose soft guidance in the\\nrecomposition stage which controls the retention of original motion to adjust\\nshape constraints, aiding subject shape adaptation and semantic transformation.\\nUnlike previous methods, ConMo unlocks a wide range of applications, including\\nsubject size and position editing, subject removal, semantic modifications, and\\ncamera motion simulation. Extensive experiments demonstrate that ConMo\\nsignificantly outperforms state-of-the-art methods in motion fidelity and\\nsemantic consistency. The code is available at\\nhttps://github.com/Andyplus1/ConMo.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-03T10:15:52Z\"}"}

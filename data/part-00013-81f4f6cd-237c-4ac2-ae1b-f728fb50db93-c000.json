{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06235v1\", \"title\": \"Decentralized Federated Domain Generalization with Style Sharing: A\\n  Formal Modeling and Convergence Analysis\", \"summary\": \"Much of the federated learning (FL) literature focuses on settings where\\nlocal dataset statistics remain the same between training and testing time.\\nRecent advances in domain generalization (DG) aim to use data from source\\n(training) domains to train a model that generalizes well to data from unseen\\ntarget (testing) domains. In this paper, we are motivated by two major gaps in\\nexisting work on FL and DG: (1) the lack of formal mathematical analysis of DG\\nobjectives and training processes; and (2) DG research in FL being limited to\\nthe conventional star-topology architecture. Addressing the second gap, we\\ndevelop $\\\\textit{Decentralized Federated Domain Generalization with Style\\nSharing}$ ($\\\\texttt{StyleDDG}$), a fully decentralized DG algorithm designed to\\nallow devices in a peer-to-peer network to achieve DG based on sharing style\\ninformation inferred from their datasets. Additionally, we fill the first gap\\nby providing the first systematic approach to mathematically analyzing\\nstyle-based DG training optimization. We cast existing centralized DG\\nalgorithms within our framework, and employ their formalisms to model\\n$\\\\texttt{StyleDDG}$. Based on this, we obtain analytical conditions under which\\na sub-linear convergence rate of $\\\\texttt{StyleDDG}$ can be obtained. Through\\nexperiments on two popular DG datasets, we demonstrate that $\\\\texttt{StyleDDG}$\\ncan obtain significant improvements in accuracy across target domains with\\nminimal added communication overhead compared to decentralized gradient methods\\nthat do not employ style sharing.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-08T17:32:56Z\"}"}

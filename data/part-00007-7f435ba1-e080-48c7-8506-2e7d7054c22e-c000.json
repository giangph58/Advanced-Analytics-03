{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01873v1\", \"title\": \"A Diffusion-Based Framework for Occluded Object Movement\", \"summary\": \"Seamlessly moving objects within a scene is a common requirement for image\\nediting, but it is still a challenge for existing editing methods. Especially\\nfor real-world images, the occlusion situation further increases the\\ndifficulty. The main difficulty is that the occluded portion needs to be\\ncompleted before movement can proceed. To leverage the real-world knowledge\\nembedded in the pre-trained diffusion models, we propose a Diffusion-based\\nframework specifically designed for Occluded Object Movement, named DiffOOM.\\nThe proposed DiffOOM consists of two parallel branches that perform object\\nde-occlusion and movement simultaneously. The de-occlusion branch utilizes a\\nbackground color-fill strategy and a continuously updated object mask to focus\\nthe diffusion process on completing the obscured portion of the target object.\\nConcurrently, the movement branch employs latent optimization to place the\\ncompleted object in the target location and adopts local text-conditioned\\nguidance to integrate the object into new surroundings appropriately. Extensive\\nevaluations demonstrate the superior performance of our method, which is\\nfurther validated by a comprehensive user study.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-02T16:29:30Z\"}"}

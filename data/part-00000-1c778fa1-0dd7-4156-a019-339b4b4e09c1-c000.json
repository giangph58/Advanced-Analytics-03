{"value":"{\"aid\": \"http://arxiv.org/abs/2503.21721v1\", \"title\": \"Evaluating Text-to-Image Synthesis with a Conditional Fr\\u00e9chet\\n  Distance\", \"summary\": \"Evaluating text-to-image synthesis is challenging due to misalignment between\\nestablished metrics and human preferences. We propose cFreD, a metric based on\\nthe notion of Conditional Fr\\\\'echet Distance that explicitly accounts for both\\nvisual fidelity and text-prompt alignment. Existing metrics such as Inception\\nScore (IS), Fr\\\\'echet Inception Distance (FID) and CLIPScore assess either\\nimage quality or image-text alignment but not both which limits their\\ncorrelation with human preferences. Scoring models explicitly trained to\\nreplicate human preferences require constant updates and may not generalize to\\nnovel generation techniques or out-of-domain inputs. Through extensive\\nexperiments across multiple recently proposed text-to-image models and diverse\\nprompt datasets, we demonstrate that cFreD exhibits a higher correlation with\\nhuman judgments compared to statistical metrics, including metrics trained with\\nhuman preferences. Our findings validate cFreD as a robust, future-proof metric\\nfor the systematic evaluation of text-to-image models, standardizing\\nbenchmarking in this rapidly evolving field. We release our evaluation toolkit\\nand benchmark in the appendix.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-27T17:35:14Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.21729v1\", \"title\": \"ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large\\n  Reasoning Models with Iterative Retrieval Augmented Generation\", \"summary\": \"Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely\\nprimarily on parametric knowledge, limiting factual accuracy. While recent\\nworks equip reinforcement learning (RL)-based LRMs with retrieval capabilities,\\nthey suffer from overthinking and lack robustness in reasoning, reducing their\\neffectiveness in question answering (QA) tasks. To address this, we propose\\nReaRAG, a factuality-enhanced reasoning model that explores diverse queries\\nwithout excessive iterations. Our solution includes a novel data construction\\nframework with an upper bound on the reasoning chain length. Specifically, we\\nfirst leverage an LRM to generate deliberate thinking, then select an action\\nfrom a predefined action space (Search and Finish). For Search action, a query\\nis executed against the RAG engine, where the result is returned as observation\\nto guide reasoning steps later. This process iterates until a Finish action is\\nchosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach\\noutperforms existing baselines on multi-hop QA. Further analysis highlights its\\nstrong reflective ability to recognize errors and refine its reasoning\\ntrajectory. Our study enhances LRMs' factuality while effectively integrating\\nrobust reasoning for Retrieval-Augmented Generation (RAG).\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-03-27T17:44:18Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07722v1\", \"title\": \"Relaxing the Markov Requirements on Reinforcement Learning Under Weak\\n  Partial Ignorability\", \"summary\": \"Incomplete data, confounding effects, and violations of the Markov property\\nare interrelated problems which are ubiquitous in Reinforcement Learning\\napplications. We introduce the concept of ``partial ignorabilty\\\" and leverage\\nit to establish a novel convergence theorem for adaptive Reinforcement\\nLearning. This theoretical result relaxes the Markov assumption on the\\nstochastic process underlying conventional $Q$-learning, deploying a\\ngeneralized form of the Robbins-Monro stochastic approximation theorem to\\nestablish optimality. This result has clear downstream implications for most\\nactive subfields of Reinforcement Learning, with clear paths for extension to\\nthe field of Causal Inference.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,stat.ME\", \"published\": \"2025-04-10T13:15:52Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04772v1\", \"title\": \"Feedback-Enhanced Hallucination-Resistant Vision-Language Model for\\n  Real-Time Scene Understanding\", \"summary\": \"Real-time scene comprehension is a key advance in artificial intelligence,\\nenhancing robotics, surveillance, and assistive tools. However, hallucination\\nremains a challenge. AI systems often misinterpret visual inputs, detecting\\nnonexistent objects or describing events that never happened. These errors, far\\nfrom minor, threaten reliability in critical areas like security and autonomous\\nnavigation where accuracy is essential.\\n  Our approach tackles this by embedding self-awareness into the AI. Instead of\\ntrusting initial outputs, our framework continuously assesses them in real\\ntime, adjusting confidence thresholds dynamically. When certainty falls below a\\nsolid benchmark, it suppresses unreliable claims. Combining YOLOv5's object\\ndetection strength with VILA1.5-3B's controlled language generation, we tie\\ndescriptions to confirmed visual data. Strengths include dynamic threshold\\ntuning for better accuracy, evidence-based text to reduce hallucination, and\\nreal-time performance at 18 frames per second.\\n  This feedback-driven design cuts hallucination by 37 percent over traditional\\nmethods. Fast, flexible, and reliable, it excels in applications from robotic\\nnavigation to security monitoring, aligning AI perception with reality.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-07T06:59:30Z\"}"}

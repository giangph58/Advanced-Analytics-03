{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24361v1\", \"title\": \"Sim-and-Real Co-Training: A Simple Recipe for Vision-Based Robotic\\n  Manipulation\", \"summary\": \"Large real-world robot datasets hold great potential to train generalist\\nrobot models, but scaling real-world human data collection is time-consuming\\nand resource-intensive. Simulation has great potential in supplementing\\nlarge-scale data, especially with recent advances in generative AI and\\nautomated data generation tools that enable scalable creation of robot behavior\\ndatasets. However, training a policy solely in simulation and transferring it\\nto the real world often demands substantial human effort to bridge the reality\\ngap. A compelling alternative is to co-train the policy on a mixture of\\nsimulation and real-world datasets. Preliminary studies have recently shown\\nthis strategy to substantially improve the performance of a policy over one\\ntrained on a limited amount of real-world data. Nonetheless, the community\\nlacks a systematic understanding of sim-and-real co-training and what it takes\\nto reap the benefits of simulation data for real-robot learning. This work\\npresents a simple yet effective recipe for utilizing simulation data to solve\\nvision-based robotic manipulation tasks. We derive this recipe from\\ncomprehensive experiments that validate the co-training strategy on various\\nsimulation and real-world datasets. Using two domains--a robot arm and a\\nhumanoid--across diverse tasks, we demonstrate that simulation data can enhance\\nreal-world task performance by an average of 38%, even with notable differences\\nbetween the simulation and real-world data. Videos and additional results can\\nbe found at https://co-training.github.io/\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO,cs.AI,cs.LG\", \"published\": \"2025-03-31T17:39:38Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02822v1\", \"title\": \"Do Two AI Scientists Agree?\", \"summary\": \"When two AI models are trained on the same scientific task, do they learn the\\nsame theory or two different theories? Throughout history of science, we have\\nwitnessed the rise and fall of theories driven by experimental validation or\\nfalsification: many theories may co-exist when experimental data is lacking,\\nbut the space of survived theories become more constrained with more\\nexperimental data becoming available. We show the same story is true for AI\\nscientists. With increasingly more systems provided in training data, AI\\nscientists tend to converge in the theories they learned, although sometimes\\nthey form distinct groups corresponding to different theories. To\\nmechanistically interpret what theories AI scientists learn and quantify their\\nagreement, we propose MASS, Hamiltonian-Lagrangian neural networks as AI\\nScientists, trained on standard problems in physics, aggregating training\\nresults across many seeds simulating the different configurations of AI\\nscientists. Our findings suggests for AI scientists switch from learning a\\nHamiltonian theory in simple setups to a Lagrangian formulation when more\\ncomplex systems are introduced. We also observe strong seed dependence of the\\ntraining dynamics and final learned weights, controlling the rise and fall of\\nrelevant theories. We finally demonstrate that not only can our neural networks\\naid interpretability, it can also be applied to higher dimensional problems.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.LG\", \"published\": \"2025-04-03T17:58:44Z\"}"}

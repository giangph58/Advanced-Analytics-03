{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07898v1\", \"title\": \"How do Large Language Models Understand Relevance? A Mechanistic\\n  Interpretability Perspective\", \"summary\": \"Recent studies have shown that large language models (LLMs) can assess\\nrelevance and support information retrieval (IR) tasks such as document ranking\\nand relevance judgment generation. However, the internal mechanisms by which\\noff-the-shelf LLMs understand and operationalize relevance remain largely\\nunexplored. In this paper, we systematically investigate how different LLM\\nmodules contribute to relevance judgment through the lens of mechanistic\\ninterpretability. Using activation patching techniques, we analyze the roles of\\nvarious model components and identify a multi-stage, progressive process in\\ngenerating either pointwise or pairwise relevance judgment. Specifically, LLMs\\nfirst extract query and document information in the early layers, then process\\nrelevance information according to instructions in the middle layers, and\\nfinally utilize specific attention heads in the later layers to generate\\nrelevance judgments in the required format. Our findings provide insights into\\nthe mechanisms underlying relevance assessment in LLMs, offering valuable\\nimplications for future research on leveraging LLMs for IR tasks.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR,cs.CL,cs.LG\", \"published\": \"2025-04-10T16:14:55Z\"}"}

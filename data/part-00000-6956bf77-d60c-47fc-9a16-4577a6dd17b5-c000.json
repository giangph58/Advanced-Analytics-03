{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23989v1\", \"title\": \"Rubric Is All You Need: Enhancing LLM-based Code Evaluation With\\n  Question-Specific Rubrics\", \"summary\": \"Since the disruption in LLM technology brought about by the release of GPT-3\\nand ChatGPT, LLMs have shown remarkable promise in programming-related tasks.\\nWhile code generation remains a popular field of research, code evaluation\\nusing LLMs remains a problem with no conclusive solution. In this paper, we\\nfocus on LLM-based code evaluation and attempt to fill in the existing gaps. We\\npropose multi-agentic novel approaches using question-specific rubrics tailored\\nto the problem statement, arguing that these perform better for logical\\nassessment than the existing approaches that use question-agnostic rubrics. To\\naddress the lack of suitable evaluation datasets, we introduce two datasets: a\\nData Structures and Algorithms dataset containing 150 student submissions from\\na popular Data Structures and Algorithms practice website, and an Object\\nOriented Programming dataset comprising 80 student submissions from\\nundergraduate computer science courses. In addition to using standard metrics\\n(Spearman Correlation, Cohen's Kappa), we additionally propose a new metric\\ncalled as Leniency, which quantifies evaluation strictness relative to expert\\nassessment. Our comprehensive analysis demonstrates that question-specific\\nrubrics significantly enhance logical assessment of code in educational\\nsettings, providing better feedback aligned with instructional goals beyond\\nmere syntactic correctness.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE,cs.AI\", \"published\": \"2025-03-31T11:59:43Z\"}"}

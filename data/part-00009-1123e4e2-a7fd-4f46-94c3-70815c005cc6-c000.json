{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07080v1\", \"title\": \"DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning\", \"summary\": \"Despite great performance on Olympiad-level reasoning problems, frontier\\nlarge language models can still struggle on high school math when presented\\nwith novel problems outside standard benchmarks. Going beyond final accuracy,\\nwe propose a deductive consistency metric to analyze chain-of-thought output\\nfrom language models (LMs).Formally, deductive reasoning involves two subtasks:\\nunderstanding a set of input premises and inferring the conclusions that follow\\nfrom them. The proposed metric studies LMs' performance on these subtasks, with\\nthe goal of explaining LMs' reasoning errors on novel problems: how well do LMs\\nunderstand input premises with increasing context lengths, and how well can\\nthey infer conclusions over multiple reasoning hops? Since existing benchmarks\\nmay be memorized, we develop a pipeline to evaluate LMs' deductive consistency\\non novel, perturbed versions of benchmark problems. On novel grade school math\\nproblems (GSM-8k), we find that LMs are fairly robust to increasing number of\\ninput premises, but suffer significant accuracy decay as the number of\\nreasoning hops is increased. Interestingly, these errors are masked in the\\noriginal benchmark as all models achieve near 100% accuracy. As we increase the\\nnumber of solution steps using a synthetic dataset, prediction over multiple\\nhops still remains the major source of error compared to understanding input\\npremises. Other factors, such as shifts in language style or natural\\npropagation of early errors do not explain the trends. Our analysis provides a\\nnew view to characterize LM reasoning -- as computations over a window of input\\npremises and reasoning hops -- that can provide unified evaluation across\\nproblem domains.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-09T17:53:55Z\"}"}

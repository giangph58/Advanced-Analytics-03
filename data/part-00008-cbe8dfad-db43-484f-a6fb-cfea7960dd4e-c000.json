{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01840v1\", \"title\": \"LARGE: Legal Retrieval Augmented Generation Evaluation Tool\", \"summary\": \"Recently, building retrieval-augmented generation (RAG) systems to enhance\\nthe capability of large language models (LLMs) has become a common practice.\\nEspecially in the legal domain, previous judicial decisions play a significant\\nrole under the doctrine of stare decisis which emphasizes the importance of\\nmaking decisions based on (retrieved) prior documents. However, the overall\\nperformance of RAG system depends on many components: (1) retrieval corpora,\\n(2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation\\nmetrics. Here we propose LRAGE, an open-source tool for holistic evaluation of\\nRAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces\\nto facilitate seamless experiments and investigate how changes in the\\naforementioned five components affect the overall accuracy. We validated LRAGE\\nusing multilingual legal benches including Korean (KBL), English (LegalBench),\\nand Chinese (LawBench) by demonstrating how the overall accuracy changes when\\nvarying the five components mentioned above. The source code is available at\\nhttps://github.com/hoorangyee/LRAGE.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-02T15:45:03Z\"}"}

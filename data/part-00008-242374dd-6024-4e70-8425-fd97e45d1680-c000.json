{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06225v1\", \"title\": \"Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via\\n  Adaptation\", \"summary\": \"While decoder-only large language models (LLMs) have shown impressive\\nresults, encoder-decoder models are still widely adopted in real-world\\napplications for their inference efficiency and richer encoder representation.\\nIn this paper, we study a novel problem: adapting pretrained decoder-only LLMs\\nto encoder-decoder, with the goal of leveraging the strengths of both\\napproaches to achieve a more favorable quality-efficiency trade-off. We argue\\nthat adaptation not only enables inheriting the capability of decoder-only LLMs\\nbut also reduces the demand for computation compared to pretraining from\\nscratch. We rigorously explore different pretraining objectives and parameter\\ninitialization/optimization techniques. Through extensive experiments based on\\nGemma 2 (2B and 9B) and a suite of newly pretrained mT5-sized models (up to\\n1.6B), we demonstrate the effectiveness of adaptation and the advantage of\\nencoder-decoder LLMs. Under similar inference budget, encoder-decoder LLMs\\nachieve comparable (often better) pretraining performance but substantially\\nbetter finetuning performance than their decoder-only counterpart. For example,\\nGemma 2B-2B outperforms Gemma 2B by $\\\\sim$7\\\\% after instruction tuning.\\nEncoder-decoder adaptation also allows for flexible combination of\\ndifferent-sized models, where Gemma 9B-2B significantly surpasses Gemma 2B-2B\\nby $>$3\\\\%. The adapted encoder representation also yields better results on\\nSuperGLUE. We will release our checkpoints to facilitate future research.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.LG\", \"published\": \"2025-04-08T17:13:41Z\"}"}

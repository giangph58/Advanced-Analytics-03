{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06256v1\", \"title\": \"Transfer between Modalities with MetaQueries\", \"summary\": \"Unified multimodal models aim to integrate understanding (text output) and\\ngeneration (pixel output), but aligning these different modalities within a\\nsingle architecture often demands complex training recipes and careful data\\nbalancing. We introduce MetaQueries, a set of learnable queries that act as an\\nefficient interface between autoregressive multimodal LLMs (MLLMs) and\\ndiffusion models. MetaQueries connects the MLLM's latents to the diffusion\\ndecoder, enabling knowledge-augmented image generation by leveraging the MLLM's\\ndeep understanding and reasoning capabilities. Our method simplifies training,\\nrequiring only paired image-caption data and standard diffusion objectives.\\nNotably, this transfer is effective even when the MLLM backbone remains frozen,\\nthereby preserving its state-of-the-art multimodal understanding capabilities\\nwhile achieving strong generative performance. Additionally, our method is\\nflexible and can be easily instruction-tuned for advanced applications such as\\nimage editing and subject-driven generation.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-08T17:58:47Z\"}"}

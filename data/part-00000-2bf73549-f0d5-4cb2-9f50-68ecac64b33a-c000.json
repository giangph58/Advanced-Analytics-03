{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23895v1\", \"title\": \"Better wit than wealth: Dynamic Parametric Retrieval Augmented\\n  Generation for Test-time Knowledge Enhancement\", \"summary\": \"Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\\nretrieving relevant documents from external sources and incorporating them into\\nthe context. While it improves reliability by providing factual texts, it\\nsignificantly increases inference costs as context length grows and introduces\\nchallenging issue of RAG hallucination, primarily caused by the lack of\\ncorresponding parametric knowledge in LLMs. An efficient solution is to enhance\\nthe knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by\\nembedding document into LLMs parameters to perform test-time knowledge\\nenhancement, effectively reducing inference costs through offline training.\\nHowever, its high training and storage costs, along with limited generalization\\nability, significantly restrict its practical adoption. To address these\\nchallenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that\\nleverages a lightweight parameter translator model to efficiently convert\\ndocuments into parametric knowledge. DyPRAG not only reduces inference,\\ntraining, and storage costs but also dynamically generates parametric\\nknowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge\\nconflicts in a plug-and-play manner at test-time. Extensive experiments on\\nmultiple datasets demonstrate the effectiveness and generalization capabilities\\nof DyPRAG, offering a powerful and practical RAG paradigm which enables\\nsuperior knowledge fusion and mitigates RAG hallucination in real-world\\napplications. Our code is available at https://github.com/Trae1ounG/DyPRAG.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-03-31T09:46:35Z\"}"}

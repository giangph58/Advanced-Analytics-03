{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02440v1\", \"title\": \"HGFormer: Topology-Aware Vision Transformer with HyperGraph Learning\", \"summary\": \"The computer vision community has witnessed an extensive exploration of\\nvision transformers in the past two years. Drawing inspiration from traditional\\nschemes, numerous works focus on introducing vision-specific inductive biases.\\nHowever, the implicit modeling of permutation invariance and fully-connected\\ninteraction with individual tokens disrupts the regional context and spatial\\ntopology, further hindering higher-order modeling. This deviates from the\\nprinciple of perceptual organization that emphasizes the local groups and\\noverall topology of visual elements. Thus, we introduce the concept of\\nhypergraph for perceptual exploration. Specifically, we propose a\\ntopology-aware vision transformer called HyperGraph Transformer (HGFormer).\\nFirstly, we present a Center Sampling K-Nearest Neighbors (CS-KNN) algorithm\\nfor semantic guidance during hypergraph construction. Secondly, we present a\\ntopology-aware HyperGraph Attention (HGA) mechanism that integrates hypergraph\\ntopology as perceptual indications to guide the aggregation of global and\\nunbiased information during hypergraph messaging. Using HGFormer as visual\\nbackbone, we develop an effective and unitive representation, achieving\\ndistinct and detailed scene depictions. Empirical experiments show that the\\nproposed HGFormer achieves competitive performance compared to the recent SoTA\\ncounterparts on various visual benchmarks. Extensive ablation and visualization\\nstudies provide comprehensive explanations of our ideas and contributions.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-03T09:58:01Z\"}"}

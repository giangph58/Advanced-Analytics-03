{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07416v1\", \"title\": \"RadZero: Similarity-Based Cross-Attention for Explainable\\n  Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability\", \"summary\": \"Recent advancements in multi-modal models have significantly improved\\nvision-language alignment in radiology. However, existing approaches struggle\\nto effectively utilize complex radiology reports for learning, rely on\\nlow-resolution images, and offer limited interpretability in attention\\nmechanisms. To address these challenges, we introduce RadZero, a novel\\nsimilarity-based cross-attention framework for vision-language alignment in\\nradiology with zero-shot multi-task capability. RadZero leverages large\\nlanguage models to extract minimal semantic sentences from radiology reports\\nand employs a multi-positive contrastive learning strategy to effectively\\ncapture relationships between images and multiple relevant textual\\ndescriptions. It also utilizes a pre-trained vision encoder with additional\\ntrainable Transformer layers, allowing efficient high-resolution image\\nprocessing. By computing similarity between text embeddings and local image\\npatch features, RadZero enables zero-shot inference with similarity probability\\nfor classification and pixel-level cross-modal similarity maps for grounding\\nand segmentation. Experimental results on public chest radiograph benchmarks\\nshow that RadZero outperforms state-of-the-art methods in zero-shot\\nclassification, grounding, and segmentation. Furthermore, cross-modal\\nsimilarity map analysis highlights its potential for improving explainability\\nin vision-language alignment. Additionally, qualitative evaluation demonstrates\\nRadZero's capability for open-vocabulary semantic segmentation, further\\nvalidating its effectiveness in medical imaging.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.CL,cs.LG\", \"published\": \"2025-04-10T03:14:17Z\"}"}

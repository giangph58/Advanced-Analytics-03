{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24290v1\", \"title\": \"Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement\\n  Learning on the Base Model\", \"summary\": \"We introduce Open-Reasoner-Zero, the first open source implementation of\\nlarge-scale reasoning-oriented RL training focusing on scalability, simplicity\\nand accessibility. Through extensive experiments, we demonstrate that a\\nminimalist approach, vanilla PPO with GAE ($\\\\lambda=1$, $\\\\gamma=1$) and\\nstraightforward rule-based rewards, without any KL regularization, is\\nsufficient to scale up both response length and benchmark performance, similar\\nto the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as\\nDeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on\\nAIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating\\nremarkable efficiency -- requiring only a tenth of the training steps, compared\\nto DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our\\nsource code, parameter settings, training data, and model weights across\\nvarious sizes.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL\", \"published\": \"2025-03-31T16:36:05Z\"}"}

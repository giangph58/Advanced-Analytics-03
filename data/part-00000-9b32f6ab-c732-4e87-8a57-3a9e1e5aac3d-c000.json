{"value":"{\"aid\": \"http://arxiv.org/abs/2503.21777v1\", \"title\": \"Test-Time Visual In-Context Tuning\", \"summary\": \"Visual in-context learning (VICL), as a new paradigm in computer vision,\\nallows the model to rapidly adapt to various tasks with only a handful of\\nprompts and examples. While effective, the existing VICL paradigm exhibits poor\\ngeneralizability under distribution shifts. In this work, we propose test-time\\nVisual In-Context Tuning (VICT), a method that can adapt VICL models on the fly\\nwith a single test sample. Specifically, we flip the role between the task\\nprompts and the test sample and use a cycle consistency loss to reconstruct the\\noriginal task prompt output. Our key insight is that a model should be aware of\\na new test distribution if it can successfully recover the original task\\nprompts. Extensive experiments on six representative vision tasks ranging from\\nhigh-level visual understanding to low-level image processing, with 15 common\\ncorruptions, demonstrate that our VICT can improve the generalizability of VICL\\nto unseen new domains. In addition, we show the potential of applying VICT for\\nunseen tasks at test time. Code: https://github.com/Jiahao000/VICT.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.LG\", \"published\": \"2025-03-27T17:59:52Z\"}"}

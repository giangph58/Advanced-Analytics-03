{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06095v1\", \"title\": \"Nonuniform-Tensor-Parallelism: Mitigating GPU failure impact for\\n  Scaled-up LLM Training\", \"summary\": \"LLM training is scaled up to 10Ks of GPUs by a mix of data-(DP) and\\nmodel-parallel (MP) execution. Critical to achieving efficiency is\\ntensor-parallel (TP; a form of MP) execution within tightly-coupled subsets of\\nGPUs, referred to as a scale-up domain, and the larger the scale-up domain the\\nbetter the performance. New datacenter architectures are emerging with more\\nGPUs able to be tightly-coupled in a scale-up domain, such as moving from 8\\nGPUs to 72 GPUs connected via NVLink. Unfortunately, larger scale-up domains\\nincrease the blast-radius of failures, with a failure of single GPU potentially\\nimpacting TP execution on the full scale-up domain, which can degrade overall\\nLLM training throughput dramatically. With as few as 0.1% of GPUs being in a\\nfailed state, a high TP-degree job can experience nearly 10% reduction in LLM\\ntraining throughput. We propose nonuniform-tensor-parallelism (NTP) to mitigate\\nthis amplified impact of GPU failures. In NTP, a DP replica that experiences\\nGPU failures operates at a reduced TP degree, contributing throughput equal to\\nthe percentage of still-functional GPUs. We also propose a rack-design with\\nimproved electrical and thermal capabilities in order to sustain power-boosting\\nof scale-up domains that have experienced failures; combined with NTP, this can\\nallow the DP replica with the reduced TP degree (i.e., with failed GPUs) to\\nkeep up with the others, thereby achieving near-zero throughput loss for\\nlarge-scale LLM training.\", \"main_category\": \"cs.DC\", \"categories\": \"cs.DC,cs.LG\", \"published\": \"2025-04-08T14:35:40Z\"}"}

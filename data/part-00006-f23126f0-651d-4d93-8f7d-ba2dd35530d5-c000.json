{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02572v1\", \"title\": \"Language Models reach higher Agreement than Humans in Historical\\n  Interpretation\", \"summary\": \"This paper compares historical annotations by humans and Large Language\\nModels. The findings reveal that both exhibit some cultural bias, but Large\\nLanguage Models achieve a higher consensus on the interpretation of historical\\nfacts from short texts. While humans tend to disagree on the basis of their\\npersonal biases, Large Models disagree when they skip information or produce\\nhallucinations. These findings have significant implications for digital\\nhumanities, enabling large-scale annotation and quantitative analysis of\\nhistorical data. This offers new educational and research opportunities to\\nexplore historical interpretations from different Language Models, fostering\\ncritical thinking about bias.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-03T13:37:45Z\"}"}

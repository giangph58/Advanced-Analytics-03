{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06982v1\", \"title\": \"SIGMAN:Scaling 3D Human Gaussian Generation with Millions of Assets\", \"summary\": \"3D human digitization has long been a highly pursued yet challenging task.\\nExisting methods aim to generate high-quality 3D digital humans from single or\\nmultiple views, but remain primarily constrained by current paradigms and the\\nscarcity of 3D human assets. Specifically, recent approaches fall into several\\nparadigms: optimization-based and feed-forward (both single-view regression and\\nmulti-view generation with reconstruction). However, they are limited by slow\\nspeed, low quality, cascade reasoning, and ambiguity in mapping low-dimensional\\nplanes to high-dimensional space due to occlusion and invisibility,\\nrespectively. Furthermore, existing 3D human assets remain small-scale,\\ninsufficient for large-scale training. To address these challenges, we propose\\na latent space generation paradigm for 3D human digitization, which involves\\ncompressing multi-view images into Gaussians via a UV-structured VAE, along\\nwith DiT-based conditional generation, we transform the ill-posed\\nlow-to-high-dimensional mapping problem into a learnable distribution shift,\\nwhich also supports end-to-end inference. In addition, we employ the multi-view\\noptimization approach combined with synthetic data to construct the HGS-1M\\ndataset, which contains $1$ million 3D Gaussian assets to support the\\nlarge-scale training. Experimental results demonstrate that our paradigm,\\npowered by large-scale training, produces high-quality 3D human Gaussians with\\nintricate textures, facial details, and loose clothing deformation.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-09T15:38:18Z\"}"}

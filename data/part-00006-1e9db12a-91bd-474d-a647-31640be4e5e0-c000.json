{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05103v1\", \"title\": \"TDFANet: Encoding Sequential 4D Radar Point Clouds Using\\n  Trajectory-Guided Deformable Feature Aggregation for Place Recognition\", \"summary\": \"Place recognition is essential for achieving closed-loop or global\\npositioning in autonomous vehicles and mobile robots. Despite recent\\nadvancements in place recognition using 2D cameras or 3D LiDAR, it remains to\\nbe seen how to use 4D radar for place recognition - an increasingly popular\\nsensor for its robustness against adverse weather and lighting conditions.\\nCompared to LiDAR point clouds, radar data are drastically sparser, noisier and\\nin much lower resolution, which hampers their ability to effectively represent\\nscenes, posing significant challenges for 4D radar-based place recognition.\\nThis work addresses these challenges by leveraging multi-modal information from\\nsequential 4D radar scans and effectively extracting and aggregating\\nspatio-temporal features.Our approach follows a principled pipeline that\\ncomprises (1) dynamic points removal and ego-velocity estimation from velocity\\nproperty, (2) bird's eye view (BEV) feature encoding on the refined point\\ncloud, (3) feature alignment using BEV feature map motion trajectory calculated\\nby ego-velocity, (4) multi-scale spatio-temporal features of the aligned BEV\\nfeature maps are extracted and aggregated.Real-world experimental results\\nvalidate the feasibility of the proposed method and demonstrate its robustness\\nin handling dynamic environments. Source codes are available.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO\", \"published\": \"2025-04-07T14:10:07Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07052v1\", \"title\": \"To Backtrack or Not to Backtrack: When Sequential Search Limits Model\\n  Reasoning\", \"summary\": \"Recent advancements in large language models have significantly improved\\ntheir reasoning abilities, particularly through techniques involving search and\\nbacktracking. Backtracking naturally scales test-time compute by enabling\\nsequential, linearized exploration via long chain-of-thought (CoT) generation.\\nHowever, this is not the only strategy for scaling test-time compute: parallel\\nsampling with best-of-n selection provides an alternative that generates\\ndiverse solutions simultaneously. Despite the growing adoption of sequential\\nsearch, its advantages over parallel sampling--especially under a fixed compute\\nbudget remain poorly understood. In this paper, we systematically compare these\\ntwo approaches on two challenging reasoning tasks: CountDown and Sudoku.\\nSurprisingly, we find that sequential search underperforms parallel sampling on\\nCountDown but outperforms it on Sudoku, suggesting that backtracking is not\\nuniversally beneficial. We identify two factors that can cause backtracking to\\ndegrade performance: (1) training on fixed search traces can lock models into\\nsuboptimal strategies, and (2) explicit CoT supervision can discourage\\n\\\"implicit\\\" (non-verbalized) reasoning. Extending our analysis to reinforcement\\nlearning (RL), we show that models with backtracking capabilities benefit\\nsignificantly from RL fine-tuning, while models without backtracking see\\nlimited, mixed gains. Together, these findings challenge the assumption that\\nbacktracking universally enhances LLM reasoning, instead revealing a complex\\ninteraction between task structure, training data, model scale, and learning\\nparadigm.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-09T17:12:49Z\"}"}

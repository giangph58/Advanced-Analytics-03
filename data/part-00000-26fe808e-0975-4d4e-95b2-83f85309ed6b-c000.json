{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24138v1\", \"title\": \"AI-Assisted Colonoscopy: Polyp Detection and Segmentation using\\n  Foundation Models\", \"summary\": \"In colonoscopy, 80% of the missed polyps could be detected with the help of\\nDeep Learning models. In the search for algorithms capable of addressing this\\nchallenge, foundation models emerge as promising candidates. Their zero-shot or\\nfew-shot learning capabilities, facilitate generalization to new data or tasks\\nwithout extensive fine-tuning. A concept that is particularly advantageous in\\nthe medical imaging domain, where large annotated datasets for traditional\\ntraining are scarce. In this context, a comprehensive evaluation of foundation\\nmodels for polyp segmentation was conducted, assessing both detection and\\ndelimitation. For the study, three different colonoscopy datasets have been\\nemployed to compare the performance of five different foundation models,\\nDINOv2, YOLO-World, GroundingDINO, SAM and MedSAM, against two benchmark\\nnetworks, YOLOv8 and Mask R-CNN. Results show that the success of foundation\\nmodels in polyp characterization is highly dependent on domain specialization.\\nFor optimal performance in medical applications, domain-specific models are\\nessential, and generic models require fine-tuning to achieve effective results.\\nThrough this specialization, foundation models demonstrated superior\\nperformance compared to state-of-the-art detection and segmentation models,\\nwith some models even excelling in zero-shot evaluation; outperforming\\nfine-tuned models on unseen data.\", \"main_category\": \"eess.IV\", \"categories\": \"eess.IV,cs.CV\", \"published\": \"2025-03-31T14:20:53Z\"}"}

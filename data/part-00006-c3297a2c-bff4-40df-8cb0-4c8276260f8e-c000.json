{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01698v1\", \"title\": \"ToM-RL: Reinforcement Learning Unlocks Theory of Mind in Small LLMs\", \"summary\": \"Recent advancements in rule-based reinforcement learning (RL), applied during\\nthe post-training phase of large language models (LLMs), have significantly\\nenhanced their capabilities in structured reasoning tasks such as mathematics\\nand logical inference. However, the effectiveness of RL in social reasoning,\\nparticularly in Theory of Mind (ToM), the ability to infer others' mental\\nstates, remains largely unexplored. In this study, we demonstrate that RL\\nmethods effectively unlock ToM reasoning capabilities even in small-scale LLMs\\n(0.5B to 7B parameters). Using a modest dataset comprising 3200 questions\\nacross diverse scenarios, our RL-trained 7B model achieves 84.50\\\\% accuracy on\\nthe Hi-ToM benchmark, surpassing models like GPT-4o and DeepSeek-v3 despite\\nsignificantly fewer parameters. While smaller models ($\\\\leq$3B parameters)\\nsuffer from reasoning collapse, larger models (7B parameters) maintain stable\\nperformance through consistent belief tracking. Additionally, our RL-based\\nmodels demonstrate robust generalization to higher-order, out-of-distribution\\nToM problems, novel textual presentations, and previously unseen datasets.\\nThese findings highlight RL's potential to enhance social cognitive reasoning,\\nbridging the gap between structured problem-solving and nuanced social\\ninference in LLMs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-02T12:58:42Z\"}"}

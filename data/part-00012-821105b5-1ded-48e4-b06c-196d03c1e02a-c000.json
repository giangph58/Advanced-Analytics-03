{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07742v1\", \"title\": \"Gradient-based Sample Selection for Faster Bayesian Optimization\", \"summary\": \"Bayesian optimization (BO) is an effective technique for black-box\\noptimization. However, its applicability is typically limited to\\nmoderate-budget problems due to the cubic complexity in computing the Gaussian\\nprocess (GP) surrogate model. In large-budget scenarios, directly employing the\\nstandard GP model faces significant challenges in computational time and\\nresource requirements. In this paper, we propose a novel approach,\\ngradient-based sample selection Bayesian Optimization (GSSBO), to enhance the\\ncomputational efficiency of BO. The GP model is constructed on a selected set\\nof samples instead of the whole dataset. These samples are selected by\\nleveraging gradient information to maintain diversity and representation. We\\nprovide a theoretical analysis of the gradient-based sample selection strategy\\nand obtain explicit sublinear regret bounds for our proposed framework.\\nExtensive experiments on synthetic and real-world tasks demonstrate that our\\napproach significantly reduces the computational cost of GP fitting in BO while\\nmaintaining optimization performance comparable to baseline methods.\", \"main_category\": \"stat.ML\", \"categories\": \"stat.ML,cs.LG\", \"published\": \"2025-04-10T13:38:15Z\"}"}

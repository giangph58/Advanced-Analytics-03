{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24191v1\", \"title\": \"Output Constraints as Attack Surface: Exploiting Structured Generation\\n  to Bypass LLM Safety Mechanisms\", \"summary\": \"Content Warning: This paper may contain unsafe or harmful content generated\\nby LLMs that may be offensive to readers. Large Language Models (LLMs) are\\nextensively used as tooling platforms through structured output APIs to ensure\\nsyntax compliance so that robust integration with existing softwares like agent\\nsystems, could be achieved. However, the feature enabling functionality of\\ngrammar-guided structured output presents significant security vulnerabilities.\\nIn this work, we reveal a critical control-plane attack surface orthogonal to\\ntraditional data-plane vulnerabilities. We introduce Constrained Decoding\\nAttack (CDA), a novel jailbreak class that weaponizes structured output\\nconstraints to bypass safety mechanisms. Unlike prior attacks focused on input\\nprompts, CDA operates by embedding malicious intent in schema-level grammar\\nrules (control-plane) while maintaining benign surface prompts (data-plane). We\\ninstantiate this with a proof-of-concept Chain Enum Attack, achieves 96.2%\\nattack success rates across proprietary and open-weight LLMs on five safety\\nbenchmarks with a single query, including GPT-4o and Gemini-2.0-flash. Our\\nfindings identify a critical security blind spot in current LLM architectures\\nand urge a paradigm shift in LLM safety to address control-plane\\nvulnerabilities, as current mechanisms focused solely on data-plane threats\\nleave critical systems exposed.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.AI\", \"published\": \"2025-03-31T15:08:06Z\"}"}

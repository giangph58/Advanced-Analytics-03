{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04869v1\", \"title\": \"Content-Aware Transformer for All-in-one Image Restoration\", \"summary\": \"Image restoration has witnessed significant advancements with the development\\nof deep learning models. Although Transformer architectures have progressed\\nconsiderably in recent years, challenges remain, particularly the limited\\nreceptive field in window-based self-attention. In this work, we propose\\nDSwinIR, a Deformable Sliding window Transformer for Image Restoration. DSwinIR\\nintroduces a novel deformable sliding window self-attention that adaptively\\nadjusts receptive fields based on image content, enabling the attention\\nmechanism to focus on important regions and enhance feature extraction aligned\\nwith salient features. Additionally, we introduce a central ensemble pattern to\\nreduce the inclusion of irrelevant content within attention windows. In this\\nway, the proposed DSwinIR model integrates the deformable sliding window\\nTransformer and central ensemble pattern to amplify the strengths of both CNNs\\nand Transformers while mitigating their limitations. Extensive experiments on\\nvarious image restoration tasks demonstrate that DSwinIR achieves\\nstate-of-the-art performance. For example, in image deraining, compared to\\nDRSformer on the SPA dataset, DSwinIR achieves a 0.66 dB PSNR improvement. In\\nall-in-one image restoration, compared to PromptIR, DSwinIR achieves over a\\n0.66 dB and 1.04 dB improvement on three-task and five-task settings,\\nrespectively. Pretrained models and code are available at our project\\nhttps://github.com/Aitical/DSwinIR.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-07T09:24:41Z\"}"}

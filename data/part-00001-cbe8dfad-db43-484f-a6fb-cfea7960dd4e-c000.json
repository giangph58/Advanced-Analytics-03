{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01833v1\", \"title\": \"YourBench: Easy Custom Evaluation Sets for Everyone\", \"summary\": \"Evaluating large language models (LLMs) effectively remains a critical\\nbottleneck, as traditional static benchmarks suffer from saturation and\\ncontamination, while human evaluations are costly and slow. This hinders timely\\nor domain-specific assessment, crucial for real-world applications. We\\nintroduce YourBench, a novel, open-source framework that addresses these\\nlimitations by enabling dynamic, automated generation of reliable, up-to-date,\\nand domain-tailored benchmarks cheaply and without manual annotation, directly\\nfrom user-provided documents. We demonstrate its efficacy by replicating 7\\ndiverse MMLU subsets using minimal source text, achieving this for under 15 USD\\nin total inference costs while perfectly preserving the relative model\\nperformance rankings (Spearman Rho = 1) observed on the original benchmark. To\\nensure that YourBench generates data grounded in provided input instead of\\nrelying on posterior parametric knowledge in models, we also introduce\\nTempora-0325, a novel dataset of over 7K diverse documents, published\\nexclusively after March 2025. Our comprehensive analysis spans 26 SoTA models\\nfrom 7 major families across varying scales (3-671B parameters) to validate the\\nquality of generated evaluations through rigorous algorithmic checks (e.g.,\\ncitation grounding) and human assessments. We release the YourBench library,\\nthe Tempora-0325 dataset, 150k+ question answer pairs based on Tempora and all\\nevaluation and inference traces to facilitate reproducible research and empower\\nthe community to generate bespoke benchmarks on demand, fostering more relevant\\nand trustworthy LLM evaluation.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,I.2.1\", \"published\": \"2025-04-02T15:40:24Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24235v1\", \"title\": \"What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\\n  Language Models\", \"summary\": \"As enthusiasm for scaling computation (data and parameters) in the\\npretraining era gradually diminished, test-time scaling (TTS), also referred to\\nas ``test-time computing'' has emerged as a prominent research focus. Recent\\nstudies demonstrate that TTS can further elicit the problem-solving\\ncapabilities of large language models (LLMs), enabling significant\\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\\ncoding, but also in general tasks like open-ended Q&A. However, despite the\\nexplosion of recent efforts in this area, there remains an urgent need for a\\ncomprehensive survey offering a systemic understanding. To fill this gap, we\\npropose a unified, multidimensional framework structured along four core\\ndimensions of TTS research: what to scale, how to scale, where to scale, and\\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\\nof methods, application scenarios, and assessment aspects, and present an\\norganized decomposition that highlights the unique functional roles of\\nindividual techniques within the broader TTS landscape. From this analysis, we\\ndistill the major developmental trajectories of TTS to date and offer hands-on\\nguidelines for practical deployment. Furthermore, we identify several open\\nchallenges and offer insights into promising future directions, including\\nfurther scaling, clarifying the functional essence of techniques, generalizing\\nto more tasks, and more attributions.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-03-31T15:46:15Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24365v1\", \"title\": \"Which LIME should I trust? Concepts, Challenges, and Solutions\", \"summary\": \"As neural networks become dominant in essential systems, Explainable\\nArtificial Intelligence (XAI) plays a crucial role in fostering trust and\\ndetecting potential misbehavior of opaque models. LIME (Local Interpretable\\nModel-agnostic Explanations) is among the most prominent model-agnostic\\napproaches, generating explanations by approximating the behavior of black-box\\nmodels around specific instances. Despite its popularity, LIME faces challenges\\nrelated to fidelity, stability, and applicability to domain-specific problems.\\nNumerous adaptations and enhancements have been proposed to address these\\nissues, but the growing number of developments can be overwhelming,\\ncomplicating efforts to navigate LIME-related research. To the best of our\\nknowledge, this is the first survey to comprehensively explore and collect\\nLIME's foundational concepts and known limitations. We categorize and compare\\nits various enhancements, offering a structured taxonomy based on intermediate\\nsteps and key issues. Our analysis provides a holistic overview of advancements\\nin LIME, guiding future research and helping practitioners identify suitable\\napproaches. Additionally, we provide a continuously updated interactive website\\n(https://patrick-knab.github.io/which-lime-to-trust/), offering a concise and\\naccessible overview of the survey.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-03-31T17:44:39Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02273v1\", \"title\": \"Reasoning Under 1 Billion: Memory-Augmented Reinforcement Learning for\\n  Large Language Models\", \"summary\": \"Recent advances in fine-tuning large language models (LLMs) with\\nreinforcement learning (RL) have shown promising improvements in complex\\nreasoning tasks, particularly when paired with chain-of-thought (CoT)\\nprompting. However, these successes have been largely demonstrated on\\nlarge-scale models with billions of parameters, where a strong pretraining\\nfoundation ensures effective initial exploration. In contrast, RL remains\\nchallenging for tiny LLMs with 1 billion parameters or fewer because they lack\\nthe necessary pretraining strength to explore effectively, often leading to\\nsuboptimal reasoning patterns. This work introduces a novel intrinsic\\nmotivation approach that leverages episodic memory to address this challenge,\\nimproving tiny LLMs in CoT reasoning tasks. Inspired by human memory-driven\\nlearning, our method leverages successful reasoning patterns stored in memory\\nwhile allowing for controlled exploration to generate novel responses.\\nIntrinsic rewards are computed efficiently using a kNN-based episodic memory,\\nallowing the model to discover new reasoning strategies while quickly adapting\\nto effective past solutions. Experiments on fine-tuning GSM8K and AI-MO\\ndatasets demonstrate that our approach significantly enhances smaller LLMs'\\nsample efficiency and generalization capability, making RL-based reasoning\\nimprovements more accessible in low-resource settings.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-03T04:46:17Z\"}"}

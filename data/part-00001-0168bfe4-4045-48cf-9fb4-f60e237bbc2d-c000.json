{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06037v1\", \"title\": \"Confidence Regularized Masked Language Modeling using Text Length\", \"summary\": \"Masked language modeling, which is a task to predict a randomly masked word\\nin the input text, is an efficient language representation learning method.\\nMasked language modeling ignores various words which people can think of for\\nfilling in the masked position and calculates the loss with a single word.\\nEspecially when the input text is short, the entropy of the word distribution\\nthat can fill in the masked position can be high. This may cause the model to\\nbe overconfident in the single answer. To address this issue, we propose a\\nnovel confidence regularizer that controls regularizing strength dynamically by\\nthe input text length. Experiments with GLUE and SQuAD datasets showed that our\\nmethod achieves better accuracy and lower expected calibration error.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-08T13:37:08Z\"}"}

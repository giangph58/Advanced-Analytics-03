{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05632v1\", \"title\": \"Reasoning Towards Fairness: Mitigating Bias in Language Models through\\n  Reasoning-Guided Fine-Tuning\", \"summary\": \"Recent advances in large-scale generative language models have shown that\\nreasoning capabilities can significantly improve model performance across a\\nvariety of tasks. However, the impact of reasoning on a model's ability to\\nmitigate stereotypical responses remains largely underexplored. In this work,\\nwe investigate the crucial relationship between a model's reasoning ability and\\nfairness, and ask whether improved reasoning capabilities can mitigate harmful\\nstereotypical responses, especially those arising due to shallow or flawed\\nreasoning. We conduct a comprehensive evaluation of multiple open-source LLMs,\\nand find that larger models with stronger reasoning abilities exhibit\\nsubstantially lower stereotypical bias on existing fairness benchmarks.\\nBuilding on this insight, we introduce ReGiFT -- Reasoning Guided Fine-Tuning,\\na novel approach that extracts structured reasoning traces from advanced\\nreasoning models and infuses them into models that lack such capabilities. We\\nuse only general-purpose reasoning and do not require any fairness-specific\\nsupervision for bias mitigation. Notably, we see that models fine-tuned using\\nReGiFT not only improve fairness relative to their non-reasoning counterparts\\nbut also outperform advanced reasoning models on fairness benchmarks. We also\\nanalyze how variations in the correctness of the reasoning traces and their\\nlength influence model fairness and their overall performance. Our findings\\nhighlight that enhancing reasoning capabilities is an effective,\\nfairness-agnostic strategy for mitigating stereotypical bias caused by\\nreasoning flaws.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-08T03:21:51Z\"}"}

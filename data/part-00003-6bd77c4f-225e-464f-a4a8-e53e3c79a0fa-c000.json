{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07646v1\", \"title\": \"On the Temporal Question-Answering Capabilities of Large Language Models\\n  Over Anonymized Data\", \"summary\": \"The applicability of Large Language Models (LLMs) in temporal reasoning tasks\\nover data that is not present during training is still a field that remains to\\nbe explored. In this paper we work on this topic, focusing on structured and\\nsemi-structured anonymized data. We not only develop a direct LLM pipeline, but\\nalso compare various methodologies and conduct an in-depth analysis. We\\nidentified and examined seventeen common temporal reasoning tasks in natural\\nlanguage, focusing on their algorithmic components. To assess LLM performance,\\nwe created the \\\\textit{Reasoning and Answering Temporal Ability} dataset\\n(RATA), featuring semi-structured anonymized data to ensure reliance on\\nreasoning rather than on prior knowledge. We compared several methodologies,\\ninvolving SoTA techniques such as Tree-of-Thought, self-reflexion and code\\nexecution, tuned specifically for this scenario. Our results suggest that\\nachieving scalable and reliable solutions requires more than just standalone\\nLLMs, highlighting the need for integrated approaches.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-10T10:48:42Z\"}"}

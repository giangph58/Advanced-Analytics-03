{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24391v1\", \"title\": \"Easi3R: Estimating Disentangled Motion from DUSt3R Without Training\", \"summary\": \"Recent advances in DUSt3R have enabled robust estimation of dense point\\nclouds and camera parameters of static scenes, leveraging Transformer network\\narchitectures and direct supervision on large-scale 3D datasets. In contrast,\\nthe limited scale and diversity of available 4D datasets present a major\\nbottleneck for training a highly generalizable 4D model. This constraint has\\ndriven conventional 4D methods to fine-tune 3D models on scalable dynamic video\\ndata with additional geometric priors such as optical flow and depths. In this\\nwork, we take an opposite path and introduce Easi3R, a simple yet efficient\\ntraining-free method for 4D reconstruction. Our approach applies attention\\nadaptation during inference, eliminating the need for from-scratch pre-training\\nor network fine-tuning. We find that the attention layers in DUSt3R inherently\\nencode rich information about camera and object motion. By carefully\\ndisentangling these attention maps, we achieve accurate dynamic region\\nsegmentation, camera pose estimation, and 4D dense point map reconstruction.\\nExtensive experiments on real-world dynamic videos demonstrate that our\\nlightweight attention adaptation significantly outperforms previous\\nstate-of-the-art methods that are trained or finetuned on extensive dynamic\\ndatasets. Our code is publicly available for research purpose at\\nhttps://easi3r.github.io/\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-31T17:59:58Z\"}"}

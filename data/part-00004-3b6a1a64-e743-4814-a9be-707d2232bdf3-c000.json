{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07089v1\", \"title\": \"OmniCaptioner: One Captioner to Rule Them All\", \"summary\": \"We propose OmniCaptioner, a versatile visual captioning framework for\\ngenerating fine-grained textual descriptions across a wide variety of visual\\ndomains. Unlike prior methods limited to specific image types (e.g., natural\\nimages or geometric visuals), our framework provides a unified solution for\\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\\nstructured visuals (e.g., documents, tables, charts). By converting low-level\\npixel information into semantically rich textual representations, our framework\\nbridges the gap between visual and textual modalities. Our results highlight\\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\\nlong-context captions of visual modalities empower LLMs, particularly the\\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\\nImproved Image Generation, where detailed captions improve tasks like\\ntext-to-image generation and image transformation; and (iii) Efficient\\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\\nperspective for bridging the gap between language and visual modalities.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.CL\", \"published\": \"2025-04-09T17:58:58Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05810v1\", \"title\": \"PaMi-VDPO: Mitigating Video Hallucinations by Prompt-Aware\\n  Multi-Instance Video Preference Learning\", \"summary\": \"Direct Preference Optimization (DPO) helps reduce hallucinations in Video\\nMultimodal Large Language Models (VLLMs), but its reliance on offline\\npreference data limits adaptability and fails to capture true video-response\\nmisalignment. We propose Video Direct Preference Optimization (VDPO), an online\\npreference learning framework that eliminates the need for preference\\nannotation by leveraging video augmentations to generate rejected samples while\\nkeeping responses fixed. However, selecting effective augmentations is\\nnon-trivial, as some clips may be semantically identical to the original under\\nspecific prompts, leading to false rejections and disrupting alignment. To\\naddress this, we introduce Prompt-aware Multi-instance Learning VDPO\\n(PaMi-VDPO), which selects augmentations based on prompt context. Instead of a\\nsingle rejection, we construct a candidate set of augmented clips and apply a\\nclose-to-far selection strategy, initially ensuring all clips are semantically\\nrelevant while then prioritizing the most prompt-aware distinct clip. This\\nallows the model to better capture meaningful visual differences, mitigating\\nhallucinations, while avoiding false rejections, and improving alignment.\\nPaMi-VDPOseamlessly integrates into existing VLLMs without additional\\nparameters, GPT-4/human supervision. With only 10k SFT data, it improves the\\nbase model by 5.3% on VideoHallucer, surpassing GPT-4o, while maintaining\\nstable performance on general video benchmarks.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-08T08:41:41Z\"}"}

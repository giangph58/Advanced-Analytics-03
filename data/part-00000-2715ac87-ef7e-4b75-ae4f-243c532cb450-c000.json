{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24190v1\", \"title\": \"Implicit In-Context Learning: Evidence from Artificial Language\\n  Experiments\", \"summary\": \"Humans acquire language through implicit learning, absorbing complex patterns\\nwithout explicit awareness. While LLMs demonstrate impressive linguistic\\ncapabilities, it remains unclear whether they exhibit human-like pattern\\nrecognition during in-context learning at inferencing level. We adapted three\\nclassic artificial language learning experiments spanning morphology,\\nmorphosyntax, and syntax to systematically evaluate implicit learning at\\ninferencing level in two state-of-the-art OpenAI models: gpt-4o and o3-mini.\\nOur results reveal linguistic domain-specific alignment between models and\\nhuman behaviors, o3-mini aligns better in morphology while both models align in\\nsyntax.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-03-31T15:07:08Z\"}"}

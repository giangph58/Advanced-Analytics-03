{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02789v1\", \"title\": \"A Framework for Robust Cognitive Evaluation of LLMs\", \"summary\": \"Emergent cognitive abilities in large language models (LLMs) have been widely\\nobserved, but their nature and underlying mechanisms remain poorly understood.\\nA growing body of research draws on cognitive science to investigate LLM\\ncognition, but standard methodologies and experimen-tal pipelines have not yet\\nbeen established. To address this gap we develop CognitivEval, a framework for\\nsystematically evaluating the artificial cognitive capabilities of LLMs, with a\\nparticular emphasis on robustness in response collection. The key features of\\nCognitivEval include: (i) automatic prompt permutations, and (ii) testing that\\ngathers both generations and model probability estimates. Our experiments\\ndemonstrate that these features lead to more robust experimental outcomes.\\nUsing CognitivEval, we replicate five classic experiments in cognitive science,\\nillustrating the framework's generalizability across various experimental tasks\\nand obtaining a cognitive profile of several state of the art LLMs.\\nCognitivEval will be released publicly to foster broader collaboration within\\nthe cognitive science community.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-03T17:35:54Z\"}"}

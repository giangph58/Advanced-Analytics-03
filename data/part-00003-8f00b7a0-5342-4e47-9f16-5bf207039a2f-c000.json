{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02279v1\", \"title\": \"MultiTSF: Transformer-based Sensor Fusion for Human-Centric Multi-view\\n  and Multi-modal Action Recognition\", \"summary\": \"Action recognition from multi-modal and multi-view observations holds\\nsignificant potential for applications in surveillance, robotics, and smart\\nenvironments. However, existing methods often fall short of addressing\\nreal-world challenges such as diverse environmental conditions, strict sensor\\nsynchronization, and the need for fine-grained annotations. In this study, we\\npropose the Multi-modal Multi-view Transformer-based Sensor Fusion (MultiTSF).\\nThe proposed method leverages a Transformer-based to dynamically model\\ninter-view relationships and capture temporal dependencies across multiple\\nviews. Additionally, we introduce a Human Detection Module to generate\\npseudo-ground-truth labels, enabling the model to prioritize frames containing\\nhuman activity and enhance spatial feature learning. Comprehensive experiments\\nconducted on our in-house MultiSensor-Home dataset and the existing MM-Office\\ndataset demonstrate that MultiTSF outperforms state-of-the-art methods in both\\nvideo sequence-level and frame-level action recognition settings.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-03T05:04:05Z\"}"}

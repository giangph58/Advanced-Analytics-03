{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01903v1\", \"title\": \"STAR-1: Safer Alignment of Reasoning LLMs with 1K Data\", \"summary\": \"This paper introduces STAR-1, a high-quality, just-1k-scale safety dataset\\nspecifically designed for large reasoning models (LRMs) like DeepSeek-R1. Built\\non three core principles -- diversity, deliberative reasoning, and rigorous\\nfiltering -- STAR-1 aims to address the critical needs for safety alignment in\\nLRMs. Specifically, we begin by integrating existing open-source safety\\ndatasets from diverse sources. Then, we curate safety policies to generate\\npolicy-grounded deliberative reasoning samples. Lastly, we apply a GPT-4o-based\\nsafety scoring system to select training examples aligned with best practices.\\nExperimental results show that fine-tuning LRMs with STAR-1 leads to an average\\n40% improvement in safety performance across four benchmarks, while only\\nincurring a marginal decrease (e.g., an average of 1.1%) in reasoning ability\\nmeasured across five reasoning tasks. Extensive ablation studies further\\nvalidate the importance of our design principles in constructing STAR-1 and\\nanalyze its efficacy across both LRMs and traditional LLMs. Our project page is\\nhttps://ucsc-vlaa.github.io/STAR-1.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-02T17:04:04Z\"}"}

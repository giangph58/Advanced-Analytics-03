{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24108v1\", \"title\": \"PolypSegTrack: Unified Foundation Model for Colonoscopy Video Analysis\", \"summary\": \"Early detection, accurate segmentation, classification and tracking of polyps\\nduring colonoscopy are critical for preventing colorectal cancer. Many existing\\ndeep-learning-based methods for analyzing colonoscopic videos either require\\ntask-specific fine-tuning, lack tracking capabilities, or rely on\\ndomain-specific pre-training. In this paper, we introduce\\n\\\\textit{PolypSegTrack}, a novel foundation model that jointly addresses polyp\\ndetection, segmentation, classification and unsupervised tracking in\\ncolonoscopic videos. Our approach leverages a novel conditional mask loss,\\nenabling flexible training across datasets with either pixel-level segmentation\\nmasks or bounding box annotations, allowing us to bypass task-specific\\nfine-tuning. Our unsupervised tracking module reliably associates polyp\\ninstances across frames using object queries, without relying on any\\nheuristics. We leverage a robust vision foundation model backbone that is\\npre-trained unsupervisedly on natural images, thereby removing the need for\\ndomain-specific pre-training. Extensive experiments on multiple polyp\\nbenchmarks demonstrate that our method significantly outperforms existing\\nstate-of-the-art approaches in detection, segmentation, classification, and\\ntracking.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-03-31T14:00:21Z\"}"}

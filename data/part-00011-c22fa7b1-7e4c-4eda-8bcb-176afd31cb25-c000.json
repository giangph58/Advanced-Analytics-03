{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04861v1\", \"title\": \"SAFT: Structure-aware Transformers for Textual Interaction\\n  Classification\", \"summary\": \"Textual interaction networks (TINs) are an omnipresent data structure used to\\nmodel the interplay between users and items on e-commerce websites, social\\nnetworks, etc., where each interaction is associated with a text description.\\nClassifying such textual interactions (TIC) finds extensive use in detecting\\nspam reviews in e-commerce, fraudulent transactions in finance, and so on.\\nExisting TIC solutions either (i) fail to capture the rich text semantics due\\nto the use of context-free text embeddings, and/or (ii) disregard the bipartite\\nstructure and node heterogeneity of TINs, leading to compromised TIC\\nperformance. In this work, we propose SAFT, a new architecture that integrates\\nlanguage- and graph-based modules for the effective fusion of textual and\\nstructural semantics in the representation learning of interactions. In\\nparticular, line graph attention (LGA)/gated attention units (GAUs) and\\npretrained language models (PLMs) are capitalized on to model the\\ninteraction-level and token-level signals, which are further coupled via the\\nproxy token in an iterative and contextualized fashion. Additionally, an\\nefficient and theoretically-grounded approach is developed to encode the local\\nand global topology information pertaining to interactions into structural\\nembeddings. The resulting embeddings not only inject the structural features\\nunderlying TINs into the textual interaction encoding but also facilitate the\\ndesign of graph sampling strategies. Extensive empirical evaluations on\\nmultiple real TIN datasets demonstrate the superiority of SAFT over the\\nstate-of-the-art baselines in TIC accuracy.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-07T09:19:12Z\"}"}

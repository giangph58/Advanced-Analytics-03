{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04814v1\", \"title\": \"Explainability of AI Uncertainty: Application to Multiple Sclerosis\\n  Lesion Segmentation on MRI\", \"summary\": \"Trustworthy artificial intelligence (AI) is essential in healthcare,\\nparticularly for high-stakes tasks like medical image segmentation. Explainable\\nAI and uncertainty quantification significantly enhance AI reliability by\\naddressing key attributes such as robustness, usability, and explainability.\\nDespite extensive technical advances in uncertainty quantification for medical\\nimaging, understanding the clinical informativeness and interpretability of\\nuncertainty remains limited. This study introduces a novel framework to explain\\nthe potential sources of predictive uncertainty, specifically in cortical\\nlesion segmentation in multiple sclerosis using deep ensembles. The proposed\\nanalysis shifts the focus from the uncertainty-error relationship towards\\nrelevant medical and engineering factors. Our findings reveal that\\ninstance-wise uncertainty is strongly related to lesion size, shape, and\\ncortical involvement. Expert rater feedback confirms that similar factors\\nimpede annotator confidence. Evaluations conducted on two datasets (206\\npatients, almost 2000 lesions) under both in-domain and distribution-shift\\nconditions highlight the utility of the framework in different scenarios.\", \"main_category\": \"eess.IV\", \"categories\": \"eess.IV,cs.CV\", \"published\": \"2025-04-07T08:09:27Z\"}"}

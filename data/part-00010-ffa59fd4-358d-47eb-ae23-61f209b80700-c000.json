{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07792v1\", \"title\": \"Breaking the Barriers: Video Vision Transformers for Word-Level Sign\\n  Language Recognition\", \"summary\": \"Sign language is a fundamental means of communication for the deaf and\\nhard-of-hearing (DHH) community, enabling nuanced expression through gestures,\\nfacial expressions, and body movements. Despite its critical role in\\nfacilitating interaction within the DHH population, significant barriers\\npersist due to the limited fluency in sign language among the hearing\\npopulation. Overcoming this communication gap through automatic sign language\\nrecognition (SLR) remains a challenge, particularly at a dynamic word-level,\\nwhere temporal and spatial dependencies must be effectively recognized. While\\nConvolutional Neural Networks have shown potential in SLR, they are\\ncomputationally intensive and have difficulties in capturing global temporal\\ndependencies between video sequences. To address these limitations, we propose\\na Video Vision Transformer (ViViT) model for word-level American Sign Language\\n(ASL) recognition. Transformer models make use of self-attention mechanisms to\\neffectively capture global relationships across spatial and temporal\\ndimensions, which makes them suitable for complex gesture recognition tasks.\\nThe VideoMAE model achieves a Top-1 accuracy of 75.58% on the WLASL100 dataset,\\nhighlighting its strong performance compared to traditional CNNs with 65.89%.\\nOur study demonstrates that transformer-based architectures have great\\npotential to advance SLR, overcome communication barriers and promote the\\ninclusion of DHH individuals.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-10T14:27:25Z\"}"}

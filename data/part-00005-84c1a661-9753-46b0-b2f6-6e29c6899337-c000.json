{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02697v1\", \"title\": \"Learning Phase Distortion with Selective State Space Models for Video\\n  Turbulence Mitigation\", \"summary\": \"Atmospheric turbulence is a major source of image degradation in long-range\\nimaging systems. Although numerous deep learning-based turbulence mitigation\\n(TM) methods have been proposed, many are slow, memory-hungry, and do not\\ngeneralize well. In the spatial domain, methods based on convolutional\\noperators have a limited receptive field, so they cannot handle a large spatial\\ndependency required by turbulence. In the temporal domain, methods relying on\\nself-attention can, in theory, leverage the lucky effects of turbulence, but\\ntheir quadratic complexity makes it difficult to scale to many frames.\\nTraditional recurrent aggregation methods face parallelization challenges.\\n  In this paper, we present a new TM method based on two concepts: (1) A\\nturbulence mitigation network based on the Selective State Space Model\\n(MambaTM). MambaTM provides a global receptive field in each layer across\\nspatial and temporal dimensions while maintaining linear computational\\ncomplexity. (2) Learned Latent Phase Distortion (LPD). LPD guides the state\\nspace model. Unlike classical Zernike-based representations of phase\\ndistortion, the new LPD map uniquely captures the actual effects of turbulence,\\nsignificantly improving the model's capability to estimate degradation by\\nreducing the ill-posedness. Our proposed method exceeds current\\nstate-of-the-art networks on various synthetic and real-world TM benchmarks\\nwith significantly faster inference speed. The code is available at\\nhttp://github.com/xg416/MambaTM.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,eess.IV\", \"published\": \"2025-04-03T15:33:18Z\"}"}

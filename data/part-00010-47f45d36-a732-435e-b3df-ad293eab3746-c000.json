{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02411v1\", \"title\": \"Adapting Large Language Models for Multi-Domain\\n  Retrieval-Augmented-Generation\", \"summary\": \"Retrieval-Augmented Generation (RAG) enhances LLM factuality, but\\nmulti-domain applications face challenges like lack of diverse benchmarks and\\npoor out-of-domain generalization. The first contribution of this work is to\\nintroduce a diverse benchmark comprising a variety of question-answering tasks\\nfrom 8 sources and covering 13 domains. Our second contribution consists in\\nsystematically testing out-of-domain generalization for typical RAG tuning\\nstrategies. While our findings reveal that standard fine-tuning fails to\\ngeneralize effectively, we show that sequence-level distillation with\\nteacher-generated labels improves out-of-domain performance by providing more\\ncoherent supervision. Our findings highlight key strategies for improving\\nmulti-domain RAG robustness.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-03T09:03:40Z\"}"}

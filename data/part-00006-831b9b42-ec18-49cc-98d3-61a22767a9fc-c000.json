{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04973v1\", \"title\": \"Ensuring Safety in an Uncertain Environment: Constrained MDPs via\\n  Stochastic Thresholds\", \"summary\": \"This paper studies constrained Markov decision processes (CMDPs) with\\nconstraints against stochastic thresholds, aiming at safety of reinforcement\\nlearning in unknown and uncertain environments. We leverage a Growing-Window\\nestimator sampling from interactions with the uncertain and dynamic environment\\nto estimate the thresholds, based on which we design Stochastic\\nPessimistic-Optimistic Thresholding (SPOT), a novel model-based primal-dual\\nalgorithm for multiple constraints against stochastic thresholds. SPOT enables\\nreinforcement learning under both pessimistic and optimistic threshold\\nsettings. We prove that our algorithm achieves sublinear regret and constraint\\nviolation; i.e., a reward regret of $\\\\tilde{\\\\mathcal{O}}(\\\\sqrt{T})$ while\\nallowing an $\\\\tilde{\\\\mathcal{O}}(\\\\sqrt{T})$ constraint violation over $T$\\nepisodes. The theoretical guarantees show that our algorithm achieves\\nperformance comparable to that of an approach relying on fixed and clear\\nthresholds. To the best of our knowledge, SPOT is the first reinforcement\\nlearning algorithm that realises theoretical guaranteed performance in an\\nuncertain environment where even thresholds are unknown.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,stat.ML\", \"published\": \"2025-04-07T11:58:19Z\"}"}

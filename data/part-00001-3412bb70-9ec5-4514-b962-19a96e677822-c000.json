{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24282v1\", \"title\": \"Style Quantization for Data-Efficient GAN Training\", \"summary\": \"Under limited data setting, GANs often struggle to navigate and effectively\\nexploit the input latent space. Consequently, images generated from adjacent\\nvariables in a sparse input latent space may exhibit significant discrepancies\\nin realism, leading to suboptimal consistency regularization (CR) outcomes. To\\naddress this, we propose \\\\textit{SQ-GAN}, a novel approach that enhances CR by\\nintroducing a style space quantization scheme. This method transforms the\\nsparse, continuous input latent space into a compact, structured discrete proxy\\nspace, allowing each element to correspond to a specific real data point,\\nthereby improving CR performance. Instead of direct quantization, we first map\\nthe input latent variables into a less entangled ``style'' space and apply\\nquantization using a learnable codebook. This enables each quantized code to\\ncontrol distinct factors of variation. Additionally, we optimize the optimal\\ntransport distance to align the codebook codes with features extracted from the\\ntraining data by a foundation model, embedding external knowledge into the\\ncodebook and establishing a semantically rich vocabulary that properly\\ndescribes the training dataset. Extensive experiments demonstrate significant\\nimprovements in both discriminator robustness and generation quality with our\\nmethod.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-03-31T16:28:44Z\"}"}

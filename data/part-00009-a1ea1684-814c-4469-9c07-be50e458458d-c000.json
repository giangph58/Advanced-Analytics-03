{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04736v1\", \"title\": \"Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use\", \"summary\": \"Reinforcement learning has been shown to improve the performance of large\\nlanguage models. However, traditional approaches like RLHF or RLAIF treat the\\nproblem as single-step. As focus shifts toward more complex reasoning and\\nagentic tasks, language models must take multiple steps of text generation,\\nreasoning and environment interaction before generating a solution. We propose\\na synthetic data generation and RL methodology targeting multi-step\\noptimization scenarios. This approach, called Step-Wise Reinforcement Learning\\n(SWiRL), iteratively generates multi-step reasoning and tool use data, and then\\nlearns from that data. It employs a simple step-wise decomposition that breaks\\neach multi-step trajectory into multiple sub-trajectories corresponding to each\\naction by the original model. It then applies synthetic data filtering and RL\\noptimization on these sub-trajectories. We evaluated SWiRL on a number of\\nmulti-step tool use, question answering, and mathematical reasoning tasks. Our\\nexperiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%,\\n14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA,\\nMuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits\\ngeneralization across tasks: for example, training only on HotPotQA (text\\nquestion-answering) improves zero-shot performance on GSM8K (a math dataset) by\\na relative 16.9%.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.CL,cs.LG\", \"published\": \"2025-04-07T05:20:58Z\"}"}

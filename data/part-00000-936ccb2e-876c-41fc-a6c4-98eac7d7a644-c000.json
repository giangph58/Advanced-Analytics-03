{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06564v1\", \"title\": \"Do Reasoning Models Show Better Verbalized Calibration?\", \"summary\": \"Large reasoning models (LRMs) have recently shown impressive capabilities in\\ncomplex reasoning by leveraging increased test-time computation and exhibiting\\nbehaviors akin to human-like deliberation. Despite these advances, it remains\\nan open question whether LRMs are better calibrated - particularly in their\\nverbalized confidence - compared to instruction-tuned counterparts. In this\\npaper, we investigate the calibration properties of LRMs trained via supervised\\nfine-tuning distillation on long reasoning traces (henceforth SFT reasoning\\nmodels) and outcome-based reinforcement learning for reasoning (henceforth RL\\nreasoning models) across diverse domains. Our findings reveal that LRMs\\nsignificantly outperform instruction-tuned models on complex reasoning tasks in\\nboth accuracy and confidence calibration. In contrast, we find surprising\\ntrends in the domain of factuality in particular. On factuality tasks, while\\nDeepseek-R1 shows strong calibration behavior, smaller QwQ-32B shows no\\nimprovement over instruct models; moreover, SFT reasoning models display worse\\ncalibration (greater overconfidence) compared to instruct models. Our results\\nprovide evidence for a potentially critical role of reasoning-oriented RL\\ntraining in improving LLMs' capacity for generating trustworthy, self-aware\\noutputs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-09T03:58:19Z\"}"}

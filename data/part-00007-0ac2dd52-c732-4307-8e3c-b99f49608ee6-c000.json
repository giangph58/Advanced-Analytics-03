{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05690v1\", \"title\": \"STAGE: Stemmed Accompaniment Generation through Prefix-Based\\n  Conditioning\", \"summary\": \"Recent advances in generative models have made it possible to create\\nhigh-quality, coherent music, with some systems delivering production-level\\noutput.Yet, most existing models focus solely on generating music from scratch,\\nlimiting their usefulness for musicians who want to integrate such models into\\na human, iterative composition workflow.In this paper we introduce STAGE, our\\nSTemmed Accompaniment GEneration model, fine-tuned from the state-of-the-art\\nMusicGen to generate single-stem instrumental accompaniments conditioned on a\\ngiven mixture. Inspired by instruction-tuning methods for language models, we\\nextend the transformer's embedding matrix with a context token, enabling the\\nmodel to attend to a musical context through prefix-based conditioning.Compared\\nto the baselines, STAGE yields accompaniments that exhibit stronger coherence\\nwith the input mixture, higher audio quality, and closer alignment with textual\\nprompts.Moreover, by conditioning on a metronome-like track, our framework\\nnaturally supports tempo-constrained generation, achieving state-of-the-art\\nalignment with the target rhythmic structure--all without requiring any\\nadditional tempo-specific module.As a result, STAGE offers a practical,\\nversatile tool for interactive music creation that can be readily adopted by\\nmusicians in real-world workflows.\", \"main_category\": \"cs.SD\", \"categories\": \"cs.SD,eess.AS\", \"published\": \"2025-04-08T05:24:11Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05040v1\", \"title\": \"InstructionBench: An Instructional Video Understanding Benchmark\", \"summary\": \"Despite progress in video large language models (Video-LLMs), research on\\ninstructional video understanding, crucial for enhancing access to\\ninstructional content, remains insufficient. To address this, we introduce\\nInstructionBench, an Instructional video understanding Benchmark, which\\nchallenges models' advanced temporal reasoning within instructional videos\\ncharacterized by their strict step-by-step flow. Employing GPT-4, we formulate\\nQ\\\\&A pairs in open-ended and multiple-choice formats to assess both\\nCoarse-Grained event-level and Fine-Grained object-level reasoning. Our\\nfiltering strategies exclude questions answerable purely by common-sense\\nknowledge, focusing on visual perception and analysis when evaluating Video-LLM\\nmodels. The benchmark finally contains 5k questions across over 700 videos. We\\nevaluate the latest Video-LLMs on our InstructionBench, finding that\\nclosed-source models outperform open-source ones. However, even the best model,\\nGPT-4o, achieves only 53.42\\\\% accuracy, indicating significant gaps in temporal\\nreasoning. To advance the field, we also develop a comprehensive instructional\\nvideo dataset with over 19k Q\\\\&A pairs from nearly 2.5k videos, using an\\nautomated data generation framework, thereby enriching the community's research\\nresources.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-07T13:05:09Z\"}"}

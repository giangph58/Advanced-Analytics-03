{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02710v1\", \"title\": \"A Numerically Efficient Method to Enhance Model Predictive Control\\n  Performance with a Reinforcement Learning Policy\", \"summary\": \"We propose a novel approach for combining model predictive control (MPC) with\\nreinforcement learning (RL) to reduce online computation while achieving high\\nclosed-loop tracking performance and constraint satisfaction. This method,\\ncalled Policy-Enhanced Partial Tightening (PEPT), approximates the optimal\\nvalue function through a Riccati recursion around a state-control trajectory\\nobtained by evaluating the RL policy. The result is a convex quadratic terminal\\ncost that can be seamlessly integrated into the MPC formulation. The proposed\\ncontroller is tested in simulations on a trajectory tracking problem for a\\nquadcopter with nonlinear dynamics and bounded state and control. The results\\nhighlight PEPT's effectiveness, outperforming both pure RL policies and several\\nMPC variations. Compared to pure RL, PEPT achieves 1000 times lower constraint\\nviolation cost with only twice the feedback time. Against the best MPC-based\\npolicy, PEPT reduces constraint violations by 2 to 5 times and runs nearly 3\\ntimes faster while maintaining similar tracking performance. The code is\\nopen-source at www.github.com/aghezz1/pept.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC\", \"published\": \"2025-04-03T15:50:47Z\"}"}

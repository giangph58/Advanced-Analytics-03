{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01839v1\", \"title\": \"A Randomized Zeroth-Order Hierarchical Framework for Heterogeneous\\n  Federated Learning\", \"summary\": \"Heterogeneity in federated learning (FL) is a critical and challenging aspect\\nthat significantly impacts model performance and convergence. In this paper, we\\npropose a novel framework by formulating heterogeneous FL as a hierarchical\\noptimization problem. This new framework captures both local and global\\ntraining process through a bilevel formulation and is capable of the following:\\n(i) addressing client heterogeneity through a personalized learning framework;\\n(ii) capturing pre-training process on server's side; (iii) updating global\\nmodel through nonstandard aggregation; (iv) allowing for nonidentical local\\nsteps; and (v) capturing clients' local constraints. We design and analyze an\\nimplicit zeroth-order FL method (ZO-HFL), provided with nonasymptotic\\nconvergence guarantees for both the server-agent and the individual\\nclient-agents, and asymptotic guarantees for both the server-agent and\\nclient-agents in an almost sure sense. Notably, our method does not rely on\\nstandard assumptions in heterogeneous FL, such as the bounded gradient\\ndissimilarity condition. We implement our method on image classification tasks\\nand compare with other methods under different heterogeneous settings.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC,cs.LG\", \"published\": \"2025-04-02T15:44:59Z\"}"}

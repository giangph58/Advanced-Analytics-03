{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06036v1\", \"title\": \"Multi-Sense Embeddings for Language Models and Knowledge Distillation\", \"summary\": \"Transformer-based large language models (LLMs) rely on contextual embeddings\\nwhich generate different (continuous) representations for the same token\\ndepending on its surrounding context. Nonetheless, words and tokens typically\\nhave a limited number of senses (or meanings). We propose multi-sense\\nembeddings as a drop-in replacement for each token in order to capture the\\nrange of their uses in a language. To construct a sense embedding dictionary,\\nwe apply a clustering algorithm to embeddings generated by an LLM and consider\\nthe cluster centers as representative sense embeddings. In addition, we propose\\na novel knowledge distillation method that leverages the sense dictionary to\\nlearn a smaller student model that mimics the senses from the much larger base\\nLLM model, offering significant space and inference time savings, while\\nmaintaining competitive performance. Via thorough experiments on various\\nbenchmarks, we showcase the effectiveness of our sense embeddings and knowledge\\ndistillation approach. We share our code at\\nhttps://github.com/Qitong-Wang/SenseDict\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-08T13:36:36Z\"}"}

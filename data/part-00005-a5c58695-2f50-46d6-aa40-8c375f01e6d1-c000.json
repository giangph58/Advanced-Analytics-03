{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01735v1\", \"title\": \"AdPO: Enhancing the Adversarial Robustness of Large Vision-Language\\n  Models with Preference Optimization\", \"summary\": \"Large Vision-Language Models (LVLMs), such as GPT-4o and LLaVA, have recently\\nwitnessed remarkable advancements and are increasingly being deployed in\\nreal-world applications. However, inheriting the sensitivity of visual neural\\nnetworks, LVLMs remain vulnerable to adversarial attacks, which can result in\\nerroneous or malicious outputs. While existing efforts utilize adversarial\\nfine-tuning to enhance robustness, they often suffer from performance\\ndegradation on clean inputs. In this paper, we proposes AdPO, a novel\\nadversarial defense strategy for LVLMs based on preference optimization. For\\nthe first time, we reframe adversarial training as a preference optimization\\nproblem, aiming to enhance the model's preference for generating normal outputs\\non clean inputs while rejecting the potential misleading outputs for\\nadversarial examples. Notably, AdPO achieves this by solely modifying the image\\nencoder, e.g., CLIP ViT, resulting in superior clean and adversarial\\nperformance in a variety of downsream tasks. Considering that training involves\\nlarge language models (LLMs), the computational cost increases significantly.\\nWe validate that training on smaller LVLMs and subsequently transferring to\\nlarger models can achieve competitive performance while maintaining efficiency\\ncomparable to baseline methods. Our comprehensive experiments confirm the\\neffectiveness of the proposed AdPO, which provides a novel perspective for\\nfuture adversarial defense research.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-02T13:43:21Z\"}"}

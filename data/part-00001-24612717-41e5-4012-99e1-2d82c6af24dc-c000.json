{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01519v1\", \"title\": \"Chain of Correction for Full-text Speech Recognition with Large Language\\n  Models\", \"summary\": \"Full-text error correction with Large Language Models (LLMs) for Automatic\\nSpeech Recognition (ASR) has gained increased attention due to its potential to\\ncorrect errors across long contexts and address a broader spectrum of error\\ntypes, including punctuation restoration and inverse text normalization.\\nNevertheless, many challenges persist, including issues related to stability,\\ncontrollability, completeness, and fluency. To mitigate these challenges, this\\npaper proposes the Chain of Correction (CoC) for full-text error correction\\nwith LLMs, which corrects errors segment by segment using pre-recognized text\\nas guidance within a regular multi-turn chat format. The CoC also uses\\npre-recognized full text for context, allowing the model to better grasp global\\nsemantics and maintain a comprehensive overview of the entire content.\\nUtilizing the open-sourced full-text error correction dataset ChFT, we\\nfine-tune a pre-trained LLM to evaluate the performance of the CoC framework.\\nExperimental results demonstrate that the CoC effectively corrects errors in\\nfull-text ASR outputs, significantly outperforming baseline and benchmark\\nsystems. We further analyze how to set the correction threshold to balance\\nunder-correction and over-rephrasing, extrapolate the CoC model on extremely\\nlong ASR outputs, and investigate whether other types of information can be\\nemployed to guide the error correction process.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,eess.AS\", \"published\": \"2025-04-02T09:06:23Z\"}"}

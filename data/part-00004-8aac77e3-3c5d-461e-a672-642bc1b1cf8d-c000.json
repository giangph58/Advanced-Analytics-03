{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07598v1\", \"title\": \"On Model and Data Scaling for Skeleton-based Self-Supervised Gait\\n  Recognition\", \"summary\": \"Gait recognition from video streams is a challenging problem in computer\\nvision biometrics due to the subtle differences between gaits and numerous\\nconfounding factors. Recent advancements in self-supervised pretraining have\\nled to the development of robust gait recognition models that are invariant to\\nwalking covariates. While neural scaling laws have transformed model\\ndevelopment in other domains by linking performance to data, model size, and\\ncompute, their applicability to gait remains unexplored. In this work, we\\nconduct the first empirical study scaling on skeleton-based self-supervised\\ngait recognition to quantify the effect of data quantity, model size and\\ncompute on downstream gait recognition performance. We pretrain multiple\\nvariants of GaitPT - a transformer-based architecture - on a dataset of 2.7\\nmillion walking sequences collected in the wild. We evaluate zero-shot\\nperformance across four benchmark datasets to derive scaling laws for data,\\nmodel size, and compute. Our findings demonstrate predictable power-law\\nimprovements in performance with increased scale and confirm that data and\\ncompute scaling significantly influence downstream accuracy. We further isolate\\narchitectural contributions by comparing GaitPT with GaitFormer under\\ncontrolled compute budgets. These results provide practical insights into\\nresource allocation and performance estimation for real-world gait recognition\\nsystems.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-10T09:51:22Z\"}"}

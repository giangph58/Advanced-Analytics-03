{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02402v1\", \"title\": \"EvMic: Event-based Non-contact sound recovery from effective\\n  spatial-temporal modeling\", \"summary\": \"When sound waves hit an object, they induce vibrations that produce\\nhigh-frequency and subtle visual changes, which can be used for recovering the\\nsound. Early studies always encounter trade-offs related to sampling rate,\\nbandwidth, field of view, and the simplicity of the optical path. Recent\\nadvances in event camera hardware show good potential for its application in\\nvisual sound recovery, because of its superior ability in capturing\\nhigh-frequency signals. However, existing event-based vibration recovery\\nmethods are still sub-optimal for sound recovery. In this work, we propose a\\nnovel pipeline for non-contact sound recovery, fully utilizing spatial-temporal\\ninformation from the event stream. We first generate a large training set using\\na novel simulation pipeline. Then we designed a network that leverages the\\nsparsity of events to capture spatial information and uses Mamba to model\\nlong-term temporal information. Lastly, we train a spatial aggregation block to\\naggregate information from different locations to further improve signal\\nquality. To capture event signals caused by sound waves, we also designed an\\nimaging system using a laser matrix to enhance the gradient and collected\\nmultiple data sequences for testing. Experimental results on synthetic and\\nreal-world data demonstrate the effectiveness of our method.\", \"main_category\": \"cs.SD\", \"categories\": \"cs.SD,cs.AI,eess.AS\", \"published\": \"2025-04-03T08:51:17Z\"}"}

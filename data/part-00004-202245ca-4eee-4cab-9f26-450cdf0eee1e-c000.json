{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02620v1\", \"title\": \"Efficient Model Editing with Task-Localized Sparse Fine-tuning\", \"summary\": \"Task arithmetic has emerged as a promising approach for editing models by\\nrepresenting task-specific knowledge as composable task vectors. However,\\nexisting methods rely on network linearization to derive task vectors, leading\\nto computational bottlenecks during training and inference. Moreover,\\nlinearization alone does not ensure weight disentanglement, the key property\\nthat enables conflict-free composition of task vectors. To address this, we\\npropose TaLoS which allows to build sparse task vectors with minimal\\ninterference without requiring explicit linearization and sharing information\\nacross tasks. We find that pre-trained models contain a subset of parameters\\nwith consistently low gradient sensitivity across tasks, and that sparsely\\nupdating only these parameters allows for promoting weight disentanglement\\nduring fine-tuning. Our experiments prove that TaLoS improves training and\\ninference efficiency while outperforming current methods in task addition and\\nnegation. By enabling modular parameter editing, our approach fosters practical\\ndeployment of adaptable foundation models in real-world applications.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL,cs.CV\", \"published\": \"2025-04-03T14:20:06Z\"}"}

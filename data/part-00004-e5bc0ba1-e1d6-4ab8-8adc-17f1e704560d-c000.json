{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04953v1\", \"title\": \"M-Prometheus: A Suite of Open Multilingual LLM Judges\", \"summary\": \"The use of language models for automatically evaluating long-form text\\n(LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are\\noptimized exclusively for English, with strategies for enhancing their\\nmultilingual evaluation capabilities remaining largely unexplored in the\\ncurrent literature. This has created a disparity in the quality of automatic\\nevaluation methods for non-English languages, ultimately hindering the\\ndevelopment of models with better multilingual capabilities. To bridge this\\ngap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from\\n3B to 14B parameters that can provide both direct assessment and pairwise\\ncomparison feedback on multilingual outputs. M-Prometheus models outperform\\nstate-of-the-art open LLM judges on multilingual reward benchmarks spanning\\nmore than 20 languages, as well as on literary machine translation (MT)\\nevaluation covering 4 language pairs. Furthermore, M-Prometheus models can be\\nleveraged at decoding time to significantly improve generated outputs across\\nall 3 tested languages, showcasing their utility for the development of better\\nmultilingual models. Lastly, through extensive ablations, we identify the key\\nfactors for obtaining an effective multilingual judge, including backbone model\\nselection and training on natively multilingual feedback data instead of\\ntranslated data. We release our models, training dataset, and code.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-07T11:37:26Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06560v1\", \"title\": \"NeedleInATable: Exploring Long-Context Capability of Large Language\\n  Models towards Long-Structured Tables\", \"summary\": \"Processing structured tabular data, particularly lengthy tables, constitutes\\na fundamental yet challenging task for large language models (LLMs). However,\\nexisting long-context benchmarks primarily focus on unstructured text,\\nneglecting the challenges of long and complex structured tables. To address\\nthis gap, we introduce NeedleInATable (NIAT), a novel task that treats each\\ntable cell as a \\\"needle\\\" and requires the model to extract the target cell\\nunder different queries. Evaluation results of mainstream LLMs on this\\nbenchmark show they lack robust long-table comprehension, often relying on\\nsuperficial correlations or shortcuts for complex table understanding tasks,\\nrevealing significant limitations in processing intricate tabular data. To this\\nend, we propose a data synthesis method to enhance models' long-table\\ncomprehension capabilities. Experimental results show that our synthesized\\ntraining data significantly enhances LLMs' performance on the NIAT task,\\noutperforming both long-context LLMs and long-table agent methods. This work\\nadvances the evaluation of LLMs' genuine long-structured table comprehension\\ncapabilities and paves the way for progress in long-context and table\\nunderstanding applications.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-09T03:46:56Z\"}"}

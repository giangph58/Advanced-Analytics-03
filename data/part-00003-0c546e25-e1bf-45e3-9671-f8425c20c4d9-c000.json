{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01389v1\", \"title\": \"De Novo Molecular Design Enabled by Direct Preference Optimization and\\n  Curriculum Learning\", \"summary\": \"De novo molecular design has extensive applications in drug discovery and\\nmaterials science. The vast chemical space renders direct molecular searches\\ncomputationally prohibitive, while traditional experimental screening is both\\ntime- and labor-intensive. Efficient molecular generation and screening methods\\nare therefore essential for accelerating drug discovery and reducing costs.\\nAlthough reinforcement learning (RL) has been applied to optimize molecular\\nproperties via reward mechanisms, its practical utility is limited by issues in\\ntraining efficiency, convergence, and stability. To address these challenges,\\nwe adopt Direct Preference Optimization (DPO) from NLP, which uses molecular\\nscore-based sample pairs to maximize the likelihood difference between high-\\nand low-quality molecules, effectively guiding the model toward better\\ncompounds. Moreover, integrating curriculum learning further boosts training\\nefficiency and accelerates convergence. A systematic evaluation of the proposed\\nmethod on the GuacaMol Benchmark yielded excellent scores. For instance, the\\nmethod achieved a score of 0.883 on the Perindopril MPO task, representing a\\n6\\\\% improvement over competing models. And subsequent target protein binding\\nexperiments confirmed its practical efficacy. These results demonstrate the\\nstrong potential of DPO for molecular design tasks and highlight its\\neffectiveness as a robust and efficient solution for data-driven drug\\ndiscovery.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,physics.chem-ph,q-bio.BM\", \"published\": \"2025-04-02T06:00:21Z\"}"}

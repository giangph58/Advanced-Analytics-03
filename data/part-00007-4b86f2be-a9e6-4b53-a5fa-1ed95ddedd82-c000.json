{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24193v1\", \"title\": \"Text2Tracks: Prompt-based Music Recommendation via Generative Retrieval\", \"summary\": \"In recent years, Large Language Models (LLMs) have enabled users to provide\\nhighly specific music recommendation requests using natural language prompts\\n(e.g. \\\"Can you recommend some old classics for slow dancing?\\\"). In this setup,\\nthe recommended tracks are predicted by the LLM in an autoregressive way, i.e.\\nthe LLM generates the track titles one token at a time. While intuitive, this\\napproach has several limitation. First, it is based on a general purpose\\ntokenization that is optimized for words rather than for track titles. Second,\\nit necessitates an additional entity resolution layer that matches the track\\ntitle to the actual track identifier. Third, the number of decoding steps\\nscales linearly with the length of the track title, slowing down inference. In\\nthis paper, we propose to address the task of prompt-based music recommendation\\nas a generative retrieval task. Within this setting, we introduce novel,\\neffective, and efficient representations of track identifiers that\\nsignificantly outperform commonly used strategies. We introduce Text2Tracks, a\\ngenerative retrieval model that learns a mapping from a user's music\\nrecommendation prompt to the relevant track IDs directly. Through an offline\\nevaluation on a dataset of playlists with language inputs, we find that (1) the\\nstrategy to create IDs for music tracks is the most important factor for the\\neffectiveness of Text2Tracks and semantic IDs significantly outperform commonly\\nused strategies that rely on song titles as identifiers (2) provided with the\\nright choice of track identifiers, Text2Tracks outperforms sparse and dense\\nretrieval solutions trained to retrieve tracks from language prompts.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR\", \"published\": \"2025-03-31T15:09:19Z\"}"}

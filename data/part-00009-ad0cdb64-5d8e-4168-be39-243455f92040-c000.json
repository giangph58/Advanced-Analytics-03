{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01871v1\", \"title\": \"Interpreting Emergent Planning in Model-Free Reinforcement Learning\", \"summary\": \"We present the first mechanistic evidence that model-free reinforcement\\nlearning agents can learn to plan. This is achieved by applying a methodology\\nbased on concept-based interpretability to a model-free agent in Sokoban -- a\\ncommonly used benchmark for studying planning. Specifically, we demonstrate\\nthat DRC, a generic model-free agent introduced by Guez et al. (2019), uses\\nlearned concept representations to internally formulate plans that both predict\\nthe long-term effects of actions on the environment and influence action\\nselection. Our methodology involves: (1) probing for planning-relevant\\nconcepts, (2) investigating plan formation within the agent's representations,\\nand (3) verifying that discovered plans (in the agent's representations) have a\\ncausal effect on the agent's behavior through interventions. We also show that\\nthe emergence of these plans coincides with the emergence of a planning-like\\nproperty: the ability to benefit from additional test-time compute. Finally, we\\nperform a qualitative analysis of the planning algorithm learned by the agent\\nand discover a strong resemblance to parallelized bidirectional search. Our\\nfindings advance understanding of the internal mechanisms underlying planning\\nbehavior in agents, which is important given the recent trend of emergent\\nplanning and reasoning capabilities in LLMs through RL\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-02T16:24:23Z\"}"}

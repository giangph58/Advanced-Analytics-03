{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02565v1\", \"title\": \"MAD: A Magnitude And Direction Policy Parametrization for Stability\\n  Constrained Reinforcement Learning\", \"summary\": \"We introduce magnitude and direction (MAD) policies, a policy\\nparameterization for reinforcement learning (RL) that preserves Lp closed-loop\\nstability for nonlinear dynamical systems. Although complete in their ability\\nto describe all stabilizing controllers, methods based on nonlinear Youla and\\nsystem-level synthesis are significantly affected by the difficulty of\\nparameterizing Lp-stable operators. In contrast, MAD policies introduce\\nexplicit feedback on state-dependent features - a key element behind the\\nsuccess of RL pipelines - without compromising closed-loop stability. This is\\nachieved by describing the magnitude of the control input with a\\ndisturbance-feedback Lp-stable operator, while selecting its direction based on\\nstate-dependent features through a universal function approximator. We further\\ncharacterize the robust stability properties of MAD policies under model\\nmismatch. Unlike existing disturbance-feedback policy parameterizations, MAD\\npolicies introduce state-feedback components compatible with model-free RL\\npipelines, ensuring closed-loop stability without requiring model information\\nbeyond open-loop stability. Numerical experiments show that MAD policies\\ntrained with deep deterministic policy gradient (DDPG) methods generalize to\\nunseen scenarios, matching the performance of standard neural network policies\\nwhile guaranteeing closed-loop stability by design.\", \"main_category\": \"eess.SY\", \"categories\": \"eess.SY,cs.LG,cs.SY\", \"published\": \"2025-04-03T13:26:26Z\"}"}

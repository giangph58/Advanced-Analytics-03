{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01559v1\", \"title\": \"RealityAvatar: Towards Realistic Loose Clothing Modeling in Animatable\\n  3D Gaussian Avatars\", \"summary\": \"Modeling animatable human avatars from monocular or multi-view videos has\\nbeen widely studied, with recent approaches leveraging neural radiance fields\\n(NeRFs) or 3D Gaussian Splatting (3DGS) achieving impressive results in\\nnovel-view and novel-pose synthesis. However, existing methods often struggle\\nto accurately capture the dynamics of loose clothing, as they primarily rely on\\nglobal pose conditioning or static per-frame representations, leading to\\noversmoothing and temporal inconsistencies in non-rigid regions. To address\\nthis, We propose RealityAvatar, an efficient framework for high-fidelity\\ndigital human modeling, specifically targeting loosely dressed avatars. Our\\nmethod leverages 3D Gaussian Splatting to capture complex clothing deformations\\nand motion dynamics while ensuring geometric consistency. By incorporating a\\nmotion trend module and a latentbone encoder, we explicitly model\\npose-dependent deformations and temporal variations in clothing behavior.\\nExtensive experiments on benchmark datasets demonstrate the effectiveness of\\nour approach in capturing fine-grained clothing deformations and motion-driven\\nshape variations. Our method significantly enhances structural fidelity and\\nperceptual quality in dynamic human reconstruction, particularly in non-rigid\\nregions, while achieving better consistency across temporal frames.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-02T09:59:12Z\"}"}

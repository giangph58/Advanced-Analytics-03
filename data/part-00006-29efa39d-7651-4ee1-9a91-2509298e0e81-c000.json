{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02521v1\", \"title\": \"UNDO: Understanding Distillation as Optimization\", \"summary\": \"Knowledge distillation has emerged as an effective strategy for compressing\\nlarge language models' (LLMs) knowledge into smaller, more efficient student\\nmodels. However, standard one-shot distillation methods often produce\\nsuboptimal results due to a mismatch between teacher-generated rationales and\\nthe student's specific learning requirements. In this paper, we introduce the\\nUNDO: UNderstanding Distillation as Optimization framework, designed to bridge\\nthis gap by iteratively identifying the student's errors and prompting the\\nteacher to refine its explanations accordingly. Each iteration directly targets\\nthe student's learning deficiencies, motivating the teacher to provide tailored\\nand enhanced rationales that specifically address these weaknesses. Empirical\\nevaluations on various challenging mathematical and commonsense reasoning tasks\\ndemonstrate that our iterative distillation method, UNDO, significantly\\noutperforms standard one-step distillation methods, achieving performance gains\\nof up to 20%. Additionally, we show that teacher-generated data refined through\\nour iterative process remains effective even when applied to different student\\nmodels, underscoring the broad applicability of our approach. Our work\\nfundamentally reframes knowledge distillation as an iterative teacher-student\\ninteraction, effectively leveraging dynamic refinement by the teacher for\\nbetter knowledge distillation.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-03T12:18:51Z\"}"}

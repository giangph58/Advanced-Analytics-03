{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24379v1\", \"title\": \"Any2Caption:Interpreting Any Condition to Caption for Controllable Video\\n  Generation\", \"summary\": \"To address the bottleneck of accurate user intent interpretation within the\\ncurrent video generation community, we present Any2Caption, a novel framework\\nfor controllable video generation under any condition. The key idea is to\\ndecouple various condition interpretation steps from the video synthesis step.\\nBy leveraging modern multimodal large language models (MLLMs), Any2Caption\\ninterprets diverse inputs--text, images, videos, and specialized cues such as\\nregion, motion, and camera poses--into dense, structured captions that offer\\nbackbone video generators with better guidance. We also introduce Any2CapIns, a\\nlarge-scale dataset with 337K instances and 407K conditions for\\nany-condition-to-caption instruction tuning. Comprehensive evaluations\\ndemonstrate significant improvements of our system in controllability and video\\nquality across various aspects of existing video generation models. Project\\nPage: https://sqwu.top/Any2Cap/\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-03-31T17:59:01Z\"}"}

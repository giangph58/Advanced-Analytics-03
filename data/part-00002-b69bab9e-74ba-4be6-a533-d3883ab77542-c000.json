{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01719v1\", \"title\": \"Beyond Non-Expert Demonstrations: Outcome-Driven Action Constraint for\\n  Offline Reinforcement Learning\", \"summary\": \"We address the challenge of offline reinforcement learning using realistic\\ndata, specifically non-expert data collected through sub-optimal behavior\\npolicies. Under such circumstance, the learned policy must be safe enough to\\nmanage \\\\textit{distribution shift} while maintaining sufficient flexibility to\\ndeal with non-expert (bad) demonstrations from offline data.To tackle this\\nissue, we introduce a novel method called Outcome-Driven Action Flexibility\\n(ODAF), which seeks to reduce reliance on the empirical action distribution of\\nthe behavior policy, hence reducing the negative impact of those bad\\ndemonstrations.To be specific, a new conservative reward mechanism is developed\\nto deal with {\\\\it distribution shift} by evaluating actions according to\\nwhether their outcomes meet safety requirements - remaining within the state\\nsupport area, rather than solely depending on the actions' likelihood based on\\noffline data.Besides theoretical justification, we provide empirical evidence\\non widely used MuJoCo and various maze benchmarks, demonstrating that our ODAF\\nmethod, implemented using uncertainty quantification techniques, effectively\\ntolerates unseen transitions for improved \\\"trajectory stitching,\\\" while\\nenhancing the agent's ability to learn from realistic non-expert data.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.RO\", \"published\": \"2025-04-02T13:27:44Z\"}"}

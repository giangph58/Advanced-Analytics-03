{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02658v1\", \"title\": \"MiLo: Efficient Quantized MoE Inference with Mixture of Low-Rank\\n  Compensators\", \"summary\": \"A critical approach for efficiently deploying Mixture-of-Experts (MoE) models\\nwith massive parameters is quantization. However, state-of-the-art MoE models\\nsuffer from non-negligible accuracy loss with extreme quantization, such as\\nunder 4 bits. To address this, we introduce MiLo, a novel method that augments\\nhighly quantized MoEs with a mixture of low-rank compensators. These\\ncompensators consume only a small amount of additional memory but significantly\\nrecover accuracy loss from extreme quantization. MiLo also identifies that\\nMoEmodels exhibit distinctive characteristics across weights due to their\\nhybrid dense-sparse architectures, and employs adaptive rank selection policies\\nalong with iterative optimizations to close the accuracy gap. MiLo does not\\nrely on calibration data, allowing it to generalize to different MoE models and\\ndatasets without overfitting to a calibration set. To avoid the hardware\\ninefficiencies of extreme quantization, such as 3-bit, MiLo develops Tensor\\nCore-friendly 3-bit kernels, enabling measured latency speedups on 3-bit\\nquantized MoE models. Our evaluation shows that MiLo outperforms existing\\nmethods on SoTA MoE models across various tasks.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-03T14:54:17Z\"}"}

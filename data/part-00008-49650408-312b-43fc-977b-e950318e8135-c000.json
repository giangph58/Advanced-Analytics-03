{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07951v1\", \"title\": \"Scaling Laws for Native Multimodal Models Scaling Laws for Native\\n  Multimodal Models\", \"summary\": \"Building general-purpose models that can effectively perceive the world\\nthrough multimodal signals has been a long-standing goal. Current approaches\\ninvolve integrating separately pre-trained components, such as connecting\\nvision encoders to LLMs and continuing multimodal training. While such\\napproaches exhibit remarkable sample efficiency, it remains an open question\\nwhether such late-fusion architectures are inherently superior. In this work,\\nwe revisit the architectural design of native multimodal models (NMMs)--those\\ntrained from the ground up on all modalities--and conduct an extensive scaling\\nlaws study, spanning 457 trained models with different architectures and\\ntraining mixtures. Our investigation reveals no inherent advantage to\\nlate-fusion architectures over early-fusion ones, which do not rely on image\\nencoders. On the contrary, early-fusion exhibits stronger performance at lower\\nparameter counts, is more efficient to train, and is easier to deploy.\\nMotivated by the strong performance of the early-fusion architectures, we show\\nthat incorporating Mixture of Experts (MoEs) allows for models that learn\\nmodality-specific weights, significantly enhancing performance.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-10T17:57:28Z\"}"}

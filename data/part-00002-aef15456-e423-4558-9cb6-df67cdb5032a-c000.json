{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07086v1\", \"title\": \"A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths\\n  to Reproducibility\", \"summary\": \"Reasoning has emerged as the next major frontier for language models (LMs),\\nwith rapid advances from both academic and industrial labs. However, this\\nprogress often outpaces methodological rigor, with many evaluations relying on\\nbenchmarking practices that lack transparency, robustness, or statistical\\ngrounding. In this work, we conduct a comprehensive empirical study and find\\nthat current mathematical reasoning benchmarks are highly sensitive to subtle\\nimplementation choices - including decoding parameters, random seeds, prompt\\nformatting, and even hardware and software-framework configurations.\\nPerformance gains reported in recent studies frequently hinge on unclear\\ncomparisons or unreported sources of variance. To address these issues, we\\npropose a standardized evaluation framework with clearly defined best practices\\nand reporting standards. Using this framework, we reassess recent methods and\\nfind that reinforcement learning (RL) approaches yield only modest improvements\\n- far below prior claims - and are prone to overfitting, especially on\\nsmall-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT)\\nmethods show consistently stronger generalization. To foster reproducibility,\\nwe release all code, prompts, and model outputs, for reasoning benchmarks,\\nestablishing more rigorous foundations for future work.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL\", \"published\": \"2025-04-09T17:58:17Z\"}"}

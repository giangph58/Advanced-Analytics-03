{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02337v1\", \"title\": \"LPA3D: 3D Room-Level Scene Generation from In-the-Wild Images\", \"summary\": \"Generating realistic, room-level indoor scenes with semantically plausible\\nand detailed appearances from in-the-wild images is crucial for various\\napplications in VR, AR, and robotics. The success of NeRF-based generative\\nmethods indicates a promising direction to address this challenge. However,\\nunlike their success at the object level, existing scene-level generative\\nmethods require additional information, such as multiple views, depth images,\\nor semantic guidance, rather than relying solely on RGB images. This is because\\nNeRF-based methods necessitate prior knowledge of camera poses, which is\\nchallenging to approximate for indoor scenes due to the complexity of defining\\nalignment and the difficulty of globally estimating poses from a single image,\\ngiven the unseen parts behind the camera. To address this challenge, we\\nredefine global poses within the framework of Local-Pose-Alignment (LPA) -- an\\nanchor-based multi-local-coordinate system that uses a selected number of\\nanchors as the roots of these coordinates. Building on this foundation, we\\nintroduce LPA-GAN, a novel NeRF-based generative approach that incorporates\\nspecific modifications to estimate the priors of camera poses under LPA. It\\nalso co-optimizes the pose predictor and scene generation processes. Our\\nablation study and comparisons with straightforward extensions of NeRF-based\\nobject generative methods demonstrate the effectiveness of our approach.\\nFurthermore, visual comparisons with other techniques reveal that our method\\nachieves superior view-to-view consistency and semantic normality.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-03T07:18:48Z\"}"}

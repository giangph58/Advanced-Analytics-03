{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02666v1\", \"title\": \"BECAME: BayEsian Continual Learning with Adaptive Model MErging\", \"summary\": \"Continual Learning (CL) strives to learn incrementally across tasks while\\nmitigating catastrophic forgetting. A key challenge in CL is balancing\\nstability (retaining prior knowledge) and plasticity (learning new tasks).\\nWhile representative gradient projection methods ensure stability, they often\\nlimit plasticity. Model merging techniques offer promising solutions, but prior\\nmethods typically rely on empirical assumptions and carefully selected\\nhyperparameters. In this paper, we explore the potential of model merging to\\nenhance the stability-plasticity trade-off, providing theoretical insights that\\nunderscore its benefits. Specifically, we reformulate the merging mechanism\\nusing Bayesian continual learning principles and derive a closed-form solution\\nfor the optimal merging coefficient that adapts to the diverse characteristics\\nof tasks. To validate our approach, we introduce a two-stage framework named\\nBECAME, which synergizes the expertise of gradient projection and adaptive\\nmerging. Extensive experiments show that our approach outperforms\\nstate-of-the-art CL methods and existing merging strategies.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CV\", \"published\": \"2025-04-03T15:07:28Z\"}"}

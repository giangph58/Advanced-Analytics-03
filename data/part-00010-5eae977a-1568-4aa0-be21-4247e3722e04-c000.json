{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06122v1\", \"title\": \"Leanabell-Prover: Posttraining Scaling in Formal Reasoning\", \"summary\": \"Recent advances in automated theorem proving (ATP) through LLMs have\\nhighlighted the potential of formal reasoning with Lean 4 codes. However, ATP\\nhas not yet be revolutionized by the recent posttraining scaling as\\ndemonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the\\nentire posttraining of ATP, aiming to align it with breakthroughs in reasoning\\nmodels in natural languages.To begin, we continual train current ATP models\\nwith a hybrid dataset, which consists of numerous statement-proof pairs, and\\nadditional data aimed at incorporating cognitive behaviors that emulate human\\nreasoning and hypothesis refinement. Next, we explore reinforcement learning\\nwith the use of outcome reward returned by Lean 4 compiler. Through our\\ndesigned continual training and reinforcement learning processes, we have\\nsuccessfully improved existing formal provers, including both\\nDeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance\\nin the field of whole-proof generation. For example, we achieve a 59.8% pass\\nrate (pass@32) on MiniF2F. This is an on-going project and we will\\nprogressively update our findings, release our data and training details.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI\", \"published\": \"2025-04-08T15:15:26Z\"}"}

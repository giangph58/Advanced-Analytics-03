{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04950v1\", \"title\": \"A Unified Pairwise Framework for RLHF: Bridging Generative Reward\\n  Modeling and Policy Optimization\", \"summary\": \"Reinforcement Learning from Human Feedback (RLHF) has emerged as a important\\nparadigm for aligning large language models (LLMs) with human preferences\\nduring post-training. This framework typically involves two stages: first,\\ntraining a reward model on human preference data, followed by optimizing the\\nlanguage model using reinforcement learning algorithms. However, current RLHF\\napproaches may constrained by two limitations. First, existing RLHF frameworks\\noften rely on Bradley-Terry models to assign scalar rewards based on pairwise\\ncomparisons of individual responses. However, this approach imposes significant\\nchallenges on reward model (RM), as the inherent variability in prompt-response\\npairs across different contexts demands robust calibration capabilities from\\nthe RM. Second, reward models are typically initialized from generative\\nfoundation models, such as pre-trained or supervised fine-tuned models, despite\\nthe fact that reward models perform discriminative tasks, creating a mismatch.\\nThis paper introduces Pairwise-RL, a RLHF framework that addresses these\\nchallenges through a combination of generative reward modeling and a pairwise\\nproximal policy optimization (PPO) algorithm. Pairwise-RL unifies reward model\\ntraining and its application during reinforcement learning within a consistent\\npairwise paradigm, leveraging generative modeling techniques to enhance reward\\nmodel performance and score calibration. Experimental evaluations demonstrate\\nthat Pairwise-RL outperforms traditional RLHF frameworks across both internal\\nevaluation datasets and standard public benchmarks, underscoring its\\neffectiveness in improving alignment and model behavior.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-07T11:34:48Z\"}"}

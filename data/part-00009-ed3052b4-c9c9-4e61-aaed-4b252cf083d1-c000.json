{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07454v1\", \"title\": \"How Can Objects Help Video-Language Understanding?\", \"summary\": \"How multimodal large language models (MLLMs) perceive the visual world\\nremains a mystery. To one extreme, object and relation modeling may be\\nimplicitly implemented with inductive biases, for example by treating objects\\nas tokens. To the other extreme, empirical results reveal the surprising\\nfinding that simply performing visual captioning, which tends to ignore spatial\\nconfiguration of the objects, serves as a strong baseline for video\\nunderstanding. We aim to answer the question: how can objects help\\nvideo-language understanding in MLLMs? We tackle the question from the object\\nrepresentation and adaptation perspectives. Specifically, we investigate the\\ntrade-off between representation expressiveness (e.g., distributed versus\\nsymbolic) and integration difficulty (e.g., data-efficiency when learning the\\nadapters). Through extensive evaluations on five video question answering\\ndatasets, we confirm that explicit integration of object-centric representation\\nremains necessary, and the symbolic objects can be most easily integrated while\\nbeing performant for question answering. We hope our findings can encourage the\\ncommunity to explore the explicit integration of perception modules into MLLM\\ndesign. Our code and models will be publicly released.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-10T04:59:28Z\"}"}

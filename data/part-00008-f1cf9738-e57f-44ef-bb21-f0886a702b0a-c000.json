{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07934v1\", \"title\": \"SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual\\n  Reasoning Self-Improvement\", \"summary\": \"In this paper, we present an effective method to enhance visual reasoning\\nwith significantly fewer training samples, relying purely on self-improvement\\nwith no knowledge distillation. Our key insight is that the difficulty of\\ntraining data during reinforcement fine-tuning (RFT) is critical. Appropriately\\nchallenging samples can substantially boost reasoning capabilities even when\\nthe dataset is small. Despite being intuitive, the main challenge remains in\\naccurately quantifying sample difficulty to enable effective data filtering. To\\nthis end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS)\\nto achieve that. Starting from our curated 70k open-source training samples, we\\nintroduce an MCTS-based selection method that quantifies sample difficulty\\nbased on the number of iterations required by the VLMs to solve each problem.\\nThis explicit step-by-step reasoning in MCTS enforces the model to think longer\\nand better identifies samples that are genuinely challenging. We filter and\\nretain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our\\nfinal model, ThinkLite-VL. Evaluation results on eight benchmarks show that\\nThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%,\\nusing only 11k training samples with no knowledge distillation. This\\nsignificantly outperforms all existing 7B-level reasoning VLMs, and our fairly\\ncomparable baselines that use classic selection methods such as accuracy-based\\nfiltering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of\\n75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are\\navailable at https://github.com/si0wang/ThinkLite-VL.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-10T17:49:05Z\"}"}

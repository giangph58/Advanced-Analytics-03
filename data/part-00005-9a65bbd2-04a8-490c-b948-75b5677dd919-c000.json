{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06661v1\", \"title\": \"Domain-Conditioned Scene Graphs for State-Grounded Task Planning\", \"summary\": \"Recent robotic task planning frameworks have integrated large multimodal\\nmodels (LMMs) such as GPT-4V. To address grounding issues of such models, it\\nhas been suggested to split the pipeline into perceptional state grounding and\\nsubsequent state-based planning. As we show in this work, the state grounding\\nability of LMM-based approaches is still limited by weaknesses in granular,\\nstructured, domain-specific scene understanding. To address this shortcoming,\\nwe develop a more structured state grounding framework that features a\\ndomain-conditioned scene graph as its scene representation. We show that such\\nrepresentation is actionable in nature as it is directly mappable to a symbolic\\nstate in classical planning languages such as PDDL. We provide an instantiation\\nof our state grounding framework where the domain-conditioned scene graph\\ngeneration is implemented with a lightweight vision-language approach that\\nclassifies domain-specific predicates on top of domain-relevant object\\ndetections. Evaluated across three domains, our approach achieves significantly\\nhigher state estimation accuracy and task planning success rates compared to\\nthe previous LMM-based approaches.\", \"main_category\": \"cs.RO\", \"categories\": \"cs.RO\", \"published\": \"2025-04-09T07:51:46Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02287v1\", \"title\": \"MultiSensor-Home: A Wide-area Multi-modal Multi-view Dataset for Action\\n  Recognition and Transformer-based Sensor Fusion\", \"summary\": \"Multi-modal multi-view action recognition is a rapidly growing field in\\ncomputer vision, offering significant potential for applications in\\nsurveillance. However, current datasets often fail to address real-world\\nchallenges such as wide-area environmental conditions, asynchronous data\\nstreams, and the lack of frame-level annotations. Furthermore, existing methods\\nface difficulties in effectively modeling inter-view relationships and\\nenhancing spatial feature learning. In this study, we propose the Multi-modal\\nMulti-view Transformer-based Sensor Fusion (MultiTSF) method and introduce the\\nMultiSensor-Home dataset, a novel benchmark designed for comprehensive action\\nrecognition in home environments. The MultiSensor-Home dataset features\\nuntrimmed videos captured by distributed sensors, providing high-resolution RGB\\nand audio data along with detailed multi-view frame-level action labels. The\\nproposed MultiTSF method leverages a Transformer-based fusion mechanism to\\ndynamically model inter-view relationships. Furthermore, the method also\\nintegrates a external human detection module to enhance spatial feature\\nlearning. Experiments on MultiSensor-Home and MM-Office datasets demonstrate\\nthe superiority of MultiTSF over the state-of-the-art methods. The quantitative\\nand qualitative results highlight the effectiveness of the proposed method in\\nadvancing real-world multi-modal multi-view action recognition.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-03T05:23:08Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07448v1\", \"title\": \"LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation\", \"summary\": \"Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient\\nfine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs\\nnotable overhead and suffers from parameter interference in multi-task\\nscenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet\\neffective approach that freezes the projection matrices $A$ as random\\nprojections and sparsifies the matrices $B$ using task-specific masks. This\\ndesign substantially reduces the number of trainable parameters while\\nmaintaining strong task performance. Moreover, LoRI minimizes cross-task\\ninterference in adapter merging by leveraging the orthogonality between adapter\\nsubspaces, and supports continual learning by using sparsity to mitigate\\ncatastrophic forgetting. Extensive experiments across natural language\\nunderstanding, mathematical reasoning, code generation, and safety alignment\\ntasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT\\nmethods, while using up to 95% fewer trainable parameters than LoRA. In\\nmulti-task experiments, LoRI enables effective adapter merging and continual\\nlearning with reduced cross-task interference. Code is available at:\\nhttps://github.com/juzhengz/LoRI\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL\", \"published\": \"2025-04-10T04:46:04Z\"}"}

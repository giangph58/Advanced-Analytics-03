{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06939v1\", \"title\": \"FeedbackEval: A Benchmark for Evaluating Large Language Models in\\n  Feedback-Driven Code Repair Tasks\", \"summary\": \"Code repair is a fundamental task in software development, facilitating\\nefficient bug resolution and software maintenance. Although large language\\nmodels (LLMs) have demonstrated considerable potential in automated code\\nrepair, their ability to comprehend and effectively leverage diverse types of\\nfeedback remains insufficiently understood. To bridge this gap, we introduce\\nFeedbackEval, a systematic benchmark for evaluating LLMs' feedback\\ncomprehension and performance in code repair tasks. We conduct a comprehensive\\nempirical study on five state-of-the-art LLMs, including GPT-4o, Claude-3.5,\\nGemini-1.5, GLM-4, and Qwen2.5, to evaluate their behavior under both\\nsingle-iteration and iterative code repair settings. Our results show that\\nstructured feedback, particularly in the form of test feedback, leads to the\\nhighest repair success rates, while unstructured feedback proves significantly\\nless effective. Iterative feedback further enhances repair performance, though\\nthe marginal benefit diminishes after two or three rounds. Moreover, prompt\\nstructure is shown to be critical: incorporating docstrings, contextual\\ninformation, and explicit guidelines substantially improves outcomes, whereas\\npersona-based, chain-of-thought, and few-shot prompting strategies offer\\nlimited benefits in single-iteration scenarios. This work introduces a robust\\nbenchmark and delivers practical insights to advance the understanding and\\ndevelopment of feedback-driven code repair using LLMs.\", \"main_category\": \"cs.SE\", \"categories\": \"cs.SE\", \"published\": \"2025-04-09T14:43:08Z\"}"}

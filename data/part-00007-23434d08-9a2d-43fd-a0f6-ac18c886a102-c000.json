{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07863v1\", \"title\": \"Robust Hallucination Detection in LLMs via Adaptive Token Selection\", \"summary\": \"Hallucinations in large language models (LLMs) pose significant safety\\nconcerns that impede their broader deployment. Recent research in hallucination\\ndetection has demonstrated that LLMs' internal representations contain\\ntruthfulness hints, which can be harnessed for detector training. However, the\\nperformance of these detectors is heavily dependent on the internal\\nrepresentations of predetermined tokens, fluctuating considerably when working\\non free-form generations with varying lengths and sparse distributions of\\nhallucinated entities. To address this, we propose HaMI, a novel approach that\\nenables robust detection of hallucinations through adaptive selection and\\nlearning of critical tokens that are most indicative of hallucinations. We\\nachieve this robustness by an innovative formulation of the Hallucination\\ndetection task as Multiple Instance (HaMI) learning over token-level\\nrepresentations within a sequence, thereby facilitating a joint optimisation of\\ntoken selection and hallucination detection on generation sequences of diverse\\nforms. Comprehensive experimental results on four hallucination benchmarks show\\nthat HaMI significantly outperforms existing state-of-the-art approaches.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-10T15:39:10Z\"}"}

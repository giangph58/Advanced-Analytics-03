{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05902v1\", \"title\": \"Defending Deep Neural Networks against Backdoor Attacks via Module\\n  Switching\", \"summary\": \"The exponential increase in the parameters of Deep Neural Networks (DNNs) has\\nsignificantly raised the cost of independent training, particularly for\\nresource-constrained entities. As a result, there is a growing reliance on\\nopen-source models. However, the opacity of training processes exacerbates\\nsecurity risks, making these models more vulnerable to malicious threats, such\\nas backdoor attacks, while simultaneously complicating defense mechanisms.\\nMerging homogeneous models has gained attention as a cost-effective\\npost-training defense. However, we notice that existing strategies, such as\\nweight averaging, only partially mitigate the influence of poisoned parameters\\nand remain ineffective in disrupting the pervasive spurious correlations\\nembedded across model parameters. We propose a novel module-switching strategy\\nto break such spurious correlations within the model's propagation path. By\\nleveraging evolutionary algorithms to optimize fusion strategies, we validate\\nour approach against backdoor attacks targeting text and vision domains. Our\\nmethod achieves effective backdoor mitigation even when incorporating a couple\\nof compromised models, e.g., reducing the average attack success rate (ASR) to\\n22% compared to 31.9% with the best-performing baseline on SST-2.\", \"main_category\": \"cs.CR\", \"categories\": \"cs.CR,cs.CL\", \"published\": \"2025-04-08T11:01:07Z\"}"}

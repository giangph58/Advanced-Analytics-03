{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02508v1\", \"title\": \"APHQ-ViT: Post-Training Quantization with Average Perturbation Hessian\\n  Based Reconstruction for Vision Transformers\", \"summary\": \"Vision Transformers (ViTs) have become one of the most commonly used\\nbackbones for vision tasks. Despite their remarkable performance, they often\\nsuffer significant accuracy drops when quantized for practical deployment,\\nparticularly by post-training quantization (PTQ) under ultra-low bits.\\nRecently, reconstruction-based PTQ methods have shown promising performance in\\nquantizing Convolutional Neural Networks (CNNs). However, they fail when\\napplied to ViTs, primarily due to the inaccurate estimation of output\\nimportance and the substantial accuracy degradation in quantizing post-GELU\\nactivations. To address these issues, we propose \\\\textbf{APHQ-ViT}, a novel PTQ\\napproach based on importance estimation with Average Perturbation Hessian\\n(APH). Specifically, we first thoroughly analyze the current approximation\\napproaches with Hessian loss, and propose an improved average perturbation\\nHessian loss. To deal with the quantization of the post-GELU activations, we\\ndesign an MLP Reconstruction (MR) method by replacing the GELU function in MLP\\nwith ReLU and reconstructing it by the APH loss on a small unlabeled\\ncalibration set. Extensive experiments demonstrate that APHQ-ViT using linear\\nquantizers outperforms existing PTQ methods by substantial margins in 3-bit and\\n4-bit across different vision tasks. The source code is available at\\nhttps://github.com/GoatWu/APHQ-ViT.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-03T11:48:56Z\"}"}

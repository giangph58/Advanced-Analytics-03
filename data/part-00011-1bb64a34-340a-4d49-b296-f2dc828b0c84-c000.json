{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02436v1\", \"title\": \"SkyReels-A2: Compose Anything in Video Diffusion Transformers\", \"summary\": \"This paper presents SkyReels-A2, a controllable video generation framework\\ncapable of assembling arbitrary visual elements (e.g., characters, objects,\\nbackgrounds) into synthesized videos based on textual prompts while maintaining\\nstrict consistency with reference images for each element. We term this task\\nelements-to-video (E2V), whose primary challenges lie in preserving the\\nfidelity of each reference element, ensuring coherent composition of the scene,\\nand achieving natural outputs. To address these, we first design a\\ncomprehensive data pipeline to construct prompt-reference-video triplets for\\nmodel training. Next, we propose a novel image-text joint embedding model to\\ninject multi-element representations into the generative process, balancing\\nelement-specific consistency with global coherence and text alignment. We also\\noptimize the inference pipeline for both speed and output stability. Moreover,\\nwe introduce a carefully curated benchmark for systematic evaluation, i.e, A2\\nBench. Experiments demonstrate that our framework can generate diverse,\\nhigh-quality videos with precise element control. SkyReels-A2 is the first\\nopen-source commercial grade model for the generation of E2V, performing\\nfavorably against advanced closed-source commercial models. We anticipate\\nSkyReels-A2 will advance creative applications such as drama and virtual\\ne-commerce, pushing the boundaries of controllable video generation.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-03T09:50:50Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24067v1\", \"title\": \"TransMamba: Flexibly Switching between Transformer and Mamba\", \"summary\": \"Transformers are the cornerstone of modern large language models, but their\\nquadratic computational complexity limits efficiency in long-sequence\\nprocessing. Recent advancements in Mamba, a state space model (SSM) with linear\\ncomplexity, offer promising efficiency gains but suffer from unstable\\ncontextual learning and multitask generalization. This paper proposes\\nTransMamba, a novel framework that unifies Transformer and Mamba through shared\\nparameter matrices (e.g., QKV and CBx), and thus could dynamically switch\\nbetween attention and SSM mechanisms at different token lengths and layers. We\\ndesign the Memory converter to bridge Transformer and Mamba by converting\\nattention outputs into SSM-compatible states, ensuring seamless information\\nflow at TransPoints where the transformation happens. The TransPoint scheduling\\nis also thoroughly explored for further improvements. We conducted extensive\\nexperiments demonstrating that TransMamba achieves superior training efficiency\\nand performance compared to baselines, and validated the deeper consistency\\nbetween Transformer and Mamba paradigms, offering a scalable solution for\\nnext-generation sequence modeling.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-03-31T13:26:24Z\"}"}

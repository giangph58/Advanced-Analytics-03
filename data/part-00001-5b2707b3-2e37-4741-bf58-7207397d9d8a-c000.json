{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24370v1\", \"title\": \"Effectively Controlling Reasoning Models through Thinking Intervention\", \"summary\": \"Reasoning-enhanced large language models (LLMs) explicitly generate\\nintermediate reasoning steps prior to generating final answers, helping the\\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\\nemerging generation framework offers a unique opportunity for more fine-grained\\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\\ndesigned to explicitly guide the internal reasoning processes of LLMs by\\nstrategically inserting or revising specific thinking tokens. We conduct\\ncomprehensive evaluations across multiple tasks, including instruction\\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\\naccuracy gains in instruction-following scenarios, 15.4% improvements in\\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\\nopens a promising new research avenue for controlling reasoning LLMs.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL\", \"published\": \"2025-03-31T17:50:13Z\"}"}

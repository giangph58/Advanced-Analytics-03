{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02536v1\", \"title\": \"A Sensorimotor Vision Transformer\", \"summary\": \"This paper presents the Sensorimotor Transformer (SMT), a vision model\\ninspired by human saccadic eye movements that prioritize high-saliency regions\\nin visual input to enhance computational efficiency and reduce memory\\nconsumption. Unlike traditional models that process all image patches\\nuniformly, SMT identifies and selects the most salient patches based on\\nintrinsic two-dimensional (i2D) features, such as corners and occlusions, which\\nare known to convey high-information content and align with human fixation\\npatterns. The SMT architecture uses this biological principle to leverage\\nvision transformers to process only the most informative patches, allowing for\\na substantial reduction in memory usage that scales with the sequence length of\\nselected patches. This approach aligns with visual neuroscience findings,\\nsuggesting that the human visual system optimizes information gathering through\\nselective, spatially dynamic focus. Experimental evaluations on Imagenet-1k\\ndemonstrate that SMT achieves competitive top-1 accuracy while significantly\\nreducing memory consumption and computational complexity, particularly when a\\nlimited number of patches is used. This work introduces a saccade-like\\nselection mechanism into transformer-based vision models, offering an efficient\\nalternative for image analysis and providing new insights into biologically\\nmotivated architectures for resource-constrained applications.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-03T12:37:44Z\"}"}

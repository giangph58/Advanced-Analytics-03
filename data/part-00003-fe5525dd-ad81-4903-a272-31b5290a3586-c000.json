{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02328v1\", \"title\": \"Refining CLIP's Spatial Awareness: A Visual-Centric Perspective\", \"summary\": \"Contrastive Language-Image Pre-training (CLIP) excels in global alignment\\nwith language but exhibits limited sensitivity to spatial information, leading\\nto strong performance in zero-shot classification tasks but underperformance in\\ntasks requiring precise spatial understanding. Recent approaches have\\nintroduced Region-Language Alignment (RLA) to enhance CLIP's performance in\\ndense multimodal tasks by aligning regional visual representations with\\ncorresponding text inputs. However, we find that CLIP ViTs fine-tuned with RLA\\nsuffer from notable loss in spatial awareness, which is crucial for dense\\nprediction tasks. To address this, we propose the Spatial Correlation\\nDistillation (SCD) framework, which preserves CLIP's inherent spatial structure\\nand mitigates the above degradation. To further enhance spatial correlations,\\nwe introduce a lightweight Refiner that extracts refined correlations directly\\nfrom CLIP before feeding them into SCD, based on an intriguing finding that\\nCLIP naturally captures high-quality dense features. Together, these components\\nform a robust distillation framework that enables CLIP ViTs to integrate both\\nvisual-language and visual-centric improvements, achieving state-of-the-art\\nresults across various open-vocabulary dense prediction benchmarks.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-03T07:04:56Z\"}"}

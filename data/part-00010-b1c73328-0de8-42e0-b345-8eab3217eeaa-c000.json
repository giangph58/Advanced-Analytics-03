{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01789v1\", \"title\": \"OpenThaiGPT 1.6 and R1: Thai-Centric Open Source and Reasoning Large\\n  Language Models\", \"summary\": \"We present OpenThaiGPT 1.6 and R1 (OTG-1.6 and OTG-R1), Thai-centric Large\\nLanguage Models (LLMs) developed through distinct methodologies to enhance\\ngeneralization and reasoning capabilities. OTG-1.6 employs Task Arithmetic\\nmodel merging for broad generalization, while OTG-R1 integrates multi-stage\\ntraining with the Less-Is-More Reasoning Hypothesis (LIMO) for advanced\\nreasoning. Benchmark evaluations demonstrate superior performance across Thai\\nlanguage tasks, achieving competitive results against larger-scale open-source\\nThai LLMs. This paper details the proposed models, training processes,\\nbenchmarks, and results, highlighting improvements over previous models and\\nestablishing new performance standards for Thai-centric LLMs.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-02T14:55:52Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23786v1\", \"title\": \"MGD-SAM2: Multi-view Guided Detail-enhanced Segment Anything Model 2 for\\n  High-Resolution Class-agnostic Segmentation\", \"summary\": \"Segment Anything Models (SAMs), as vision foundation models, have\\ndemonstrated remarkable performance across various image analysis tasks.\\nDespite their strong generalization capabilities, SAMs encounter challenges in\\nfine-grained detail segmentation for high-resolution class-independent\\nsegmentation (HRCS), due to the limitations in the direct processing of\\nhigh-resolution inputs and low-resolution mask predictions, and the reliance on\\naccurate manual prompts. To address these limitations, we propose MGD-SAM2\\nwhich integrates SAM2 with multi-view feature interaction between a global\\nimage and local patches to achieve precise segmentation. MGD-SAM2 incorporates\\nthe pre-trained SAM2 with four novel modules: the Multi-view Perception Adapter\\n(MPAdapter), the Multi-view Complementary Enhancement Module (MCEM), the\\nHierarchical Multi-view Interaction Module (HMIM), and the Detail Refinement\\nModule (DRM). Specifically, we first introduce MPAdapter to adapt the SAM2\\nencoder for enhanced extraction of local details and global semantics in HRCS\\nimages. Then, MCEM and HMIM are proposed to further exploit local texture and\\nglobal context by aggregating multi-view features within and across\\nmulti-scales. Finally, DRM is designed to generate gradually restored\\nhigh-resolution mask predictions, compensating for the loss of fine-grained\\ndetails resulting from directly upsampling the low-resolution prediction maps.\\nExperimental results demonstrate the superior performance and strong\\ngeneralization of our model on multiple high-resolution and normal-resolution\\ndatasets. Code will be available at https://github.com/sevenshr/MGD-SAM2.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-03-31T07:02:32Z\"}"}

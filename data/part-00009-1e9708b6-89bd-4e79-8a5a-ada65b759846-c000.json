{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01468v1\", \"title\": \"HH-PIM: Dynamic Optimization of Power and Performance with\\n  Heterogeneous-Hybrid PIM for Edge AI Devices\", \"summary\": \"Processing-in-Memory (PIM) architectures offer promising solutions for\\nefficiently handling AI applications in energy-constrained edge environments.\\nWhile traditional PIM designs enhance performance and energy efficiency by\\nreducing data movement between memory and processing units, they are limited in\\nedge devices due to continuous power demands and the storage requirements of\\nlarge neural network weights in SRAM and DRAM. Hybrid PIM architectures,\\nincorporating non-volatile memories like MRAM and ReRAM, mitigate these\\nlimitations but struggle with a mismatch between fixed computing resources and\\ndynamically changing inference workloads. To address these challenges, this\\nstudy introduces a Heterogeneous-Hybrid PIM (HH-PIM) architecture, comprising\\nhigh-performance MRAM-SRAM PIM modules and low-power MRAM-SRAM PIM modules. We\\nfurther propose a data placement optimization algorithm that dynamically\\nallocates data based on computational demand, maximizing energy efficiency.\\nFPGA prototyping and power simulations with processors featuring HH-PIM and\\nother PIM types demonstrate that the proposed HH-PIM achieves up to $60.43$\\npercent average energy savings over conventional PIMs while meeting application\\nlatency requirements. These results confirm the suitability of HH-PIM for\\nadaptive, energy-efficient AI processing in edge devices.\", \"main_category\": \"cs.AR\", \"categories\": \"cs.AR,cs.AI\", \"published\": \"2025-04-02T08:22:32Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01764v1\", \"title\": \"Dual-stream Transformer-GCN Model with Contextualized Representations\\n  Learning for Monocular 3D Human Pose Estimation\", \"summary\": \"This paper introduces a novel approach to monocular 3D human pose estimation\\nusing contextualized representation learning with the Transformer-GCN\\ndual-stream model. Monocular 3D human pose estimation is challenged by depth\\nambiguity, limited 3D-labeled training data, imbalanced modeling, and\\nrestricted model generalization. To address these limitations, our work\\nintroduces a groundbreaking motion pre-training method based on contextualized\\nrepresentation learning. Specifically, our method involves masking 2D pose\\nfeatures and utilizing a Transformer-GCN dual-stream model to learn\\nhigh-dimensional representations through a self-distillation setup. By focusing\\non contextualized representation learning and spatial-temporal modeling, our\\napproach enhances the model's ability to understand spatial-temporal\\nrelationships between postures, resulting in superior generalization.\\nFurthermore, leveraging the Transformer-GCN dual-stream model, our approach\\neffectively balances global and local interactions in video pose estimation.\\nThe model adaptively integrates information from both the Transformer and GCN\\nstreams, where the GCN stream effectively learns local relationships between\\nadjacent key points and frames, while the Transformer stream captures\\ncomprehensive global spatial and temporal features. Our model achieves\\nstate-of-the-art performance on two benchmark datasets, with an MPJPE of 38.0mm\\nand P-MPJPE of 31.9mm on Human3.6M, and an MPJPE of 15.9mm on MPI-INF-3DHP.\\nFurthermore, visual experiments on public datasets and in-the-wild videos\\ndemonstrate the robustness and generalization capabilities of our approach.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV,cs.AI\", \"published\": \"2025-04-02T14:17:57Z\"}"}

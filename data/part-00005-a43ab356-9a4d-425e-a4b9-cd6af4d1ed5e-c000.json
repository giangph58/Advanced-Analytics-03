{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07624v1\", \"title\": \"ConceptFormer: Towards Efficient Use of Knowledge-Graph Embeddings in\\n  Large Language Models\", \"summary\": \"Retrieval Augmented Generation (RAG) has enjoyed increased attention in the\\nrecent past and recent advancements in Large Language Models (LLMs) have\\nhighlighted the importance of integrating world knowledge into these systems.\\nCurrent RAG methodologies often modify the internal architecture of pre-trained\\nlanguage models (PLMs) or rely on textifying knowledge graphs (KGs), which is\\ninefficient in terms of token usage. This paper introduces ConceptFormer, a new\\napproach to augment LLMs with structured knowledge from KGs, such as Wikidata,\\nwithout altering their internal structure or relying on textual input of KGs.\\nConceptFormer operates in the LLM embedding vector space, creating and\\ninjecting \\\\emph{concept vectors} that encapsulate the information of the KG\\nnodes directly. Trained in conjunction with a frozen LLM, ConceptFormer\\ngenerates a comprehensive lookup table that maps KG nodes to their respective\\nconcept vectors. The approach aims to enhance the factual recall capabilities\\nof LLMs by enabling them to process these concept vectors natively, thus\\nenriching them with structured world knowledge in an efficient and scalable\\nmanner. Our experiments demonstrate that the addition of concept vectors to\\nGPT-2 0.1B substantially increases its factual recall ability (Hit@10) by up to\\n272\\\\% when tested on sentences from Wikipedia and up to 348\\\\% on synthetically\\ngenerated sentences. Even injecting only a single concept vector into the\\nprompt increases factual recall ability (Hit@10) by up to 213\\\\% on Wikipedia\\nsentences, significantly outperforming RAG with graph textification while\\nconsuming 130x fewer input tokens.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.IR\", \"published\": \"2025-04-10T10:17:08Z\"}"}

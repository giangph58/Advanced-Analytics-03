{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07835v1\", \"title\": \"Pychop: Emulating Low-Precision Arithmetic in Numerical Methods and\\n  Neural Networks\", \"summary\": \"Motivated by the growing demand for low-precision arithmetic in computational\\nscience, we exploit lower-precision emulation in Python -- widely regarded as\\nthe dominant programming language for numerical analysis and machine learning.\\nLow-precision training has revolutionized deep learning by enabling more\\nefficient computation and reduced memory and energy consumption while\\nmaintaining model fidelity. To better enable numerical experimentation with and\\nexploration of low precision computation, we developed the Pychop library,\\nwhich supports customizable floating-point formats and a comprehensive set of\\nrounding modes in Python, allowing users to benefit from fast, low-precision\\nemulation in numerous applications. Pychop also introduces interfaces for both\\nPyTorch and JAX, enabling efficient low-precision emulation on GPUs for neural\\nnetwork training and inference with unparalleled flexibility.\\n  In this paper, we offer a comprehensive exposition of the design,\\nimplementation, validation, and practical application of Pychop, establishing\\nit as a foundational tool for advancing efficient mixed-precision algorithms.\\nFurthermore, we present empirical results on low-precision emulation for image\\nclassification and object detection using published datasets, illustrating the\\nsensitivity of the use of low precision and offering valuable insights into its\\nimpact. Pychop enables in-depth investigations into the effects of numerical\\nprecision, facilitates the development of novel hardware accelerators, and\\nintegrates seamlessly into existing deep learning workflows. Software and\\nexperimental code are publicly available at\\nhttps://github.com/inEXASCALE/pychop.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.NA,math.NA\", \"published\": \"2025-04-10T15:12:29Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02495v1\", \"title\": \"Inference-Time Scaling for Generalist Reward Modeling\", \"summary\": \"Reinforcement learning (RL) has been widely adopted in post-training for\\nlarge language models (LLMs) at scale. Recently, the incentivization of\\nreasoning capabilities in LLMs from RL indicates that $\\\\textit{proper learning\\nmethods could enable effective inference-time scalability}$. A key challenge of\\nRL is to obtain accurate reward signals for LLMs in various domains beyond\\nverifiable questions or artificial rules. In this work, we investigate how to\\nimprove reward modeling (RM) with more inference compute for general queries,\\ni.e. the $\\\\textbf{inference-time scalability of generalist RM}$, and further,\\nhow to improve the effectiveness of performance-compute scaling with proper\\nlearning methods. For the RM approach, we adopt pointwise generative reward\\nmodeling (GRM) to enable flexibility for different input types and potential\\nfor inference-time scaling. For the learning method, we propose Self-Principled\\nCritique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs\\nthrough online RL, to generate principles adaptively and critiques accurately,\\nresulting in $\\\\textbf{DeepSeek-GRM}$ models. Furthermore, for effective\\ninference-time scaling, we use parallel sampling to expand compute usage, and\\nintroduce a meta RM to guide voting process for better scaling performance.\\nEmpirically, we show that SPCT significantly improves the quality and\\nscalability of GRMs, outperforming existing methods and models in various RM\\nbenchmarks without severe biases, and could achieve better performance compared\\nto training-time scaling. DeepSeek-GRM still meets challenges in some tasks,\\nwhich we believe can be addressed by future efforts in generalist reward\\nsystems. The models will be released and open-sourced.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI,cs.LG\", \"published\": \"2025-04-03T11:19:49Z\"}"}

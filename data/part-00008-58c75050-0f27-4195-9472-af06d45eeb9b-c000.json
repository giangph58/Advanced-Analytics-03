{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02725v1\", \"title\": \"ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference\\n  Optimization\", \"summary\": \"Recent advancements in large language models (LLMs) have accelerated progress\\ntoward artificial general intelligence, yet their potential to generate harmful\\ncontent poses critical safety challenges. Existing alignment methods often\\nstruggle to cover diverse safety scenarios and remain vulnerable to adversarial\\nattacks. In this work, we propose Ex-Ante Reasoning Preference Optimization\\n(ERPO), a novel safety alignment framework that equips LLMs with explicit\\npreemptive reasoning through Chain-of-Thought and provides clear evidence for\\nsafety judgments by embedding predefined safety rules. Specifically, our\\napproach consists of three stages: first, equipping the model with Ex-Ante\\nreasoning through supervised fine-tuning (SFT) using a constructed reasoning\\nmodule; second, enhancing safety, usefulness, and efficiency via Direct\\nPreference Optimization (DPO); and third, mitigating inference latency with a\\nlength-controlled iterative preference optimization strategy. Experiments on\\nmultiple open-source LLMs demonstrate that ERPO significantly enhances safety\\nperformance while maintaining response efficiency.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-03T16:07:38Z\"}"}

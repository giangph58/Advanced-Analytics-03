{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06193v1\", \"title\": \"Heuristic Methods are Good Teachers to Distill MLPs for Graph Link\\n  Prediction\", \"summary\": \"Link prediction is a crucial graph-learning task with applications including\\ncitation prediction and product recommendation. Distilling Graph Neural\\nNetworks (GNNs) teachers into Multi-Layer Perceptrons (MLPs) students has\\nemerged as an effective approach to achieve strong performance and reducing\\ncomputational cost by removing graph dependency. However, existing distillation\\nmethods only use standard GNNs and overlook alternative teachers such as\\nspecialized model for link prediction (GNN4LP) and heuristic methods (e.g.,\\ncommon neighbors). This paper first explores the impact of different teachers\\nin GNN-to-MLP distillation. Surprisingly, we find that stronger teachers do not\\nalways produce stronger students: MLPs distilled from GNN4LP can underperform\\nthose distilled from simpler GNNs, while weaker heuristic methods can teach\\nMLPs to near-GNN performance with drastically reduced training costs. Building\\non these insights, we propose Ensemble Heuristic-Distilled MLPs (EHDM), which\\neliminates graph dependencies while effectively integrating complementary\\nsignals via a gating mechanism. Experiments on ten datasets show an average\\n7.93% improvement over previous GNN-to-MLP approaches with 1.95-3.32 times less\\ntraining time, indicating EHDM is an efficient and effective link prediction\\nmethod.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-08T16:35:11Z\"}"}

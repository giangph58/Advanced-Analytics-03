{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05646v1\", \"title\": \"Lattice: Learning to Efficiently Compress the Memory\", \"summary\": \"Attention mechanisms have revolutionized sequence learning but suffer from\\nquadratic computational complexity. This paper introduces Lattice, a novel\\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\\nstructure of K-V matrices to efficiently compress the cache into a fixed number\\nof memory slots, achieving sub-quadratic complexity. We formulate this\\ncompression as an online optimization problem and derive a dynamic memory\\nupdate rule based on a single gradient descent step. The resulting recurrence\\nfeatures a state- and input-dependent gating mechanism, offering an\\ninterpretable memory update process. The core innovation is the orthogonal\\nupdate: each memory slot is updated exclusively with information orthogonal to\\nits current state hence incorporation of only novel, non-redundant data, which\\nminimizes the interference with previously stored information. The experimental\\nresults show that Lattice achieves the best perplexity compared to all\\nbaselines across diverse context lengths, with performance improvement becoming\\nmore pronounced as the context length increases.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI\", \"published\": \"2025-04-08T03:48:43Z\"}"}

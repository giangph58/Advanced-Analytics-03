{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01739v1\", \"title\": \"Understanding Cross-Model Perceptual Invariances Through Ensemble\\n  Metamers\", \"summary\": \"Understanding the perceptual invariances of artificial neural networks is\\nessential for improving explainability and aligning models with human vision.\\nMetamers - stimuli that are physically distinct yet produce identical neural\\nactivations - serve as a valuable tool for investigating these invariances. We\\nintroduce a novel approach to metamer generation by leveraging ensembles of\\nartificial neural networks, capturing shared representational subspaces across\\ndiverse architectures, including convolutional neural networks and vision\\ntransformers. To characterize the properties of the generated metamers, we\\nemploy a suite of image-based metrics that assess factors such as semantic\\nfidelity and naturalness. Our findings show that convolutional neural networks\\ngenerate more recognizable and human-like metamers, while vision transformers\\nproduce realistic but less transferable metamers, highlighting the impact of\\narchitectural biases on representational invariances.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-02T13:51:19Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04823v1\", \"title\": \"Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\\n  Models\", \"summary\": \"Recent advancements in reasoning language models have demonstrated remarkable\\nperformance in complex tasks, but their extended chain-of-thought reasoning\\nprocess increases inference overhead. While quantization has been widely\\nadopted to reduce the inference cost of large language models, its impact on\\nreasoning models remains understudied. In this study, we conduct the first\\nsystematic study on quantized reasoning models, evaluating the open-sourced\\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\\nactivation quantization using state-of-the-art algorithms at varying\\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\\nfindings reveal that while lossless quantization can be achieved with W8A8 or\\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\\nfurther identify model size, model origin, and task difficulty as critical\\ndeterminants of performance. Contrary to expectations, quantized models do not\\nexhibit increased output lengths. In addition, strategically scaling the model\\nsizes or reasoning steps can effectively enhance the performance. All quantized\\nmodels and codes will be open-sourced in\\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-04-07T08:22:45Z\"}"}

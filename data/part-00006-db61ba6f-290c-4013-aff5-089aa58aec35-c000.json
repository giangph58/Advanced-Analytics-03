{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07521v1\", \"title\": \"Why We Feel: Breaking Boundaries in Emotional Reasoning with Multimodal\\n  Large Language Models\", \"summary\": \"Most existing emotion analysis emphasizes which emotion arises (e.g., happy,\\nsad, angry) but neglects the deeper why. We propose Emotion Interpretation\\n(EI), focusing on causal factors-whether explicit (e.g., observable objects,\\ninterpersonal interactions) or implicit (e.g., cultural context, off-screen\\nevents)-that drive emotional responses. Unlike traditional emotion recognition,\\nEI tasks require reasoning about triggers instead of mere labeling. To\\nfacilitate EI research, we present EIBench, a large-scale benchmark\\nencompassing 1,615 basic EI samples and 50 complex EI samples featuring\\nmultifaceted emotions. Each instance demands rationale-based explanations\\nrather than straightforward categorization. We further propose a Coarse-to-Fine\\nSelf-Ask (CFSA) annotation pipeline, which guides Vision-Language Models\\n(VLLMs) through iterative question-answer rounds to yield high-quality labels\\nat scale. Extensive evaluations on open-source and proprietary large language\\nmodels under four experimental settings reveal consistent performance\\ngaps-especially for more intricate scenarios-underscoring EI's potential to\\nenrich empathetic, context-aware AI applications. Our benchmark and methods are\\npublicly available at: https://github.com/Lum1104/EIBench, offering a\\nfoundation for advanced multimodal causal analysis and next-generation\\naffective computing.\", \"main_category\": \"cs.AI\", \"categories\": \"cs.AI,cs.MM\", \"published\": \"2025-04-10T07:33:49Z\"}"}

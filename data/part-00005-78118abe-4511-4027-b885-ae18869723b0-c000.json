{"value":"{\"aid\": \"http://arxiv.org/abs/2504.01337v1\", \"title\": \"Advancing MoE Efficiency: A Collaboration-Constrained Routing (C2R)\\n  Strategy for Better Expert Parallelism Design\", \"summary\": \"Mixture-of-Experts (MoE) has successfully scaled up models while maintaining\\nnearly constant computing costs. By employing a gating network to route input\\ntokens, it selectively activates a subset of expert networks to process the\\ncorresponding token embeddings. However, in practice, the efficiency of MoE is\\nchallenging to achieve due to two key reasons: imbalanced expert activation,\\nwhich leads to substantial idle time during model or expert parallelism, and\\ninsufficient capacity utilization; massive communication overhead, induced by\\nnumerous expert routing combinations in expert parallelism at the system level.\\nPrevious works typically formulate it as the load imbalance issue characterized\\nby the gating network favoring certain experts over others or attribute it to\\nstatic execution which fails to adapt to the dynamic expert workload at\\nruntime. In this paper, we exploit it from a brand new perspective, a\\nhigher-order view and analysis of MoE routing policies: expert collaboration\\nand specialization where some experts tend to activate broadly with others\\n(collaborative), while others are more likely to activate only with a specific\\nsubset of experts (specialized). Our experiments reveal that most experts tend\\nto be overly collaborative, leading to increased communication overhead from\\nrepeatedly sending tokens to different accelerators. To this end, we propose a\\nnovel collaboration-constrained routing (C2R) strategy to encourage more\\nspecialized expert groups, as well as to improve expert utilization, and\\npresent an efficient implementation of MoE that further leverages expert\\nspecialization. We achieve an average performance improvement of 0.51% and\\n0.33% on LLaMA-MoE and Qwen-MoE respectively across ten downstream NLP\\nbenchmarks, and reduce the all2all communication costs between GPUs, bringing\\nan extra 20%-30% total running time savings on top of the existing SoTA, i.e.\\nMegaBlocks.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.AI,cs.CL\", \"published\": \"2025-04-02T03:51:59Z\"}"}

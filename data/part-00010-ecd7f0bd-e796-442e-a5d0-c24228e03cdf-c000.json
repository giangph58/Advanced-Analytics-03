{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24296v1\", \"title\": \"Fair Dynamic Spectrum Access via Fully Decentralized Multi-Agent\\n  Reinforcement Learning\", \"summary\": \"We consider a decentralized wireless network with several source-destination\\npairs sharing a limited number of orthogonal frequency bands. Sources learn to\\nadapt their transmissions (specifically, their band selection strategy) over\\ntime, in a decentralized manner, without sharing information with each other.\\nSources can only observe the outcome of their own transmissions (i.e., success\\nor collision), having no prior knowledge of the network size or of the\\ntransmission strategy of other sources. The goal of each source is to maximize\\ntheir own throughput while striving for network-wide fairness. We propose a\\nnovel fully decentralized Reinforcement Learning (RL)-based solution that\\nachieves fairness without coordination. The proposed Fair Share RL (FSRL)\\nsolution combines: (i) state augmentation with a semi-adaptive time reference;\\n(ii) an architecture that leverages risk control and time difference\\nlikelihood; and (iii) a fairness-driven reward structure. We evaluate FSRL in\\nmore than 50 network settings with different number of agents, different\\namounts of available spectrum, in the presence of jammers, and in an ad-hoc\\nsetting. Simulation results suggest that, when we compare FSRL with a common\\nbaseline RL algorithm from the literature, FSRL can be up to 89.0% fairer (as\\nmeasured by Jain's fairness index) in stringent settings with several sources\\nand a single frequency band, and 48.1% fairer on average.\", \"main_category\": \"cs.NI\", \"categories\": \"cs.NI,cs.LG\", \"published\": \"2025-03-31T16:42:11Z\"}"}

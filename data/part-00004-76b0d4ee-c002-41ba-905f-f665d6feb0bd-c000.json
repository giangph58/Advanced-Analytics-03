{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04966v1\", \"title\": \"Few Dimensions are Enough: Fine-tuning BERT with Selected Dimensions\\n  Revealed Its Redundant Nature\", \"summary\": \"When fine-tuning BERT models for specific tasks, it is common to select part\\nof the final layer's output and input it into a newly created fully connected\\nlayer. However, it remains unclear which part of the final layer should be\\nselected and what information each dimension of the layers holds. In this\\nstudy, we comprehensively investigated the effectiveness and redundancy of\\ntoken vectors, layers, and dimensions through BERT fine-tuning on GLUE tasks.\\nThe results showed that outputs other than the CLS vector in the final layer\\ncontain equivalent information, most tasks require only 2-3 dimensions, and\\nwhile the contribution of lower layers decreases, there is little difference\\namong higher layers. We also evaluated the impact of freezing pre-trained\\nlayers and conducted cross-fine-tuning, where fine-tuning is applied\\nsequentially to different tasks. The findings suggest that hidden layers may\\nchange significantly during fine-tuning, BERT has considerable redundancy,\\nenabling it to handle multiple tasks simultaneously, and its number of\\ndimensions may be excessive.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-07T11:53:16Z\"}"}

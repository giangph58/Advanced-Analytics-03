{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05294v1\", \"title\": \"Truthful or Fabricated? Using Causal Attribution to Mitigate Reward\\n  Hacking in Explanations\", \"summary\": \"Chain-of-thought explanations are widely used to inspect the decision process\\nof large language models (LLMs) and to evaluate the trustworthiness of model\\noutputs, making them important for effective collaboration between LLMs and\\nhumans. We demonstrate that preference optimization - a key step in the\\nalignment phase - can inadvertently reduce the faithfulness of these\\nexplanations. This occurs because the reward model (RM), which guides\\nalignment, is tasked with optimizing both the expected quality of the response\\nand the appropriateness of the explanations (e.g., minimizing bias or adhering\\nto safety standards), creating potential conflicts. The RM lacks a mechanism to\\nassess the consistency between the model's internal decision process and the\\ngenerated explanation. Consequently, the LLM may engage in \\\"reward hacking\\\" by\\nproducing a final response that scores highly while giving an explanation\\ntailored to maximize reward rather than accurately reflecting its reasoning. To\\naddress this issue, we propose enriching the RM's input with a causal\\nattribution of the prediction, allowing the RM to detect discrepancies between\\nthe generated self-explanation and the model's decision process. In controlled\\nsettings, we show that this approach reduces the tendency of the LLM to\\ngenerate misleading explanations.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-07T17:49:23Z\"}"}

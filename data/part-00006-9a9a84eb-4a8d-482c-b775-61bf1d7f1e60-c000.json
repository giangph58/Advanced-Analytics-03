{"value":"{\"aid\": \"http://arxiv.org/abs/2504.07070v1\", \"title\": \"A Survey on Personalized and Pluralistic Preference Alignment in Large\\n  Language Models\", \"summary\": \"Personalized preference alignment for large language models (LLMs), the\\nprocess of tailoring LLMs to individual users' preferences, is an emerging\\nresearch direction spanning the area of NLP and personalization. In this\\nsurvey, we present an analysis of works on personalized alignment and modeling\\nfor LLMs. We introduce a taxonomy of preference alignment techniques, including\\ntraining time, inference time, and additionally, user-modeling based methods.\\nWe provide analysis and discussion on the strengths and limitations of each\\ngroup of techniques and then cover evaluation, benchmarks, as well as open\\nproblems in the field.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-04-09T17:39:58Z\"}"}

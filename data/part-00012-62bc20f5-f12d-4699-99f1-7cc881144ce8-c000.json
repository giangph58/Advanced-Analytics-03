{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24310v1\", \"title\": \"BEATS: Bias Evaluation and Assessment Test Suite for Large Language\\n  Models\", \"summary\": \"In this research, we introduce BEATS, a novel framework for evaluating Bias,\\nEthics, Fairness, and Factuality in Large Language Models (LLMs). Building upon\\nthe BEATS framework, we present a bias benchmark for LLMs that measure\\nperformance across 29 distinct metrics. These metrics span a broad range of\\ncharacteristics, including demographic, cognitive, and social biases, as well\\nas measures of ethical reasoning, group fairness, and factuality related\\nmisinformation risk. These metrics enable a quantitative assessment of the\\nextent to which LLM generated responses may perpetuate societal prejudices that\\nreinforce or expand systemic inequities. To achieve a high score on this\\nbenchmark a LLM must show very equitable behavior in their responses, making it\\na rigorous standard for responsible AI evaluation. Empirical results based on\\ndata from our experiment show that, 37.65\\\\% of outputs generated by industry\\nleading models contained some form of bias, highlighting a substantial risk of\\nusing these models in critical decision making systems. BEATS framework and\\nbenchmark offer a scalable and statistically rigorous methodology to benchmark\\nLLMs, diagnose factors driving biases, and develop mitigation strategies. With\\nthe BEATS framework, our goal is to help the development of more socially\\nresponsible and ethically aligned AI models.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL,cs.AI\", \"published\": \"2025-03-31T16:56:52Z\"}"}

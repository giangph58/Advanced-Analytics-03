{"value":"{\"aid\": \"http://arxiv.org/abs/2504.04781v1\", \"title\": \"OCC-MLLM-CoT-Alpha: Towards Multi-stage Occlusion Recognition Based on\\n  Large Language Models via 3D-Aware Supervision and Chain-of-Thoughts Guidance\", \"summary\": \"Comprehending occluded objects are not well studied in existing large-scale\\nvisual-language multi-modal models. Current state-of-the-art multi-modal large\\nmodels struggles to provide satisfactory results in understanding occluded\\nobjects through universal visual encoders and supervised learning strategies.\\nTherefore, we propose OCC-MLLM-CoT-Alpha, a multi-modal large vision language\\nframework that integrates 3D-aware supervision and Chain-of-Thoughts guidance.\\nParticularly, (1) we build a multi-modal large vision-language model framework\\nwhich is consisted of a large multi-modal vision-language model and a 3D\\nreconstruction expert model. (2) the corresponding multi-modal\\nChain-of-Thoughts is learned through a combination of supervised and\\nreinforcement training strategies, allowing the multi-modal vision-language\\nmodel to enhance the recognition ability with learned multi-modal\\nchain-of-thoughts guidance. (3) A large-scale multi-modal chain-of-thoughts\\nreasoning dataset, consisting of $110k$ samples of occluded objects held in\\nhand, is built. In the evaluation, the proposed methods demonstrate decision\\nscore improvement of 15.75%,15.30%,16.98%,14.62%, and 4.42%,3.63%,6.94%,10.70%\\nfor two settings of a variety of state-of-the-art models.\", \"main_category\": \"cs.CV\", \"categories\": \"cs.CV\", \"published\": \"2025-04-07T07:15:26Z\"}"}

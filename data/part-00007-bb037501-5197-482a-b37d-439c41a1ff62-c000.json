{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05019v1\", \"title\": \"Mixture-of-Personas Language Models for Population Simulation\", \"summary\": \"Advances in Large Language Models (LLMs) paved the way for their emerging\\napplications in various domains, such as human behavior simulations, where LLMs\\ncould augment human-generated data in social science research and machine\\nlearning model training. However, pretrained LLMs often fail to capture the\\nbehavioral diversity of target populations due to the inherent variability\\nacross individuals and groups. To address this, we propose \\\\textit{Mixture of\\nPersonas} (MoP), a \\\\textit{probabilistic} prompting method that aligns the LLM\\nresponses with the target population. MoP is a contextual mixture model, where\\neach component is an LM agent characterized by a persona and an exemplar\\nrepresenting subpopulation behaviors. The persona and exemplar are randomly\\nchosen according to the learned mixing weights to elicit diverse LLM responses\\nduring simulation. MoP is flexible, requires no model finetuning, and is\\ntransferable across base models. Experiments for synthetic data generation show\\nthat MoP outperforms competing methods in alignment and diversity metrics.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,cs.CL\", \"published\": \"2025-04-07T12:43:05Z\"}"}

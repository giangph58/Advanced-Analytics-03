{"value":"{\"aid\": \"http://arxiv.org/abs/2504.02692v1\", \"title\": \"GPTQv2: Efficient Finetuning-Free Quantization for Asymmetric\\n  Calibration\", \"summary\": \"We introduce GPTQv2, a novel finetuning-free quantization method for\\ncompressing large-scale transformer architectures. Unlike the previous GPTQ\\nmethod, which independently calibrates each layer, we always match the\\nquantized layer's output to the exact output in the full-precision model,\\nresulting in a scheme that we call asymmetric calibration. Such a scheme can\\neffectively reduce the quantization error accumulated in previous layers. We\\nanalyze this problem using optimal brain compression to derive a close-formed\\nsolution. The new solution explicitly minimizes the quantization error as well\\nas the accumulated asymmetry error. Furthermore, we utilize various techniques\\nto parallelize the solution calculation, including channel parallelization,\\nneuron decomposition, and Cholesky reformulation for matrix fusion. As a\\nresult, GPTQv2 is easy to implement, simply using 20 more lines of code than\\nGPTQ but improving its performance under low-bit quantization. Remarkably, on a\\nsingle GPU, we quantize a 405B language transformer as well as EVA-02 the rank\\nfirst vision transformer that achieves 90% pretraining Imagenet accuracy. Code\\nis available at github.com/Intelligent-Computing-Lab-Yale/GPTQv2.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG\", \"published\": \"2025-04-03T15:30:43Z\"}"}

{"value":"{\"aid\": \"http://arxiv.org/abs/2504.06814v1\", \"title\": \"Revisit Gradient Descent for Geodesically Convex Optimization\", \"summary\": \"In a seminal work of Zhang and Sra, gradient descent methods for geodesically\\nconvex optimization were comprehensively studied. In particular, based on a\\nrefined use of the triangle comparison theorem of Toponogov, Zhang and Sra\\nderived a comparison inequality that relates the current iterate, the next\\niterate and the optimum point. Since their seminal work, numerous follow-ups\\nhave studied different downstream usages of their comparison lemma. However,\\nall results along this line relies on strong assumptions, such as bounded\\ndomain assumption or curvature bounded below assumption.\\n  In this work, we introduce the concept of quasilinearization to optimization,\\npresenting a novel framework for analyzing geodesically convex optimization. By\\nleveraging this technique, we establish state-of-the-art convergence rates --\\nfor both deterministic and stochastic settings -- under substantially weaker\\nassumptions than previously required.\", \"main_category\": \"math.OC\", \"categories\": \"math.OC\", \"published\": \"2025-04-09T12:08:43Z\"}"}

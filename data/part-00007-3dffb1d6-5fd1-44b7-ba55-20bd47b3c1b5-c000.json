{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05174v1\", \"title\": \"Learning symmetries in datasets\", \"summary\": \"We investigate how symmetries present in datasets affect the structure of the\\nlatent space learned by Variational Autoencoders (VAEs). By training VAEs on\\ndata originating from simple mechanical systems and particle collisions, we\\nanalyze the organization of the latent space through a relevance measure that\\nidentifies the most meaningful latent directions. We show that when symmetries\\nor approximate symmetries are present, the VAE self-organizes its latent space,\\neffectively compressing the data along a reduced number of latent variables.\\nThis behavior captures the intrinsic dimensionality determined by the symmetry\\nconstraints and reveals hidden relations among the features. Furthermore, we\\nprovide a theoretical analysis of a simple toy model, demonstrating how, under\\nidealized conditions, the latent space aligns with the symmetry directions of\\nthe data manifold. We illustrate these findings with examples ranging from\\ntwo-dimensional datasets with $O(2)$ symmetry to realistic datasets from\\nelectron-positron and proton-proton collisions. Our results highlight the\\npotential of unsupervised generative models to expose underlying structures in\\ndata and offer a novel approach to symmetry discovery without explicit\\nsupervision.\", \"main_category\": \"cs.LG\", \"categories\": \"cs.LG,hep-ph\", \"published\": \"2025-04-07T15:17:41Z\"}"}

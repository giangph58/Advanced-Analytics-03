{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23913v1\", \"title\": \"Entropy-Based Adaptive Weighting for Self-Training\", \"summary\": \"The mathematical problem-solving capabilities of large language models have\\nbecome a focal point of research, with growing interests in leveraging\\nself-generated reasoning paths as a promising way to refine and enhance these\\nmodels. These paths capture step-by-step logical processes while requiring only\\nthe correct answer for supervision. The self-training method has been shown to\\nbe effective in reasoning tasks while eliminating the need for external models\\nand manual annotations. However, optimizing the use of self-generated data for\\nmodel training remains an open challenge. In this work, we propose\\nEntropy-Based Adaptive Weighting for Self-Training (EAST), an adaptive\\nweighting strategy designed to prioritize uncertain data during self-training.\\nSpecifically, EAST employs a mapping function with a tunable parameter that\\ncontrols the sharpness of the weighting, assigning higher weights to data where\\nthe model exhibits greater uncertainty. This approach guides the model to focus\\non more informative and challenging examples, thereby enhancing its reasoning\\nability. We evaluate our approach on GSM8K and MATH benchmarks. Empirical\\nresults show that, while the vanilla method yields virtually no improvement\\n(0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K,\\nEAST attains a further 1-2% performance boost compared to the vanilla method.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-03-31T10:04:35Z\"}"}

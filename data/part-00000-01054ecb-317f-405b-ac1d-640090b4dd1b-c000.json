{"value":"{\"aid\": \"http://arxiv.org/abs/2503.24020v1\", \"title\": \"Optimal low-rank approximations for linear Gaussian inverse problems on\\n  Hilbert spaces, Part I: posterior covariance approximation\", \"summary\": \"For linear inverse problems with Gaussian priors and Gaussian observation\\nnoise, the posterior is Gaussian, with mean and covariance determined by the\\nconditioning formula. Using the Feldman-Hajek theorem, we analyse the\\nprior-to-posterior update and its low-rank approximation for\\ninfinite-dimensional Hilbert parameter spaces and finite-dimensional\\nobservations. We show that the posterior distribution differs from the prior on\\na finite-dimensional subspace, and construct low-rank approximations to the\\nposterior covariance, while keeping the mean fixed. Since in infinite\\ndimensions, not all low-rank covariance approximations yield approximate\\nposterior distributions which are equivalent to the posterior and prior\\ndistribution, we characterise the low-rank covariance approximations which do\\nyield this equivalence, and their respective inverses, or `precisions'. For\\nsuch approximations, a family of measure approximation problems is solved by\\nidentifying the low-rank approximations which are optimal for various losses\\nsimultaneously. These loss functions include the family of R\\\\'enyi divergences,\\nthe Amari $\\\\alpha$-divergences for $\\\\alpha\\\\in(0,1)$, the Hellinger metric and\\nthe Kullback-Leibler divergence. Our results extend those of Spantini et al.\\n(SIAM J. Sci. Comput. 2015) to Hilbertian parameter spaces, and provide\\ntheoretical underpinning for the construction of low-rank approximations of\\ndiscretised versions of the infinite-dimensional inverse problem, by\\nformulating discretization independent results.\", \"main_category\": \"math.ST\", \"categories\": \"math.ST,math.PR,stat.TH\", \"published\": \"2025-03-31T12:48:24Z\"}"}

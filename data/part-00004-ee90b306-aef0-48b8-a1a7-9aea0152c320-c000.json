{"value":"{\"aid\": \"http://arxiv.org/abs/2504.05805v1\", \"title\": \"Why is Normalization Necessary for Linear Recommenders?\", \"summary\": \"Despite their simplicity, linear autoencoder (LAE)-based models have shown\\ncomparable or even better performance with faster inference speed than neural\\nrecommender models. However, LAEs face two critical challenges: (i) popularity\\nbias, which tends to recommend popular items, and (ii) neighborhood bias, which\\noverly focuses on capturing local item correlations. To address these issues,\\nthis paper first analyzes the effect of two existing normalization methods for\\nLAEs, i.e., random-walk and symmetric normalization. Our theoretical analysis\\nreveals that normalization highly affects the degree of popularity and\\nneighborhood biases among items. Inspired by this analysis, we propose a\\nversatile normalization solution, called Data-Adaptive Normalization (DAN),\\nwhich flexibly controls the popularity and neighborhood biases by adjusting\\nitem- and user-side normalization to align with unique dataset characteristics.\\nOwing to its model-agnostic property, DAN can be easily applied to various\\nLAE-based models. Experimental results show that DAN-equipped LAEs consistently\\nimprove existing LAE-based models across six benchmark datasets, with\\nsignificant gains of up to 128.57% and 12.36% for long-tail items and unbiased\\nevaluations, respectively. Refer to our code in https://github.com/psm1206/DAN.\", \"main_category\": \"cs.IR\", \"categories\": \"cs.IR\", \"published\": \"2025-04-08T08:37:32Z\"}"}

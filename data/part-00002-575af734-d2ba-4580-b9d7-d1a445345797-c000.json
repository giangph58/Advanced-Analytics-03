{"value":"{\"aid\": \"http://arxiv.org/abs/2503.23714v1\", \"title\": \"Building Instruction-Tuning Datasets from Human-Written Instructions\\n  with Open-Weight Large Language Models\", \"summary\": \"Instruction tuning is crucial for enabling Large Language Models (LLMs) to\\nsolve real-world tasks. Prior work has shown the effectiveness of\\ninstruction-tuning data synthesized solely from LLMs, raising a fundamental\\nquestion: Do we still need human-originated signals for instruction tuning?\\nThis work answers the question affirmatively: we build state-of-the-art\\ninstruction-tuning datasets sourced from human-written instructions, by simply\\npairing them with LLM-generated responses. LLMs fine-tuned on our datasets\\nconsistently outperform those fine-tuned on existing ones. Our data\\nconstruction approach can be easily adapted to other languages; we build\\ndatasets for Japanese and confirm that LLMs tuned with our data reach\\nstate-of-the-art performance. Analyses suggest that instruction-tuning in a new\\nlanguage allows LLMs to follow instructions, while the tuned models exhibit a\\nnotable lack of culture-specific knowledge in that language. The datasets and\\nfine-tuned models will be publicly available. Our datasets, synthesized with\\nopen-weight LLMs, are openly distributed under permissive licenses, allowing\\nfor diverse use cases.\", \"main_category\": \"cs.CL\", \"categories\": \"cs.CL\", \"published\": \"2025-03-31T04:28:38Z\"}"}
